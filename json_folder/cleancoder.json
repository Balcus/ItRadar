[
{"url": "https://blog.cleancoder.com/uncle-bob/2016/11/10/TDD-Doesnt-work.html", "title": "TDD Doesn't Work", "content": "\n       TDD Doesn’t work. \n\n \n   It doesn’t?  That’s odd.  I’ve always found it to work quite well. \n \n\n Not according to  a new study . \n\n \n   Another study? \n \n\n Yeah, an in-depth study that repeated another study that was done a few years back.  Both showed that TDD doesn’t work.  The new one uses a multi-site, blind analysis, approach.  It looks conclusive. \n\n \n   Do the authors consider it conclusive? \n \n\n The authors recommend more study.  But they’re probably just being humble.  The data is pretty convincing. \n\n \n   What is the data? \n \n\n The study shows that the claims about TDD are false.  TDD doesn’t make you go faster; it doesn’t reduce defects; and it doesn’t help you to write better code. \n\n \n   That’s very strange.  I think TDD does make me go faster, improve my code, and my accuracy.  I know others who have said the same.  So I’m puzzled about why this study would show something different. \n \n\n Well, it did.  DHH was right.   TDD is Dead . \n\n \n   Hmmm.  OK, so what exactly did the authors study?  How did they come to this conclusion? \n \n\n I don’t know, I just know there was a study. \n\n \n   How did you find out about the study? \n \n\n I read a  blog  about it.  At the end the author said that the study has made him reconsider TDD.  He used to think it worked. \n\n \n   OK, well, let’s look at the study.  Hmmm.  Yes, right here it says that they compared TDD to TLD. \n \n\n What’s TLD? \n\n \n   Test LAST development.  That’s when you write your unit tests _after  you write your code._ \n \n\n See? So the study showed that it’s better to write your tests last! \n\n \n   Hmmm.  No, that doesn’t seem to be what the study showed.  In fact, the study found that there was no significant difference. \n \n\n OK, fine.  So if I write my code, and then write my tests it’s just as good as TDD. \n\n \n   Well, no, not quite.  At least that’s not what the study showed.  The study asked the folks doing TLD to work in “small chunks”. \n \n\n Small Chunks? \n\n \n   Yes.  The folks doing TLD would write a little bit of production code, followed by a little bit of test code. \n \n\n Oh.  I see.  So they’d write production code for 10 minutes and then write unit tests for ten minutes or something like that. \n\n \n   Well, maybe.  But, see here, it says that all the participants were trained in TDD.  And then some of them were asked to do TLD in small chunks. \n \n\n Right.  OK.  So, my statement still holds.  They wrote production code, then they wrote unit tests; and it didn’t matter. \n\n \n   So let me ask you how you would write unit tests, _after  production code; but in small chunks._ \n \n\n I’d write some production code – enough to pass a test or two – and then I’d write those tests. \n\n \n   How would you know how much code would pass a test or two? \n \n\n I’d think of a couple of tests to pass, then I’d write the code that would pass those tests.  Then I’d write the tests. \n\n \n   And since you had been trained in TDD; that kind of thought process would be natural to you; wouldn’t it? \n \n\n Um.  Hmmm.  I think I see your point.  The TLDers were doing TDD in their heads, and then just reversing the order. \n\n \n   _Right.  In order to work in small chunks, they had to imagine the tests that they’d be writing; so that they could write production code that was _testable. \n \n\n So maybe this study wasn’t studying what they thought they were studying. \n\n \n   It seems to me that they were trying to study the _order  of writing the tests, more than the process of TDD.  In their effort to reduce the number of variables they inadvertently eliminated them all.  They forced the participants doing TLD to use the TDD  process  of short cycles, and that forced the participants to drive the production code by thinking about tests first._ \n \n\n OK.  Maybe.  But still, those TLDers  did write their tests last .  So at least the study showed that you don’t really have to write the tests first – so long as you work in very short cycles. \n\n \n   Sure.  The really effective part of TDD is the size of the cycle, not so much whether you write the test first.  The reason we write the tests first is that it encourages us to keep the cycles really short. \n \n\n So what the study showed is that people who work in short cycles don’t have to worry about writing tests first, so long as they continue to work in short cycles. \n\n \n   That’s probably a fair statement.  However, look here.  The problem that the participants were solving was _The Bowling Game . This is a very small problem.  In fact, they said the entire programming session took three hours._ \n \n\n Is that important? \n\n \n   Sure.  The benefit of writing the tests first is _disciplinary .  Writing the test first keeps your cycles short; and keeps your coverage high, over long periods of time._ \n \n\n OK, but if you had enough internal discipline to keep your cycles short, then the study shows that it doesn’t matter if you write your tests first. \n\n \n   That’s a big “if”; but sure.  The study shows that if you take a group of people, trained in TDD, and then tell them to keep everything the same, including the size of their cycles, and just change the ordering of the tests, then in three hours of programming you won’t see much difference. \n \n\n Yeah.  Yeah.  That’s what the study shows. \n\n \n   So, really, the study was making a distinction without a difference. \n \n\n Well..  Heh, heh, they  found  no difference, so I guess that’s right. \n\n \n   So the study didn’t show that TDD doesn’t work, did it? \n \n\n No, I guess not. \n\n \n   What _did  it show?_ \n \n\n I think it showed that you can’t interpret the conclusions of a study without  reading  the study. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/03/06/TestingLikeTheTSA.html", "title": "Testing Like the TSA", "content": "\n       I was very glad to read in DHH’s  recent post  that he is actually still using TDD***.  I’m glad he has realized that TDD is not, in fact,  dead . \n\n This blog is a simple response; just to state a couple of disagreements.  But I have to say, I agree more than I disagree. \n\n DHH presented seven points.  I have reproduced them below, along with my comments.  And since DHH did not justify his opinions, I’ll not justify mine. \n\n \n   (1. Don’t aim for 100% coverage. \n \n\n \n   Disagree.  Aim as high as you can.  Treat 100% as an asymptotic goal.  No other number makes a reasonable goal.  There is never a reason to stop driving your coverage number higher. \n \n\n \n   (2. Code-to-test ratios above 1:2 is a smell, above 1:3 is a stink. \n \n\n \n   Disagree.  1:1 LOC is about right.  If you have 20K lines of code, you probably ought to have about 20K lines of tests.  (BTW, I can’t make any sense of DHH’s ratios there.  I think he meant the first one the other way around (2:1) and I think he meant the second one to be (1.5:1).  Or maybe I’m just dense. \n \n\n \n   (3. You’re probably doing it wrong if testing is taking more than 1/3 of your time. You’re definitely doing it wrong if it’s taking up more than half. \n \n\n \n   Agree.  Testing should take none of your time.  Writing tests, with TDD, requires negative time (i.e. it saves you a boatload)  Every worthwhile test you don’t write costs you time. \n \n\n \n   (4. Don’t test standard Active Record associations, validations, or scopes. \n \n\n \n   Agree.  There’s generally no reason to test your framework; so long as you trust it.  If you don’t trust it (and there are many frameworks out there that are not trustworthy) then writing some tests against the framework might be worthwhile.  Consider it equivalent to “Incomming Inspection”. \n \n\n \n   (5. Reserve integration testing for issues arising from the integration of separate elements (aka don’t integration test things that can be unit tested instead). \n \n\n \n   Agree.  Keep your tests focused.  Don’t test through UIs.  Don’t test through web servers.  Test as close to the code as you can. \n \n\n \n   (6. Don’t use Cucumber unless you live in the magic kingdom of non-programmers-writing-tests (and send me a bottle of fairy dust if you’re there!) \n \n\n \n   Agree and Disagree.  Cucumber (Gherkin) is worth it only if you have business people and/or QA who are willing to read your tests.  If they will also write your acceptance tests then: ABSOLUTELY send that fairy dust far and wide; because it’s worth its weight in diamonds. \n \n\n \n   (7. Don’t force yourself to test-first every controller, model, and view (my ratio is typically 20% test-first, 80% test-after). \n \n\n \n   Agree…  Sort of.  Some controllers, models, and views are too stupid to bother to test.  If they are obviously correct, because they are one line of code, then testing them might be superfluous.  But be careful.  Sometimes one line of code has 20 lines of semantics. \n \n\n \n *** It has been pointed out to me that the “TSA” post actually predates the “TDD is Dead” post by several years.  Somehow or another I got the two backwards.  (sigh). \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/01/11/TheDarkPath.html", "title": "The Dark Path", "content": "\n       Over the last few months I’ve dabbled in two new languages.   Swift  and  Kotlin .  These two languages have a number of similarities.  Indeed, the similarities are so stark that I wonder if this isn’t a new trend in our language  churn .  If so,  it is a dark path . \n\n Both languages have integrated some functional characteristics.  For example, they both have lambdas.  This is a good thing, in general.  The more we learn about functional programming, the better.  These languages are both a far cry from a truly functional programming language; but every step in that direction is a good step. \n\n My problem is that both languages have doubled down on strong static typing.  Both seem to be intent on closing  every single type hole  in their parent languages.  In the case of Swift, the parent language is the bizarre typeless hybrid of C and Smalltalk called  Objective-C ; so perhaps the emphasis on typing is understandable.  In the case of Kotlin the parent is the already rather strongly typed Java. \n\n Now I don’t want you to think that I’m opposed to statically typed languages.  I’m not.  There are definite advantages to both dynamic and static languages; and I happily use both kinds.   I have a slight preference for dynamic typing; and so I use Clojure quite a bit.  On the other hand, I probably write more Java than Clojure.  So you can consider me bi-typical.  I walk on both sides of the street – so to speak. \n\n It’s not the fact that Swift and Kotlin are statically typed that has me concerned.  Rather, it is the  depth  of that static typing. \n\n I would not call Java a strongly opinionated language when it comes to static typing.  You can create structures in Java that follow the type rules nicely; but you can also violate many of the type rules whenever you want or need to.  The language complains a bit when you do; and throws up a few roadblocks; but not so many as to be obstructionist. \n\n Swift and Kotlin, on the other hand, are completely inflexible when it comes to their type rules.  For example, in Swift, if you declare a function to throw an  exception , then  by God  every call to that function,  all the way up the stack , must be adorned with a  do-try  block, or a  try! , or a  try? .  There is no way, in this language, to silently throw an exception all the way to the top level; without paving a super-hiway for it up through the entire calling tree.  (You can watch Justin and I struggle with this in our  Mobile Application Case Study  videos.) \n\n Now, perhaps you think this is a good thing.  Perhaps you think that there have been a lot of bugs in systems that have resulted from un-corralled exceptions.  Perhaps you think that exceptions that aren’t escorted, step by step, up the calling stack are risky and error prone.  And, of course, you would be right about that.  Undeclared and unmanaged exceptions are very risky. \n\n The question is: Whose job is it to manage that risk?  Is it the language’s job?  Or is it the programmer’s job? \n\n In Kotlin, you cannot derive from a class, or override a function, unless you adorn that class or function as  open .  You also cannot override a function unless the overriding function is adorned with  override .  If you neglect to adorn a class with  open , the language will not allow you to derive from it. \n\n Now, perhaps you think this is a good thing.  Perhaps you believe that inheritance and derivation hierarchies that are allowed to grow without bound are a source of error and risk.  Perhaps you think we can eliminate whole classes of bugs by forcing programmers to explicitly declare their classes to be  open .  And you may be right.  Derivation and inheritance  are  risky things.  Lots can go wrong when you override a function in a derived class. \n\n The question is: Whose job is it to manage that risk?  Is it the language’s job?  Or is it the programmer’s job. \n\n Both Swift and Kotlin have incorporated the concept of  nullable  types.  The fact that a variable can contain a  null  becomes part of the  type  of that variable.  A variable of type  String  cannot contain a  null ; it can only contain a reified  String .  On the other hand, a variable of type  String?  has a  nullable  type and  can  contain a  null . \n\n The rules of the language insist that when you use a nullable variable, you  must  first check that variable for  null .  So if  s  is a  String?  then  var l = s.length()  won’t compile.  Instead you have to say  var l = s.length() ?: 0  or  var l = if (s!=null) s.length() else 0 . \n\n Perhaps you think this is a good thing.  Perhaps you have seen enough  NPE s in your lifetime.  Perhaps you know, beyond a shadow of a doubt, that unchecked  null s are the cause of billions and billions of dollars of software failures.  (Indeed, the Kotlin documentation calls the  NPE  the  “Billion Dollar Bug” ).  And, of course, you are right.  It is very risky to have  null s rampaging around the system out of control. \n\n The question is: Whose job is it to manage the  null s.  The language?  Or the programmer? \n\n These languages are like the little Dutch boy sticking his fingers in the dike.  Every time there’s a new kind of bug, we add a language feature to prevent that kind of bug.  And so these languages accumulate more and more fingers in holes in dikes.  The problem is, eventually you run out of fingers and toes. \n\n But before you run out of fingers and toes, you have created languages that contain dozens of keywords, hundreds of constraints, a tortuous syntax, and a reference manual that reads like a law book.  Indeed, to become an expert in these languages, you must become a  language lawyer  (a term that was invented during the  C++  era.) \n\n This is the wrong path! \n\n Ask yourself why we are trying to plug defects with language features.  The answer ought to be obvious.  We are trying to plug these defects because these defects happen too often. \n\n Now, ask yourself why these defects happen too often.  If your answer is that our languages don’t prevent them, then I strongly suggest that you quit your job and never think about being a programmer again; because defects are  never  the fault of our languages.  Defects are the fault of  programmers .   It is  programmers  who create defects – not languages. \n\n And what is it that programmers are supposed to do to prevent defects?  I’ll give you one guess.  Here are some hints.  It’s a verb.  It starts with a “T”.  Yeah.  You got it.   TEST! \n\n You  test  that your system does not emit unexpected  null s.  You  test  that your system handles  null s at it’s inputs.  You  test  that every exception you can throw is caught somewhere. \n\n Why are these languages adopting all these features?  Because programmers  are not testing  their code.  And because programmers are not testing their code, we now have languages that  force  us to put the word  open  in front of every class we want to derive from.  We now have languages that  force  us to adorn every function, all the way up the calling tree, with  try! .   We now have languages that are so constraining, and so over-specified, that you have to design the whole system up front before you can code any of it. \n\n Consider:  How do I know whether a class is  open  or not?  How do I know if somewhere down the calling tree someone might throw an exception?  How much code will I have to change when I finally discover that someone really needs to return a  null  up the calling tree? \n\n All these constraints, that these languages are imposing, presume that the programmer has perfect knowledge of the system;  before the system is written .  They presume that you  know  which classes will need to be  open  and which will not.  They presume that you  know  which calling paths will throw exceptions, and which will not.  They presume that you  know  which functions will produce  null  and which will not. \n\n And because of all this presumption, they  punish  you when you are wrong.  They force you to go back and change massive amounts of code, adding  try!  or  ?:  or  open  all the way up the stack. \n\n And how do you avoid being punished?  There are two ways.  One that works; and one that doesn’t.  The one that doesn’t work is to design everything up front before coding.  The one that  does  avoid the punishment is to  override all the safeties . \n\n And so you will declare  all  your classes and  all  your functions  open .  You will  never  use exceptions.  And you will get used to using lots and lots of  !  characters to override the  null  checks and allow  NPE s to rampage through your systems. \n\n \n\n Why did the nuclear plant at Chernobyl catch fire, melt down, destroy a small city, and leave a large area uninhabitable?   They overrode all the safeties.   So don’t depend on safeties to prevent catastrophes.  Instead, you’d better get used to writing lots and lots of tests, no matter what language you are using! \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/03/03/TDD-Harms-Architecture.html", "title": "TDD Harms Architecture", "content": "\n       The idea that TDD damages design and architecture is not new.  DHH suggested as much several years ago with his notion of  Test Induced Design Damage ; in which he compares the design he prefers to a design created by  Jim Weirich  that is “testable”.  The argument, boils down to separation and indirection.  DHH’s concept of good design  minimizes  these attributes, whereas Weirich’s  maximizes  them. \n\n I strongly urge you to read DHH’s article,  and watch Weirich’s video , and judge for yourself which design you prefer. \n\n Recently I’ve seen the argument resurface on twitter; though not in reference to DHH’s ideas; but instead in reference to a  very old interview  between James Coplien and myself.  In this case the argument is about using TDD to allow architecture to  emerge .  As you’ll discover, if you read through that interview, Cope and I agree that architecture does not emerge from TDD.  The term I used, in that interview was, I believe – Horse shit. \n\n Still another common argument is that as the number of tests grows, a single change to the production code can cause hundreds of tests to require corresponding changes.  For example, if you add an argument to a method, every test that calls that method must be changed to add the new argument.  This is known as  The Fragile Test Problem . \n\n A related argument is: The more tests you have, the harder it is to change the production code; because so many tests can break and require repair.  Thus, tests make the production code rigid. \n\n What’s behind this? \n\n Is there anything to these concerns?  Are they real?  Does TDD really damage design and architecture? \n\n There are too many issues to simply disregard.  So what’s going on here? \n\n Before I answer that, let’s look at a simple diagram.  Which of these two designs is better? \n\n \n\n Yes, it’s true, I’ve given you a hint by coloring the left (sinister) side red, and the right (dexter) side green.  I hope it is clear that the right hand solution is generally better than the left. \n\n Why?  Coupling, of course.  In the left solution the users are directly coupled to a multitude of services.  Any change to a service, regardless of how trivial, will likely cause many users to require change.  So the left side is fragile. \n\n Worse, the left side users act as anchors that impede the ability of the developers to make changes to the services.  Developers fear that too many users may be affected by simple changes. So the left side is rigid. \n\n The right side, on the other hand, decouples the users from the services by using an API.  What’s more, the services  implement  the API using inheritance, or some other form of polymorphism.  (That is the meaning of the closed triangular arrows – a UMLism.)  Thus a large number of changes can be made to the services without affecting either the API or the users.  What’s more the users are not an anchor making the services rigid. \n\n The principles at play here are the  Open-Closed Principle (OCP)  and the  Dependency Inversion Principle (DIP) . \n\n Note, that the design on the left is the design that DHH was advocating in his article; whereas the design on the right was the topic of Weirich’s exploration.  DHH likes the directness of the design on the left.  Weirich likes the separation and isolation of the design on the right. \n\n The Critical Substitution \n\n Now, in your mind, I want you to make a simple substitution.  Look at that diagram, and substitute the word “TEST” for the word “USER”  – and then  think . \n\n Yes.  That’s right.   Tests need to be designed .  Principles of design apply to tests just as much as they apply to regular code.  Tests are  part of the system ; and they must be maintained to the same standards as any other part of the system. \n\n One-to-One Correspondence. \n\n If you’ve been following me for any length of time you know that I describe TDD using  three laws .  These laws force you to write your tests and your production code simultaneously, virtually line by line.  One line of test, followed by one line of production code, around, and around and around.  If you’ve never seen or experienced this, you might want to watch  this  video. \n\n Most people who are new to TDD, and the three laws, end up writing tests that look like the diagram on the left.  They create a kind of one-to-one correspondence between the production code and the test code.  For example, they may create a  test class  for every production code class.  They may create  test methods  for every production code method. \n\n Of course this makes sense,  at first .  After all, the goal of any test suite is to test the elements of the system.  Why wouldn’t you create tests that had a one-to-one correspondence with those elements?  Why wouldn’t you create a test class for each class, and a set of test methods for each method?  Wouldn’t that be the  correct  solution? \n\n And, indeed, most of the books, articles, and demonstrations of TDD show precisely that approach.  They show tests that have a strong structural correlation to the system being tested.  So, of course, developers trying to adopt TDD will follow that advice. \n\n The problem is – and I want you to think carefully about this next statement –  a one-to-one correspondence implies extremely tight coupling . \n\n Think of it!  If the structure of the tests follows the structure of the production code, then the tests are inextricably coupled to the production code – and they follow the sinister red picture on the left! \n\n FitNesse \n It, frankly, took me many years to realize this.  If you look at the structure of  FitNesse , which we began writing in 2001, you will see a strong one-to-one correspondence between the test classes and the production code classes.  Indeed, I used to tout this as an advantage because I could find every unit test by simply putting the word “Test” after the class that was being tested. \n\n And, of course, we experienced some of the problems that you would expect with such a sinister design.  We had fragile tests.  We had structures made rigid by the tests.  We felt the pain of TDD.  And, after several years, we started to understand that the cause of that pain was that we were not designing our tests to be decoupled. \n\n If you look at part of FitNesse written after 2008 or so, you’ll see that there is a significant drop in the one-to-one correspondence.  The tests and code look more like the green design on the right. \n\n Emergence. \n\n The idea that the high level design and architecture of a system emerge from TDD is, frankly, absurd.  Before you begin to code any software project, you need to have some architectural vision in place.  TDD will not, and can not, provide this vision.  That is not TDD’s role. \n\n However, this does not mean that designs do not emerge from TDD – they do; just not at the highest levels.  The designs that emerge from TDD are one or two steps above the code; and they are intimately connected to the code, and to the red-green-refactor cycle. \n\n It works like this: As some programmers begin to develop a new class or module, they start by writing simple tests that describe the most degenerate behaviors.  These tests check the absurdities, such as what the system does when no input is provided.  The production code that solves these tests is trivial, and gradually grows as more and more tests are added. \n\n At some point, relatively early in the process, the programmers look at the production code and decide that the structure is a bit messy.  So the programmers extract a few methods, rename a few others, and generally clean things up.  This activity will have  little or no effect on the tests.   The tests are still testing all that code, regardless of the fact that the structure of that code is changing. \n\n This process continues.  As tests of ever greater complexity and constraint are added to the suite, the production code continues to grow in response.  From time to time, relatively frequently, the programmers clean that production code up.  They may extract new classes.  They may even pull out new modules.  And yet the tests remain unchanged.  The tests still cover the production code;  but they no longer have a similar structure . \n\n And so, to bridge the different structure between the tests and the production code,  an API emerges .  This API serves to allow the two streams of code to evolve in very different directions, responding to the opposing forces that press upon tests and production code. \n\n Forces in Opposition \n\n I said, above, that the tests remain unchanged during the process.  This isn’t actually true.  The tests are also refactored by the programmers on a fairly frequent basis.  But the direction of the refactoring is  very different  from the direction that the production code is refactored.  The difference can be summarized by this simple statement: \n\n \n   As the tests get more specific, the production code gets more generic. \n \n\n This is (to me) one of the most important revelations about TDD in the last 16 years.  These two streams of code evolve in opposite directions.  Programmers refactor tests to become more and more concrete and specific.  They refactor the production code to become more and more abstract and general. \n\n Indeed,  this is why  TDD works.  This is why designs can emerge from TDD.  This is why algorithms can be derived by TDD.  These things happen as a direct result of programmers pushing the tests and production code in opposite directions. \n\n Of course  designs emerge, if you are using design principles to push the production code to be more and more generic.   Of course  APIs emerge if you are pulling these two streams of communicating code towards opposite extremes of specificity and generality.   Of course  algorithms can be derived if the tests grow ever more constraining while the production code grows ever more general. \n\n And, of course, highly specific code  cannot have  a one-to-one correspondence with highly generic code. \n\n Conclusion \n\n What makes TDD work?  You do.  Following the three laws provides no guarantee.  The three laws are a discipline, not a solution.  It is  you , the programmer, who makes TDD work.  And you make it work by understanding that tests are part of the system, that tests must be designed, and that test code evolves towards ever greater specificity, while production code evolves towards ever greater generality. \n\n Can TDD harm your design and architecture?   Yes!  If you don’t employ design principles to evolve your production code, if you don’t evolve the tests and code in opposite directions, if you don’t treat the tests as part of your system, if you don’t think about decoupling, separation and isolation, you will damage your design and architecture – TDD or no TDD. \n\n You see, it is not TDD that creates bad designs.  It is not TDD that creates good designs.  It’s you.  TDD is a discipline.  It’s a way to organize your work.  It’s a way to ensure test coverage.  It is a way to ensure appropriate generality in response to specificity. \n\n TDD is important. TDD works.  TDD is a professional discipline that all programmers should learn and practice.  But it is not TDD that causes good or bad designs.   You  do that. \n\n Is is only programmers, not TDD, that can do harm to designs and architectures. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/01/13/TypesAndTests.html", "title": "Types and Tests", "content": "\n       \n   Friday the 13th! \n \n\n The response to my  Dark Path  blog has been entertaining.  It has ranged from effusive agreement to categorical disagreement.  It also elicited a few abusive insults.  I appreciate the passion. A nice vocal debate is always the best way to learn. As for the insulters: you guys need to move out of your Mom’s basement and get a life. \n\n To be clear, and at the risk of being repetitive, that blog was not an indictment of static typing.  I rather enjoy static typing.  I’ve spent the last 30 years working in statically typed languages and have gotten rather used to them. \n\n My intent, with that blog, was to complain about how far the pendulum has swung.  I consider the static typing of Swift and Kotlin to have swung too far in the statically type-checked direction.  They have, IMHO, passed the point where the tradeoff between expressiveness and constraint is profitable. \n\n \n\n One of the more common responses to that blog was: “Hey, man, like…  Types are Tests! ” \n\n No, types are not tests.  Type systems are not tests.  Type checking is not testing.  Here’s why. \n\n A computer program is a specification of behavior.  The purpose of a program is to make a machine behave in a certain way.  The text of the program consists of the instructions that the machine follows.  The sum of those instructions is the behavior of the program. \n\n Types do not specify behavior.  Types are constraints placed, by the programmer, upon the textual elements of the program.  Those constraints reduce the number of ways that different parts of the program text can refer to each other. \n\n Now this kind of constraint system can be very useful in reducing the incidence of textual errors in a program.  If you specify that function  f  must be called with an  Int  then the type system will ensure that no other part of the program text will invoke  f  with a   Double .  If such an error were allowed to escape into a running program (as was common back in the good old  C  days) then a runtime error would likely result. \n\n You might therefore say that the type system is a kind of “test” that fails for all inappropriate invocations of  f .  I might concede that point except for one thing –  the way  f  is called has nothing to do with the required behavior of the system .  Rather it is a test of an arbitrary constraint imposed by the programmer.  A constraint that was likely  over  specified from the point of view of the system requirements. \n\n The system requirements likely do not depend on the fact that the argument of  f  is an  Int .  We could very likely change the declaration of  f  to take a  Double , without affecting the observed behavior of the program at all. \n\n So what the type system is checking is not the external behavior of the program.  It is checking only the internal consistency of the program text. \n\n Now this is no small thing.  Writing program text that is internally consistent is pretty important.  Inconsistencies and ambiguities in program text can lead to misbehaviors of all kinds.  So I don’t want to downplay this at all. \n\n On the other hand, internal self-consistency does not mean the program exhibits the correct behavior.  Behavior and self-consistency are orthogonal concepts. Well behaved programs can be, and have been, written in languages with high ambiguity and low internal consistency.  Badly behaved programs have been written in languages that are deeply self-consistent and tolerate few ambiguities. \n\n So just how internally consistent do we need the program text to be?  Does every line of text need to be  precisely  60 characters long?  Must indentations always be multiples of two spaces?  Must every floating point number have a dot?  Should all positive numbers begin with a  + ?  Or should we allow certain ambiguities in our program text and allow the language to make assumptions that resolve them?  Should we relax the specificity of the language and allow it to tolerate certain easily resolvable ambiguities?  Should we allow this even if sometimes those ambiguities are resolved incorrectly? \n\n Clearly, every language chooses the latter option.  No language forces the program text to be  absolutely  unambiguous and self consistent.  Indeed, such a language would likely be impossible to create.  And even if it weren’t, it would likely be impossible to use.  Absolute precision and consistency has never been, nor should it ever be, the goal. \n\n So how much internal unambiguous self-consistency do we need?  It would be easy to say that we need as much as we can get.  It might seem obvious that the more unambiguous and internally consistent a language is, the fewer defects programs written in that language will have.  But is that true? \n\n The problem with increasing the level of precision and internal self consistency, is that it implies an increase in the number of constraints.  But constraints need to be specified; and specification requires notation.  Therefore as the number of constraints grows, so does the complexity of the notation.  The syntax and the semantics of a language grows as a function of the internal self-consistency and specificity of the language. \n\n As notation and semantics grow in complexity, the chance for unintended consequences grows.  Among  the worst of those consequences are Open-Closed violations. \n\n Imagine that there is a language named  TDP  that is ultimately self-consistent and specific.  In  TDP  every single line of code is self-consistent with, and specific to, every other line of code.  A change to one line forces a change to every other line in order to maintain that self-consistency and specificity. \n\n Do languages like this exist?  No; but the more type-safe a language is, the more internally consistent and specific it forces the programmers to be, the more it approaches that ultimate  TDP  condition. \n\n Consider the  const  keyword in  C++ .  When I was first learning  C++  I didn’t use it.  It was just too much on top of everything else there was to learn.  But as I gained in knowledge and comfort with the language the day came when I used my first  const .  And down the rathole I went, fixing one compile error after another, changing hundreds and hundreds of lines of code, until the system I was working on was  const -correct. \n\n Did I stop using  const  because of this experience?  No, of course not.  I just made sure that I knew, up front, which fields and functions were going to be  const .  This required a lot of up-front design; but that was worth the alternative.  Did that make the problem go away?  Of course not.  I frequently found myself running around inside the system smearing  const  all over the place. \n\n Is  TDP  a good condition to be in?  Do you want to have to change every line of code every time anything at all changes?  Clearly not.  This violates the OCP, and would create a nightmare for maintenance. \n\n Perhaps you think I’m setting up a straw man argument.  After all,  TDP  does not exist.  My claim, however, is that Swift and Kotlin have taken a step in that undesirable direction.  That’s why I called it:  The Dark Path . \n\n Every step down that path increases the difficulty of using and maintaining the language.  Every step down that path forces users of the language to get their type models “right” up front; because changing them later is too expensive.  Every step down that path forces us back into the regime of Big Design Up Front. \n\n But does that mean we should never take even a single step down that path?  Does that mean our languages should have no types and no specific internal self consistency.  Should we all be programming in  Lisp ? \n\n \n   (That was a joke, all you guys living in your Mom’s basement can keep your insults to yourself please; and stay off my lawn.  As for  Lisp  the answer is: Yes, we probably should all be programming in Lisp; but for different reasons.) \n \n\n Type safety has a number of benefits that, at first, outweigh the costs.  A few steps down the dark path allow us to gather some pretty nice low hanging fruit.  We can gain a decent amount of specificity and self consistency without huge violations of the OCP.  Type models can also enhance expressivity and readability.  And type models definitely help IDEs with refactorings and other mechanical operations. \n\n But there is a  balance point  after which every step down  The Dark Path  increases the cost over the benefit. \n\n I think Java and C# have done a reasonable job at hovering near the balance point.  (If you ignore the horrible syntax for generics, and the ridiculous proscription against multiple inheritance.)  In my opinion those languages have gone just a bit too far; but the costs of type-safety aren’t too high to tolerate.  Ruby, Groovy, and Javascript, on the other hand, hover on the other side of the balance point.  They are, perhaps, a bit too permissive; a bit too ambiguous (Does anybody really understand the sub-object graph in Ruby?). \n\n So, a little type safety, like a little salt, is a good thing.  Too much, on the other hand, can have unfortunate consequences. \n\n \n\n Does every step down  The Dark Path  mean that you can ignore a certain number of unit tests?  Does programming in  Dark Path  languages mean that you don’t have to test as much? \n\n No.  A thousand times: NO.  Type models do not specify behavior.  The correctness of your type model has no bearing on the correctness of the behavior you have specified.  At best the type system will prevent some mechanistic failures of representation (e.g.  Double  vs.  Int ); but you still have to specify every single bit of behavior; and you still have to  test  every bit of behavior. \n\n So, no, type systems do not decrease the testing load.  Not even the tiniest bit.  But they  can  prevent some errors that unit tests might not see. (e.g.  Double  vs.  Int ) \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/03/07/SymmetryBreaking.html", "title": "Symmetry Breaking", "content": "\n       Imagine that you are an accountant.  You are responsible for manipulating arcane symbols, concepts, and procedures in order to create deeply complicated and detailed financial models for your business.  The stakes are enormous.  Accuracy is essential.  Millions wait to be lost or gained based upon your rare and esoteric skills. \n\n How do you ensure your performance?  Upon what disciplines do you depend?  How will you make sure that the models you build, and the advice they imply, are faithful to your profession, and profitable for your business? \n\n For the last 500 years, accountants have been using the discipline of  double-entry bookkeeping .  The idea is simple; but the execution is challenging.  Each transaction is recorded, concurrently within a system of accounts; once as a debit, and then again as a credit.  These debits and credits follow separate but complimentary mathematical pathways, through a system of categorized accounts, until they converge on the balance sheet in a subtraction that must yield a zero.  Anything other than a zero implies an error was made somewhere along one of those pathways. \n\n We, programmers have a similar problem.  We manipulate arcane symbols, concepts, and procedures in order to create deeply complicated and detailed models of behavior for our businesses.  The stakes are enormous.  Accuracy is essential.  Millions wait to be lost or gained based upon our rare and esoteric skills. \n\n How do we ensure our performance?  Upon what disciplines do we depend?  How will we make sure that the models we build, and the behavior they elicit, are faithful to our profession, and profitable for our businesses. \n\n It has long been asserted that Test Driven Development (TDD) is the equivalent of double-entry bookkeeping.  There are some undeniable parallels.  Under the discipline of TDD every desired behavior is written twice; once in test code that verifies the behavior, and once in production code that exhibits that behavior.  The two streams of code are written concurrently, and follow complimentary, yet separate execution pathways until they converge in the count of defects - a count that must be zero. \n\n Another parallel is the granularity of the two disciplines.  Double-entry bookkeeping operates at the extremely fine granularity of individual transactions.  TDD operates at the equivalently fine granularity of individual behaviors and assertions.  In both cases the division between the granules is natural and obvious. There is no other granule for accounting; and it is hard to imagine a more appropriate granule for software. \n\n Still another parallel is the immediate feedback of the two approaches.  Errors are detected at every granule.  Accountants are taught to check the results for each and every transaction.  Programmers using TDD are taught to check the tests for every assertion.  Therefore, when properly executed, no error can infiltrate into, and thereby corrupt, large swathes of the models.  The rapid feedback, in both instances, prevents long hours of debugging and rework. \n\n But as similar as these two disciplines appear to be on the surface, there are some deep differences between them.  Some are obvious; such as the fact that one deals with numbers and accounts, whereas the other deals with functions and assertions.  Other differences are less obvious and much more profound. \n\n Asymmetry \n\n Double-entry bookkeeping is symmetrical.  Debits and credits have no relative priority.  Each is derivable from the other.  If you know the credited accounts and the transactions, then you can derive a reasonable set of debited accounts, and vice versa.  Therefore, there is no reason that accountants must enter a credit or a debit first.  The choice is arbitrary.  The subtraction will work in either case. \n\n This is not true of TDD.  There is an obvious arrow between the tests and the production code.  The tests depend upon the production code; the production code does not depend upon the tests.  This is true both at compile time, and at run time.  The arrow points in one direction, and one direction only. \n\n This asymmetry leads to the inescapable conclusion that the equivalence to double entry bookkeeping only works if the tests are written first.  There is no way to create an equivalent discipline if the production code is written before the tests. \n\n This may be difficult to see at first.  So let’s use the old mathematical trick of reduction to an absurdity.   But before we do that, let’s state TDD with the formality that can be inverted; the formality of the three laws.  Those laws are: \n\n \n   You are not allowed to write any production code without first writing a test that fails because the production code does not exist. \n   You are not allowed to write more of a test than is sufficient to fail; including failure of compilation. \n   You are not allowed to write more production code than is sufficient to pass the currently failing test. \n \n\n Following these three laws results in a very orderly and discrete procedure: \n\n \n   You must decide what production code function you intend to create. \n   You must write a test that fails because that production code doesn’t exist. \n   You must stop writing that test as soon as it fails for any reason, including compilation errors. \n   You must write only the production code that makes the test pass. \n   Repeat ad infinitum. \n \n\n Note how the discipline enforces the fine granularity of individual behaviors and assertions; including compile time assertions.  Notice that there is very little ambiguity about how much code to write at any given point; and whether that code should be production or test code.  Those three laws tie you down into a very tightly constrained behavior. \n\n It should be very clear that following the process dictated by the three laws is the logical equivalent of double-entry bookkeeping. \n\n Reductio ad Absurdum \n\n Now, let’s assume that a similar discipline can be defined that inverts the order, so that test code is written  after  production code.  How would we write such a discipline? \n\n We could start by simply inverting the three laws.  But as soon as we do we run into trouble: \n\n \n   1)  You are not allowed to write any test code without first writing production code that… \n \n\n How do you complete that rule?  In the un-inverted rule the sentence is completed by demanding that the test must fail because the production code doesn’t yet exist.  But what is the condition for our new, inverted, rule?  We could choose something arbitrary like “ …is a complete function. ”  However, this is not really a proper inverse of the first law of TDD. \n\n Indeed,  there is no proper inverse .  The first law cannot be inverted.  The reason is that the first law presumes that you know what production feature you are about to create – but so must  any  first law, including any inverted first law. \n\n For example, we could try to invert the first law as follows: \n\n \n   1)  You are not allowed to write any test code without first writing production code that will fail the test code for the behavior you are writing. \n \n\n I think you can see why this is not actually an inversion.  In order to follow this rule, you’d have to write the test code in your mind,  first , and then write the production code that failed it.  In essence the test has been identified before the production code is written.  The test has still come first. \n\n You might object by noting that it is always possible to write tests after production code; and that in fact programmers have been doing just that for years.  That’s true; but our goal was to write a  rule  that was the inverse of the first law of TDD.  A rule that constrained us to the same level of granularity of behaviors and assertions; but that had us inventing the tests last.  That rule does not appear to exist. \n\n The second rule has similar problems. \n\n \n   2)  You are not allowed to write more production code than is sufficient to… \n \n\n How do you complete that sentence?  There is no obvious limit to the amount of production code you can write.  Again, if we choose a predicate, that predicate will be arbitrary.  For example:  …complete a single function.   But, of course, that function could be huge, or tiny, or any size at all.  We have lost the obvious and natural granularity of individual behaviors and assertions. \n\n So once again, the rule is not invertible. \n\n Notice that these failures of invertibility are all about granularity.  When tests come first the granularity is naturally constrained.  When production code comes first, there is no constraint. \n\n This unconstrained granularity implies something deeper.  Note that the third law of TDD forces us to make the currently failing test, and only the currently failing test, pass.  This means that the production code we are about to write  will be derived  from the failing test.  But if you invert the third law you end up with nonsense: \n\n \n   3)  You are not allowed to write more test code than is sufficient to pass the current production code. \n \n\n What does that mean?  I can write a test that passes the current production code by writing a test with no assertions – or a test with no code at all.  Is this rule asking us to test every possible assertion?  What assertions are those? They haven’t been identified. \n\n This leads us to the conclusion that tests at fine granularity cannot obviously be derived from production code. \n\n Let’s state this more precisely.  It is straight forward, using the three laws of TDD, to derive the entirety of the production code from a series of individual assertion tests; but it is not straight forward to derive the entirety of a test suite from the completed production code. \n\n This is not to say that you cannot impute tests from production code.  Of course you can.  What this is telling us is: (and anyone who has ever tried to write tests from legacy code knows this) it is remarkably difficult, if not utterly impractical, to write fine-grained, comprehensive, tests from production code.  In order to write such tests from production code, you must first understand the entirety of that production code; because any part of that production code can affect the test you are trying to write.  And second, the production code must be decoupled in a way that allows the fine granularity. \n\n When tests are written first, granularity and decoupling are trivial to achieve.  When tests follow production code, decoupling and granularity are much more difficult to achieve. \n\n Irreversibility \n\n This means that tests and production code are irreversible.  Accountants don’t have this problem.  Debited accounts and credited accounts are mutually reversible.  You can derive one from the other.  But tests and production code progress in one direction. \n\n Why should this be? \n\n The answer lies in yet another asymmetry between tests and production code: their structure.  The structure of the production code is vastly different from the structure of the test code. \n\n The production code forms a system with interacting components.  That system operates as a single whole. It is subdivided into components, separated by abstraction layers, and organized with communication pathways all of which support the operation, throughput, and maintainability of that system. \n\n The tests, on the other hand,  do not form a system .  They are, instead, a set of unrelated assertions.  Each assertion is independent of all the others.  Each small test in the test suite stands alone.  Each test can execute on its own.  Indeed, the tests have no preferred order of execution; and many test frameworks enforce this by executing the tests in a random order. \n\n The discipline of TDD tells us to build the production code one small test case at a time.  That discipline also gives us guidance on the order in which to write those tests.  We choose the simplest tests at first, and only increase the complexity of the tests when all simpler tests have been written and passed. \n\n This ordering is important.  Novices to TDD often get the ordering wrong and find that they have written a test that forces them to implement too much production code.  The rules of TDD tell us that if a test cannot be made to pass by a trivial addition or change to the production code; then a simpler test should be chosen. \n\n Thus, the tests, and their ordering, form the assembly instructions for the production code.  If those tests are made to pass in that order, then the production code will be assembled through a series of trivial steps. \n\n But, as we all know, assembly instructions are not reversible.  It is difficult to look at an airplane, for example, and derive the assembly procedure.  On the other hand, given the assembly procedure, an airplane can be built one piece at a time. \n\n Thus, the conversion of the test suite into production code is a trap-door function; rather like multiplying two large prime numbers.  It can be trivially executed in one direction; but is very difficult, if not completely impractical, to execute in the other.  Tests can trivially drive production code; but production code cannot practicably drive the equivalent test suite. \n\n Bottom Line \n\n What we can conclude from this is that there is a well defined discipline of test-first that is equivalent to double-entry bookkeeping; but  there is no such discipline for test-after.   It is possible to test after, of course, but there’s no way to define it as a  discipline .  The discipline only works in one direction.  Test-first. \n\n As I said at the start: The stakes are enormous.  Millions are waiting to be gained or lost.  Lives and fortunes are at stake.  Our businesses, and indeed our whole society, are depending upon us.  What discipline will we use to ensure that we do not let them down? \n\n If accountants can do it, can’t we? \n\n Of course we can. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/02/23/NecessaryComments.html", "title": "Necessary Comments", "content": "\n       It is well known that I prefer code that has few comments.  I code by the principle that good code does not require many comments.  Indeed, I have often suggested that every comment represents a failure to make the code self explanatory.  I have advised programmers to consider comments as a last resort. \n\n I recently came across that last resort. \n\n I was working with a member of our  cleancoders.com  team, on a particular issue with our site.  We were trying to implement a feature that would inform a customer who bought one of our videos, what other videos they might want to purchase as well.  The feature would find all the videos that had been purchased by others who had bought that video, and would pick the most popular and recommend it to the customer. \n\n The algorithm to do that can take several seconds to run; and we didn’t want the customer to have to wait.  So we decided to cache the result and run the function no more often than every N minutes. \n\n Our strategy was to wrap the long running function in  another  function that would return the previous result from the cache and if more than the N minute had passed, would run the algorithm in a separate thread, and cache the new result.  We called this “choking”. \n\n The long running function was called the “Chokeable Function”, and the wrapping function was called the “Choked Function”.  The Choked Function had the same signature and return value as the Chokeable function; but implemented our choking behavior. \n\n Trying to explain this in code is very difficult.  So we wrote the following comment at the start of the choking module: \n\n ; A Choked function is a way to throttle the execution of a long running\n; algorithm -- a so-called \"Chokeable Function\".  The Choked function\n; returns the last result from the Chokeable Function; and only allows\n; the Chokeable function to be called if more than the Choke-time has \n; elapsed since its last invocation.\n \n\n Now imagine the unit tests!  At first, as we were writing them, we tried to name those tests with nice long names.  But the names kept getting longer and more obscure.  Moreover, as we tried to explain the tests to each other, we found that we needed to fall back on diagrams and pictures. \n\n In the end we drew a timing diagram that showed all the conditions that we’d have to deal with in the unit tests. \n\n We realized that nobody else would understand our tests unless they could see that timing diagram as well.  So we drew that timing diagram, along with explanatory text, into the comments in the test module. \n\n ; Below is the timing diagram for how Choked Functions are tested.\n; (See the function-choke module for a description of Choked Functions.)\n;\n; The first line of the timing diagram represents calls to the choked\n; function.  Each such call is also the subject of a unit test; so they\n; are identified by a test#\n;\n; In this test, the chokeable function counts the number of times it has\n; been invoked, and the choked function returns that counter.  The expected\n; return value is the below the first horizontal line in the timing\n; diagram.\n\n; 1  2  3   3.5  4  5    6  Test#\n;    aaaaa\n;---------------------------\n; n  n  1    1   1  1    2\n;---------------------------------------------\n;AAAAA           AAAAA\n;     CCCCCCC         CCCCCCC\n;\n; Where: A is the algorithm time. (The chokeable function run time)\n;        C is the choke time\n;        n is nil\n;        1 is \"result-1\"\n;        2 is \"result-2\"\n \n\n The names of the tests then became, simply,  test1  and  test2 , up to  test6 ; referring back to the diagram. \n\n We were pretty pleased with the result; both the code and the comments. \n\n So, this is one of those cases where there was no good way for the code to be self documenting.  Had we left the comments out of these modules, we would have lost the intent and the rationale behind what we did. \n\n This doesn’t happen all the time.  Indeed, I have found this kind of thing to be relatively rare.  But it  does  happen; and when it does nothing can be more helpful than a well written, well thought through, comment. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/05/05/TestDefinitions.html", "title": "First-Class Tests.", "content": "\n       I believe it may be my fate to find blogs written by people who have fallen prey to unfortunate disciplines that have led them to give up on unit testing.   This blog  is just another one of those. \n\n The author tells of how his unit tests are fragile because he mocks out all the collaborators.  (sigh).  Every time a collaborator changes, the mocks have to be changed.  And this, of course, makes the unit tests fragile. \n\n The author goes on to tell us how he abandoned unit testing, and started doing, what he called “System Testing” instead.  In his vocabluary, “System Tests” were simply tests that are more end-to-end than “unit tests”. \n\n So first, some definitions.  Pardon me for my hubris, but there are so many different definitions of “unit test” and “system test” and “acceptance test” out there that it seems to me someone ought to provide a single authoritative definition.  I don’t know if these definitions will stick; but I hope  some  set of definitions does in the near future. \n\n \n   \n     Unit Test: A test written by a programmer for the purpose of ensuring that the production code does what the programmer expects it to do. (For the moment we will ignore the notion that unit tests also aid the design, etc.) \n   \n   \n     Acceptance Test: A test written by the business for the purpose of ensuring that the production code does what the business expects it to do.  The authors of these tests are business people, or technical people who represent the business.  i.e. Business Analysts, and QA. \n   \n   \n     Integration Test: A test written by architects and/or technical leads for the purpose of ensuring that a sub-assembly of system components operates correctly.  These are  plumbing  tests.  They are  not  business rule tests.  Their purpose is to confirm that the sub-assembly has been integrated and configured correctly. \n   \n   \n     System Test: An integration test written for the purpose of ensuring that the  internals  of the whole integrated system operate as planned.  These are  plumbing  tests.  They are  not  business rule tests.  Their purpose is to confirm that the system has been integrated and configured correctly. \n   \n   \n     Micro-test: A term coined by Mike Hill (@GeePawHill).  A unit test written at a very small scope.  The purpose is to test a single function, or small grouping of functions. \n   \n   \n     Functional Test: A unit test written at a larger scope, with appropriate mocks for slow components. \n   \n \n\n Given these definitions, the author of the above blog has given up on badly written micro-tests, in favor of badly written functional tests. Why badly written?  Because in both cases the author describes these tests as coupled to things that they should not be coupled to.  In the case of his micro-tests, he was using too many mocks, and deeply coupling his tests to the implementation of the production code.  That, of course, leads to fragile tests. \n\n In the case of his functional tests, the author described them as going all the way from the UI to the Database; and made reference to the fact that they were slow.  He cheered the notion that his tests could sometimes run in as little as  15 minutes .  15 minutes is much too long to wait for the kind of rapid feedback that unit tests are supposed to give us.  TDDers are not in the habit of waiting for the continuous build system to find out if the tests pass. \n\n Skilled TDDers understand that neither micro-tests, nor functional tests, (nor Acceptance tests for that matter) should be coupled to the implementation of the system.  They should (as the blog’s author urges us) be considered as part of the system and  “…treated like a first-class citizen: [They] should be treated in the same way as one would treat production code.” \n\n What the blog author does not seem to recognize is that first class citizens of the system should not be  coupled .  Someone who treats their tests like first class citizens will take great pains to ensure that those tests are not strongly coupled to the production code. \n\n Decoupling micro-tests and functional tests from the production code is not particularly difficult.  It  does  require some skill at software design; and some knowledge of decoupling techniques.  Generally, a good dose of OO design and dependency inversion, along with the judicious use of a few Design Patterns (like Facade and Strategy) are sufficient to decouple even the most pernicious of tests. \n\n Unfortunately, too many programmers think that the rules for unit tests are different – that they can be “junk code” written using any ad hoc style that they find convenient.  Also, too many programmers have read the books on mocking, and have bought in to the notion that mocking tools are an intrinsic, and necessary, part of unit testing.  Nothing could be further from the truth. \n\n I, for example, seldom use a mocking tool.  When I need a mock (or, rather, a Test Double) I write it myself.  It’s not very hard to write test doubles.  My IDE helps me a lot with that.  What’s more, writing the Test Double myself encourages me not to write tests with Test Doubles, unless it is really necessary.  Instead of using Test Doubles, I back away a bit from micro-testing, and write tests that are a bit closer to functional tests.  This too helps me to decouple the tests from the internals of the production code. \n\n The bottom line is that when people give up on unit tests, it’s usually because they haven’t followed the above author’s advice.  They have not treated the tests like first-class citizens.  They have not treated the tests as though they were part of the system.  They have not maintained those tests to the same standards that they apply to the rest of the system.  Instead, they have allowed the tests to rot, to become coupled, to become rigid, and fragile, and slow.  And then, in frustration, they give up on the tests. \n\n Moral:  Keep your tests clean.  Treat them as first-class citizens of the system. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/03/16/DrCalvin.html", "title": "Is Dr. Calvin in the Room?", "content": "\n       \n\n I saw Grady’s tweet today, and I read the  article  it referenced; and it got me to thinking.  Is the training of a neural net a kind of programming?  It is the kind of activity that an agile team could plan, estimate, and execute in iterations?  Or is it something entirely different? \n\n Before I try to answer that question, let’s set our definitions.  What is programming? \n\n Alan Turing defined computer programming in his  1936 paper .  Since that time we have used his definition without significant alteration.  According to Turing a computer program is a sequence of simple statements of the following form: \n\n \n   Given  that the system is in state  S1 , \n When  event  E  occurs, \n Then  set the system to state  S2  and invoke behavior  B . \n \n\n This statement is very obviously a transition of a finite state machine (FSM).  And, indeed, every program ever written can be reduced to a linear sequence of such statements.  We can, in fact, claim that the act of computer programming is simply the enumeration of all the transitions within the finite state machine that expresses the behavior desired by the programmer. \n\n Of course we don’t often use FSM language to write our code.  We use, instead, the language of Dijkstra:  Sequence ,  Selection , and  Iteration .  However, it is not very difficult to prove that these two forms of expression are equivalent. \n\n As a non-mathematical proof consider that the format of the transition above uses the same  Given / When / Then  triplet that has become so popular for the specification of behavior in  Behavior Driven Development  (BDD). One could, therefore, make the claim that BDD is simply the act of specifying the transitions of the state machine that the programmers will implement using  Sequence ,  Selection , and  Iteration . \n\n As further proof we could look at the discipline of  Test Driven Development  (TDD) in which no production code can be written unless it is to satisfy a failing unit test. \n\n Those unit tests always follow the pattern known as  Arrange ,  Act , and  Assert  (AAA).  First we  arrange  the system into the state appropriate for the test.  Then we  Act  upon the system.  Then we  Assert  that the results of that action were correct. \n\n If you think carefully about this you will realize that AAA is simply an alternative expression for the  Given / When / Then  of BDD. \n\n \n   Given  that we have  arranged  the system into the testable state,   \n When  we  act  upon that system, \n Then  we expect our  assertions  to pass. \n \n\n And, of course, that means that TDD is just a way to fully enumerate all the transitions of the FSM that expresses the desired behavior of the program we are writing. \n\n So it should be no surprise that the act of programming a computer is the act of defining a finite state machine.  What programmers do is to define the necessary states, events, and behaviors.  Every computer program, no matter how simple or complex, is nothing more than a network of states, transitions, and behaviors. \n\n And this is true, even if the program is a neural net. \n\n Our Expectations are high. \n\n The latest buzz words are things like  Machine Learning , and  Deep Learning .  These terms represent the massive problem of designing and  training  a neural network.  But if a neural network is really just an FSM, then why do we have to train it?  Why don’t we simply specify all the states and transitions like any programmer would? \n\n The answer is that  we don’t know  all the states and transitions.  We have no way to enumerate them all.  And so we expect the training of the neural network to cause it to  infer  those states and transitions. \n\n Actually, that’s not quite right.  The neural network may not know what state it is in.  It may also not know what events it is receiving.  It may only know the states and events in a statistical sense.  And we expect it to infer the correct behavior from those statistics. \n\n I can explain this by using a neural network that you are intimately familiar with – your brain. \n\n Imagine that you are driving a car down a highway at a time when there’s very little traffic.  What you see ahead of you is empty road.  Your speed is stable.  The weather is nice.  The Sun is shining above.  You  know  your state. You see the road curve ahead.  You realize that, given your current state, you should adjust the angle of the wheel ever so slightly.  This puts you in a new state; and you know what state that is.  Your behavior is almost entirely deterministic.  You could write the FSM. \n\n Now imagine that that you are driving on a busy city street in the rain.  There are cars jockeying for position around you.  Traffic lights, one-way streets, construction, pedestrians, scooters etc.  The amount of data you must process on a second by second basis is daunting.  You must keep looking left and right, checking the car ahead, the car behind, the traffic lights, and signs.  You  never  really know what state you are in.  You have a statistical feel for your state.  You can assign a probability to the states you might be in.  The car in front of you lurches for some unexpected reason.  You quickly increase pressure on your brake.  “No,” you decide, “it wasn’t stopping..  It just hit a pothole.” \n\n Somehow, in the chaos of all that conflicting data, you are expected to make good driving decisions, and execute concrete behaviors.  It is your job, as the neural net in charge of that car, to extract and express order from  chaos.  No programmer could write the FSM to do this. \n\n Training \n So how do you train a neural net?  The simple answer to that question is that you present the neural net with a sequence of events, and then “reward” or “punish” its behavior.  In this case, the words “reward” and “punish” amount to incrementing or decrementing an “appropriateness” value.  The neural net learns to maximize “appropriateness” based on current and previous events.  In effect, the neural net builds up within it’s memory a fuzzy finite state machine.  I say, in effect, because you could not find that FSM in the memory of the neural net.  It is hidden within the variables and coefficients, and interconnections of the program.  It’s there; but it’s encoded in a way that is very hard to decode. \n\n So, to address Grady’s question.  Is training a neural net programming?  Is training a neural net a story, or a set of stories, for an agile team to prioritize and execute?  Can you estimate how long it will take to train a neural net? \n\n The answer to that, at least at the moment, is clearly “No”.  Training neural nets is something of a black art.  There may be a science behind it; but that science is not particularly rigorous yet. \n\n Training a neural net is nothing like programming.  It is not the enumeration of transitions into a finite state machine.  Rather, it is an attempt to find enough events to present to the neural network, and a corresponding attempt to measure how appropriately the neural net behaves.  And it is that last measurement that is the most fraught with uncertainty and danger. \n\n Expectations too high? \n Are we going to have self driving cars pretty soon?  Are we going to see true machine  intelligence  in the near future?  Will autonomous taxis be driving us from arbitrary point to arbitrary point within the decade? \n\n Well, let’s do a little math. \n\n The human brain has about 1E14 neurons.  Each neuron connects to around 1E3 other neurons.  If we consider each connection to roughly correspond to a bit of memory; then our brains have on the order of 1E17 bits of memory.  That corresponds roughly to 10 petabytes of  RAM .  Don’t confuse this with disk or SSD.  We’re talking about instant access memory.  RAM. \n\n On top of that each of those 1E17 connections can switch every 10ms or so.  That means that the human brain can process around 1E19 bits per second.  My laptop, with 4 cores running at 2.8ghz, and a 64 bit buss can process only 1E15 bits per second. So the human brain can outperform my laptop by four orders of magnitude in sheer processing power. \n\n To mimic the processing capability of the human brain, I would need 10,000 apple powerbook laptops that were intimately connected (no, wifi wouldn’t do) to each other, and 10 petabytes of RAM. That is not a hardware technology that we currently possess. \n\n Indeed, the structure of our computers is so vastly different from the structure of the brain, that making the comparison is laughable.  The human brain is not a computer in any sense that we would recognize the word.  It does not have addresses, it does not have instructions, it does not have stacks, integers, floating point numbers, busses, master clocks, or anything else we would recognize as part of a computer.  In fact, the human brain is not even digital.  It is an analog data processor composed of 100 billion components that are massively interconnected. \n\n So, no.  We are not going to see Hal 9000 any time soon.  Johnny #5 will not soon be alive.  Dialing MycroftXXX on your phone will not connect you with Mike, the leader of the revolution.  Collossus and Guardian are not going to co-conspire to take over the world.  Don’t expect to meet Neo in the Matrix.  Don’t expect Kit to speak out of the radio of your car.  Our computers are simply not going to wake up any time soon. \n\n Autonomous Taxis \n But can our computers drive cars?  Can we make neural nets powerful enough to safely drive human passengers from any arbitrary point, to any other arbitrary point? \n\n Let’s do a little more math.  My brain weighs about 1 pound.  Since it is 10,000 times more powerful than my laptop, I infer that if my laptop were made out of neurons, it would weight 0.0001 pounds, or about a twentieth of a gram.  This is roughly the mass of the brain of a honeybee. \n\n Actually, the million neurons in the honeybee’s brain are packed ten times more densely than ours, and operate much faster than ours.  Which means the honeybee’s brain is significantly more powerful than my laptop. \n\n So, do you believe that you could train a honeybee’s brain to drive your car?  Safely?  Legally?  From any arbitrary point, to any other arbitrary point? \n\n Perhaps that’s not a fair question.  So let’s multiply that brain size by a factor of 600.  A cat’s brain weighs 30 grams.  It would require 600 of my laptops, intimately connected to each other and to terabytes of RAM in order to mimic a cat’s brain.  Do you believe you could train your cat to drive you from place to place?  Safely? \n\n Or is it possible, that in order to properly train a neural net to safely drive a car, a neural net with the capabilities of the human brain will be required? \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/01/09/DiamondSquare.html", "title": "TDD Lesson - Terrain Generation", "content": "\n       Have you ever wondered how the terrain in games like Minecraft is generated?  There’s an old algorithm called the  Diamond-Square Algorithm  that is in common use.  I thought it would be fun to implement this algorithm using TDD. Here’s a sample output from the result: \n\n \n\n So I thought you might like to see the TDD steps I used to build this algorithm.  Before we start, make sure you understand the algorithm by reading the link above.  Don’t worry, it’s a very simple algorithm; and the linked article is extremely easy to read. \n\n So how do you test-drive an algorithm like this?  At first it might seem simple.  For each test, create an small array of doubles, predict the results, and then write the test that compares the predicted results with the output of the algorithm. \n\n There are several problems with this approach. The first is data-overload.  Even the smallest array is a 3X3 with 9 different values to check.  The next smallest is 5X5 with 25 values.  Then 9X9 with 81 values.  Trying to write a comprehensive set of tests, even for the 3X3 case, would be tedious at best; and very difficult for someone else to understand. \n\n The second problem is that test-driving from the raw results forces us to write the whole algorithm very early in the testing process.  There’s no way to proceed incrementally.  You may recall from  previous blogs  I’ve written that I call this: “Getting Stuck”. \n\n Yet another problem is that those tests don’t prove the the algorithm is working the way it is supposed to.  They just prove that the resulting values are right.  Now in most cases that’s not a problem.  I’m a strong adherent of the  Classical School  of TDD, and I tend not to use my tests to inspect the internal workings of the software I am writing.  The output values are usually good enough.  In this case, however, the algorithm is the overriding concern. \n\n Now, being that I am a classicist, I still want to check values.  However, the values I want to check are not the output values of the algorithm, but the internal values that the algorithm uses.  Specifically the coordinates of the cells that are being manipulated. \n\n Consider:  I want to know that the coordinates of the midpoint of the square are being calculated properly.  I also want to ensure that the value of the midpoint is set by taking the average of the four corners.  So I’m very interested in the calculation of the coordinates of those four corners.  On the other hand, once I know that those coordinates are calculated properly, I don’t really care what the values of the cells are, or whether the average is calculated properly.  I mean, averages are pretty trivial to test.  So I really don’t need to test the average calculation yet. \n\n The  Diamond  half of the algorithm has similar concerns.  I want to make sure that the coordinates of the diamond points are being calculated properly, and that the coordinates for the inputs to those averages are calculated correctly.  I also want to make sure that the diamonds at the edges of the array only draw from the three real points; and don’t try to reach outside the array for the fourth. \n\n So, here’s the code for my tests.  I’m not going to show you the code for the actual algorithm; because it’s not particularly interesting.  Indeed, I suggest you use these tests to drive your own implementation.  ;-) \n\n The Code \n\n We begin by using Stephan Bechtold’s wonderful  HierarchicalContextRunner  to help us organize our tests.  If you aren’t familiar with this runner – shame on you. <grin> \n\n Note the  String  named  actions .  This is going to be the heart of our strategy. \n\n @RunWith(HierarchicalContextRunner.class)\npublic class TerrainInterpolatorTest {\n  private String actions = \"\";\n  private double[][] dummy;\n  private TerrainInterpolator interpolator;\n \n\n We start with simple validations.  Our setup for this set of tests creates an instance of  TerrainInterpolatorSpy , which you’ll see down at the end of the code.  This test double is going to load that  actions  string with the values that we want to test. \n\n This is a rather different slant an a  spy  test double.  Rather than giving us booleans and flags to inspect about the calls of individual functions, this spy is going to load the  actions  string with the sequence of things that happened as the algorithm proceeded. \n\n The first three tests are just very simple checks to make sure that the algorithm behaves properly when given invalid inputs. \n\n   public class SimpleValidations {\n    @Before\n    public void setUp() throws Exception {\n      dummy = new double[1][1];\n      interpolator = new TerrainInterpolatorSpy();\n    }\n \n\n First we test that when we are given an array that’s too small, no actions take place. \n\n     @Test\n    public void terminalCondition_sizeOne() throws Exception {\n      interpolator.interpolate(dummy, 1);\n      assertThat(actions, isEmptyString());\n    }\n \n\n The next two tests make sure we can detect that the array has an invalid size. \n\n     @Test(expected = TerrainInterpolator.BadSize.class)\n    public void sizeMustBePowerOfTwoPlus1() throws Exception {\n      interpolator.interpolate(dummy, 2);\n    }\n\n    @Test\n    public void Check_isPowerOfTwo() throws Exception {\n      assertThat(interpolator.isPowerOfTwo(2), is(true));\n      assertThat(interpolator.isPowerOfTwo(4), is(true));\n      assertThat(interpolator.isPowerOfTwo(8), is(true));\n\n      assertThat(interpolator.isPowerOfTwo(1), is(false));\n      assertThat(interpolator.isPowerOfTwo(7), is(false));\n      assertThat(interpolator.isPowerOfTwo(18), is(false));\n    }\n \n\n Now, on to the real work.  We’re going to look at the coordinate calculations.  We start by making sure that the midpoint of a 3X3 square is calculated properly, and that the points passed to the average function are the correct points.  Stare at that string below until you understand what it means.  The code that builds the  action  string is in the  TerrainInterpolatorSpy  below. \n\n     public class SquareDiamondCoordinateCalculations {\n      @Test\n      public void simpleThreeByThree_SquarePass() {\n        interpolator.interpolate(dummy, 3);\n        assertThat(actions, startsWith(\n\t\t\"Square(0,0,3): A([0,0],[2,0],[0,2],[2,2])->[1,1].\"));\n      }\n \n\n Next, we make sure that the diamond points are calculated properly, that the inputs to the average function come from the right locations, and that, since all the diamonds on a 3X3 are on the edges, that the averages are taken from only three points. \n\n       @Test\n      public void simpleThreeByThree_DiamondPass() {\n        interpolator.interpolate(dummy, 3);\n        assertThat(actions, endsWith(\"Diamond(0,0,3): \"+\n          \"A([0,0],[2,0],[1,1])->[1,0]. \" +\n          \"A([1,1],[0,0],[0,2])->[0,1]. \" +\n          \"A([0,2],[2,2],[1,1])->[1,2]. \" +\n          \"A([1,1],[2,0],[2,2])->[2,1]. \"));\n      }\n \n\n Next, let’s look at a 5X5 and make sure that the initial square and diamond coordinates are properly calculated. \n\n       @Test\n      public void DiamondSquare_FirstPass() throws Exception {\n        interpolator.interpolate(dummy, 5);\n        assertThat(actions, startsWith(\n         \"Square(0,0,5): A([0,0],[4,0],[0,4],[4,4])->[2,2]. \"+\n            \"Diamond(0,0,5): \" +\n            \"A([0,0],[4,0],[2,2])->[2,0]. \" +\n            \"A([2,2],[0,0],[0,4])->[0,2]. \" +\n            \"A([0,4],[4,4],[2,2])->[2,4]. \" +\n            \"A([2,2],[4,0],[4,4])->[4,2]. \"));\n      }\n\n    }\n \n\n OK, now we have to make sure that the square and diamonds are properly repeated, with the right size and locations.  At this point we don’t care about the averages anymore because we know the coordinates for the inputs are calculated correctly.  All we care about now is that the starting points, and sizes for the squares and diamonds are calculated properly.  So we’re going to use a different spy.  The  TerrainInterpolatorDiamondSquareSpy  loads the  actions  string with only the square and diamond actions, but not the averages. \n\n public class SquareDiamondRepetition {\n  @Before\n  public void setup() {\n    dummy = new double[1][1];\n    interpolator = \n      new TerrainInterpolatorDiamondSquareSpy();\n  }\n   \n @Test\n public void FiveByFive() throws Exception {\n  interpolator.interpolate(dummy, 5);\n  assertThat(actions, is(\n  \"\" +\n  \"Square(0,0,5) Diamond(0,0,5) \" +\n  \"Square(0,0,3) Square(0,2,3) Square(2,0,3) Square(2,2,3) \" +\n  \"Diamond(0,0,3) Diamond(0,2,3) Diamond(2,0,3) Diamond(2,2,3) \"\n  ));\n }    }\n \n\n Now, finally, we can look at some  real  values.  This will help us test-drive the simple stuff, like computing averages from a list of points. \n\n     public class Averages {\n      @Before\n      public void setup() {\n        dummy = new double[3][3];\n        interpolator = new TerrainInterpolator();\n      }\n \n\n The call to  interpolate  with just two arguments, turns off any randomization.  Thus, if we start with an array of all zeros; our result should be an array of all zeros. \n\n       @Test\n      public void zero() throws Exception {\n        interpolator.interpolate(dummy, 3);\n        assertThat(dummy, is(new double[][]\n\t      {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}}));\n      }\n \n\n Of course the same is true of all ones.  Indeed, an array of all X will produce an array of all X. \n\n       @Test\n      public void allOnes() throws Exception {\n        dummy[0][0] = dummy[2][0] = \n        dummy[0][2] = dummy[2][2] = 1;\n        interpolator.interpolate(dummy, 3);\n        assertThat(dummy, is(new double[][]{\n\t      {1, 1, 1}, \n\t      {1, 1, 1}, \n\t      {1, 1, 1}}));\n      }\n \n\n Now for some real math. \n\n       @Test\n      public void ramp() throws Exception {\n        dummy[0][0] = 0;\n        dummy[2][0] = 12;\n        dummy[0][2] = 12;\n        dummy[2][2] = 24;\n        interpolator.interpolate(dummy,3);\n        assertThat(dummy, is(new double[][]{\n\t      {0, 8, 12}, \n\t      {8, 12, 16}, \n\t      {12, 16, 24}}));\n      }\n    }\n \n\n OK, finally, let’s make sure the random factor and offset are being added properly.  The offset is my own creation, so that I can drive the values up or down with a fixed offset.   The third argument to the  interpolate  call is the random amplitude.  The fourth is the offset.   The  TerrainInterpolatorWithFixedRandom  test double simply turns off the randomness, and uses the absolute values of the random amplitude instead. \n\n This test took a lot of hand calculation to create.  And, of course, I did the calculation wrong, at first.  So then I had to check the results from the algorithm, and modify the test. \n\n     public class RandomsAndOffsets {\n      @Before\n      public void setup() {\n        dummy = new double[5][5];\n        interpolator = \n          new TerrainInterpolatorWithFixedRandom();\n      }\n\n      @Test\n      public void volcano() throws Exception {\n        interpolator.interpolate(dummy, 5, 2,4);\n        assertThat(dummy, is(new double[][]{\n          {0,8.5,8,8.5,0},\n          {8.5,8.5,10.75,8.5,8.5},\n          {8,10.75,6,10.75,8},\n          {8.5,8.5,10.75,8.5,8.5},\n          {0,8.5,8,8.5,0}\n        }));\n      }\n    }\n  }\n \n\n The  TerrainInterpolatorSpy  loads the  actions  variable with the square, diamond, set, and average actions invoked by the algorithm.  This test double simply overrides those functions of the main algorithm with others that load the  actions  variable. \n\n   private class TerrainInterpolatorSpy \n    extends TerrainInterpolator {\n    void doSquare(int x, int y, int size) {\n      actions += String.format(\"Square(%d,%d,%d): \",x,y,size);\n      super.doSquare(x, y, size);\n    }\n\n    void doDiamond(int x, int y, int size) {\n      actions += String.format(\n\t    \"Diamond(%d,%d,%d): \",x,y,size);\n      super.doDiamond(x, y, size);\n    }\n\n    void set(int x, int y, double value) {\n      actions += String.format(\"->[%d,%d]. \", x, y);\n    }\n\n    double get(int x, int y) {\n      return -1;\n    }\n\n    double average(Integer... points) {\n      actions += \"A(\";\n      for (int i = 0; i < points.length; i += 2)\n        actions += String.format(\n\t      \"[%d,%d],\", points[i], points[i + 1]);\n      actions = actions.substring(0, actions.length()-1)+\")\";\n      return 0;\n    }\n  }\n \n\n The  TerrainInterploatorDiamondSquareSpy  test double simply loads the  actions  variable with the square and diamond actions; but not with the set or average actions. \n\n   private class TerrainInterpolatorDiamondSquareSpy \n    extends TerrainInterpolator {\n    void doSquare(int x, int y, int size) {\n      actions += String.format(\"Square(%d,%d,%d) \",x,y,size);\n    }\n\n    void doDiamond(int x, int y, int size) {\n      actions += String.format(\"Diamond(%d,%d,%d) \",x,y,size);\n\n    }\n  }\n \n\n The  TerrainInterpolatorWithFixedRandom  test double simply overrides the  random  function to return the fixed  randomAmplitude  instead of returning a random number between + and -  randomAmplitude . \n\n   private class TerrainInterpolatorWithFixedRandom \n    extends TerrainInterpolator {\n    double random() {\n      return randomAmplitude;\n    }\n  }\n}\n \n\n From these tests I created a fully functioning DiamondSquare algorithm.  I bet you can too.  Give it a try. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/07/11/PragmaticFunctionalProgramming.html", "title": "Pragmatic Functional Programming", "content": "\n       The move to functional programming began, in earnest, about a decade ago.  We saw languages like Scala, Clojure, and F# start to attract attention. This move was more than just the normal “Oh cool, a new language!” enthusiasm.  There was something real driving it – or so we thought. \n\n Moore’s law told us that the speed of computers would double every 18 months.  This law held true from the 1960s until 2000.  And then it stopped.  Cold.  Clock rates reached 3ghz, and then plateaued.  The speed of light limit had been reached.  Signals could not propagate across the surface of the chip fast enough to allow higher speeds. \n\n So the hardware designers changed their strategy.  In order to get more throughput, they added more processors (cores).  In order to make room for those cores they removed much of the cacheing and pipelining hardware from the chips.  Thus, the processors were a bit slower than before; but there were more of them.  Throughput increased. \n\n I got my first dual core machine 8 years ago.  Two years later I got a four core machine.  And so the proliferation of the cores had begun.  And we all understood that this would impact software development in ways that we couldn’t imagine. \n\n One of our responses was to learn Functional Programming (FP).  FP strongly discourages changing the state of a variable once initialized.  This has a profound effect upon concurrency.  If you can’t change the state of a variable, you can’t have a race condition.  If you can’t update the value of a variable, you can’t have a concurrent update problem. \n\n This, of course, was thought to be the solution to the multi-core problem.  As cores proliferated, concurrency, NAY!,  simultaneity  would become a significant issue.  FP ought to provide the programming style that would mitigate the problems of dealing with 1024 cores in a single processor. \n\n So everyone started learning Clojure, or Scala, or F#, or Haskell; because they knew the freight train was on the tracks heading for them, and they wanted to be prepared when it arrived. \n\n But the freight train never came.  Six years ago I got a four core laptop.  I’ve had two more since then.  The next laptop I get looks like it will be a four core laptop too.  Are we seeing another plateau? \n\n \n   As an aside, I watched a movie from 2007 last night.  The heroine was using a laptop, viewing pages on a fancy browser, using google, and getting text messages on her flip phone.  It was all too familiar.  Oh, it was dated – I could see that the laptop was an older model, that the browser was an older version, and the flip phone was a far cry from the smart phones of today.  Still – the change wasn’t as dramatic as the change from 2000 to 2011 would have been.  And not nearly as dramatic as the change from 1990 - 2000 would have been.  Are we seeing a plateau in the rate of computer and software technology? \n \n\n So, perhaps, FP is not as critical a skill as we once thought.  Maybe we aren’t going to be inundated with cores.  Maybe we don’t have to worry about chips with 32,768 cores on them.  Maybe we can all relax and go back to updating our variables again. \n\n I think that would be a mistake.  A big one.  I think it would be as big a mistake as rampant use of  goto .  I think it would be as dangerous as abandoning dynamic dispatch. \n\n Why?  We can start with the reason we got interested in the first place.  FP makes concurrency much safer.  If you are building a system with lots of threads, or processes, then using FP will strongly reduce the issues you might have with race conditions and concurrent updates. \n\n Why else?  Well, FP is easier to write, easier to read, easier to test, and easier to understand.  Now I imagine that a few of your are waving your hands and shouting at the screen.  You’ve tried FP and you have found it anything but easy.  All those maps and reduces and all the recursion – especially the  tail  recursion are anything but  easy .  Sure.  I get it.  But that’s just a problem with familiarity.  Once you are familiar with those concepts – and it doesn’t take long to develop that familiarity – programming gets a  lot  easier. \n\n Why does it get easier?  Because you don’t have to keep track of the state of the system.  The state of variables can’t change; so the state of the system remains unaltered.  And it’s not just the system that you don’t have to keep track of.  You don’t need to keep track of the state of a list, or the state of a set, or the state of a stack, or a queue; because these data structures cannot be changed.  When you push an element onto a stack in an FP language, you get a new stack, you don’t change the old one.  This means that the programmer has to juggle fewer balls in the air at the same time.  There’s less to remember.  Less to keep track of.  And therefore the code is much simpler to write, read, understand, and test. \n\n So what FP language should you use?  My favorite is Clojure.  The reason is that Clojure is absurdly simple.  It’s a dialect of Lisp, which is a beautifully simple language.  Here, let me show you. \n\n Here’s a function in Java:   f(x); \n\n Now, to turn this into a function in Lisp, you simply move the first parenthesis to the left:  (f x) . \n\n Now you know 95% of Lisp, and you know 90% of Clojure.  That silly little parentheses syntax really is just about all the syntax there is to these languages.  They are  absurdly  simple. \n\n Now I know, maybe you’ve seen Lisp programs before and you don’t like all those parentheses.  And maybe you don’t like  CAR  and  CDR  and  CADR , etc.  Don’t worry.  Clojure has a bit more punctuation than Lisp, so there are fewer parentheses.  Clojure also replaced  CAR  and  CDR  and  CADR  with  first  and  rest  and  second .  What’s more, Clojure is built on the JVM, and allows complete access to the full Java library, and any other Java framework or library you care to use.  The interoperability is quick and easy.  And, better still, Clojure allows full access to the OO features of the JVM. \n\n “But wait!” I hear you say.  “FP and OO are mutually incompatible!”  Who told you that?  That’s nonsense!  Oh, it’s true that in FP you cannot change the state of an object; but so what?  Just as pushing an integer onto a stack gives you a new stack, when you call a method that adjusts the value of an object, you get a new object instead of changing the old one.  This is very easy to deal with, once you get used to it. \n\n But back to OO.  One of the features of OO that I find most useful, at the level of software architecture, is dynamic polymorphism.  And Clojure provides complete access to the dynamic polymorphism of Java.  Perhaps an example would explain this best. \n\n (defprotocol Gateway\n  (get-internal-episodes [this])\n  (get-public-episodes [this]))\n \n\n The code above defines a polymorphic  interface  for the JVM.  In Java this interface would look like this: \n\n public interface Gateway {\n\tList<Episode> getInternalEpisodes();\n\tList<Episode> getPublicEpisodes();\n}\n \n\n At the JVM level the byte-code produced is identical.  Indeed, a program written in Java would  implement  the interface just as if it was written in Java.  By the same token a Clojure program can implement a Java interface.  In Clojure that looks like this: \n\n (deftype Gateway-imp [db]\n  Gateway\n  (get-internal-episodes [this]\n    (internal-episodes db))\n\n  (get-public-episodes [this]\n    (public-episodes db)))\n \n\n Note the constructor argument  db , and how all the methods can access it.  In this case the implementations of the interface simply delegate to some local functions, passing the  db  along. \n\n Best of all, perhaps, is the fact that Lisp, and therefore Clojure, is (wait for it)  Homoiconic , which means that the code is data that the program can manipulate.  This is easy to see.  The following code:  (1 2 3)  represents a list of three integers.  If the first element of a list happens to be a function, as in:  (f 2 3)  then it becomes a function call.  Thus, all function calls in Clojure are lists; and lists can be directly manipulated by the code.  Thus, a program can construct and execute other programs. \n\n The bottom line is this.  Functional programming is important.  You should learn it.  And if you are wondering what language to use to learn it, I suggest Clojure. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/07/24/DriveMeToTorontoHal.html", "title": "Drive me to Toronto, Hal.", "content": "\n       I keep reading articles in the news about the imminent inevitability of driverless cars.  The financial news is all abuzz over the idea.  So are the techies.  The predictions are that truck drivers, cab drivers, and Uberists will all be out of work in the next five years. \n\n I have one word for all these predictions:   Toronto! \n\n Do you remember when IBM Watson played, and won, Jeopardy?  This was back in February of 2011.  The achievement was dramatic.  Watson so vastly outperformed the human competitors that it was really kind of sad.  Afterwards, Ken Jennings, one of Watson’s opponents who had previously won 74 games in a row, acquiesced by welcoming “our new computer overlords.” \n\n But as remarkable as the IBM achievement was, it wasn’t without it’s embarrassments.  And the embarrassments were telling.  Very telling.  The mistakes that Watson made were not the kind of mistakes that a human would make.  Indeed, the mistakes were eyebrow raising – and given the implications perhaps even hair raising. \n\n One such event occurred when the contestants were asked to name a US city that had one airport named after a WW2 hero, and another airport named after a WW2 battle.  Think about this for a second.  Walk through the top three cities in the US.  New York?  No, JFK, LGA, and Newark don’t fit.   LA? No, LAX, John Wayne, Ontario don’t fit.  Chicago?  Aha!  O’Hare and Midway!  That’s the one.  So what US city did Watson choose? \n\n Toronto. \n\n Now, of course, there was a reason that Watson chose Toronto over Chicago.  Watson was, after all, a computer; and computers always have absolutely discrete and unambiguous reasons for what they do.  A series of  if  statements comparing weighted values through a complex tree of associations yielded a final, definite result.  So there was certainly a reason.  A good reason. \n\n I don’t know what the details of that reason were.  I’d like to know because I think the answer would be interesting from a technical point of view.  On the other hand I don’t much care what the reasons were; because whatever the reasons, the answer that Watson gave was supremely stupid. \n\n No human of moderate intelligence and education would have made that mistake.  No such human could understand another such human making that mistake.  Indeed, any human who insisted on that answer, as Watson surely would have, might very well be deemed legally insane. \n\n So here’s the dilemma:  Watson outperformed the human Jeopardy contestants by a significant margin.  The claim has been made that driverless cars will outperform humans by a significant margin too.  Driverless cars will decrease the accident and fatality rates.  Driverless cars will make the roads safer. Those are the claims. \n\n But the inevitable tragedy will eventually occur.  We can imagine it.  Perhaps, one day, a driverless car will run down a two-year-old who strayed into the street. \n\n The car will have had a good reason to kill that child.  The car, after all, is a computer; and a computer always has an absolutely discrete and unambiguous reason for what it does.  And, believe me,  everyone  is going to want to know the reason that the little child had to die. \n\n Imagine the courtroom scene.  The distraught parents, the angry press, the subdued lawyers representing the company who made the car.  The car’s computer is on the stand. It is about to answer the question.  There’s a hush as everyone leans in.  The prosecutor pointedly asks: “Why did that child have to die?”  And the computer, parsing through all the data in it’s memory, running through a chain of  if  statements comparing all the carefully weighted values, finally, and definitively answers: “The reason was:  Toronto. ” \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/07/28/TheBrainProblem.html", "title": "The Brain Problem", "content": "\n       Imagine an information processing system composed of over 100 billion components that are deeply interconnected to one another.  The interconnections are physical and dedicated – they aren’t on a buss. Imagine that the entire system is an  analog  computer.  Analog computers are much faster, if less accurate, than digital computers.  Imagine that the immense memory capacity of this device is also  analog .  The whole system is composed of myriads of feedback loops, filters, routers, sorters, and many more functions that we cannot even begin to imagine. \n\n Imagine that each one of those 100 billion components are, themselves, highly complex and powerful  analog  information processors capable of transforming and communicating immense amounts of data in very short times.  Imagine that at the core of each of these massive analog components there is an array of tens of thousands of digital processors executing a digital program that is tuned and customized just for it. \n\n That’s what a human brain is.  A human brain contains one hundred billion neurons. Each neuron is physically connected to thousands upon thousands of other neurons.  Neuronal signals are analog in nature.  Information is contained in the  rate  at which neurons fire.  It is the pulse rate, not the pulses themselves, that contain and transmit information. \n\n Each Neuron is, itself, a fantastically complicated information processor.  Chemical signals, analog in nature, are communicated through an intricate labyrinth of fibers, tubules, channels, and membranes that are continuously forming and changing, and that specifically interconnect the components within the cell in the most dynamic fashion.  The signals are contained in the quantity and nature of the tens of thousands of proteins that the cell can create, along with ions and other molecule concentrations.  Virtually every molecule in a cell is a component in this processor.  And each molecule can operate at the speed of molecular interactions.  (i.e.  fast ).  The raw information processing power of a single cell is vastly superior to a modern laptop. \n\n What’s more cells signal each other using chemicals.  Some simply send chemical signals to their neighbors.  Others dump chemicals into the bloodstream to communicate with cells on the other side of the body.  The number and form of chemical interconnections between and within cells is daunting. \n\n If you want to get a flavor for how impressive this cellular machinery is, just watch  this  video.  No.  Don’t just watch it.   Study it! \n\n At the heart of each neuron there is a vast array of tens of thousands of digital processors, working to copy, repair, transmit, and execute the digital instructions contained within the DNA.  These processors are, themselves, influenced by the analog chemical signaling within the cell. \n\n So here’s the bottom line.  Your brain is a highly efficient analog computer composed of a deeply interconnected array of one hundred billion analog processors of immense computational power, each driven by tens of thousands of digital processors that are monitored and controlled through interactions both from within and without. \n\n Or, to say this another way, virtually every molecule, if not every atom, in your brain is an active component in the information processing going on between your ears. \n\n The idea that our meager internet, which indirectly interconnects only a trifling few hundreds of millions of pitifully weak serial processors, could process the information of a single brain is absurd. \n\n No, we will not be achieving artificial sentience any time soon. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/08/09/ThoughtPolice.html", "title": "Thought Police", "content": "\n       Let me tell you the sad story of Edward.  Poor Edward is an escalator operator working for a company named Escalators-R-Us (ERU).  ERU had noticed that people less than the average height (short people) tend not to become escalator operators.  In fact, at ERU, only 20% of escalator operators are short.  So ERU held classes for short people, encouraging them to take escalator operator jobs.  ERU also adjusted their hiring practices so as to prefer short people over tall people.  ERU’s goal was to increase the height diversity of escalator operators.  The managers at ERU felt that the height gap was a result of deeply ingrained societal heightism.  They believed their policies were a way to promote social justice. \n\n Edward, being tall, didn’t think this was the right approach.  He felt that the issue was not really a problem.  His idea was that there’s something biological about short people that predisposes them against operating escalators.  So Edward wrote his ideas in a memo and published it on one of the internal discussion groups. \n\n The memo said that, on average, short people are more anxious about operating escalators than tall people are.  It went on to say that there is lots of overlap between short and tall people when it comes to escalators, and that there are lots of short people who have no problem with escalators.  It’s just that  on average  short people tend to be slighly more anxious about escalators and so tend not to be interested in escalator operator jobs. \n\n Edward went on to recommend that ERUs policy of giving preference to short people was discriminatory and harmful to the company. \n\n Poor Edward.  It wasn’t a new idea, of course; and the research had already been done, and was conclusive.  Short people had  never  been shown to be more anxious about operating escalators.  Moreover, short people had been conclusively shown to be just as capable, and sanquine, about operating escalators as tall people.  So Edward’s idea was  wrong .  He really should have done his homework better. \n\n Now, predictably, lots of short and tall people were outraged by Edward’s memo.  They thought his ideas were offensive.  They thought his ideas were harmful.  They were firmly convinced that the reason for the height gap was that heightism is deeply embedded in our society.  Some were so offended that they raged and moaned and declared that Edward was a heightist and should be fired for positing such a toxic idea. \n\n And so, poor Edward was fired.  He was fired for expressing an idea.  A disagreeable and incorrect idea, to be sure; but an idea nonetheless. \n\n Instruction Manual for Creating a Monoculture. \n\n \n   \n     Fire people who express ideas you disagree with. \n   \n \n\n End of Instruction Manual. \n\n Now this story was fictional.  But if it were real, and if I were an escalator operator at ERU, then I would  RUN! \n\n I would run as far and as fast as I could!  I would get the hell out of there! \n\n Because a company who fires people for expressing ideas is dying. It is dying because expressing ideas has become intolerable.  So all creativity, all imagination, all risk taking, all the things that make a company, a community, and a society,  vital  is being suppressed. \n\n \n   If the people in power fire those who express bad ideas then no one other than the people in power will ever express an idea. \n \n\n If this were not fiction, and I was chairman of the Board of Directors of ERU, I would – in no uncertain terms: \n\n \n   Fire the people who fired Edward.  I would Fire them with extreme prejudice.  I would gather their belongings and put them on the sidewalk, NOW!  I would cancel their keycards.  I would not let them anywhere near the company ever, ever, again!  And then, maybe… maybe…  the company would survive. \n \n\n Because, you see, there is a simple rule about vital and free companies, communities, and societies: \n\n You never punish bad ideas. \n\n Instead, you counter bad ideas with better ideas. \n\n \n The people and companies in this posting are fictional.  Any resemblance to any real individuals or companies is coincidental.  The events described herein are solely about the fictional individuals and entities, and should not be interpreted in any other context. \n\n So there. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/08/10/OnTheInabilityToDiscussThingsRationally.html", "title": "On the Diminished Capacity to Discuss Things Rationally", "content": "\n       I can divide the negative responses to my previous blog into the following categories: \n\n \n   The Outraged Haters.  “You F-ing Clown.” \n   The Condescending Elitists.  “Oh, Bob, I’m so dissapointed.” \n   The Unprepared True Believers.  “Subduction can’t be tolerated.” \n   The Rational Disagreers.  “But in this particular context…“ \n \n\n I appreciate the last group.  They engaged in a reasonable discussion. They added to the conversation.  They made a difference.  Thank you. \n\n Unfortunately the last group was vanishingly small.  There were only half a dozen, or less, out of the hundreds of responses overall. \n\n Group 2 was also very small.  Three or four at most.  I’m encouraged by those numbers.  The fewer elitists the better. \n\n The biggest group was number 3.  They had read something about what someone else had written about what someone else had said…  They had formulated their conclusions.  They  knew  the truth!  And they stepped forward to assert their superior knowledge with statements that, whether true or false, were irrelevant to the topic. \n\n Group 1. was also quite large.  The number of Fs was impressive.  It’s a word they seem to know so well, and use so frequently that one wonders if they know many others. They offered no reason or rationale for their Fs.  They simply dumped their truckload of hate and moved on.  I suppose that’s typical; but I find it discouraging. \n\n Then, of course, there was the group of positive responses.  Of these, there were quite a few, and I appreciate them all.  Thank you. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2011/09/30/Screaming-Architecture.html", "title": "Screaming Architecture", "content": "\n       Imagine that you are looking at the blueprints of a building. This document, prepared by an architect, tells you the plans for the building. What do these plans tell you? \n\n If the plans you are looking at are for a single family residence, then you’ll likely see a front entrance, a foyer leading to a living room and perhaps a dining room. There’ll likely be a kitchen a short distance away, close to the dining room. Perhaps a dinette area next to the kitchen, and probably a family room close to that. As you looked at those plans, there’d be no question that you were looking at a  house . The architecture would  scream :  house . \n\n Or if you were looking at the architecture of a library, you’d likely see a grand entrance, an area for check-in-out clerks, reading areas, small conference rooms, and gallery after gallery capable of holding bookshelves for all the books in the library. That architecture would  scream :  Library . \n\n So what does the architecture of your application scream? When you look at the top level directory structure, and the source files in the highest level package; do they scream:  Health Care System , or  Accounting System , or  Inventory Management System ? Or do they scream:  Rails , or  Spring/Hibernate , or  ASP ? \n\n The Theme of an Architecture \n\n Go back and read Ivar Jacobson’s seminal work on software architecture: Object Oriented Software Engineering. Notice the subtitle of the book:  A use case driven approach.  In this book Ivar makes the point that software architectures are structures that support the use cases of the system. Just as the plans for a house or a library scream about the use cases of those buildings, so should the architecture of a software application scream about the use cases of the application. \n\n Architectures are not (or should not) be about frameworks. Architectures should not be  supplied  by frameworks. Frameworks are tools to be used, not architectures to be conformed to. If your architecture is based on frameworks, then it  cannot  be based on your use cases. \n\n The Purpose of an Architecture \n\n The reason that good architectures are centered around use-cases is so that architects can safely describe the structures that support those use-cases without committing to frameworks, tools, and environment. Again, consider the plans for a house. The first concern of the architect is to make sure that the house is usable, it is not to ensure that the house is made of bricks. Indeed, the architect takes pains to ensure that the homeowner can decide about bricks, stone, or cedar  later , after the plans ensure that the use cases are met. \n\n A good software architecture allows decisions about frameworks, databases, web-servers, and other environmental issues and tools, to be deferred and delayed. A good architecture makes it unnecessary to decide on Rails, or Spring, or Hibernate, or Tomcat or MySql, until  much  later in the project. A good architecture makes it easy to change your mind about those decisions too. A good architecture emphasizes the use-cases and decouples them from peripheral concerns. \n\n But what about the Web? \n\n Is the web an architecture? Does the fact that your system is delivered on the web dictate the architecture of your system?  Of course not!  The Web is a delivery mechanism, and your application architecture should treat it as such. The fact that your application is delivered over the web is a  detail  and should not dominate your system structure. Indeed, the fact that your application is delivered over the web is something you should  defer . Your system architecture should be as ignorant as possible about how it is to be delivered. You should be able to deliver it as a console app, or a web app, or a thick client app, or even a web service app, without undue complication or change to the fundamental architecture. \n\n Frameworks are tools, not ways of life. \n\n Frameworks can be very powerful and very useful. Framework authors often  believe  in their frameworks. The examples they write for how to use their frameworks are told from the point of view of a  true believer . Other authors who write about the framework also tend to be disciples of the true belief. They show you  the way  to use the framework. Often it is an all-encompassing, all-pervading, let-the-framework-do-everything position. This is not the position you want to take. \n\n Look at each framework with a jaded eye. View it skeptically. Yes, it might help, but at what cost. How should I use it, and how should I protect myself from it. How can I preserve the use-case emphasis of my architecture? How can I prevent the framework from taking over that architecture. \n\n Testable Architectures. \n\n If you system architecture is all about the use cases, and if you have kept your frameworks at arms-length. Then you should be able to unit-test all those use cases without any of the frameworks in place. You shouldn’t need the web server running in order to run your tests. You shouldn’t need the database connected in order to run your tests. Your business objects should be plain old objects that have no dependencies on frameworks or databases or other complications. Your use case objects should coordinate your business objects. And all of them together should be testable in-situ, without any of the complications of frameworks. \n\n Conclusion \n\n Your architectures should tell readers about the system, not about the frameworks you used in your system. If you are building a health-care system, then when new programmers look at the source repository, their first impression should be: “Oh, this is a heath-care system”. Those new programmers should be able to learn all the use cases of the system, and still not know how the system is delivered. They may come to you and say: “We see some things that look sorta like models, but where are the views and controllers”, and you should say: “Oh, those are details that needn’t concern you at the moment, we’ll show them to you later.” \n\n \n\n For more on this topic, see Episode VII - Architecture, Use-cases, and High Level Design, at  cleancoders.com . \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/08/14/WomenInTech.html", "title": "Women in Tech", "content": "\n       I started my career as a programmer in 1969 at a company called A.S.C. Tabulating in Lake Bluff, Illinois.  ASC had two IBM 360s that they rented out to customers.  They also provided programming services for those customers.  I was hired as a COBOL programmer at the tender age of 18.  I was horrible at it. \n\n The other programmers there were all in their thirties or forties.  And, perhaps, a third of them were women. \n\n I worked there for a couple of years and got to know the men and women pretty well.  They taught me a lot.  One of the women had to take over a program that I couldn’t seem to get right.  She saved my bacon.  But that was common, we all helped each other when we could. \n\n A few months after being hired, some of us, including two of the women, began working on a minicomputer system that offered a real-time accounting system to Local 705 Trucker’s union in Chicago.  We were replacing a GE Datanet 30 with a Varian 620/f.  We had no libraries, no frameworks, no nothing.  The code was all written on punched cards in assembler. \n\n Again, the women were just part of the team.  Nobody treated them differently.  They were just as talented and dedicated as anyone else.  At least that’s the way my 18 year old brain percieved the situation. \n\n In fact, my 18 year old brain didn’t give it a lot of thought.  The issue wasn’t present.  Nobody was talking about the problem of  Women in Tech  back then.  The situation just was what it was.  I took it to be completely normal.  I took it for granted. \n\n I changed jobs a couple of times in the following years.  Looking back on it, I can identify two trends.  First, every time I changed jobs the average age of the programmers went down.  The thirties and forties gave way to the twenty-somethings.  Second, the number of women significantly decreased from one third to one tenth, or less. \n\n These changes were not something I noticed at the time.  I only see them now by looking back.  Again, in the 70s and 80s the issue of women in tech had not entered my awareness.  It wasn’t on my mind, or any of the minds around me.  I was obvlivious to the issue. \n\n In the early 90s I started a contract programming company.  RCM Consulting Inc.  I hired four of the best programmers I knew to help out with it.  Two men, and two women.  I didn’t think anything at all about the genders.  What mattered to me was the talent.  The women and men were paid and treated identically.  We were all part of a team. \n\n The systems we wrote were  very  technical.  Some were Windows GUI apps on the order of Autocad.  Others were deeply mathematical computational geometry applications.  This was real techie, nerdy, stuff.  I loved it. \n\n The code in those systems had no gender bias.  You could not look at a source file and tell whether a man or a woman wrote it.  It was just code.  And it was good code, for the day, too.  We delivered those systems, and they rocked! \n\n I did not become aware of the women in tech issue until the day of my, now notorious,  keynote talk  at RailsConf’09.  In this keynote I made a bad joke about C++ being the testosterone of languages, and Java being the estrogen of languages.  When I invented that joke, I didn’t think it was offensive because I was making fun of the stereotypes.  You can watch it at about 9 minutes into the video.  It did not occurr to me that anyone could interpret that as a serious gender slur.  It seemed the opposite to me. \n\n Of course I was wrong about that.  Some folks took significant exception to it.  After giving it some thought I realized that my attempt at gender stereotype humor was inartful, and I apologized.  I also began to realize that there was an  issue  that I had stumbled upon. \n\n You may wonder how I could have been ignorant of that issue.  It’s not like the women in tech issue began in 2009.  My excuse, for what it’s worth, is that, up to that point, I did not focus on the social issues of programming much.  I was then, and am now, much more interested in the technology itself.  So the issue took me by surprise. \n\n Now I am not always the sharpest tack in the drawer – especially about people issues.  So the ‘09 Rails talk was not the last time I managed to offend a group of people regarding gender issues.  I won’t go into all the details.  Suffice it to say that I managed to “step in it” a few more times and have been corrected, more than once.  Those corrections were sometimes made politely, and sometimes not.  In every case, however, I thought about the complaints and issued appropriate apologies if I thought they had a reasonable point. \n\n This brings us to James Damore’s now infamous memo at Google for which he was fired.  After a careful reading of that memo, I came to the conclusion that the firing was a huge mistake, and that the CEO of Google should resign or be fired.  A company that depends upon innovation and creative thinking cannot survive if it stifles creative thought by firing people who disagree. \n\n I’m not here to argue that point.  I made the point in a previous blog, and in subsequent tweets and facebook posts. \n\n I enjoy a good debate.  I like to argue.  It’s one of the most important ways that I learn.  So I engaged those who disagreed with me with enthusiatic energy.  Some folks engaged back. Some folks swore and made nasty accusations.  Some blocked or unfollowed me.  Some regretted buying my books.  etc. etc.  This is all pretty normal stuff on the internet nowadays. \n\n But today I stumbled across a long twitter thread that I can only describe as intentional character assassination.  The author of this thread is misrepresenting facts and making some pretty nasty accusations.  Again this is not all that unusual, except for the fact that I was not invited to defend myself. \n\n Usually people call me out on twitter by using my twitter handle.  This allows me to see and respond to their complaints and accusations.  But the author of this particular thread, and all the participants therein, were assiduously avoiding this practice. \n\n Now, of course, nothing on Twitter is private.  Anyone who says something mean about someone else must know that their words will eventually get back to the person they are talking about.  So I can’t imagine that the author really wanted long term privacy.  I think what the author really wanted was  momentum .  It is difficult to defend yourself against a frothy mob. \n\n The gist of this author’s thread is that I am a misogynist; and that I should not be taken seriously in any regard.  I understand that efforts have been made to have me excluded from conferences, and to boycott the publisher of my books, etc. \n\n I am not a misogynist.  I do not hate women.  I do not think women are less capable than men.  I do not think women are less able to program than men.  I am a 64 year old white male who grew up with  Bewitched ,  Father Knows Best , and  Petticoat Junction  and this has certainly colored my sense of humor and outlook.  I am working on that. But, more importantly, I am in no way opposed to women becoming programmers and leaders. \n\n I don’t engage in character assasination.  I don’t try to undercut people simply becuase I disagree with them.  I don’t drive twitter slur campaigns.  I’d have a tough time looking myself in the mirror if I did. \n\n Is character assasination really the strategy that is most likely to help women in tech?  I don’t think so. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2011/01/17/software-craftsmanship-is-about.html", "title": "What Software Craftsmanship is about", "content": "\n       TL;DR. \n \n\n I’ve gone from  Dan North’s post , to  Gil Zilberfeld’s  to  Michael Feather’s  to  Jason Gorman’s .  It would appear that we, in the software craftsmanship movement have not been clear. I hope this blog clears a few things up. \n\n Why is there a software craftsmanship movement?  What motivated it?  What drives it now?  One thing; and one thing only. \n\n \n   We are tired of writing crap. \n \n\n That’s it.  The fat lady sang.  Good nite Gracy. Over and out. \n\n We’re tired of writing crap. We are tired of embarrassing ourselves and our employers by delivering lousy software.  We have had enough of telling our customers to reboot at midnight.  We don’t want bug lists that are a thousand pages long.  We don’t want code that grows more tangled and corrupt with every passing day.  We’re tired of doing a bad job.  We want to start doing a good job. \n\n That’s … what … this … is … about.  Nothing else. \n\n What we are not doing: \n\n \n   We are not putting code at the center of everything. \n   We are not turning inward and ignoring the business and the customer. \n   We are not inspecting our navels. \n   We are not offering cheap certifications. \n   We are not forgetting that our job is to delight our customers. \n \n\n What we will not do anymore: \n\n \n   We will not make messes in order to meet a schedule. \n   We will not accept the stupid old lie about cleaning things up later. \n   We will not believe the claim that quick means dirty. \n   We will not accept the option to do it wrong. \n   We will not allow anyone to force us to behave unprofessionally. \n \n\n What we will do from now on: \n\n \n   We will meet our schedules by knowing that the only way to go fast is to go well. \n   We will delight our customers by writing the best code we can. \n   We will honor our employers by creating the best designs we can. \n   We will honor our team by testing everything that can be tested. \n   We will be humble enough to write those tests first. \n   We will practice so that we become better at our craft. \n \n\n We will remember what our grandmothers and grandfathers told us: \n\n \n   Anything worth doing is worth doing well. \n   Slow and steady wins the race. \n   Measure twice cut once. \n   Practice, Practice, Practice. \n \n\n I suppose that some people might look askance at our code katas and our code retreats, and our practice sessions.  They might think that we’re turning inwards and abandoning our customers.  They might think that we’ve given up on the real world and have yielded to the temptation to entertain ourselves.  I can see how someone might come to that conclusion. \n\n But they are as wrong as the day is long.  We are doing this because we care about the customer.  We are dedicating time and effort to being the best that we can be so that our employers will get the best possible value out of us. \n\n Do you think the only time musicians play their instruments is when they are on stage?  Do you think the only time that batters hit balls is during games?  Do you think the only time lawyers give a closing is at trial?  Of course not.  These people are professionals; and professionals practice!  Professionals study the minutia of their disciplines.  Professionals know all the little tricks and quirks.  They know the history, the theories, the anecdotes.  They know techniques and methods.  They know good options and bad options and how to tell them apart.  And they know all this stuff because they practice, practice practice. \n\n So when you see someone wearing a green wrist-band that says “Clean Code” or “Test First” or “Test Obsessed”, it’s not because they’ve joined a movement, or signed a manifesto, or that they somehow feel superior to everyone else.  They aren’t participants in a holy war.  They aren’t trying to join a tribe and huddle around a campfire.  The green band is a personal thing.  It’s a promise made to one’s self:  “I will do a good job.  I will not rush.  I will write tests.  I will go fast by going well.  I will not write crap. And I will practice, practice practice so that I can be a professional.” \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2011/01/19/individuals-and-interactions.html", "title": "Bringing Balance to the Force", "content": "\n       I read Martin Fowler’s  contribution  to the craftsmanship thread with interest.  He spoke of the so-called “crevasse” between developers and customers.  He reiterated Dan North’s fear that the craftsmanship movement could widen this crevasse. \n\n We are programmers.  We need to talk about programming from time to time. We need time and space to focus on our primary discipline: programming.  That discussion, and that focus, is a very positive thing.  It means we care about what we do.  But have we gone too far?  Are we too technical?  Is the Software Craftsmanship movement a movement only about technical practice and details?  Have we forgotten the customer? \n\n The Software Craftsmanship Manifesto’s  fourth statement: “We value not only customer collaboration but also productive partnerships” should be enough to quell those fears. Software Craftsmanship is a movement dedicated to partnering with customers.  That means taking on the customer’s problems as our own.  That means putting ourselves in the position of our customers.  Their pain becomes our pain, their problems our problems, their victories, our victories.  That’s craftsmanship!  That’s what we want.  We want to be able to do our job with professionalism and skill, and to partner with our customers to achieve the best possible outcomes. \n\n Software Craftsmanship is not, as Martin said: “A place where programming can be front and central again.”  It is not a movement that “underplays the vital role of customer communication”.  After all, those of us in the Software Craftsmanship movement have not abandoned Agile.  We still read the Agile papers.  We still follow the Agile threads.  We still go to the Agile conferences.  We are still part of the Agile community.  So we are steeped in “the vital role of customer communication.”  So much so that we amplified that role to one of partnership. \n\n No, the Software Craftsmanship movement is not overplaying the technical role; rather it is trying to recapture the balance that the Agile movement has lost. \n\n Martin made an amazing point about this in his article.  He said that the craftsmanship movement was spawned as a reaction to the rise of non-programming topics within agile.  I completely agree.  Indeed, I made exactly that point just a week ago while attending an Agile Coach Camp in Norway.  I, for one, consider the agile movement to have been inundated by a vocal and enthusiastic cohort of project managers, waving their scrum-master certificates, or their Lean and Kanban books.  They have overwhelmed the original movement and changed it into something new.   Agile is no longer about a balance between the technical and non-technical parts of development.  Rather it has become a discussion almost entirely dedicated to non-technical issues.  Agile is no longer about healing the divide, or closing the crevasse.  The agile movement now represents one side of the crevasse. \n\n The argument has been made that the technical issues are the simple part.  That the real hard parts of software development are the people issues. So therefore we should focus on the people issues, on the customers and employers, and keep quiet about the technical stuff.  If we talk too loudly about the technical stuff, then the customers may feel that we’re not paying due attention to them. \n\n Bollocks!  Bollocks I say!  Yes, the people part is hard.  The people part is complicated.  The people part needs lots of work.  We should be talking a lot about the people part.  But anybody who thinks the technical stuff isn’t just as hard, and just as worthy of conversation, is misguided.  We need both.  And we need both sides to listen to each other and to trust each other.  We need balance! \n\n The imbalance is the crevasse!  One side thinks their issues are more important that the other’s.  One side thinks their issues should dominate.  And when the other side tries to talk about their issues, they are told to shush because they might alienate the other side and “widen the crevasse”. \n\n But neither side is more important than the other.  Neither side should dominate.  Neither side’s issues should be toned down.  Neither side should be told to shush for fear of what the other side might say.  The only way to bring the crevasse together is to realize that both sides need each other, and need each other to be skilled and professional.  Each side should be glad that the other is talking about their own issues.  And each side should be willing to listen to the other side’s issues. Each side must respect the other side’s desire to increase their skill and professionalism.  If we do that enough, maybe we’ll realize that there’s actually only one side. \n\n So the next time you see some programmers talking about code retreats or koans or katas or TDD or some other deeply technical topic, congratulate them for giving due diligence to their practice.  The next time you see an agile coach talking about Kanban, or Lean, or Iteration length, or story points, congratulate them for their dedication to their discipline.  Remember, these are your team-mates.  You want them to be able to play their positions with skill and professionalism.  You want them to be good at their jobs.  And, if you want them to respect your role,  you must first respect theirs. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2011/11/22/Clean-Architecture.html", "title": "Clean Architecture", "content": "\n       In the weeks since I started talking about the need to clean up our architecture, I’ve noticed a surprising resistance to the idea. Apparently the notion that it’s a good idea to hide the framework, UI, or database from the application code is not universally accepted. \n\n I first blogged about this topic  here  , I did a whole cleancoders.com  episode  on the topic. I’ve also done several keynotes on the topic, the slides for which are  here , and a video recording of which is  here . \n\n One somewhat dissenting view, written by  The Frustrated Architect  in his  coding {the} architecture  blog is  here . He shows a picture, which I’ll repeat:\n \nThe point he’s trying to make is that if the UI and Database are a much larger part of the system than the business rules, then the architecture of the system should be more oriented around those larger elements. Of course I disagree. No matter how large any one part of the system is, the other parts should be decoupled from it. \n\n Other dissenting (or perhaps a better word is “skeptical”) views have been less formal. One person simply asked me: “Have you ever actually done this –  in a Rails project ”, as if Rails was somehow special and changed the game so much that the normal rules of good design don’t apply. \n\n Other folks have worried that the net result of my advice would be lots of duplicated code, and lots of rote copying of data from one data structure to another across the layers of the system. Certainly I don’t want this either; and nothing I have suggested would inevitably lead to repetition of data structures and an inordinate of field copying. I’ll explain why below. \n\n One particularly colorful complaint was: “ This sounds like a dogmatic rant, I want to see code. ” While I sympathize with that view, the concepts here are just not that difficult to grasp; and, in fact, lots of code would obscure them more than help. \n\n Not Rocket Science. \n\n This isn’t rocket science. The basic idea is very simple. You separate the UI from the business rules by passing simple data structures between the two. You don’t let your controllers know anything about the business rules. Instead, the controllers unpack the HttpRequest object into a simple vanilla data structure, and then pass that data structure to an interactor object that implements the use case by invoking business objects. The interactor then gathers the response data into another vanilla data structure and passes it back to the UI. The views do not know about the business objects. They just look in that data structure and present the response. There are, of course, more details than that; and they are well described in the references above. But at the bottom, that’s all there is to it. \n\n The benefit should be obvious. The application code is completely decoupled from the UI. You can test the application code without the UI present. You don’t need to fire up the web server, or the container, or Rails, or any of the other frameworks in order to run your tests. What’s more, if you don’t like your current UI, you can change it by replacing it with another. \n\n Is this a perfect scheme? Of course not. Might one kind of UI be so different from another that they couldn’t share a common interface? Of course that’s possible. Does that mean this kind of decoupling is a waste? Are you kidding me? \n\n It’ll slow me down. \n\n No it won’t. For goodness sake, it will speed you up. You’ll be able to run your tests without delay. You’ll be able to defer decisions about the UI. You’ll be able to test business rules without the UI present. That kind of flexibility and decoupling  always  speeds you up. If there’s one thing we’ve learned about coupling over the last fifty years it that nothing is better as slowing you down. \n\n We shouldn’t defer decisions. \n\n One of the more strident comments I’ve made about architecture is that a good architecture allows you to defer critical decisions like the UI, frameworks, database, etc. One point made by several people is that customers don’t want the UI deferred. DBAs don’t want the database deferred. From iteration to iteration they want to see the whole system working, including the UI, the Database, and the frameworks. They don’t want an iteration to be spent solely on business rules. Indeed, good agile practices specifically demand long skinny slices through the entire architecture. \n\n Of course I agree with all of that. However long skinny slices don’t have to be coupled. A good architecture  allows  you to defer critical decisions, it doesn’t  force  you to defer them. However, if you  can  defer them, it means you have lots of flexibility. For example, you could create an interim simple UI for the first few sprints, and then replace it with a more capable UI later. \n\n What about convention over configuration? \n\n The Rails mantra of depending on conventions rather than configuring everything is a powerful idea; and one that I fully agree with. But that doesn’t mean you should couple your system. Conventions do not necessarily cause coupling! There is no reason, for example, why the model objects in Rails should hold business rule methods. Keeping the business rules separate and decoupled from the ActiveRecord derivatives does not violate any  Rails  conventions. Even if it did, I think decoupling trumps convention. \n\n What about GOOS? \n\n One of the better books about software design is “Growing Object Oriented Software” by Steve Freeman and Nat Pryce. They recommend an outside-in approach to developing systems. You start at the interface and work your way in to the business rules. \n\n At first this sounds like it contradicts my advice. After all, I focus on the use-cases and consider the UI to be a annoying little detail. However, there’s nothing wrong with working on the annoying little details first, so long as you  decouple  your business rules from them. There’s nothing in the GOOS ideology that opposed decoupling the business rules from the UI. \n\n Now, truth be told, I don’t use the GOOS methodology. I prefer an inside-out approach. I like to focus on the business rules first, and then put a UI around it later. But that does’t mean that the GOOS technique is bad (it’s not) or that if you follow GOOS you won’t have a decoupled architecture (you’d better!). \n\n Oh no, it’s Big Up Front Design! \n\n No it’s not. I’m not telling you to spend months and months drawing UML diagrams. I’m telling you to  decouple . You can do that decoupling while you are writing your code and making your tests pass. You don’t need a big up front plan in order to create a nicely decoupled architecture. All you have to do is  think  about the problem while you are coding it. \n\n However, I should point out that there is nothing whatever wrong with spending a few hours, or even a day or two, pondering the shape of your system. There’s nothing wrong with drawing some UML, or other diagrams in order to get some ideas about how your system should be structured. I don’t want you doing this for months, but there’s nothing wrong with  thinking . (see:  Hammock Driven Development ) \n\n This ain’t new. \n\n I’ve been surprised by the reactions to these ideas. I understand that people naturally resist change; and that lots of programmers aren’t used to the ideas of decoupling ( read that clause several times and weep ). But this is not some new idea that occurred to me out of the blue. These ideas are  old . They come from people like David Parnas, Tom Demarco, Grady Booch, Ivar Jacobson, and many, many others. The fact that they are old doesn’t necessarily mean that they are good; but in this case –  they are . \n\n What happened to us? How did we forget these rules? When did the old rules of coupling and cohesion evaporate from our awareness? Are we really so naive as to think that the best way to write a complex system is to throw a bunch of components into a bag and shake it until it works? \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2011/11/06/Double-Entry-Bookkeeping-Dilemma-Should-I-Invest-or-Not.html", "title": "Double Entry Bookkeeping Dilemma. Should I Invest or Not?", "content": "\n       A few days ago I found  this  blog, which makes the case that\nsoftware developers who use TDD, should be pragmatic about the costs and benefits. You should read the blog now. Notice how reasonable and\nbalanced it sounds. Now read what follows and ask yourself if  that too  seems reasonable and balanced. \n\n \n   Every single transaction must be doubly entered! \n \n\n This sound advice rather seems quite extreme to me. IMHO a skilled accountant pragmatically decides when to invest in double entry bookkeeping. \n\n After practicing double entry bookkeeping for over a decade, I’m a strong believer and proponent of double entry bookkeeping. \n\n However over the years I’ve realized that double entry bookkeeping does have four, very important, costs associated with it: \n\n \n   Cost of entering transactions twice in the first place \n   Cost of summing both credit and debit regularly to get feedback \n   Cost of maintaining and updating both entries as and when required \n   Cost of understanding other’s account structures. \n \n\n One also starts to recognize some other subtle costs associated with double entry bookkeeping: \n\n \n   Illusion of safety: While double entry bookkeeping gives you a great safety net, at times, it can also create an illusion of safety leading to accountants too heavily relying on just the dual set of books (possibly doing more harm than good.) \n \n\n \n\n \n   Opportunity cost: If I did not invest in doubly entering this particular transaction, what else could I have done in that time? Flip side of this argument is the opportunity cost of repetitive checking and rechecking or even worse not checking at all. \n \n\n \n\n \n   Getting in the way: While double entry helps you drive your account structure, at times, it gets in the way of restructuring the accounts. Many times, I’ve refrained from restructuring the accounts because I get intimidated by the sheer effort of restructure/re-enter a large number of my transactions as well. (I’ve learned many patterns to reduce this pain over the years, but the pain still exists.) \n \n\n \n\n \n   Obscures a simpler structure: Many times, I find myself so engrossed in my accounts and the structure they lead to, that I become ignorant to a better, more simpler structure. Also sometimes half-way through, even if I realize that there might be an alternative structure, because I’ve already invested in a structure (plus all the complementary entries), its harder to throw away the structure. In retrospect this always seems like a bad choice. \n \n\n \n\n \n   If we consider all these factors, would you agree with me that:\nDouble entry bookkeeping is extremely important, but each accountant has to make a conscious, pragmatic decision when to invest in entering something twice. \n \n\n Its easy to say:  always practice double entry bookkeeping , but it takes years of first-hand experience to judge where to draw the line. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2011/10/20/Simple-Hickey.html", "title": "Simple Hickey", "content": "\n       Rich Hickey gave a great  talk  at  Strange Loop  entitled  Simple Made Easy . I strongly recommend you spend an hour and listen to this talk. It’s worth every second you give it. There will be some things in this talk that you will disagree with. When that happens, stop and think - think real hard - you probably don’t actually disagree. And if you do, you probably shouldn’t. \n\n For instance, Rich says some seemingly disparaging things about TDD and Agile and Refactoring - the sacred cows of the Agile Community. If you are wedded to this community, you might react negatively. Don’t. Rich is not disparaging the practices. He  is  disparaging the religion - the mindlessness - the  thoughtlessness . \n\n Rich compares unit tests to guard rails. Then he makes a very good point. He says, when you have a bug, that bug got past your tests. And now what? Now you have to find the bug. And if the system isn’t simple, that’s not going to be easy. (Note I used the words simple and easy here. The start of Rich’s talk is about the very different definitions that these words have. I suggest you stop at this point and listen to the first ten minutes of his talk and then come back to this paragraph again.) \n\n Rich makes the point that sprinters run fast, but not long. Then he says that Agile “solved” this problem by just firing the starting gun over and over again in quick succession. He grins, and the audience laughs. Then he goes on to say that continuous sprinting does not necessarily makes systems simple, and simplicity is the real key to speed. \n\n He’s right of course. This is the same point that Martin Fowler made in his  Flaccid Scrum  article. And it’s the point that many of us in the Agile community have been making. That short iterations, without good technical practices, does not lead to fast development. Rather, it leads to a mess. \n\n Rich makes fun of the idea that a suite of tests let’s you change the code. He says that tests are a safety net, nothing more. We TDDers know that a suite of tests is  essential  if we want to fearlessly change the code. But Rich is right about the safety net. A safety net can help you keep a system simple, if it’s already simple. But a safety net below a big ball of mud is going to be of marginal assistance in detangling the mess. Oh, don’t get me wrong. I want those tests! But the job ain’t gonna be easy. (again, that word). \n\n Here’s another talk from Rich:  Hammock Driven Development , in which he encourages us to  think  instead of just writing gobs and gobs of code. \n\n So here’s the deal. Rich is concerned, and rightly so, that we have a culture of complexity. That when programmers are given a task, they race ahead and write masses of tangled code, using “easy” frameworks and tools, without giving the problem  due thought . That we confuse easiness, with simplicity. (e.g. Rails is easy,  it is not simple .) His complaint about tests is that we used them to replace thought. That we feel good about ourselves because we’ve written tests, and yet we haven’t actually given the time to the problem that the problem deserves. We haven’t made the problem simple. We’ve just done what was easy. \n\n Now, truth be told, the Agile community, and the entire software community  is  infected with this disease. All too often we do what’s easy, at the expense of what’s simple. And so we make a mess. But that is not now, nor was it  ever , a value of agile development. And it was  certainly not  a value of software craftsmanship! Indeed, doing what is simple as opposed to what is easy is one of the  defining  characteristics of a software craftsman. \n\n In the end, I think Rich’s perception of TDD is skewed by what he sees out in the industry. Frankly, I think he’s missing a bet. I imagine he’d find that the practice was as helpful to him as it has been to me. Not as a way to avoid thinking and rushing towards a mess; but rather as a disciplined way of being thorough, careful, and thoughtful. \n\n Now, ask yourself what TDD means to you. Is TDD a discipline you use to make things easy? Or is it a discipline you use in order to be thoughtful, careful, and to keep things simple? \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2011/12/11/The-Barbarians-are-at-the-Gates.html", "title": "The Barbarians are at the Gates", "content": "\n       There’s a revolution coming. I wonder if anyone has noticed. It’s not subtle. It won’t be sublime, or easy to integrate. Indeed, it’ll probably muck up everything for a long long time. But it’s coming nonetheless, and you’d better be strapped in and ready for the impact. \n\n The challenge for computing is  power . How much processing can you get done per unit of time. Over the last fifty years we’ve seen this power increase by a scale that we usually reserve for thermonuclear weapons, supermassive black holes, or core-collapse supernovae. We might characterize that power increase with the following formula: \n\n \n   Power-increase = speed-increase * memory-increase * disk-increase * watts-decrease * volume-decrease * cost-decrease. \n \n\n This formula is woefully incomplete. We could add memory speed, and disk speed, and network interconnectivity, and a whole range of other power multipliers. But never mind that; the formula is already scary enough. When I use this formula to compare my current laptop to the PDP-8 that I used 40 years ago, I get  twenty-two orders of magnitude . My little laptop is at least  ten septillion  (10,000,000,000,000,000,000,000) times more powerful than that PDP-8. \n\n What do we do with those 22 zeros? A lot! I can watch HD video on my laptop streaming down from the web, and it barely utilizes one of it’s four cores. I can edit high resolution video at 32,000 feet over the Atlantic, while sipping a scotch and checking twitter. I can play Dungeon’s and Dragons, with dazzling video effects on three high resolution screens, in real time with dozens of people scattered around the world. So, yeah, 22 orders of magnitude seems about right. \n\n Where has all that power come from? It’s come from the hardware! The hardware designers have steadily improved our machines. For the last fifty years they’ve made them faster, smaller, cheaper, and better. They’ve created miracles in technology. They’ve doubled, or more than doubled the capabilities of our computers every 18 months or so[1]. \n\n In the last fifty years the hardware of computers has undergone revolution after revolution after revolution. From Vaccuum Tubes, to Transistors, to Integrated Circuits, to Solid State Ram, to Winchester disks, to Liquid Crystal and LED displays, to Ethernet, to Wifi and bluetooth. I could go on, and on. \n\n And we software people…what have we done with all that power? Has it driven us through similar revolutions?  NO!  Quite the opposite in fact. All that dramatic increase in hardware power has allowed us software people to  avoid  any similar major revolution. We software developers have had a virtually infinite reserve of computer power laid out in front of us by the hardware developers. Every time we needed more capability, they were there with the answer. And so  our  technology – the technology of software development, has changed very little. \n\n You might disagree with that at first. You might object and say that software development has changed a lot over the last fifty years. And it’s true, of course, that software development nowadays is quite a bit different from the 1960s. Or is it? \n\n Consider this. Take a computer hardware designer from 1960 and transport him to 2011. Could he design modern processor chips without years of re-education? Clearly not. The technological gulf between those two eras is so vast, that the poor hardware engineer would need to start his career over from very close to the beginning. \n\n Now do that thought experiment with a software developer from 1960. Transport him to 2011. It would take him a day or so to get over the shock. Perhaps he’d need another day or two to learn vim, or Eclipse. He might need a few weeks to learn Java or C#. But then he’d start in writing if statements, assignment statements, and while loops just like the rest of us. He’d have to learn HTML, and XML, and Maven, and a whole suite of other tools; but he could do that on the job like the rest of us. He’d be at a disadvantage, there’s no doubt about that; but he wouldn’t have to start his career over; because he knows how to code; and coding just hasn’t changed that much in the last fifty years. We all still use the same old tools: sequence, selection, and iteration. \n\n The Sapir-Whorf trap \n\n Our computer languages are all geared towards the same thing. They view the computer as a single, hyperfast, hyperaccurate, clerk. Computer languages are simply the languages we use to direct the operation of that clerk. What’s more, the most popular computer languages map cleanly to the hardware. They create an abstract image of that hardware; but are strongly related to it. \n\n Did you know, for example, that the ++ operator in C, C++, Java, and C# derived from the fact that there were special registers in early computers that automatically incremented their contents every time you accessed them? The compiler tried to make use of these auto-increment registers whenever you used a ++ operator. \n\n Consider the data types in C++: int, double, char. These are, or are very closely related to, the data types of the hardware. The data types in Java and C# are only slightly more abstract and still have a strong correlation to the underlying hardware. \n\n The assignment statements, if statements, and loop operators in our languages are all easily mappable to machine instructions in the hardware. \n\n In short, our languages are projections of the hardware. Some languages are more abstract projections than others; but the most popular languages, C, Pascal, C++, Delphi, Java, and C# are very closely related to the hardware. Even langauges like Smalltalk, Python, and Ruby, while more abstract than the others, still show their hardware roots. \n\n The reason for this is clear. The closer you are to the hardware, the faster your programs execute. Why is so much embedded work done in C and C++? Speed. Why are DSPs programmed in assembler, or C? Speed. Why is Java closely related to the hardware, even though it executes in a virtual machine? Speed. \n\n And so our languages, like it or not, have been constrained to stay close to the hardware by (ahem) the need for speed[2]. But this leads us into a trap. The Sapir-Whorf hypothesis tells us that our view of the world is strongly affected by the languages we use. When we think in a given language, that language acts as a filter. Concepts it can’t express are removed from our awareness. Our mode of expression constrains us to only those thoughts and concepts that can easily be expressed within it. \n\n And that should make you wonder. Since we’ve been constrained by these close-to-hardware languages for the last fifty years, what have we been missing? What concepts have eluded us? How warped is our world view? \n\n We’re about to find out. \n\n There’s a Freight Train Coming. \n\n A few years ago hardware engineers hit a brick wall known as the speed of light. Signals can propagate at 3E8 meters per second at most. That’s about 1ns per foot. For the last fifty years clock rates had been doubling every 18 months; but in the early 2000s that stopped cold, and it’ll never start up again. From now on, computer chips will  not  be getting much faster. Oh we might see some new materials that give us a speedup. – Maybe. But the exponential curve of 2X every 18 months is over. It’ll never be back.[3] \n\n But density is another matter entirely. Chips are still getting exponentially denser, even if they aren’t getting much faster. What’s more, there’s no reason that density has to be limited to a chip. If we have to, we can cram lots of chips on boards, and lots of boards in boxes, and lots of boxes in rooms. And we have lots and lots of rooms. So the amount of computer power a room can contain is vast. We haven’t even begun to scratch that surface. \n\n But this leads to the dilemma. This is why the barbarians are at the gates. This is why those languages that have served us so well are about to become anchors around our necks. \n\n In the coming decades, if you want your program to run fast, you are going to have to write it so that it shares thousands of processors. Not two. Not four. Thousands. –  Thousands. \n\n Imagine an XBox with 1024 processors, each running at a ghz. Imagine your laptop with 2048 cores. Or a server farm in a room, and each server has 4096 cores. If the next 50 years is to see another 22 orders of magnitude increase in power,  that’s  where that power is going to have to come from. Not just one hyperfast, hyperaccurate clerk; but thousands upon thousands; all sharing the load; all with programs written to share the load. \n\n How are we going to write that code? What language are we going to use to express the concepts that execute in thousands of machines. \n\n Of course the current answer to that is “Functional Programming”. OK, maybe –  maybe . But I gotta tell ya, the new functional languages out there aren’t looking too good to me. Scala and F# are still closely tied to the hardware. Scala  is  java with a few cute twists. And F#? Is it really the language that’s going to take us into the next age? My fear is that these languages suffer from the Sapir/Whorf trap. Their mode of expression does not sufficiently change our world view. They are too much like Java and C# and all the other old languages. \n\n Clojure \n\n The only language I’ve seen so far that comes close to a different[4] mode of expression, and that is capable for use in the enterprise, is Clojure; which, after all, is just Lisp. But writing code in Lisp  is  different. It’s a different mode of expression, and it makes you see concepts that are difficult, if not impossible, to conceive of in Java, or C. The Clojure data types[5], the control structures, the state management, the homoiconicity, and just the overall mathematical flavor; make this language different from what most Java/C#/Ruby programmers are used to. \n\n Is Clojure the language that will take us into the massively-multicore regime? I don’t know. In fact, I rather doubt it. What I am reasonably sure of, however, is that it will be an important step on the way. It is different enough that, by using it, we might break out of the Sapir-Whorf trap and get a glimmer of the real solution. \n\n [1] Moore’s Law. \n\n [2] I feel the need. \n\n [3] You can talk to me about quantum computers when we manage to get more than a handful of qbits stable for more than a few seconds. \n\n [4] Notice I didn’t say “new”. \n\n [5] Recently some compromises have been made in Clojure, tying it a bit closer to the hardware for the sake of speed. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/01/11/Flipping-the-Bit.html", "title": "Flipping the Bit", "content": "\n       There’s a bit you need to flip in your head. It’s just one bit. In this blog we’ll call it “THE BIT”. Some of you have already flipped THE BIT. The rest of you need to flip THE BIT as soon as possible. \n\n Once you flip THE BIT your life will be easier, your code will be cleaner, you’ll get done with software faster, your software will work better, and fuzzy bunnies will happily hop over hillsides overarched by double rainbows all the way! \n\n Before I tell you what THE BIT is, let me show you an example of an article produced without the benefit of THE BIT being flipped. The article is  here , and was written by someone named Tom Fischer. As I write this blog, Fischer’s article has been read eleven thousand times. That’s eleven thousand people who’ve been influenced by the wrong polarity of THE BIT. \n\n Fischer’s article makes the point that unit testing isn’t always necessary. He uses graphs and charts with clever little colorings to support his arguments. Those arguments are two-fold. \n\n \n   Unit testing takes time and can delay projects causing them to lose money due to lost opportunity[1]. \n   Unit tests don’t find all bugs because many bugs are  integration  bugs, not bugs in the unit-tested components. \n \n\n Fischer goes on to say that unit tests are great and can solve lots of problems, etc, etc, blah, blah. But then he erases all those positive and friendly words by concluding with this: \n\n \n   We can no longer rely on a general acceptance of the myth that unit testing is a universal panacea, but need to focus unit testing on aspects of development where it is most effective, and be prepared to actively justify its use. \n \n\n Oh my goodness! We have to be prepared to  actively justify unit testing?  Be warned! The  Unit Test Compliance Squads  will be making the rounds soon. Anyone who cannot actively justify their unit tests will be duly punished! < samuri scowl > \n\n Who’s going to write unit tests if we have to  actively justify  their use? Who’d be fool enough to take that risk? So we can summarize Mr. Fischer’s article with the following sentence: “Don’t you dare write unit tests unless you KNOW, and can  actively justify , that you have enough time.” \n\n So the question is:  Who’s got enough time?  Who, out there, has schedules that are so forgiving that you can  actively justify  the risk of writing unit tests? Who out there is working in an environment where opportunity costs and time-to-market aren’t an issue? Who out there has a cushy-dreamy-stress-free-unicorn-riding project in which you can safely absorb the terrible cost of unit tests? Answer: Unicorns and fuzzy bunnies. But no humans that I am aware of. \n\n Therefore, the only real conclusion of Mr. Fischer’s article is that nobody should ever write unit tests no matter how beneficial they might sometimes be, because the risk and cost is so great that you must  actively justify  their use. (Note, that there seems to be no need to actively justify their lack…) \n\n Mr Fischer has not flipped THE BIT in his brain. He still thinks that unit tests take time. Apparently he thinks they take a  lot  of time. He still thinks that there are situations in which unit tests aren’t helpful. He hasn’t flipped THE BIT. What planet is he living on? \n\n THE BIT \n\n What is THE BIT? THE BIT is the boolean variable within your subconscious that represents your belief that unit tests take time. I want you to flip that bit so that your belief changes. I want you to believe that  Test Driven Development saves time in every case and every situation without exception amen . \n\n Notice that I did not say: “Unit tests save time”. I said: “TDD saves time”. I think that’s an important distinction. I don’t want you believing that unit tests written after the fact are anywhere near as beneficial as the  discipline  of TDD. \n\n Here’s the test for THE BIT. You can have a friend ask you these questions. If you answer them as shown below, you’ll know that THE BIT has been flipped: \n\n \n   Given a task, will you finish faster using TDD?:  YES. \n   Are there any tasks that you can finish faster without using TDD?:  NO. \n   I understand that TDD might help in the long term, but what if the job is really short term? Would you still use TDD?:  Yes, because TDD is faster even in the shortest term. \n   What if the schedule is really tight and the boss is breathing down your neck, would you still use TDD?:  YES. \n   In every case?:  YES. \n   Is there any case where you would not use TDD:  NO. \n   What if you were on the Starship Enterprise and the warp coil was seconds away from an anti-matter explosion and all you needed to do was invert one IF statement to save the ship. Would you use TDD for  that ?:  Yes. \n   Why?:  Because I’d get done faster. \n   Even for just one IF statement?:  Yes, even for just one IF statement. \n   Do you mean to tell me that there is no case, no case at all, in which you would not use TDD?:  Well, I might not use TDD if I needed the task to take a really long time to finish, and have lots of bugs. But other than that? No, no case at all. \n   What about GUIs?:  Ah, GUIs![2] \n \n\n That’s THE BIT. When you have flipped THE BIT in your head, then when your boss tells you to stop writing tests because you don’t have time, you’ll refuse and tell him that without TDD it will take you longer to get done – and you’ll believe it! The more the pressure is on, the more you’ll rely on the TDD discipline. \n\n How do you flip THE BIT? I can tell you how I flipped  my  BIT. I used TDD for a month or so; and my BIT flipped. That’s all it took for me – just doing it. Maybe that’ll be enough for you. \n\n [1] It seems to me that you can lose a lot of money by shipping code that doesn’t work too, but – oh well. \n\n [2] We’ll talk about GUIs in more detail later. The short answer is:  Yes . The medium answer is:  Yes, for all the dynamics. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/01/20/Fecophiles.html", "title": "Fecophiles", "content": "\n       I got an interesting email yesterday. It contained the following paragraph describing an email he had sent to his co-workers about a refactoring he had done: \n\n \n   When I originally sent this email to my co-workers, I got the following reaction; 1 team member thought it was better & 2 team members thought it was harder to understand. Of the 2 who thought it was worse; 1 thinks I should change it back, and the other is willing to put up with my change to shut me up :-) \n \n\n Here’s the letter he sent to them, showing them the before and after of the code he had refactored. \n\n Subject: FW: Refactoring of the day\n\n\nAm I missing something?  Or did I just refactor a 31 loc method to 2 loc?\n\n------- before ------\npublic static string GetLtsCode(IventoryBinItem item)\n{\n    string ltsCode = null;\n    if (!string.IsNullOrEmpty(item.ParentItemNo)) //child item\n    {\n\n        ltsCode = \"PzT\";\n        var isNfoOrDisc = item.IsNfoItem || item.IsDiscontinuedItem;\n        //if (isNfoOrDisc && item.ParentIsNfoOrDiscontinuedItem ||\n        //    !isNfoOrDisc && !item.ParentIsNfoOrDiscontinuedItem)\n        if (!item.ParentIsNfoOrDiscontinuedItem)\n        {\n            ltsCode = \"PzT\";\n        }\n        //else if (item.ParentIsNfoOrDiscontinuedItem && !isNfoOrDisc)\n        else if (item.ParentIsNfoOrDiscontinuedItem) //always capture demand for children of nfo (april19 change)\n        //and childitem is not, then mark it as regular, otehrwise both PzT\n        {\n            ltsCode = null;\n        }\n    }\n    else //parent\n    {\n        ltsCode = null;\n        if (item.IsNfoItem || item.IsDiscontinuedItem)\n        {\n            //april 19 change\n            //ltsCode = \"PzT\";\n            ltsCode = null;\n        }\n\n    }\n\n    return ltsCode;\n}\n-----------------\n\n---- after ------\npublic static string GetLtsCode(IventoryBinItem item)\n{\n    bool isPzT = item != null\n              && !item.ParentItemNo.IsNullEmptyOrWhiteSpace()\n              && !item.ParentIsNfoOrDiscontinuedItem;\n    return isPzT ? \"PzT\" : null;\n}\n------------------\n\n---- And later refactored to ----\npublic static string GetLtsCode(IventoryBinItem item)\n{\n    return IsPzT(item) ? \"PzT\" : null;\n}\n\nprivate static bool IsPzT(IventoryBinItem item)\n{\n    return item != null\n       && !item.ParentItemNo.IsNullEmptyOrWhiteSpace()\n       && !item.ParentIsNfoOrDiscontinuedItem;\n}\n------------------\n \n\n Now let me be clear. The first code is crap. Major crap. I mean it’s a really stinky brown and greasy pile of poop. It looks like it came out of a very sick dog. \n\n I was astounded that anyone might think that crap was better than the rather nice refactoring. I guess some people just like to smell poop. I call them  Fecophiles . \n\n I thought that perhaps there was something wrong with their noses. After all, if you live in crap, you might just stop smelling it after awhile. So I determined to shove their noses in the fetid pile by investigating the smell in all it’s rich and pungent detail. So I sent the following letter back to my friend. \n\n \n\n Let’s just walk through that previous code step by step and really  smell  it! \n\n \n   We start out buy initializing ltsCode to null. Fine. \n   Then we encounter a double negative (if not null/empty) and an indirection (item.ParentItemNo). We need a comment to understand it. Apparently if the ParentItemNo of the item is null or empty then it means that the item has no parent. If we negate that, then the if statement is trying to say “if this is a child” The comment tries to point this out, but the grammar is bad. It would be nicer if the if statement were if(isChild(item)) or if (item.isChild()). \n   In the body of the if statement we are a child, so we set the ltsCode to PzT. \n   Ignore commented out code. \n   Set the var isNfoOrDisc to an interesting expression. Oddly, I can’t see any place where that variable is used. It  used  to be used, in the commented out code, so maybe someone just forgot to comment out that var too, eh? \n   A negative if statement (it’s easy to miss that bang). \n   The if statement checks whether the parent is not nfo or discontinued. If so, sets ltsCode to PzT if . But ltsCode is  already  PzT. Maybe someone forgot to comment out this whole if statement too…. <bah!> \n   More commented out code. ugh. \n   Ah! Now in the else clause of the “parent Not nfo or Discontinue” if statement we check the boolean opposite. Does this programmer not know how if/else works? Why in hell is he checking a tautology. We already KNOW, in the else clause, that the parent is nfo or Discontinued. Sheesh. \n   And in that else clause, we set ltsCode to null. Wait… Wasn’t it already null? \n   (pant, pant, pant) ok, we’ve reached an else. What else is it? It’s the else from the double negative if statement in point 2 above. Ah, so this is where we have an item with no parent. That //parent comment is strange. Does it mean that the item is a parent? How would we know that. All we know is that the item does not have non-null or empty parent item number. So what’s with the //parent comment? It’s a lie, or at least a wild mistruth. I think that comment needs to be commented out. (grrrr). \n   We set the ltsCode to null. Hmmm. Let’s see. What was the ltsCode be before we set it to null? Why, I believe it must be null already! Is there any pathway through this horrible rats nest of code that might set ltsCode to something other than null at this point? No! \n   and now the coup-de-gras! Another if statement that checks whether the item is nfo or Disconnected. And what does this if statement do if that’s so? Why, it sets the ltsCode to … NULL. Of course! NULL. Brilliant! \n \n\n Summary. This code is a mess. A whole-hearted unmitigated indisputable mess. It’s full of inaccurate and ambiguous comments, useless variables, if statements that have no purpose, and some truly convoluted logic. And what was the goal? The goal was: \n\n \n   return PzT if the item exists and has a parent that is not nfo or Disconnected. Otherwise return null. \n \n\n OR, to put it in simple code: \n\n  public static string GetLtsCode(IventoryBinItem item) {\n  return IsPzT(item) ? \"PzT\" : null;\n }\n\n private static bool IsPzT(IventoryBinItem item) {\n  return item != null\n    && item.HasParent()\n    && item.ParentIsNotnfoOrDiscontinuedItem;\n }\n \n\n I am ashamed of anyone who thinks the original code is simpler and easier to understand. \n\n \n\n I don’t know if they got the message. But even if they did, and they could suddenly smell their home for the first time, it’s a drop in the bucket. There are lots and lots of fecophiles out there who need to take a step back and inhale deeply through their noses. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/04/20/Why-Is-Estimating-So-Hard.html", "title": "Why is Estimating so Hard?", "content": "\n       Consider the Gettysburg Address: \n\n “Four score and seven years ago our fathers brought forth upon this continent a new nation, conceived in liberty and dedicated to the proposition that all men are created equal…” \n\n Let’s ignore the profundity and melody of those remarkable words, and focus instead on the formatting. I’d like to fit the entire address on a bookmark measuring 1.5” X 8”. I’ll use a mono-font that fits 10 characters per inch. And I’ll leave 0.1” on the right and left. So I can fit 13 characters across each line. The text contains 237 words. How long do you think it would take you to manually break the address up into 13 character lines, breaking those lines at appropriate spaces? \n\n The calculation isn’t difficult. If you spent one second per word determining whether or not that word is the appropriate break point for a line, it would take you just under five minutes to break the entire address up into lines that are 13 characters or less. And the odds are you’d do it perfectly. \n\n So how long would it take you to write a program to do it? \n\n Now remember, you  know  this algorithm. You can execute it manually intuitively. During that five minute manual execution there would be no surprises, no stumbling blocks, no backing up and redoing old lines. This is an algorithm that you can execute without even thinking of it as an algorithm. You’d just  do  it. \n\n So how long would it take you to write a program to do it. \n\n Keep in mind that a program is nothing more than the detailed instructions for following a procedure; and this is a procedure you already  know ! \n\n I’ll be kind to you. Don’t give me a single estimate. Give me three estimates. Tell me how long it will take in the best case, the worst case, and the nominal case. Go ahead, write these three numbers down. Now. \n\n Got em? OK, now write the program. Make sure it works. I’ll wait here until you are done. \n\n \n\n Done? How’d you do? Most people need 30-45 minutes to get this working. I’ve seen it done in 15, and I’ve seen it done in 90. Did you fall within your estimate range? Or did you blow the range completely? \n\n Of course lots of people blow the range completely. One of the reasons they blow it is that they estimate it based on how easy the manual task appears to be. You think to yourself: “I could split those lines in 5 minutes by hand, so writing the program ought to be trivial.” We are sadly mistaken. \n\n I remember sitting down with Kent Beck one afternoon to write this algorithm just for fun. I figured it would take us 10-15 minutes. He and I paired on it, test first, for 30 minutes, and got nowhere. Eventually we gave up because we were teaching a class together and actually had to spend time with the students. \n\n But the experience stuck with me. Why was that algorithm so hard - for us - at that particular time? Why was it so hard to write down the procedure for doing something so basic and intuitive? \n\n Answer: Because when we do it manually, we don’t follow a procedure. What we do instead it continuously evaluate the output and adjust it until it’s right. A procedure is blind. It doesn’t look at the output to see if it’s right. If the procedure is wrong, the output will be wrong. Period. But we, humans, are goal seekers. The goal is to split the lines up to no greater than 13 characters, and so we evaluate every line. We look it over and adjust it until it meets the goal. And we can do that in 5 minutes. \n\n It turns out that we  don’t  know the procedure. We haven’t got any clue to just how difficult the procedure is. We aren’t computers. We don’t follow procedures. And so comparing the complexity of the manual task, to the complexity of the procedure is invalid. \n\n This is one of the reasons that estimates are so hard, and why we get them wrong so often. We look at a task that seems easy and estimate it on that basis, only to find that writing down the procedure is actually quite intricate. We blow the estimate because we estimate the wrong thing. \n\n Try this. Break some long string of text up into columns that are 10 characters long. Each time you break a line, record the position of the break, and why you decided to use that position. If you are good at abstracting, you’ll likely come up with three different scenarios for breaking a line. 1. you break it at the 10th character of a word  if  that word is longer than 10 characters. 2. You break it at the 11th character  if  that character is a space. 3. You look backwards from the 10th character looking for a space and if there is one, you break it there. \n\n These three scenarios still need to be arranged into a procedure, but at least you now know how many elements that procedure contains. Knowing that makes the procedure much easier to estimate. \n\n The moral of this story is that tasks that appear easy for a human to solve are often described by complex procedures. So when estimating, make sure you aren’t affected by the apparent ease of that task. Look below the surface to try to enumerate the number of procedural elements. \n\n And if anyone tries to tell you that your estimate is bogus because the task is so simple, ask them to write down the procedure for tying their shoes. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/01/12/The-Letter.html", "title": "The Letter", "content": "\n       We need to become a self-regulating and self-policing profession. The stakes are simply too high to allow software to remain in the current ad-hoc limbo of hackers, heroics, and hermits. \n\n Consider how much software you interact with every day. Your alarm clock, your cell phone, the television and cable box, the remote control, your toaster oven, your watch, your car, the train to work, the cash register at Starbucks, the coffee maker at Starbucks, the elevator you ride, etc. etc. The list is virtually endless. Nearly every aspect of our daily lives, nearly ever corner of our civilization is somehow touched, controlled, managed, or influenced by software. \n\n Think about that again. Virtually every aspect of our lives has a software component; and yet we exert absolutely no regulatory control over that writing of that software. Any Harry Hacker with a “J” in his name can get hired to write Java code. And the strongest likelihood is that Harry J. Hacker’s code be crap, will be wrong, and will not be explicitly tested before it is shipped. \n\n Plumbers are regulated. Electricians are regulated. Architects, lawyers, and doctors are regulated. Why aren’t we? Don’t get me wrong, I don’t want government to be the regulator; I want us to self-regulate. But if we software developers don’t figure out how to do that, then government will certainly step in. And then life will get  really  bad. \n\n Yesterday I received a very scary, letter that underscores this point rather dramatically. I thought you’d like to read it. I worked with the developer to sanitize it so that the innocent people in this story are not punished. I regret that I can’t splash the guilty parties’ names all over twitter though. \n\n \n   Hello “Uncle Bob”, \n \n\n \n\n I’m a 34 year old freelance programmer who has been developing software for 15 years. \n\n \n\n Some time ago I was hired as a team leader for a safety critical embedded system that controlled a medical surgery device. Everything was fine during the first months. I put a great deal of architectural effort into safety. I used Active-Objects for safer threading, UML-Generated State-Machines for stateful safety-checks, simulation, and lots of Unit-Testing (though not with full coverage). \n\n \n\n Oh, things weren’t perfect. We had been asked to finish a four year project in less than 1 year; so time pressure was very high. Even so I had a good feeling that the device would be safe for millions of treatments. \n\n \n\n Eventually it became clear that we could not deliver the full release on time. When we told our manager, he began to worry about his bonus. (At least that’s my personal suspicion). When I told him we couldn’t meet the schedule, he told me I was not allowed to make any more estimates. He shouted a lot all day long and forced everyone to work 7 days a week. He was in such a rage that he even scared the validation team into concluding the official medical validation after just a few days; which was far too early. \n\n \n\n He knew that safety was the most important issue for me, so he began to cut my responsibilities. He eventually gave full control of the project to the\nyoungest and most impressionable programmer on my team. I continued to code in the main-branch of the project; so the manager made his own code branch and, together with the rookie, produced his own version of the software. \n\n \n\n So I terminated my contract with the company. \n\n \n\n A few days later the first bug occurred during a human trial. Fortunately, during the early days of the project, I had made the software robust enough that the bug didn’t harm the patient. The device just stopped operating before starting the automated surgery. You’d think that would have been a wakeup call, but the company didn’t even analyze the bug (not to mention stopping the unstable product). More bugs were found later. Even obvious bugs like mixing up the surgery directions (upwards/downwards) were found during the first human treatments. It is a real possibility, and one of my great fears, that tomorrow some patient will become severely disabled. \n\n \n\n The other programmers didn’t quit. They told me: “the boss is the boss – we just do what he tells us – it’s his responsibility.”. So I figured I must have been crazy for quitting. I felt weak and worthless, like I couldn’t handle the pressure. But lately I started reading “Clean Coder”. And it’s made me consider that, perhaps, I’m not weak – but the opposite: strong. \n\n I am, of course, furious about the manager who drove his organization to such unprofessional depths. He is an idiot and a criminal, and I hope he winds up in prison. But I am even angrier at the developers. Not only were they accomplices to that criminal idiocy; they made the one guy who took a stand feel stupid and weak. They are the ones who are stupid and weak. They should not be programmers. Programmers are better than that. \n\n Aren’t we? \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/04/18/After-The-Disaster.html", "title": "After the Disaster", "content": "\n       There’s a disaster coming. I don’t know when. I don’t know what. But it’s coming, and there’s nothing we can do to stop it; but there  is  something we can do to mitigate it. \n\n How much software is running near you at the moment? Of course you are reading this on a web browser, and that computer you are sitting next to has lots of software running in it. And there are probably many other computers near you with software running on them. Is there software in your cell phone? Of course. How about in your watch? Likely. In the light switch on the wall? How about in the light bulbs? The intercom? The doorbell? The thermostat? Your furnace? Your air conditioner? Your refrigerator, dishwasher, washing-machine, drier? How about your car; and all the other cars on the road? How about the traffic signals? Did you ride an elevator today? Get in a plane or a train? How about an escalator? Do you have a pacemaker? An insulin pump? \n\n When you think about it, it’s  everywhere  And it’s spreading… \n\n How many times per day do you put your life in the hands of an ‘if’ statement written by some twenty-two year old at three in the morning, while strung out on vodka and redbull? \n\n Some time in the not too distant future, there’s going to be an event. Thousands of people will die. And it will be the fault of some errant piece of code written by some poor schmuck under hellish pressure facing impossible deadlines. Perhaps it will be an airline crash, or a cruise ship sinking. Perhaps it’ll be an explosion at a factory, or a train accident involving toxins. Perhaps it’ll be a simple clerical error at a medical research lab that causes a vial of smallpox or ebola to be improperly disposed of. \n\n It doesn’t matter what it is. What is sure is that it’ll come. The probability is not high that it will happen today, or even this year; but the probability is also not zero. It  will  happen. \n\n And when it happens, when thousands of people are killed by a stupid software error, the governments of the world will act. They’ll have to. The population will scream for protection, and the lawmakers will respond with self-righteous indignation. In their toolkit they’ll have regulations, restrictions, licensing requirements, and certification tests. They might take control of our education. They might specify who can be hired and who can’t. \n\n What tools they bring to bear upon us depends upon us. When the disaster happens, what will the congressional and world court investigations find? Will they find a software industry that has defined a set of professional disciplines that it requires of it’s members? Will they find that the software industry has done the due diligence to ensure that it’s members are educated, trained, and skilled? Will they find that the software industry is composed of serious professionals who reliably follow their disciplines and practices? \n\n Or will they find the chaos that exists today? Will they find developers who don’t write tests? (Can you hear the strutting politicians making hay with  that ?  “Mr. President, it is my sad duty to inform this august body that these developers have no record that they test their code…“ ) Will they find that developers work at all hours of the day and night, are under hellish pressure and impossible deadlines? Will they find that there are no professional standard, practices, or disciplines. Will they discover that we are all really just a bunch of undisciplined hacks? \n\n The answer to that question will determine which, if any, of those regulatory tools those self-righteous posturing politicians will foist upon us when the accident happens. If they find that we are disciplined, and self regulating, then perhaps they’ll leave us mostly alone. But if they find that we are undisciplined hacks then you know that they’ll impose all manner of horrible regulation upon us. \n\n They might tell us what languages to use. They might tell us what process to use. They might tell us what our working hours must be. They could give us a dress code. They could turn us all into civil servants. They could do anything that want. \n\n We need to get ahead of this one before it happens. Otherwise we’ll work in a government regulated profession. And then life will be hell. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/02/01/Service-Oriented-Agony.html", "title": "Service Oriented Agony", "content": "\n       I sat down with a group of developers today to do a retrospective on a project. They told me that project management has been complaining about their velocity. It wasn’t serious, but the developers felt bad because things  do  seem to take longer than they should. When I asked them why, they said that the system was very complex, and making changes to it was time consuming. \n\n I asked them to draw the system on the whiteboard. Here’s what they drew. \n\n \n\n The system has several front end processes, each is its own Rails app. They talk to each other by throwing URLs at each other, and loading cookies, etc. Pretty normal. \n\n Each of the front ends talks to some gems which act as interfaces for the services. The services are individual Rails apps that communicate through HTTP using JSON. These services each manage a backend database. \n\n I suspected the answer, but I asked the developers why this structure slowed them down. They told me that every time there is a new feature to add, they have to add it in three different places. They have to change the front end to add the UI gestures. They have to add a new function to one of the gems in order to provide the UI with the new service interface. Then they have to add a new MVC triplet to one of the services that the gem will use to access the data. \n\n I asked them if this was true even if there weren’t any schema changes; and they told me that it was because the gems and the services had evolved with the whole application, and only had functions for the current features. New features often require new functions even though there’s no schema change. \n\n This is a pretty typical structure. I’ve seen it many times before. I’ve seen it in Java apps, C++ apps, and now even Rails apps. The structure seems obvious to system designers who have grown tired of single monolithic systems and want to break those systems up into components and services. What could be more natural than to break the system along the lines of data base managment? \n\n Unfortunately this is a huge violation of the Single Responsibility Principle – or its big brother the Common Closure Principle. These principles tell us to group together things that change together, and keep apart things that change for different reasons. Unfortunately the above design separates things that change for the same reasons, and groups together things that change for different reasons. No wonder the developers feel like they are going slow! \n\n When you separate things that change for the same reasons, you have to make changes in many different places in the system. If those places are in different applications, then you have to switch mental contexts for each change. Testing the change is hard because you have to have all the components running to test it end-to-end. And you have to make sure that all the interface points work as desired. So it’s a lot of work just to get  anything  working. \n\n Moreover, when you group together things that change for different reasons, you expose the components of the system to collateral damage, thrashing, CM collisions, and a whole host of other problems. Changing one of those gems, for example, has an impact on all the front ends, even though some of those front ends won’t use the part of the gem that was changed. \n\n So what’s the solution? First of all, I question whether the system needed to be partitioned into services. Services are expensive and complicated, you should only create them if you  absolutely  need to. It’s always easier to live in a single process. Remember Martin Fowler’s first law of distributed objects:  Don’t distribute your objects. \n\n Let’s assume, however, that the system  did  need to be partitioned into services. Are these the right services? Dividing a system into UI and DB services is a  physical  partitioning. What we should be looking for is families of  business rules  that are separate from each other, and can be partitioned into true business services. \n\n Finally, let’s assume that the physical partitioning was necessary for some reason. Then we need to find a way to make the interfaces of the gems and DB services generic so that every new feature doesn’t require a change throughout the whole system. \n\n Like I said, I see this kind of partitioning a lot; and it always has the same outcome. It slows development because it smears features through many different layers. Many systems could be streamlined, and development made much faster, if the system designers paid more attention to the Single Responsibility Principle. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/01/31/The-Ruby-Colored-Box.html", "title": "The Ruby Colored Box", "content": "\n       I recently read a  blog  about a guy who had a bad interview. The blog was well written and pretty funny. He metaphorically described the interview in terms of a mexican chef being grilled about how to make crème brûlée. He made a bunch of interesting points about how unfair the interview was, and how you shouldn’t ask a mexican chef how to cook pastry. \n\n The blog was in two parts, the mexican chef metaphor, and then a rant. By the time I got to the end of the metaphor I was pretty much thinking that the author was a bit of a whiner. OK, so the interview was unfair. What interview isn’t? Why would any employer want to conduct a “fair” interview. I’m certainly not fair when I interview people. I put them under all kinds of stresses, and throw lots of strange issues at them. I want to see how they behave when confronted with unfamiliar problems. I want to know if they can extrapolate their experience into a new domain. \n\n But I also understood his point. I’ve been to interviews where I’ve bombed, and can remember thinking they were unfair at the time. They weren’t, it’s just that interviewers aren’t trying to find the first person who might fit. They’re trying to find the one person who stands out. That means they’re going to pass over a lot of good people. That’s just the breaks of the game. \n\n But then I got to the rant, and I read this: \n\n \n   My attitude is if I’m a good Ruby programmer, and you’re trying to hire me when the supply for Ruby programmers is low and demand is high, that before you even talk to me you’ve spent at least 10 minutes Googling for my name, looking at my code, and figuring out who I am, rather than spending an hour subjecting me to a series of ad hoc programming questions in areas I may or may not specialize in. That 10 minutes of Google will tell you a lot more than asking me to come in and scribble stuff on a whiteboard. \n \n\n \n   I think this process has left me a bit more discerning about the companies I’ll actually interview with. When you’re trying to hire talented developers in a scarce market, please do your due diligence and don’t insult somebody skilled by asking them to do a degrading whiteboard interview instead of looking at code they have freely available on the Internet or just looking over their shoulder as they code on a computer, preferably their own, at least the first time you get to know them. You may even learn something. \n \n\n The last thing I want is to hire someone who thinks I owe them a job. Does this guy really think he’s so hot that I’m going to do research on him  before  I meet him? Am I supposed to kneel down and kiss his sandals for granting me an audience? Just who does he think he is? \n\n Oh! He’s a  Ruby Programmer ! That explains it! Of course I should kiss his sandals. He’s a  Ruby Programmer . He’s on  GitHub . Oooooohhhh!. \n\n Dear Employers in search of Ruby Programmers: \n\n You don’t need Ruby Programmers! You just need programmers! Any decent programmer can learn Ruby in a week. And Rails only takes a little longer.  Don’t  hire these prima donnas who think they’re God’s gift to Ruby shops. They’re not. They’re just generally inexperienced kids who happened to learn a pretty easy language and a pretty easy framework. \n\n Instead, find some programmers who have been around the block a few times. They should know multiple languages, and be able to deal with many different and novel situations. Don’t worry if they don’t know Ruby, they’ll know it, and Rails, within a month. If you like you can temporarily hire some super-expensive consultant to teach them if you like. They’ll learn quickly, with or without. \n\n What you are looking for, if I may drop back into the metaphor, are mexican chefs who don’t panic when you ask them how to prepare crème brûlée. It’s ok if they don’t know (though don’t let  them  know that!). What you are looking for is whether or not they can extrapolate from their experience. In other words, to drop back out of the metaphor, if you are interviewing a Ruby programmer, your first questions to him should  not be about Ruby . \n\n Ask him something outside his comfort zone, and see if he can extrapolate. Ask him, to write  The Sieve of Eratosthenes  in C. If he doesn’t know C, give him a copy of K&R and come back in 20 min. He won’t have the right answer; but that’s not what you are looking for. You are looking to see whether or not he has the experience and resourcefulness to fruitfully attack the problem. You are looking to see if he can think outside the Ruby colored box. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html", "title": "The Clean Architecture", "content": "\n       \n\n Over the last several years we’ve seen a whole range of ideas regarding the architecture of systems. These include: \n\n \n   Hexagonal Architecture  (a.k.a. Ports and Adapters) by Alistair Cockburn and adopted by Steve Freeman, and Nat Pryce in their wonderful book  Growing Object Oriented Software \n   Onion Architecture  by Jeffrey Palermo \n   Screaming Architecture  from a blog of mine last year \n   DCI  from James Coplien, and Trygve Reenskaug. \n   BCE  by Ivar Jacobson from his book  Object Oriented Software Engineering: A Use-Case Driven Approach \n \n\n Though these architectures all vary somewhat in their details, they are very similar. They all have the same objective, which is the separation of concerns. They all achieve this separation by dividing the software into layers. Each has at least one layer for business rules, and another for interfaces. \n\n Each of these architectures produce systems that are: \n\n \n   Independent of Frameworks. The architecture does not depend on the existence of some library of feature laden software. This allows you to use such frameworks as tools, rather than having to cram your system into their limited constraints. \n   Testable. The business rules can be tested without the UI, Database, Web Server, or any other external element. \n   Independent of UI. The UI can change easily, without changing the rest of the system. A Web UI could be replaced with a console UI, for example, without changing the business rules. \n   Independent of Database. You can swap out Oracle or SQL Server, for Mongo, BigTable, CouchDB, or something else. Your business rules are not bound to the database. \n   Independent of any external agency. In fact your business rules simply don’t know anything at all about the outside world. \n \n\n The diagram at the top of this article is an attempt at integrating all these architectures into a single actionable idea. \n\n The Dependency Rule \n\n The concentric circles represent different areas of software. In general, the further in you go, the higher level the software becomes. The outer circles are mechanisms. The inner circles are policies. \n\n The overriding rule that makes this architecture work is  The Dependency Rule . This rule says that  source code dependencies  can only point  inwards . Nothing in an inner circle can know anything at all about something in an outer circle. In particular, the name of something declared in an outer circle must not be mentioned by the code in the an inner circle. That includes, functions, classes. variables, or any other named software entity. \n\n By the same token, data formats used in an outer circle should not be used by an inner circle, especially if those formats are generate by a framework in an outer circle. We don’t want anything in an outer circle to impact the inner circles. \n\n Entities \n\n Entities encapsulate  Enterprise wide  business rules. An entity can be an object with methods, or it can be a set of data structures and functions. It doesn’t matter so long as the entities could be used by many different applications in the enterprise. \n\n If you don’t have an enterprise, and are just writing a single application, then these entities are the business objects of the application. They encapsulate the most general and high-level rules. They are the least likely to change when something external changes. For example, you would not expect these objects to be affected by a change to page navigation, or security. No operational change to any particular application should affect the entity layer. \n\n Use Cases \n\n The software in this layer contains  application specific  business rules. It encapsulates and implements all of the use cases of the system. These use cases orchestrate the flow of data to and from the entities, and direct those entities to use their  enterprise wide  business rules to achieve the goals of the use case. \n\n We do not expect changes in this layer to affect the entities. We also do not expect this layer to be affected by changes to externalities such as the database, the UI, or any of the common frameworks. This layer is isolated from such concerns. \n\n We  do , however, expect that changes to the operation of the application  will  affect the use-cases and therefore the software in this layer. If the details of a use-case change, then some code in this layer will certainly be affected. \n\n Interface Adapters \n\n The software in this layer is a set of adapters that convert data from the format most convenient for the use cases and entities, to the format most convenient for some external agency such as the Database or the Web. It is this layer, for example, that will wholly contain the MVC architecture of a GUI. The Presenters, Views, and Controllers all belong in here. The models are likely just data structures that are passed from the controllers to the use cases, and then back from the use cases to the presenters and views. \n\n Similarly, data is converted, in this layer, from the form most convenient for entities and use cases, into the form most convenient for whatever persistence framework is being used. i.e. The Database. No code inward of this circle should know anything at all about the database. If the database is a SQL database, then all the SQL should be restricted to this layer, and in particular to the parts of this layer that have to do with the database. \n\n Also in this layer is any other adapter necessary to convert data from some external form, such as an external service, to the internal form used by the use cases and entities. \n\n Frameworks and Drivers. \n\n The outermost layer is generally composed of frameworks and tools such as the Database, the Web Framework, etc. Generally you don’t write much code in this layer other than glue code that communicates to the next circle inwards. \n\n This layer is where all the details go. The Web is a detail. The database is a detail. We keep these things on the outside where they can do little harm. \n\n Only Four Circles? \n\n No, the circles are schematic. You may find that you need more than just these four. There’s no rule that says you must always have just these four. However,  The Dependency Rule  always applies. Source code dependencies always point inwards. As you move inwards the level of abstraction increases. The outermost circle is low level concrete detail. As you move inwards the software grows more abstract, and encapsulates higher level policies. The inner most circle is the most general. \n\n Crossing boundaries. \n\n At the lower right of the diagram is an example of how we cross the circle boundaries. It shows the Controllers and Presenters communicating with the Use Cases in the next layer. Note the flow of control. It begins in the controller, moves through the use case, and then winds up executing in the presenter. Note also the source code dependencies. Each one of them points inwards towards the use cases. \n\n We usually resolve this apparent contradiction by using the  Dependency Inversion Principle . In a language like Java, for example, we would arrange interfaces and inheritance relationships such that the source code dependencies oppose the flow of control at just the right points across the boundary. \n\n For example, consider that the use case needs to call the presenter. However, this call must not be direct because that would violate  The Dependency Rule : No name in an outer circle can be mentioned by an inner circle. So we have the use case call an interface (Shown here as Use Case Output Port) in the inner circle, and have the presenter in the outer circle implement it. \n\n The same technique is used to cross all the boundaries in the architectures. We take advantage of dynamic polymorphism to create source code dependencies that oppose the flow of control so that we can conform to  The Dependency Rule  no matter what direction the flow of control is going in. \n\n What data crosses the boundaries. \n\n Typically the data that crosses the boundaries is simple data structures. You can use basic structs or simple Data Transfer objects if you like. Or the data can simply be arguments in function calls. Or you can pack it into a hashmap, or construct it into an object. The important thing is that isolated, simple, data structures are passed across the boundaries. We don’t want to cheat and pass  Entities  or Database rows. We don’t want the data structures to have any kind of dependency that violates  The Dependency Rule . \n\n For example, many database frameworks return a convenient data format in response to a query. We might call this a RowStructure. We don’t want to pass that row structure inwards across a boundary. That would violate  The Dependency Rule  because it would force an inner circle to know something about an outer circle. \n\n So when we pass data across a boundary, it is always in the form that is most convenient for the inner circle. \n\n Conclusion \n\n Conforming to these simple rules is not hard, and will save you a lot of headaches going forward. By separating the software into layers, and conforming to  The Dependency Rule , you will create a system that is intrinsically testable, with all the benefits that implies. When any of the external parts of the system become obsolete, like the database, or the web framework, you can replace those obsolete elements with a minimum of fuss. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/05/15/NODB.html", "title": "NO DB", "content": "\n       In the United States, in 1920, the manufacture, sale, and importation of alcoholic beverages was prohibited by a constitutional amendment. That amendment was repealed thirteen years later. During that period of prohibition, the beer industry died. \n\n In 1933, when prohibition was lifted, a few giant grain companies started brewing beer. They completely cornered the market. And for nearly 50 years, we in the United State drank this fizzy bodily effluent and called it “beer”. The only way to tolerate the flavor was to drink it very cold. \n\n As a teenager in the ‘60s, I never understood the attraction. Why beer? It was a pale, yellow, distasteful fluid derived from the urine of sick boars, and had no redeeming qualities that I could see. \n\n In 1984, I went to England; and the scales dropped from my eyes. At last I understood. I had tasted beer for the first time; and I found it to be good. \n\n Since those days the beer situation in the United States has improved dramatically. New beer companies are springing up all over the country; and in many cases the beer they make is actually quite good. We don’t have anything quite so nice as a good english bitter; but we’re getting close. \n\n In the ‘80s a few giant database companies cornered the market. They did this by promulgating fear, uncertainty, and doubt amongst managers and marketing people. The word “relational” became synonymous with “good”; and any other kind of data storage mechanism was prohibited. \n\n I was the lead developer in a startup in those days. Our product measured the quality of T1 communications lines. Our data model was relatively simple, and we kept the data in flat files. It worked fine. \n\n But our marketing guy kept on telling us that we had to have a relational database. He said that customers would demand it. I found that to be a strange claim since we hadn’t sold even one system at that time, and no customer had ever mentioned our data storage technology. But the marketing guy was adamant. We just had to have a relational database. Flat files were prohibited. \n\n As the lead developer, responsible for the quality of the software, my view of a relational database was that it would be a big, stogy, slow, expensive pain in the rear. We didn’t have complex queries. We didn’t need massive reporting capabilities. We certainly didn’t need a process with a multi-megabyte footprint sitting in memory and burning cycles. (Remember, this was the ‘80s). So I fought against this idea with everything I had; because it was the wrong technical solution. \n\n This was not a politically astute move for me. Over a period of several months, a hardware engineer who managed to write a few lines of code, was moved into the software group. He was gradually given more and more responsibility, and was eventually named my co-manager. He and I would “share” the responsibility for leading the software team. \n\n Uh huh. Sure. Right. A hardware guy with no real software experience was going to “help” me lead the team. And what do you think his first issue was? Why it was to get a relational database into our system! \n\n I left a month later and started my consulting career. It was best career move I have ever made. The company I left no longer exists. I don’t think they ever made a dime. \n\n I watched the relational database market grow during the ‘90s. I watched as all other data storage technologies, like the object databases, and the B-tree databases dwindled and died; like the beer companies in the 20s. By the end of the ‘90s, only the giants were left. \n\n Those giants were marketing up a storm. They were gods. They were rulers. During the dot com bubble, one of them actually had the audacity to buy television ads that claimed that their product was “the power that drove the internet”. That reminded me of a beer slogan from the ‘70s “Ya gotta grab for all the gusto in life ya can.” Oh brother. \n\n During this time I watched in horror as team after team put the database at the center of their system. They had been convinced by the endless marketing hype that the data model was the most important aspect of the architecture, and that the database was the heart and soul of the design. \n\n I witnessed the rise of a new job function. The DBA! Mere programmers could not be entrusted with the data – so the marketing hype told us. The data is too precious, too fragile, too easily corrupted by those undisciplined louts. We need  special  people to manage the data. People trained by the database companies. People who would safeguard and promulgate the giant database companies’ marketing message: that the database belongs in the center. The center of the system, the enterprise, the world, the very universe. MUAHAHAHAHAHAHA! \n\n I watched as SQL slipped through every crack and crevice in the system. I ran screaming from systems in which SQL had leaked into the UI. I railed endlessly against the practice of moving all business rules into stored procedures. I quailed and quaked and ranted and raved as I read through entire mail-merge programs written in SQL. \n\n I hammered and hammered as I saw tables and rows permeating the source code of system after system. I hammered out danger. I hammered out a warning. I hammered out that the schema had become “The Blob”, consuming everything in sight. But I knew all my hammering was just slinging pebbles at a behemoth. \n\n And then, in the first decade of the 21st century, the prohibition was lifted, and the NOSQL movement was born. I considered it a kind of miracle, a light shining forth in the wilderness. Finally, someone realized that there might just be some systems in the world that did not require a big, fat, horky, slow, expensive, bodily effluent, memory hog of a relational database! \n\n I watched in glee as I saw BigTable, Mongo, CouchDB, and all the other cute little data storage systems begin to spring up; like little micro-breweries in the ‘80s. The beer was back! And it was starting to taste good. \n\n But then I noticed something. Some of the systems using these nice, simple, tasty, non-relational databases were being designed  around those databases . The database, wrapped in shiny new frameworks, was still sitting at the center of the design! That poisonous old relational marketing hype was still echoing through the minds of the designers.  They were still making the fatal mistake. \n\n “Stop  You don’t understand. You don’t understand.” But the momentum was too great. An enormous wave of frameworks rose up and smashed down on our industry, washing over the land. Those frameworks wrapped up the databases and fought to grab and hold the center of our applications. They claimed to master and tame the databases. They even claimed to be able to turn a relational database into a NoSQL database. And the frameworks cried out with a great voice heard all over the land: “Depend on me, and I’ll set you free!” \n\n \n\n The name of this article is “No DB”. Perhaps after that rant you are getting an inkling of why I named it that. \n\n The center of your application is not the database. Nor is it one or more of the frameworks you may be using.  The center of your application are the use cases of your application. \n\n It makes me crazy when I hear a software developer describe his system as a “Tomcat system using Spring and Hibernate using Oracle”. The very wording puts the frameworks and the database at the center. \n\n What do you think the architecture of that system would look like? Do you think you’d find the use cases at the center of the design? Or would you find the source code arranged to fit nicely into the pattern of the frameworks? Would you find business objects that looked suspiciously like database rows? Would the schema and the frameworks pollute everything? \n\n Here’s what an application should look like. The use cases should be the highest level and most visible architectural entities. The use cases are at the center. Always! Databases and frameworks are details! You don’t have to decide upon them up front. You can push them off until later, once you’ve got all the use cases and business rules figured out, written,  and tested . \n\n What is the best time to determine your data model? When you know what the data entities are, how they are related, and how they are used. When do you know that? When you’ve gotten all the use cases and business rules written  and tested . By that time you will have identified all the queries, all the relationships, all the data elements, and you’ll be able to construct a data model that fits nicely into a database. \n\n Does this change if you are using a NoSql database? Of course not! You still focus on getting the use cases working and tested before you even think about the database; no matter what kind of database it ends up being. \n\n If you get the database involved early, then it will warp your design. It’ll fight to gain control of the center, and once there it will hold onto the center like a scruffy terrier. You have to work hard to keep the database out of the center of your systems. You have to continuously say “No” to the temptation to get the database working early. \n\n We are heading into an interesting time. A time when the prohibition against different data storage mechanisms has been lifted, and we are free to experiment with many novel new approaches. But as we play with our CouchDBs and our Mongos and BigTables, remember this:  The database is just a detail that you don’t need to figure out right away. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/12/19/Three-Paradigms.html", "title": "Three Paradigms", "content": "\n       In the last 40 years computer hardware technology has increased the computing power of our machines by well over twenty orders of magnitude. We now play Angry Birds on our phones, which have the computing power of the freon cooled supercomputer monsters of the 70s. \n\n But in that same 40 years software technology has barely changed at all. After all, we still write the same if statements, while loops, and assignment statements we did back in the ’60s. If I took a programmer from 1960 and brought him forward through time to sit at my laptop and write code; he’d need 24 hours to recover from the shock; but then he’ll be able to write the code. The concepts haven’t changed that much. \n\n But three things have changed about the act of writing software. I’m not talking about the hardware now, or the computer speed, or the incredible tools we have. I’m talking about the code itself. Three things have changed about that code. We could call these things – paradigms. And they were all “discovered” in a single decade more than 40 years ago. \n\n * 1968 –  Structured Programming . Edsger Dijkstra wrote his classic paper: “Go To Statement Considered Harmful” and a number of other papers and articles suggesting that we abandon the use of unbridled Go To, replacing it with structures such as if/then/else and while loops. \n\n * 1966 -  Object Oriented Programming . Ole-Johan Dahl and Kristen Nygaard, fiddling around with the Algol language, “discover” objects, and create the first Object Oriented Language: Simula-67. Though there are many far-reaching implications of this advance it did not add any new capabilities to our code. Indeed, it removed one. For with the advent of polymorphism, the need for pointers to functions was eliminated; and indeed deprecated. \n\n * 1957 -  Functional Programming . John McCarthy creates Lisp: the first functional language. Lisp was based on the Lambda Calculus formulated by Alonzo Church in the ’30s. Though there are many far-reaching implications of functional programming, all functional programs are dominated by one huge constraint. They don’t use assignment. \n\n Three paradigms. Three constraints. Structured Programming imposes discipline on direct transfer of control. Object Oriented Programming imposes discipline on indirect transfer of control. Functional programming imposes discipline upon assignment. Each of these paradigms took something away. None of them added any new capability. Each increased discipline and decreased capability. \n\n Can we afford another paradigm? Is there anything left to take away? \n\n There hasn’t been a new paradigm in 40 years; so perhaps that’s a good indication that there aren’t any more to find. \n\n Must we use all these paradigms, or can we pick and choose? \n\n Over time we have decided to enforce them. First structured programming was enforced by effectively eliminating the Go To statement from our languages (as Dijkstra recommended in his paper). OO has also been effectively enforced by removing pointers to functions from our most modern languages and replacing that functionality with polymorphism (e.g. Java, C#, Ruby). So for at least these two, the answer to that question seems to be that we MUST use them. All other options have been eliminated; or at least severely constrained. \n\n So what about functional programming? Are we to be consigned to using languages that have no assignment operator? Likely so! We are already consigned to writing code that must run well on multi-cores; and those cores are multiplying like Gerbils. My laptop has 4 cores. My next will likely have 8. The one after that 16. How are you going to write reliable code with 4096 processors contending for the bus? We can barely get two concurrent threads to work properly, let alone 2^n literal processors. \n\n Why is functional programming important to solving the multi-core problem? Because functional programs don’t use assignment, and therefore don’t have side effects, and therefore don’t have concurrent update problems – at least that’s the theory. \n\n We’ll talk more about the details of functional programming in later blogs. What fascinates me about the three paradigms mentioned above are their dates. They are  old ; almost older than I am. And there have been no new ones since I turned 16, 42 years ago. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/08/24/functional-programming-for-the-object-oriented-programmer.html", "title": "Functional Programming for the Object Oriented Programmer", "content": "\n       This book, written by Brian Marick, is important. Indeed, it may be  necessary . We need something to bridge the gap between the huge population of OO programmers, and the growing need for functional programmers. I’ve seen nothing else that fills this need so well. \n\n I read a lot of books, but few are so competently written. I’m a hundred pages in and I’m loving it. If you are a Java, C#, C++, Ruby, or Python programmer, and you are wondering what all this  functional programming  noise is about, this is the book for you. \n\n First, the book is a pleasure to read. Marick’s style is witty, wry, informal and, best of all, terse. His code examples are straightforward and well conceived. He makes his points quickly, and then he gets on with the next topic. There’s no dawdling. He respects his reader by unapologetically covering a lot of ground in a short time. He provides a few salient exercises at the end of each chapter, and then moves right on. The book is a challenging read; but not a challenge  to  read. \n\n The title says it all; and the pedagogical approach is ingenious. What better way to explain and expose functional programming to an OO programmer than to build an object system in a functional language? And what better language to build such an object system than Clojure? Not only will you learn a lot about functional programming in terms that make it easy to understand; you’ll also learn a lot about OO that you may not have known before. \n\n Marick realized that most of his readers won’t know a functional language. So he chose the simplest one to learn. He also realized that teaching the language would distract from the purpose of the book. So he strikes a brilliant compromise: He presents the  barest minimum  of the language he can get by with, and then gets right on with the business of describing functional programming. The technique is  very  effective; though many Clojure programmers will find the language primitives he constrains himself to a wee bit frustrating. \n\n Is there anything bad about the book? Yes; It’s not done. Marick published it on Leanpub.com and has been writing chapters over the last many months. Since the book isn’t really finished, there are a few typos here and there; and there’s a sense of a ragged end where unwritten pieces may be needed to fill some gaps. But I haven’t found this to be at all distracting so far. \n\n My recommendation is to buy this book now. You can get it  here . There’s a slider that allows you to choose the price you want to pay for the book. You should slide it all the way over to the right, like I did, and pay the $30 – it’s more than worth it. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/12/29/Brave-New-Year.html", "title": "Brave New Year", "content": "\n       This is going to be one of those annoying, rambling, touchy-feely, new-years blogs. You know the kind. One that tries to make you think about what’s passed, and what’s coming. One that tries to impress you with foresight and erudition. One that claims profundity and depth. You know the kind. \n\n Still, that’s the mood I’m in. So here goes. \n\n I was sitting at my kitchen table working on my Clean Architecture manuscript. I was writing part of Chapter One where I use the Human Body as a metaphor for software architecture. And then I started thinking about the body, the brain, nerves, computers, and - well - my train of thought went something like this. \n\n Nerve impulses are electrochemical not electrical. They don’t move at the speed of light. Indeed, the mechanism involves the motion of atoms through a liquid; which is limited by something like the speed of sound in that liquid. In any case, nerve impulses travel from one end of a nerve axon to the other at about 200 miles per hour. \n\n Now, relative to a human body, that’s very fast. A nerve impulse can get from your toe to your brain in about twenty milliseconds. That’s fast enough. On the other hand, it’s three million times slower than the speed of light. And that implies that if your nerve fibers were made of copper wire, you could be three million times bigger than you are, and still get signals from your brain to your toe in 20ms. \n\n Hmmm. Three million times 6 feet is 18 million feet. That’s 3400 miles. That’s just under half the diameter of the Earth. So if my nerve fibers were wires, I could be roughly the size of a small planet and have the reaction time of a human. Hmmm. \n\n OK, wait. What about switching time? Nerve cells have a switching rate of about a millisecond. Reaction time is about 600 ms. So it stands to reason that from the moment you see an event, until the moment you react to it, the information has passed through 600 layers of neurons. Or rather, the longest pathway taken by that information involved 600 synapses. \n\n Now imagine that those neurons were transistors that can switch signals in picoseconds; many millions of times faster than a neuron. And imagine you wanted to preserve a reaction time of 600 ms using 600 processing nodes composed of transistors. You’d have to separate those nodes by an average of about 200 miles. \n\n The human brain has about a trillion neurons. A smartphone has about one tenth that many transistors. So 10 iPhones have a trillion transistors or so. \n\n I’m sure you can see where I’m going with this. We live on a planet that is encased in a globe spanning network that connects hundreds of millions of smartphones, tablets, laptops, desktops, and mainframe computers together. The computer equipment on our planet has the reaction time, connectivity, and many millions of times the processing power, of a human brain. \n\n Those of you who read  The Moon is a Harsh Mistress  might now be guessing that my next question is: “Why doesn’t it wake up?”. But you’d be wrong. The hardware on our planet is not wired to be a brain. It’s wired to allow us to play Angry Birds. We have not connected the hardware on our planet in order to make the planet wake up. Indeed, we likely don’t know how. \n\n But that brings up a very interesting question.  Are we close to knowing how? \n\n Consider  Watson , the computer that plays Jeopardy. Playing Jeopardy is something we would normally associate with human cognition; and the fact that Watson beat the world’s champion Jeopardy players gives us pause. Was Watson thinking, or was it’s processing an analog of human thought? It’s tempting to stretch our definition of thought and give Watson the benefit of the doubt. It’s tempting to believe that if Watson could beat the best Jeopardy players, that it must be using  something  akin to human thought. \n\n But then consider this question that Watson got wrong: \n\n \n   In the category U.S. CITIES (“Its largest airport was named for a World War II hero; its second largest, for a World War II battle”). Watson’s response was “What is Toronto? \n \n\n WTF! Excuse me? What planet does Watson live on? Has Watson decided to compete for a beauty contest? Has he heard of  Chicago ? O’Hare field? Midway Airport? How in hell could a smart program like Watson make a blunder and blindingly stupid as that? \n\n But there is it. Watson was not thinking. No thinking being with access to the huge databases available to Watson would - could - make that mistake. The category was U.S.Cities. Toronto is in Canada. Toronto’s airports are named “Pearson” and “Buttonville”. And although Lester B. Pearson was an influential Prime Minister of Canada, he was not the hero of the great WWII battle of Buttonville! \n\n We programmers recognize this kind of error. We understand that computers are deeply moronic. They do  precisely  what they have been programmed to do, no more - no less. We see errors like this all the time. We call them bugs. And we know who’s to blame for them. We are. \n\n Or consider Deep Blue, the computer that beat Gary Kasparov in a chess match. In 1996 Deep Blue won a single game against Kasparov. This was the first time that a computer had  ever  beaten a grand master. A year later it won a full match of 6 games, 3.5 to 2.5. Since that time you’d think that computers would have gotten so good that they could blow away all the grand masters. But that’s not the case. Though they can now consistently win matches; they do not consistently win all the games. What’s going on? \n\n Deep Blue could search 200 million chess positions per second. Kasparov couldn’t do anything like that. Deep Blue’s strategy was based on exhaustive searching. Kasparov’s could not have been. So then how did Kasparov beat Deep Blue in even one game, let alone 2.5? If we knew the answer to that… \n\n Oh, one answer is that Kasparov learned to play differently against Deep Blue than against a human. The strategy to win against a machine is different from the strategy to win against a human. And  that  is telling. The computer does not behave like a human. It is not employing human-like thought. \n\n Why do the grand masters keep winning games against the computers? Because they  are  employing human like thought; and that thought is something that computers are still unable to approach. \n\n So, then, are we close to knowing how to build a thinking machine? After all, we  have  the hardware. Could we make that hardware think? \n\n The two examples I’ve posed are discouraging in that regard. They make it clear that we, programmers, have not yet identified  The Human Algorithm . We don’t know how to simulate human creativity, human reasoning, human learning, human inference, and/or human emotion. If we knew the algorithms, we could wire the hardware - we could wake the planet up. But that algorithm continues to elude us. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/12/22/FPBE1-Whats-it-all-about.html", "title": "FP Basics E1", "content": "\n       h4. What’s Functional Programming all about? \n\n By now you’ve almost certainly heard of functional programming. I mean, how could you miss it? Everybody’s talking about it. There are all these new functional languages coming out like Scala, F# and Clojure. People are talking about older languages too, like Erlang, Haskell, ML, and others. \n\n So what’s this all about? Why is functional programming The Next Big Thing ™? And what in blazes is it? \n\n Firstly, it’s almost certainly true that functional programming is the next big thing. There are good solid reasons for this that we’ll explore towards the end of this article. But ﬁrst, in order to understand those reasons, we need to know what functional programming is. I’m going to upset a lot of people with this next statement because I’m going to resort to extreme minimalism. I’m going to reduce functional programming down to it’s simplest core; and this isn’t really fair because the topic is rich and expressive and full of wonderful concepts. We’ll explore all those concepts in future articles. You’ll even see hints of some of them here. But for now, I’ll simply deﬁne functional programming this way: \n\n \n   Functional programming is programming without assignment statements. \n \n\n Oh no! Now I’ve gone and done it. The functional programmers out there are gathering their pitchforks and torches. They want my head for uttering such minimalist blasphemy. Meanwhile all the folks who hoped to learn what functional programming really is are about to stop reading because the above statement is so blatantly absurd. I mean: how in the world can you program without assignment? The best way to explain that is to show an example. Let’s look at a very simple program in Java: The squares of integers. \n\n public class Squint {\n  public  static  void  main(String  args[])  {\n    for  (int  i=1;  i<=25;  i++)\n      System.out.println(i*i);}\n}\n \n\n Who hasn’t written that program, or some simple variant of it? I must have written it many hundreds of times. It’s often the second program I write in a new language, and the second or third program I teach new programmers to write. Everybody knows the good old squares of integers! \n\n But let’s look at it closely. It’s just a simple loop with variable named  i  that counts up from 1 to 25. Each loop through the program causes the variable  i  to take on a new value. This is assignment. A new value is being assigned to the variable  i  every pass through the loop. If you could somehow peer into the memory of the computer and stare at the memory location that held the value of  i , you would see the value held by that memory change from one iteration of the loop to the next. \n\n If the previous paragraph seemed to have belabored an obvious point, let me point out that whole papers have been written on this topic. The concepts of identity, value, and state may seem intuitive to us; but they are actually a very rich topic in and of themselves. But I digress. \n\n Now let’s look at a functional program for the squares of integers. We’ll use the language Clojure for this; though the concepts we’re going to explore work the same in any functional language. \n\n (take 25 (squares-of (integers)))\n \n\n Yes, you are reading that correctly; and yes, that is an actual program that produces actual results. If you must see the results they are: \n\n (1 4 9 16 25 36 49 64 ... 576 625)\n \n\n There are three words in that program:  take ,  squares-of , and  integers . Each of those words refers to a function. The left parens in front of those words simply mean:  call the following function and treat everything up to the closing right paren as it’s arguments. \n\n The  take  function takes two arguments, an integer  n , and a list  l . It returns the ﬁrst  n  items of  l . The  squares-of  function takes a list of integers as it’s argument and returns a list of the squares of those integers. The  integers  function returns a list of integers in sequential order, starting at 1. Thats it. The program simply takes the ﬁrst 25 elements of a list of the squares of sequential integers starting at 1. \n\n Read that sentence again; because I did something important there. I took the three separate deﬁnitions of the functions and combined them into a single sentence. That’s called: (are you ready for the buzzword?) \n\n \n   Referential Transparency. \n \n\n [cue: Fanfare, glitter, ticker tape, cheering crowds]. \n\n Referential Transparency simply means that, in any given sentence, you can replace the words in that sentence with their deﬁnitions, and not change the meaning of the sentence. Or, more importantly for our purposes, it means that you can replace any function call with the value it returns. Let’s see this in action. \n\n The function call  (integers)  returns  (1 2 3 4 5 6 ...) . OK, I know you’ve got questions about this, right? I mean, how big is this list? The real answer is that the list is only as big as I need it to be; but let’s not think about that just now. We’ll come back to it in a later article. For the moment just accept that  (integers)  returns  (1 2 3 4 5 6 ...) ; because it does! \n\n Now, in our program, we can replace the function call  (integers)  with it’s value. So the program simply becomes: \n\n (take 25 (squares-of (1 2 3 4 5 6 ...)))\n \n\n Yes, I did that with copy and paste; and that’s also an important point. Referential Transparency is the same as copying the value of a function call and pasting it right on top of that function call. \n\n Now, let’s do the next step. The function call:  (squares-of (1 2 3 4 5 6 ...))  simply returns a list of the squares of the numbers in it’s argument list. So it returns:  (1 4 9 16 25 36 49 64 ...) . If we replace this function call with it’s value, our program simply becomes: \n\n (take 25 (1 4 9 16 25 36 49 64 ...))\n \n\n And, of course, the value of that function call is simply: \n\n (1 4 9 16 25 36 49 64 ... 576 625)\n \n\n Now let’s look at our program again: \n\n (take 25 (squares-of (integers)))\n \n\n Notice that it has no variables. Indeed, it has nothing more than three functions and one constant. Try writing the squares of integers in Java without using a variable. Oh, there’s probably a way to do it, but it certainly isn’t natural, and it wouldn’t read as nicely as my program above. \n\n More importantly, if you could peer into the computer’s memory and look at the memory locations used by my program, you’d find that those locations would be initialized as the program first used them; but then they would retain their values, unchanged, throughout the rest of the execution of the program. In other words, no new values would be assigned to those locations. \n\n Indeed, this is a necessary condition for Referential Transparency, which depends on the fact that every time you invoke a particular function call, you will get the same result. The fact that computer memory for my program does not change while my program is executing means that the call  (f 1)  will always return the same value no matter how many times it is called. And that means I can replace  (f 1) , wherever it appears, with its value. \n\n Or to say this another way: Referential Transparency means that no function can have a side effect. And, of course, that means that no variable, once initialized, can ever change its value; since assignment is the quintessential side effect. \n\n So why is this important? What’s so great about Referential Transparency? Given that it is possible to write programs without assignment, why is it important? \n\n You are almost certainly reading this on the screen of your computer. Or if not; you have a computer nearby. How many cores does it have?I’m typing this article on a MacBook Pro with 4 real cores. (They say it has 8, but I don’t count all that “hyperthreading nonsense”. It has four). My previous laptop had two cores. And the one before that had just one. The only conclusion I can draw is that my next laptop will truly have eight cores; and the one after that will likely have 16. \n\n The poor hardware engineers, who have carried us on their backs for the last four decades, have finally hit the speed of light limit. Computer clocks simply aren’t going to get much faster. After doubling every 18 months for longer than most programmers (except me) have lived, the runaway growth in computer speed has slammed to a halt; never to rise again. \n\n So those hardware engineers, in an effort to give us more and more cycles per second, have resorted to adding more processors into our chips; and there seems no end to how many processors that will lead to as the years march onwards. \n\n So let me ask you this, O skilled and competent programmer: How are you going to take advantage of every computer cycle available to you when your computer has 4096 cores in it? How will you marshal your function executions when they must run within 16384 processors all contending for the same memory bus? How will you build responsive and flexible websites when your models, views, and controllers must share 65536 processors? \n\n Honestly, we programmers can barely get two Java threads to cooperate. And threads are baby-food compared to the meat and potatoes of real processors fighting over the bus. For over half a century programmers have made the observation that the processes running in a computer are concurrent, not simultaneous. Well, boys and girls, welcome to the wonderful world of simultaneity! Now; how are you going to deal with it? \n\n And the answer to that is, simply:  Abandon all assignment, ye who enter here. \n\n Clearly, if the value of a memory location, once initialized, does not change during the course of a program execution, then there’s nothing for the 131072 processors to compete over. You don’t need semaphores if you don’t have side effects! You can’t have concurrent update (pardon me: Simultaneous Update) problems if you don’t update! \n\n So that’s the big deal about functional languages; and it is one big fricking deal. There is a freight train barreling down the tracks towards us, with multi-core emblazoned on it; and you’d better be ready by the time it gets here. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2012/09/06/I-am-Your-New-CTO.html", "title": "The New CTO", "content": "\n       “So, what did you think of  that ?” I asked as we sat down at our regular table in the cafeteria. As I scanned the other tables in the lunchroom I could see that many other teams were leaning in to their conversation and speaking in semi-hushed tones. The normally light-hearted lunchtime banter had been replaced with a new intensity. \n\n “It started pretty well.” said Jasper. “I mean he was nice enough at first, introducing himself as the new CTO and all.” \n\n “Yeah, but then it started to get weird.” said Jasmine. “I mean, how dare he imply that we’re not behaving professionally? We’ve been working our asses off!” \n\n Jerry shook his head and said: “Yeah, on the other hand, you’ve got to admit that the last few releases haven’t gone so well. I mean, he was right about the schedule slips and defect rates and all.” \n\n “The last  few  releases…” I blurted, “…you mean the last few  dozen ! Hell, it’s embarrassing! I think it’s been over a year since any of the business analysts or testers have even made  eye-contact  with me.” \n\n “Yeah, OK, so things haven’t been perfect”, Jasimine responded, “but he didn’t have to call us ‘unprofessional’. He didn’t have to say our releases were ‘shit’!” \n\n Jasper looked up from his salad and said: “Well, he didn’t really say it quite that way. He said that from now on we are all going to behave as professionals. And then he said: ‘ We will not ship shit! ’ I, for one, would  like  to stop shipping shit.” \n\n “Yeah, me too.” Jasmine retorted. “But we’ve got deadlines! It’s not our fault that we have to rush all the time.” \n\n “Yeah, but did you hear what he said about that?” I asked “He said that from now on he expected honest estimates, and that he expected us to say ‘No’ if we thought something couldn’t be done.” \n\n “If we did that, “ Jasmine shot back, “we’d be saying ‘No’ every damned day!” \n\n “Well maybe we should.” said Jerry, staring ahead at nothing. “Maybe we always should have. Maybe that’s been the problem. Maybe the reason we’ve been shipping shit (and it  has been shit ) is because we’ve been saying ‘Yes’ all the time.” \n\n “Or: ‘We’ll try.’” said Avery. “We’re always promising to  try , as though that’s going to change anything.” \n\n Jasmine slammed her fork down on the table. “Well we’ve  got  to say  something  to get those managers off our backs!” \n\n “Maybe we should just tell them the truth and stick to it.” said Jerry. “That’s what he was saying, wasn’t it? He just wants us to tell the truth. I think that might just be a good idea.” \n\n Jasper changed the subject. “Did you hear how the QA folks cheered when he said that QA should find nothing? What a hoot that was.” \n\n “How in hell is that supposed to work?” asked Jasmine angrily. She picked up her fork and started jabbing it towards Jasper. “Are we supposed to write the system  and  test it? How are we going to get  anything  done on time if we have to do it  right  all the time.” \n\n Jasmine froze with the fork still pointing at Jasper. Her eyes were wide in astonishment, and then disbelief. Everyone else fell silent for a second or two just staring at her. Then her face got red as she lowered her fork, looked away, and mumbled: “Yeah. Well, … yeah. Maybe we  ought  to be doing it right all the time. Maybe…” \n\n Avery broke the spell. “Did you hear him say that stuff about going fast and going well?” \n\n “Yeah,” I said, and I assumed a posture imitating the new CTO. “(ahem) The Only Way To Go Fast, Is To Go Well!”. Everyone chuckled. \n\n “Do you think that’s true?” Avery asked. “Do you think we’ll go faster if we write better code?” \n\n “Well,” Said Jasper. “I know I’ve been slowed down by some of  your  bad code, Avery. And  your’s  Jasmine. Hell, just yesterday I was slowed down by  my own  bad code.” He flashed that toothy grin at us. \n\n “Yeah, yours is the worst, Jasper.” Jerry said as he used a spoon to launch a grape at him. \n\n Jasper picked up a tomato slice and feigned a throw back at Jerry, but then he smiled and put it back in his bowl. Everyone laughed for a second. Even Jasmine smiled. \n\n “And then there was that thing about covering for each other.” Jasper continued. \n\n “Yeah, that was cool.” I said. “It was like he wanted us to know a lot about each other’s work so that if one of us gets sick, or goes on vacation, the rest of us can pick up where they left off.” \n\n Jerry reached over and nudged Jasmine on the shoulder: “Yeah, Jasmine, remember that time you took a week of leave and none of the rest of us even knew where your source directories were? It took us nearly the whole week just to find them.” \n\n “Yeah… That was a bad release.” said Jasper. Everyone nodded. …Even Jasmine. \n\n “And what was that bit about being afraid of the code?” asked Jasper. \n\n “You know exactly what that was.” Jerry said. “We don’t clean our code, because we’re afraid that if we do we’ll break it, and then we’ll be in trouble.” \n\n “Yeah,” replied Jasper, “he said that was really unprofessional…” \n\n The conversation lulled and we all just stared at our plates for a few seconds. \n\n “OK”, said Jasmine. “So how are we going to do all these things he said he expects of us? It’s not like he told us how. I mean… What was the list, anyway?” \n\n We all grabbed our notes and scribbled a list on our napkins. Jasmine gathered all the napkins and assembled a final list. \n\n “OK, here’s the list of things he expects:” \n\n \n   QA will find nothing. \n   We will cover for each other. \n   We will make Honest Estimates. \n   We will say ‘No’. \n   We will not be afraid of our code. \n   The only way to go fast is to go well. \n   We will not ship shit. \n \n\n “That wasn’t everything.” I said. \n\n “No, but it’s enough for now.” Jasmine shot back. “So, how do we actually  do  these things he expects.” \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/01/02/FPBE2-Whys-it-called-functional.html", "title": "FP Basics E2", "content": "\n       h4. Why’s it called Functional? \n\n In the previous episode I told you what all the functional programming hubbub is all about. Remember: Referential Transparency and the multi-core freight train? Since you are here, reading episode 2, I presume you were convinced by my arguments and want to know more. \n\n So the next question to answer is: Why’s it called “Functional” programming? The simple answer to that question is that Functional Programming is programming with functions (duh). As answers go, that one’s pretty bad. Oh it’s accurate, so far as it goes, and it sounds good enough; but it really doesn’t answer the question. After all, Java programs are programs with functions. \n\n So then why the word “functional”? \n\n I’m going to get all the math weenies upset with me now; because I’m going to use an analogy with calculus. I’m not going to let that bother me though, since I simply stole the analogy from Wikipedia. \n\n Do you know what dy/dx signifies? In particular, in the expression dy/dx, what is y? Of course, y is a function. dy/dx is the derivative of that function. So dy/dx takes a function as it’s argument and returns the derivative of that function as it’s result. Right? \n\n Consider d(x^2)/dx: it equals 2x. Notice that 2x is a function. So the argument and the return values are both functions. \n\n Wait. What do you call something that takes arguments and returns values. You call that a function. So dy/dx is a function that takes a function and returns a function. \n\n What if we had a computer language that was like that? What if we could write functions that took functions as arguments, operated upon them without evaluating them, and then returned new functions as the result? What would you call a language like that? What other word could be better than: functional? \n\n And of course now I have all the functional language purists mad at me because they know that that’s a completely inadequate way to define a functional language. But it’s a step along the path – and it’s an important one. \n\n So, what does it mean to pass a function as an argument to another function. Let’s look at our squares of integers program again. Remember it was just: \n\n (take 25 (squares-of (integers)))\n \n\n Let’s make a function out of it by giving it a name and an argument: \n\n (defn squint [n]\n  (take n (squares-of (integers))))\n \n\n I’m sure you can work out the syntax on your own. It’s not rocket science. \n\n Given this definition of squint, we can print the first 25 squares of integers with this command: \n\n (println (squint 25))\n \n\n So far this is nothing more than simple function calls and definitions. But what’s that  squares-of  function? How is it defined? \n\n (defn square [n] (* n n))\n\n(defn squares-of [list]\n  (map square list))\n \n\n Now that’s a little more interesting! The  square  function is no surprise, it just multiplies it’s argument by itself. It’s the  squares-of  definition that’s interesting; because it passes the  square  function as an argument to a function named  map . \n\n The  map  function is one of the staples of functional programming. It takes two arguments: a function  f  and a list  l ; and it returns a new list by applying  f  to every element of  l . \n\n Passing a function as an argument, like this, is not something that Java programmers do too often. On the other hand, it’s not a completely alien concept either. Any Java programmer who has studied the Command Pattern, or who has used the Listeners in Swing, will understand what’s going on here. \n\n What’s more, most languages nowadays have begun to incorporate the notion of functions passed as arguments. This feature is sometimes called “blocks” or “lambdas”. It is a very common part of any Ruby program; and has become much more important lately in C#. It is even rumored that the feature will be added to Java sometimes soon.  \n\n So then maybe we don’t have to learn a new language. Maybe our old languages are becoming more and more functional and we can just use those functional features as they become more and more available. \n\n That way lay madness, wailing, and much gnashing of teeth! A language with functional features is not a functional language; and a program written with a few lambdas here and there is not a functional program. \n\n To get a hint of why this is true, we need to look at the integers function. \n\n    (defn integers []\n      (integers-starting-at 1))\n \n\n OK, that’s not too hard. The  integers-starting-at  function simply takes an integer argument and returns a list of all integers starting with the argument. \n\n    (defn integers-starting-at [n]\n      (cons n (lazy-seq (integers-starting-at (inc n)))))\n \n\n This one’s going to take a little explaining. But don’t fret, it’s actually quite simple. \n\n First there’s the  cons  function; which is short for “construct list”. The  cons  function takes two arguments, an element, and a list. It returns a new list which is the element prepended onto the old list. So  (cons 2 [3 4])  returns  [2 3 4] . \n\n Now all the Clojure people are upset with me because that’s not exactly right. On the other hand, the difference is something we don’t need to deal with right now. For the moment, just remember this:  cons  constructs a list by prepending it’s first argument to the second; which must be a list. \n\n Now, perhaps you are thinking that  cons  just sticks the element on the front of the list; but that’s not quite right. Indeed, it’s not even close to being right. You see  cons  doesn’t change the argument list at all. Instead, it returns an entirely new list with the element prepended to the contents of the argument list. \n\n Oh boy, now the Clojure people are really mad at me; because that’s not right either. And you’re probably thinking I’m crazy too because what fool would copy a whole list into another list just to prepend an element? \n\n OK, so, yes,  cons  actually  does  prepend the element onto the list, and does not copy the list into a new list. On the other hand, it does this in such a way that you can  pretend  that the input and output lists are completely different. For all intents and purposes, the output of  cons  is an entirely new list – even though it isn’t. \n\n Confused? That’s ok, it’s not actually that difficult to understand. Consider the list  [1 2 3] . If we implement that as a linked list, it would look like this:  1->2->3 . Now let’s say we cons a  0  on the front. That gives us  0->1->2->3 . Notice, however, that the old list still exists inside the new list. That’s the secret! The old list remains unchanged inside the new list. So we can pretend that  cons  returns a whole new list, leaving the old list unchanged. \n\n This is just a hint at something that all true functional languages do. They allow you to create what appear to be wholly new data structures, while preserving the old data structures unchanged; and they do so without copying. In functional circles, this is known as persistence; and the data structures that behave this way are known as persistent data structures. Such data structures maintain their history at all times. Nothing ever gets deleted or modified within them. However, they have many different “entry-points” each of which provides a different view of the data.Think of it like a source code control system. Though you delete and modify lines of code; nothing ever gets deleted or changed in the source code repository. The entire history is preserved. You just have many different entry points into the source code repository that allow you to see snapshots of that history. \n\n So, now, let’s go back to our program. We were looking at  integers-starting-at . \n\n    (defn integers-starting-at [n]\n      (cons n (lazy-seq (integers-starting-at (inc n)))))\n \n\n What is that  cons  prepending, and what is it prepending it to? The first time through,  n  is going to be  1 , and so the  cons  will return a list that starts with  1 . That makes perfect sense because our goal is to print the square of  1 . \n\n OK, but what is the  cons  prepending the  1  onto? Clearly the  1  is being prepended to some list that’s returned by  lazy-seq . \n\n Here’s where the magic begins. You see,  lazy-seq  is a function that returns a lazy sequence. A lazy sequence is just a list - but with a twist. Instead of a linked list of values like  1->2->3 , a lazy sequence is a value linked to a function:  1->f . That function, when it is called, returns the next value of the list, linked to another function:  2->f . And what function are those values linked to? Look close! It’s the argument of  lazy-seq . \n\n The argument of  lazy-seq  is a recursive call to  integers-starting-at  with an argument of  (inc n) . The  inc  function simply returns it’s argument incremented by  1 . \n\n So, what is a lazy sequence? A lazy sequence is a list who’s elements are calculated when they are needed, and not before. Every time you ask a lazy sequence for the next item on the list, it simply calls the function that the current item is linked to. \n\n Thus, a lazy sequence has no size. You can keep on asking for more and more elements endlessly; and yet those elements won’t be calculated until they are needed. \n\n The  map  function also returns a lazy sequence. Here’s a simple implementation: \n\n    (defn map [f l]\n      (if (empty? l)\n        []\n        (cons (f (first l)) (lazy-seq (map f (rest l))))))\n \n\n The  first  function simply returns the first element of it’s argument. The  rest  function simply returns the argument list minus it’s first element; i.e. the rest of the list. So  map  is just a simple recursive function that invokes the function  f  on each element of  l  creating a new lazy sequence of the results. \n\n One thing remains before we can put this all together: the  take  function. After all we’ve been through, this one is really very simple. \n\n    (defn take [n l]\n      (if (zero? n)\n        []\n        (cons (first l) (take (dec n) (rest l)))))\n \n\n As you can see, the  take  function returns a list composed of the first  n  elements of  l . \n\n Now, lets practice a little Referential Transparency, and evaluate (by hand) the value of: \n\n    (squint 2)\n \n\n First we replace  squint  with it’s definition: \n\n    (take 2 (squares-of (integers)))\n \n\n Next we replace  take  with it’s definition. This is a bit tricky because we have to recurse. \n\n    (if (zero? 2)\n      []\n      (cons (first (squares-of (integers)))\n            (if (zero? (dec 2))\n              []\n              (cons (first (rest (squares-of (integers))))\n                    (if (zero? (dec (dec 2)))\n                      []\n                      ...)))))\n \n\n I stopped at the third  if  statement because  (dec (dec 2))  is zero. Indeed, we know the status of each of those  if  statements, so we can eliminate them all. \n\n    (cons (first (squares-of (integers)))\n          (cons (first (rest (squares-of (integers))))\n                []))\n \n\n Clearly, given that there are two calls to  cons , this is going to return a list with two elements in it. We can do a little housekeeping by replacing  integers  with its definition. \n\n (cons (first (squares-of (integers-starting-at 1)))\n     (cons (first (rest (squares-of (integers-starting-at 1))))\n           []))\n \n\n Since the call to  (squares-of (integers-starting-at 1))  occurs twice, we’ll evaluate it once and then replace the calls with it’s value. We begin by replacing  squares-of : \n\n (map square (integers-starting-at 1)\n \n\n And then  integers-starting-at : \n\n (map square (cons 1 (lazy-seq (integers-starting-at 2)))\n \n\n Now let’s replace  map . Since  map  begins with  (if (empty? l) ...) , and since the  (cons 1...)  guarantees that the list won’t be empty, we can skip the  if  statement and just use the body. \n\n (cons (square (first (cons 1 (lazy-seq...)))) \n      (map square (rest (lazy-seq (integers-starting-at 2)))))\n \n\n The call to  first  can easily be reduced: \n\n (cons (square 1) \n      (map square (rest (lazy-seq (integers-starting-at 2)))))\n \n\n Now a bit more magic. The call to  rest  returns the “rest” of the list by invoking the function passed into  lazy-seq . \n\n (cons (square 1) \n      (map square (integers-starting-at 2)))\n \n\n We can repeat this analysis for  (map square (integers-starting-at 2)) : \n\n (cons (square 1)\n      (cons (square 2) \n            (map square (integers-starting-at 3)))\n \n\n And then we can reduce the  squares . \n\n (cons 1\n      (cons 4 \n            (map square (integers-starting-at 3)))\n \n\n We left our previous analysis of the entire function in the following state: \n\n (cons (first (squares-of (integers-starting-at 1)))\n     (cons (first (rest (squares-of (integers-starting-at 1))))\n           []))\n \n\n Now we can plug in our value for  (squares-of (integers-starting-at 1)) . \n\n (cons \n  (first (cons 1\n           (cons 4 \n             (map square (integers-starting-at 3))))\n  (cons \n    (first (rest (cons 1\n                       (cons 4 \n                       (map square (integers-starting-at 3)))))\n           []))\n \n\n That first  first  is easy to reduce.  (first (cons x l))  is simply  x ; so we’ll just ignore the second argument of the  cons . \n\n (cons \n  1           \n  (cons \n    (first (rest (cons 1\n                       (cons 4 \n                       (map square (integers-starting-at 3)))))\n           []))\n \n\n The  (first (rest...))  is easy to evaluate too. \n\n (cons \n  1           \n  (cons 4 []))\n \n\n And the result of that, of course, is  [1 4] . \n\n Did you notice what happened to that  (integers-starting-at 3) ? It never got evaluated. Why? Because the original  (take 2...)  only needed 2 values, so the third was never requested. \n\n And that leads to a powerful realization. Most of us have written loops that push data through the rest of the program. But  (take 25 (squares-of (integers)))  is a loop that  pulls  data out of the rest of the program. This is a profound difference, and something we’re going to spend a lot more time on later. \n\n By this time all the Clojure people, all the functional programming people, and all the math people are furious with me; because I’ve oversimplified so much. And it’s true; there’s a bunch of stuff I’ve glossed over and waved my hands about. Still, what I’ve told you is essentially correct. It is also a pretty good demonstration of the power of treating functions as data that can be passed into and returned from functions. \n\n And that brings us to the climax of this episode; for we can now answer the question posed by the title. We call this programming style functional because it’s all about treating functions as data elements that are manipulated no differently than integers or a characters. The functional programming people refer to that as functions being  first-class citizens . A functional language is a language that has functions as first-class citizens, enforces referential transparency by eliminating assignment, and maintains data history with persistent data structures. \n\n As we’ll learn in subsequent episodes, this definition is neither complete, nor fully accurate; but for the moment… it will do. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/01/07/FPBE3-Do-the-rules-change.html", "title": "FP Basics E3", "content": "\n       h4. Do all the Rules Change? \n\n Whenever we start to use a new paradigm, we confront the problem of our old rules and our old habits. We ask ourselves whether all those rules and habits are valid in the new paradigm. Consider, for example Test Driven Development. Is it valid in Functional Programming? If so, how do you do it? \n\n The best way to figure this out it to try it; so lets try writing a simple functional program:  Word Wrap . The requirements are simple. Given a string of words separated by single spaces, and given a desired line length, insert line-end characters into the string at appropriate points to ensure that no line is longer than the length. Words may be broken apart only if they are longer than the specified length. \n\n So, if we take the Gettysburg Address as a string: \n\n “Four score and seven years ago our fathers brought forth upon this continent a new nation conceived in liberty and dedicated to the proposition that all men are created equal” \n\n And we wrap it so that no line is longer than 10 characters, the result should be: \n\n “Four score\\nand seven\\nyears ago\\nour\\nfathers\\nbrought\\nforth upon\\nthis\\ncontinent\\na new\\nnation\\nconceived\\nin liberty\\nand\\ndedicated\\nto the\\npropositio\\nn that all\\nmen are\\ncreated\\nequal” \n\n We’ll use Brian Marick’s lovely test framework:  Midje , and begin by specifying the first test. \n\n (facts\n  (wrap \"\" 1) => \"\")\n \n\n This just says that it is a fact that the call  (wrap \"\" 1)  will return  \"\" . We can make this pass by defining the  wrap  function and giving it a degenerate implementation as usual: \n\n (defn wrap [s n]\n  \"\")\n \n\n The next test forces us away from the degenerate implementation, but not by much: \n\n (wrap \"x\" 1) => \"x\"\n\n(defn wrap [s n]\n  s)\n \n\n The next test forces us to split a word that’s too long: \n\n (wrap \"xx\" 1) => \"x\\nx\"\n\n(defn wrap [s n]\n  (if (<= (length s) n)\n    s\n    (str (subs s 0 n) \"\\n\" (subs s n))))\n \n\n The  if  statement in clojure is like the  ?:  operator in  C  and  java . It returns the first expression if the predicate is true; otherwise it returns the second expression. The  subs  method should be self-explanatory, it’s similar to the  subString  method in  Java . The  str  method simply takes all it’s arguments and concatenates them into a single string \n\n The next test forces us to iterate, which we do by recursing: \n\n (wrap \"xxx\" 1) => \"x\\nx\\nx\"\n\n(defn wrap [s n]\n  (if (<= (count s) n)\n    s\n    (str (subs s 0 n) \"\\n\" (wrap (subs s n) n))))\n \n\n The next test checks the case where the character after the line break is a space. We don’t want a space at the front of the next line, so we skip over that space. \n\n (wrap \"x x\" 1) => \"x\\nx\"\n\n(defn wrap [s n]\n  (if (<= (count s) n)\n    s\n    (let [trailing-space (= \\space (get s n))\n          new-line-start (if trailing-space (inc n) n)\n          head (subs s 0 n)\n          tail (subs s new-line-start)]\n      (str head \"\\n\" (wrap tail n)))))\n \n\n The  let  clause in clojure simply allows us to create local symbols that hold values that will be used in the body of the function. These are not variables, because their values do not vary. \n\n The  get  function returns the character at position  n  within the string. The  \\space , is a character constant in clojure, representing a space. \n\n The next test forces us to search backwards to find the last space before we have to break the line. If no such space is found, we have a word longer than  n . Otherwise we split the line at that space. That backwards search is done by the  .lastIndexOf  function which is a direct call into the  Java  string library. It’s the  .  that indicates the direct  java  call. \n\n (wrap \"x x\" 2) => \"x\\nx\"\n\n(defn wrap [s n]\n  (if (<= (count s) n)\n    s\n    (let [space-before-end (.lastIndexOf s \" \" n)\n          this-line-end (if (neg? space-before-end) \n                            n \n                            space-before-end)\n          trailing-space (= \\space (get s this-line-end))\n          new-line-start (if trailing-space \n                             (inc this-line-end) \n                             this-line-end)\n          head (subs s 0 this-line-end)\n          tail (subs s new-line-start)]\n      (str head \"\\n\" (wrap tail n)))))\n \n\n This has gotten a little ugly, so let’s refactor it a bit. \n\n (defn find-start-and-end [s n]\n  (let [space-before-end (.lastIndexOf s \" \" n)\n        line-end (if (neg? space-before-end) n space-before-end)\n        trailing-space (= \\space (get s line-end))\n        line-start (if trailing-space (inc line-end) line-end)]\n    [line-start line-end]))\n\n(defn wrap [s n]\n  (if (<= (count s) n)\n    s\n    (let [[start end] (find-start-and-end s n)\n          head (subs s 0 end)\n          tail (subs s start)]\n      (str head \"\\n\" (wrap tail n)))))\n \n\n You might not be convinced that those are all the test cases. If so, I encourage you to hunt for a new test case that will fail. Indeed, I encourage you to study those six simple test cases very carefully. Most people wouldn’t write them that way. It took me a lot of time with TDD before I realized the value of well-ordered, carefully considered, extremely minimal test cases. \n\n Anyway, it would appear that, at least in this case, the rules of TDD apply just as much to functional programming as they do to any other kind of programming. So, as you start to explore functional programming, don’t throw out the baby with the bathwater.  Keep your TDD discipline! \n\n Some folks out there might complain that this algorithm isn’t functional; but I assure you that it is. There are no side effects. It is representationally transparent. It uses persistent data structures. It’s functional. \n\n It does, however, lack one of the traits that many people associate with functional programs. It is not composed of a series of transformations. \n\n Consider the squares of integers program again: \n\n (take 25 (squares-of (integers)) \n\n The sheer elegance of this code comes partially from the fact that it is a series of transformations. The first transformation is the  integers  function. It transforms nothing into a lazy sequence of integers starting at 1. The second transformation is the  squares-of  function. It transforms any list of integers into a list of their squares. The final transformation is the  take  function, which transforms a lazy sequence of indefinite size into a sequence of exactly 25 elements. \n\n Can we rewrite the word wrap problem as a series of transformations? Sure. Consider this: \n\n (defn wrap [s n]\n  (join \"\\n\" \n        (make-lines-up-to n \n                          (break-long-words n (split s #\" \")))))\n \n\n We’ll read this the way we read the squares of integers program; from the inside out. First we split the input string into a sequence of words. Then we break any words longer than  n  into two or more words. Then we append those words to lines that are no longer than  n . Finally we join those lines together with  ‘\\n’ . Simple eh? \n\n Well, it looks simple when it’s written like that. But here’s the implementation of the whole program: \n\n (defn break-long-words [n words]\n  (if (empty? words)\n    []\n    (let [word (first words)]\n      (if (>= n (count word))\n        (cons word (break-long-words n (rest words)))\n        (cons (subs word 0 n) \n              (break-long-words \n                 n \n                 (cons (subs word n) (rest words))))))))\n\n(defn make-lines-up-to\n  ([n words]\n    (make-lines-up-to n words [] []))\n  ([n words line lines]\n    (if (empty? words)\n      (conj lines (join \" \" line))\n      (let [new-line (conj line (first words))]\n        (if (>= n (count (join \" \" new-line)))\n          (make-lines-up-to n (rest words) new-line lines)\n          (make-lines-up-to n \n                            words \n                            [] \n                            (conj lines (join \" \" line))))))))\n\n(defn wrap [s n]\n  (join \"\\n\" (make-lines-up-to \n                n \n                (break-long-words n (split s #\" \")))))\n \n\n There are plenty of folks out there who will eagerly tell you that I’m not the world’s best functional programmer. They are likely correct. And so this program may be woefully inadequate as a demonstration. Still, it’s hard for me to believe that my first solution to the word wrap is inferior to the above code. This doesn’t mean that sequences of transformations are bad. On the contrary, I think they are very powerful, and we’ll be studying them in much more detail later. My point is that not all functional programs need to be sequences of transformations. The old rule of “simpler is better” still applies. And for the word wrap algorithm, at least as far as I can see (which is admittedly not very far) a transformational solution is not best. \n\n By the way, I  did not  use TDD to create the transformational solution you see above. I suppose I could have; but instead I used the method that many functional programmers prefer. I thought about it for a long time, and then I used manual testing at the console (often called the  REPL  for: Read Evaluate Print Loop) to piece the whole thing together. The entire process took at least three times longer than the TDD approach, involved a  lot  of nasty debugging and print statements, and caused many infinite recursive loops that blew the stack and crashed my console. In short, it was a pain. I don’t recommend it. \n\n So, to wrap things up: the old rules still apply. Functional programming may be a change - an important change; but it doesn’t change  everything . Programming is still programming. Discipline is still discipline. And TDD works just as well in functional programming as in any other style of programming. There may be some rules that need to be discarded when you adopt functional programming; but so far I haven’t found any. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/01/29/FPBE4-Lazy-Evaluation.html", "title": "FP Basics E4", "content": "\n       Lazy Evaluation \n\n Remember my squares of integers program in clojure? \n\n ( take   25   ( squares-of   ( integers ))) \n\n Remember that I showed you this program in Java too? \n\n public   class   Squint   { \n   public    static    void    main ( String    args [])    { \n     for    ( int    i = 1 ;    i <= 25 ;    i ++) \n       System . out . println ( i * i );} \n } \n\n Well, that wasn’t a fair comparison.  Here’s a much fairer comparison program written in Java. \n\n take ( 25 ,   squaresOf ( integers ())); \n\n Notice that all I did was move the parentheses, add a comma, and use more conventional spelling. \n\n Here’s a slighly larger context: \n\n @Test \n public   void   squaresOfIntegers ()   throws   Exception   { \n   assertEquals ( asList ( 1 ,   4 ,   9 ,   16 ,   25 ),   squint ( 5 )); \n } \n \n private   List < Integer >   squint ( int   n )   { \n   return   take ( n ,   squaresOf ( integers ())); \n } \n\n Can you write this Java program?  I’ll give you a hint: It uses lazy evaluation just like the Clojure program does.  That’s right.  That  integers()  function will return as many integers as you require.  The  squaresOf  function accepts an “infinite” list of integers and returns an “infinite” list of their squares.  Indeed, the  squaresOf  function calls a function named  map  which maps the values of one list into values of another by calling a specified function on each. \n\n This squares of integers function, that I just wrote in Java, is a purely functional program.  (Well, almost.  I cheated in one place because it was convenient; but I needn’t’ve.) \n\n So how’d I do it?   What’s the secret?  If you haven’t figured it out, you’re going to kick yourself when I tell you; because the trick is a trick that java programmers use every single day, and probably dozens of times each day.  It’s the lazy evaluator that all Java programmers know and love.  It’s the humble, lovable, iterator. \n\n That’s right, Java programmers have been doing lazy evaluation of lists since the first days of Java.  Indeed, some of us were doing it back in the good ol’ C++ days before Java was even a gleam in Gosling’s eye.  You see, lazy evaluation is really all about the iterators. \n\n OK, so here’s another hint about the implementation.  Here’s how you create an infinite list of integers without taking up any time or space: \n\n public   class   Integers   implements   Iterable < Integer >   { \n   public   Iterator < Integer >   iterator ()   { \n     return   new   Iterator < Integer >()   { \n       private   int   i   =   1 ; \n\n       public   boolean   hasNext ()   { return   true ;} \n       public   Integer   next ()   { return   i ++;} \n       public   void   remove ()   {   } \n     }; \n   } \n\n   public   static   Iterator < Integer >   get ()   { \n     return   new   Integers (). iterator (); \n   } \n } \n\n Isn’t that cool?  You can loop through an infinite list of integers like this: \n\n for   ( Integer   i   :   new   Integers ())   { \n\t // make sure you put a break in here at some point! \n } \n\n And the  map  function was even easier.  Look at this: \n\n public   class   Mapper < T >   { \n   public   Iterator < T >   map ( final   Func < T >   func ,   final   Iterator < T >   xs )   { \n     return   new   Iterator < T >()   { \n       public   boolean   hasNext ()   { return   xs . hasNext ();} \n       public   T   next ()   { return   func . f ( xs . next ());} \n       public   void   remove ()   {} \n     }; \n   } \n } \n\n And that  Func  class is really simple.  We’ve always had a form of lambda in Java: \n\n public   interface   Func < T >   { \n   T   f ( T   x ); \n } \n\n I’m sure you can work out the rest of the program for yourself now.  Once you know the trick, it’s really not that hard.  But if you’d like to see my version of all the code, you can check out my  javasquint  repo. \n\n ####Woah there Nellie!\nFrom the above you might have gotten that idea that I was about to say that you can use Java as a functional language.  Uh… No.  Oh, little programs like  squint  are easy enough to make functional; but more interesting programs are harder.  You see the problem is that Java’s data structures are mutable, and are  designed  to be mutated.  So, in fact, writing functional programs in Java is quite difficult, and generally not practical. \n\n So then why did I show you the iterator trick?  To impress upon you the fact that there’s no magic or mystery about lazily evaluated sequences in Clojure or other functional languages.  They’re just using iterators like Java would. \n\n ####Walking XML Trees\nI’m sure you’ve written code that does a depth-first walk through XML documents.  Consider the following simple XML document: \n \n\t\n<alpha>\n\t<beta>\n\t\t<gamma> 1 </gamma>\n\t\t<gamma> 2 </gamma>\n\t</beta>\n\t<beta2>3</beta2>\n</alpha>\n\n \n\n If you walked this depth-first, you’d encounter the nodes in the following order  [alpha, beta, gamma, gamma, beta2] .  Indeed, you could consider the XML document to be a linear list of nodes in that order.   Indeed  you could easily construct an iterator that walked the XML nodes in that order and gave you the illusion that you were simply walking a linear list.   INDEED  if you integrated that interator into the XML document class you could do a depth-first search with the following code: \n\n for   ( XMLNode   node   :   xmlDocument )  \n   System . out . println ( node . toString ()); \n\n Now if you walked that XML document in that fashion, and printed each node as you encountered it as shown, then the printout might look something like this: \n \n\t\n<alpha><beta><gamma>1</gamma><gamma>2</gamma></beta><beta2>3</beta2></alpha>\n<beta><gamma>1</gamma><gamma>2</gamma></beta>\n<gamma>1</gamma>\n<gamma>2</gamma>\n<beta2>3</beta2>\n\n \n\n Now stand back and look at the code and the output.  Forget that you know that the iterator of XMLNode does a depth-first traversal.  Assume that it’s like any other iterator that just does a simply sequential walk.  It looks like your  xmlDocument  class is a list of nodes with an awful lot of duplication in it, doesn’t it?  You wouldn’t ever create a list that looks like that just to walk an XML document would you?  I mean, that would just waste a lot of storage – especially if the xml document was really big. \n\n So if you think about it by considering the way it  looks , it appears foolish.  Yet if you think about it from the point of view of using an iterator, it makes perfect sense.  It’s not wasteful at all, because that absurd list simply does not exist. \n\n ####Clojure\nSo now check this out.  I have a file with that XML in it.  It’s named  doc.xml .  I can read it into Clojure using the  parse  function from  clojure.xml , and I can pretty print it with  pprint : \n\n ( pprint   ( parse   \"doc.xml\" )) \n { :tag   :alpha, \n  :attrs   nil, \n  :content \n  [{ :tag   :beta, \n    :attrs   nil, \n    :content \n    [{ :tag   :gamma,   :attrs   nil,   :content   [ \"1\" ]} \n     { :tag   :gamma,   :attrs   nil,   :content   [ \"2\" ]}]} \n   { :tag   :beta2,   :attrs   nil,   :content   [ \"3\" ]}]} \n\n The  attrs  dictionaries are for the silly attributes that you can put into XML tags (who thought  that  was a good idea?).  The  parse  function converts the XML document into a pretty simple form.  It’s a dictionary of tags that have  content  elements that are lists of other dictionaries that contain tags.  A nice recursive tree structure. \n\n Doing a depth first traversal of that structure would not be too hard, but it’d be more complicated than walking a linear list.  However, take a look what happens when I put this data structure through the  xml-seq  function: \n\n ( pprint   ( xml-seq   ( parse   \"doc.xml\" ))) \n ({ :tag   :alpha, \n   :attrs   nil, \n   :content \n   [{ :tag   :beta, \n     :attrs   nil, \n     :content \n     [{ :tag   :gamma,   :attrs   nil,   :content   [ \"1\" ]} \n      { :tag   :gamma,   :attrs   nil,   :content   [ \"2\" ]}]} \n    { :tag   :beta2,   :attrs   nil,   :content   [ \"3\" ]}]} \n  { :tag   :beta, \n   :attrs   nil, \n   :content \n   [{ :tag   :gamma,   :attrs   nil,   :content   [ \"1\" ]} \n    { :tag   :gamma,   :attrs   nil,   :content   [ \"2\" ]}]} \n  { :tag   :gamma,   :attrs   nil,   :content   [ \"1\" ]} \n  \"1\" \n  { :tag   :gamma,   :attrs   nil,   :content   [ \"2\" ]} \n  \"2\" \n  { :tag   :beta2,   :attrs   nil,   :content   [ \"3\" ]} \n  \"3\" ) \n\n Look closely. See the opening parenthesis?  Aha!  This is a list!  It’s a list of all the nodes in a depth first order!  Oh, and there’s all that duplication again!  Ugh!  How wasteful.  How awful!  How foolish to create such a bloated, redundant data structure. \n\n Oh.  But wait.  xml-seq  returns a  lazy  sequence!  The nodes aren’t evaluated  until they are asked for .  The list of nodes  does not  take up extra space!  It’s just an efficient and convenient way to walk a depth-first traversal – rather like our Java iterator. \n\n ####Conclusion\nSo the moral of the story is simply this:  When you have lazy sequences at your disposal, you can design your data structures for convenience, without undo concern for time and space; because lazy sequences are really just a clever use of iterators. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/01/30/The-Craftsman-And-The-Laborer.html", "title": "The Laborer and the Craftsman", "content": "\n       In a recent  blog  Ted Neward made this remarkable statement: \n\n \n   I’m criticizing because this is what “software craftsmanship” gets us: an imposed segregation of those who “get it” from those who “don’t” based on somebody’s arbitrary criteria of what we should or shouldn’t be doing. And if somebody doesn’t use the “right” tools or code it in the “right” way, then bam! You clearly aren’t a “craftsman” (or “craftswoman”?) and you clearly don’t care about your craft and you clearly aren’t worth the time or energy necessary to support and nourish and grow and…. \n \n\n Now, to be fair, Ted was reacting to some unfortunate behavior that managed to hurt someone else’s feelings.  You’ll see this if you read Ted’s blog, and follow the links therein.  The long and short of it is that some folks who are identified with the Software Craftsmanship movement made some rude comments about some JavaScript code they found on GitHub. The author of the code read those comments and was hurt. \n\n (BTW, There is nothing wrong with politely pointing out what you believe to be deficiencies in someone else’s code.  You don’t want to be rude or condescending when you do this; but you  do  want to do it.  How else can we learn unless someone points out where we’ve gone wrong?  So please don’t let this event stop you from valid review and critique.) \n\n But did Ted draw the correct conclusion?  Does this unfortunate event show that the Software Craftsmanship movement is an excuse to create an elite upper-crust of programmers who look down upon the unwashed masses of mediocre programmers?  Or was this event nothing more than the kind of thoughtless chatter that  everyone  does when they are behind the wheel, protected by the personal barrier of the windscreen, and yelling at the driver in front of them?  (Guys!  Remember!  Twitter ain’t private!) \n\n I think it was much more the latter.  And I think I can prove that by going to the source document for the Software Craftsmanship movement:  The Manifesto for Software Craftsmanship . \n\n \n \n   \n     As aspiring Software Craftsmen we are raising the bar of professional software development by practicing it and helping others learn the craft. Through this work we have come to value: \n\n     \n       Not only working software,\n         \n           but also well-crafted software \n         \n       \n       Not only responding to change,\n         \n           but also steadily adding value \n         \n       \n       Not only individuals and interactions,\n         \n           but also a community of professionals \n         \n       \n       Not only customer collaboration,\n         \n           but also productive partnerships \n         \n       \n     \n   \n   \n     That is, in pursuit of the items on the left we have found the items on the right to be indispensable. \n   \n \n\n \n\n In this document I don’t see any imposed segregation; any intimation of those who “get it” vs. those who don’t; or any mention of the “right” tools or the “right” way.  Indeed, what I see instead is a desire to steadily add value by writing well-crafted software while working in a community of professionals who behave as partners with their customers. That doesn’t sound like “narcissistic, high-handed, high-minded” elitism to me. \n\n Ted then goes on to present a truly remarkable dichotomy: \n\n \n   I will now coin a term that I consider to be the opposite of “software craftsman”: the “software laborer”. \n \n\n Ted defines the “software laborer” as: \n\n \n   “somebody who comes in at 9, does what they’re told, leaves at 5, and never gives a rat’s ass about programming except for what they need to know to get their job done […] who [crank] out one crappy app after another in (what else?) Visual Basic, [that] were […] sloppy, bloated, ugly […] cut-and-paste cobbled-together duct-tape wonders.” \n \n\n Now let’s look past the hyperbole, and the populist jargon, and see if we can identify just who Ted is talking about.  Firstly, they work 9-5.  Secondly, they get their job done.  Thirdly, they crank out lots of (apparently useful) apps.  And finally, they make a mess in the code.  The implication is that they are not late, have no defects, and their projects never fail. \n\n I’ve never met these people.  In my experience a mess in the code equates to lots of overtime, deep schedule overruns, intolerable defect rates, and frequent project failure – not to mention eventual redesign. \n\n Ted has created a false dichotomy that appeals to a populist ideology.  There are the elite, condescending, self-proclaimed  craftsmen , and then there are the humble, honorable,  laborers .  Ted then declares his allegiance to the latter by saying: \n\n \n   I have respect for them. \n \n\n He strengthens his identity with, and affinity for, these laborers by telling a story about a tea master and a samurai (or was it some milk and a cow) which further extends and confuses the false dichotomy. At the end of the story he boldly declares: \n\n \n   My name is Ted Neward. And I bow with respect to the “software laborers” of the world, who churn out quality code without concern for “craftsmanship”, because their lives are more than just their code. \n \n\n So I’m confused.  Is Ted equating “sloppy, bloated, ugly, crappy VB apps cobbled-together with cut-and-paste and duct-tape” with “quality code”?  Or is this just another appeal to populism? \n\n I’m not a psychoanalyst; and I don’t really want to dive deep into Ted’s psyche to unravel the contradictions and false dichotomies in his blog.  However, I will make one observation.  In his blog Ted describes his own youthful arrogance as a C++ programmer this way: \n\n \n   In my younger days, believing myself to be one of those “craftsmen”, a developer who knew C++ in and out, who understood memory management and pointers, who could create elegant and useful solutions in templates and classes and inheritance, I turned up my nose at those “laborers” […]. My app was tight, lean, and well-tuned; […] a paragon of reused code […] a shining beacon on a hill for all the world to admire. \n \n\n It seems to me that Ted is equating his own youthful bad behavior with “craftsmanship”.  He ascribes his own past arrogance and self-superiority with an entire movement.  I find that very odd and very unfortunate.  I’m not at all sure what prompted him to make such a large and disconnected leap in reasoning.  While it is true that the Software Craftsmanship movement  is  trying to raise awareness about software quality; it is certainly  not  doing so by promoting the adolescent behavior that Ted now disavows. \n\n Ted’s dichotomy is false in the following way. \n\n \n   \n     Elitism is not encouraged in the Software Craftsmanship community.  Indeed we reject the elitist attitude altogether.  Our goal is not to make others feel bad about their code.  Our goal is to teach programmers how to write better code, and behave better as professionals.  We feel that the software industry urgently needs to raise the bar of professionalism. \n   \n   \n     There are few “laborers” who fit the mold that Ted describes.  While there are many 9-5 programmers, and many others who write cut-paste code, and still others who write big, ugly, bloated code, these aren’t always the same people.  I know lots of 12-12 programmers who work hellish hours, and write bloated, ugly, cut-paste code.  I also know many 9-5 programmers who write clean and elegant code.  I know 9-5ers who don’t give a rat’s ass, and I know 9-5ers who care deeply.  I know 12-12ers who’s only care is to climb the corporate ladder, and others who work long hours for the sheer joy of making something beautiful. \n   \n \n\n ####Conclusion\n There is no white-collar / blue-collar dichotomy!   We do not have two programming Americas.  The appeal to populism is badly misplaced. \n\n What we have, instead, is a  very young  industry.  Programming is barely 60 years old.  I, personally, have been programming for 43+ of those years.  When I started, the number of programmers could likely have been numbered in the thousands.  Nowadays it’s in the tens of millions.   What that means is that most programmers are  young ; and few have had the benefit of an experienced mentor.  Most have graduated from college with the idea that the experience prepared them to be a professional.  In most cases, it has not. \n\n The result is that most programmers simply don’t know where the quality bar is.  They don’t know what disciplines they should adopt.  They don’t know the difference between good and bad code.  And, most importantly, they have not learned that writing good clean code in a disciplined manner is the fastest and best way get the job done well. \n\n We, in the Software Craftsmanship movement are trying to teach those lessons.  Our goal is to raise the awareness that software quality matters.  That doing a good job means having pride in workmanship, being careful, deliberate, and disciplined.  That the best way to miss a deadline, and lay the seeds of defeat, is to make a mess. \n\n We, in the Software Craftsmanship movement are promoting software professionalism. \n\n This isn’t elitism, it’s realism – and it’s needed. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/02/01/The-Humble-Craftsman.html", "title": "The Humble Craftsman", "content": "\n       There is another side to Ted Neward’s  blog ; and it’s a side that I agree with.  I believe Ted’s overall thesis and analysis was wrong.  Software Craftsmanship does not automatically give us a blue-collar/white-collar dichotomy; it does not automatically separate those who “get it” from those who don’t.  It does not automatically create a condescending elite who lord their self-perceived superiority over the unwashed masses.  And, in case you hadn’t noticed, I  really  disliked Ted’s manipulative appeal to populism. \n\n However, Ted was not entirely wrong either.  Because there are folks (and, to my shame, I’ve sometimes numbered among them) who have behaved badly when pointing out problems or deficiencies in other peoples’ code.  To those people (and to myself) I’d like to make the following points. \n\n It takes a  lot  of courage to put code out on github for everyone to see.  It requires a willingness to be exposed, ridiculed, and belittled.  It shows a desire to share, and a hunger to learn.  It is possibly the most honorable and selfless act a programmer can make.  That act should not be rewarded by haughty condescension.  No one should point the their fingers and snicker with their buddies.  The honorable act of sharing should be respected, not denigrated. \n\n What does it take to be a craftsman?  It takes time.  It takes experience.  It takes mentoring.  And, it takes a  lot  of trial and error.  In our industry the best, and possibly the only, way to refine your skill is to make lots and lots of mistakes; and to learn from others who have made lots and lots of mistakes.  So  thank goodness  for those mistakes, and thank goodness for the people who made them.  Without them, we’d have learned nothing.  And  especially  thank the  people  who were willing to expose their mistakes to the world. \n\n Several years ago I stumbled upon some open source code that I thought was a good example of bad code.  It was written in 2002 by David Gregory.  I wrote to David, and asked if I could use his code as an example in my book “Clean Code”.  He graciously agreed.  His is one of the most courageous acts I’ve encountered. \n\n \n   Bob: “Excuse me, David, do you mind if I rip this code, that has your name all over it, to shreds in front of a million people?” \n \n\n \n   David: “Sure, Bob.  Go right ahead.” \n \n\n Would  you  have allowed that?  Would  you  have had the courage to let  Uncle Bob  point out every minuscule problem in your code in front of a huge audience of young and eager programmers?  Could you have withstood that negative review as, year after year, it was published over and over again, in country after country, and language after language?  Could you have tolerated being  the  person who wrote  the  example of bad code? \n\n I owe a lot to David, and so does everyone who read my book and learned anything from it.  Were it not for heroes like David, we could not advance the cause of craftsmanship at all. \n\n BTW, I should point out that David’s code was not really all that bad.  For Java, in 2002, it was considerably better than average.  When one writes a book about clean code, the only examples that make sense to use are those where the difference is small enough to be seen. \n\n When someone shares their code, the respectful and honorable thing to do is to carefully critique that code.   No one’s code is above criticism.   Criticism is, after all, how we learn.  Respectfully criticizing someone’s code is one of the highest honors you can pay to the author.  Just remember, you respect the  person , not the code.  The code is fair game. \n\n Reviewing and criticizing code is a balancing act.  To do it well requires a delicate combination of ruthlessness and humility.  You have to be able to say that certain things are just silly – even stupid. \n\n For example, this is stupid: \n\n /**\n * Default Constructor\n */ \n public   MyClass ()   {} \n\n Can I say that?  Can I say “stupid”.  Yeah, I can.  Because I’ve been there.  I’ve been stupid.  And I’ll be stupid again.  I’m the guy who wrote that code.  I’m the guy who wrote a 3,000 line C function named  gi  (which stood for Graphic Interpreter).  I’m the guy who wrote an 2,000 line O(n**3) algorithm for calculating the area of a thousand-sided polygon because I couldn’t be bothered to look up the lovely 30 line, O(n) solution.  I’m the guy who got fired, while my wife was pregnant with our first child, because I couldn’t be bothered to think about schedules that were  real  important to my employer.  Yeah.  Me.  The stupid one.  So I’m allowed.  I can use that word. \n\n And if  you  use that word, or any other adjective that means the same thing, just remember you are using a word that describes yourself; because the only way you can  know  what’s stupid, is to  have done  something that stupid in the past. \n\n And that’s a good way to describe a craftsman.  A craftsman is someone who has done some really stupid things and wants to avoid doing them in the future, and to help others to avoid doing them too. \n\n If you want to bear the title of “Craftsman”, then you must respect every person who shares their code; and show them the honor that they deserve.  You treat each sharing event as a courtesy paid to you; and return that courtesy  with courtesy .  This doesn’t mean the code is above criticism.  It just means that when you criticize, you do so with courtesy, respect, and a humble acknowledgement of your own failings. \n\n So if you see some bad code out there. There’s nothing wrong with pointing it out.  Indeed, you  should  point it out.  Just remember that the only reason you recognize it as bad code is because either you, or someone who has taught you, has written bad code like that in the past.  So be humble.  Acknowledge our shared stupidity.  Commiserate just how difficult writing good clean code is, and how easy it is to do stupid things without knowing it. \n\n And never, ever, EVER, point your finger and snicker with your buddies like a gaggle of gossipy highschoolers. \n\n A special thanks to Kelly Sommers ( @kellabyte ) for sending me the email that inspired this blog. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/03/06/ThePragmaticsOfTDD.html", "title": "The Pragmatics of TDD", "content": "\n       So my last blog:  The Startup Trap  raised quite a ruckus.  Amidst the various shouts of agreement and support, there was also a group who vehemently disagreed.  I’m not going to summarize all their disagreements here, since I’ve already used up my quota of curse words for the month.  But one of those disagreements struck me as something I should address. \n\n It’s the old argument of pragmatism vs. dogmatism.  In essence, the complaint was that I was being too dogmatic.  That TDD might be great in some cases, but in others it might have too high a cost.  So you have to be pragmatic and choose wisely. \n\n At first this sounds perfectly reasonable.  After all, pragmatism is a good thing, right?  I mean, if you knew that TDD was going to make you miss your deadline; and that you could make the deadline by dropping it; you’d drop it, right? \n\n Right.  No question.  And, in fact, there are times when that’s exactly the right course of action.  The purpose of this blog is to lay out those times when  I  think TDD is too expensive. \n\n But before I do I want to make a point.  TDD is a discipline for programmers like double-entry bookkeeping is for accountants or sterile procedure is for surgeons.  Are there times when accountants don’t use double-entry bookkeeping?  Are there times when surgeons don’t use sterile procedure? \n\n The answer is yes in both cases.  I rather doubt that accountants use double-entry bookkeeping when they balance their personal checkbooks, or when checking the total on a restaurant bill.  I could be proven wrong about the former, after all, I used double-entry bookkeeping for years when balancing my checkbook.  But I eventually grew to realize that the effort wasn’t worth the risk.  As for the latter, I think we’d all agree that double-entry bookkeeping is overkill for a restaurant bill. \n\n As for surgeons and sterile procedure: I had a lipoma removed from my arm several years ago.  My wife, an RN, observed the procedure.  It was done under local in the doctors office.  As the doctor was preparing I heard her question the doctor about the fact that he wasn’t using sterile procedure.  He replied that for an operation of this size “clean procedure” was adequate.  She and I accepted that statement; and the doctor completed the operation. \n\n A couple of days later, the incision became inflamed and painful.  One of the sutures had become infected, and they had to reopen the incision and clean it out.  I don’t know if this was because of “clean procedure” but from now on I will insist that the doctors who work on me use sterile procedure and not “clean procedure”. \n\n Still, the point is valid.  There are times when TDD is too costly, and a lower discipline should be used instead.  I hope my stories have convinced you that those times are rare corner cases, and that the pragmatism meme should not be used to thwart your disciplines just because they seem inconvenient. \n\n The Pragmatics \n\n So, when do I  not  practice TDD? \n\n \n   \n     I don’t write tests for getters and setters.  Doing so is usually foolish.  Those getters and setters  will  be indirectly tested by the tests of the other methods; so there’s no point in testing them directly. \n   \n   \n     I don’t write tests for member variables.  They too will be tested indirectly. \n   \n   \n     I don’t write tests for one line functions or functions that are obviously trivial.  Again, they’ll be tested indirectly. \n   \n   I don’t write tests for GUIs.  GUIs require  fiddling .  You have to nudge them into place by changing a font size here, an RGB value there, an XY position here, a field width there.  Doing that test-first is silly and wasteful.\n     \n       However, I make sure that any significant processing in the GUI code is removed into modules that are testable.  I don’t allow significant code to go untested.  So my GUI code is little more than glue and wires that pull data into place on the screen.  (See articles on MVVM or Model View Presenter) \n     \n   \n   In general I don’t write tests for any code that I have to “fiddle” into place by trial and error.  But I separate that “fiddling” code from code that I am surer of and can write tests for.\n     \n       I have, upon occasion, fiddled code into place and then written tests after the fact. \n       I have also, upon occasion, deleted the code once “fiddled” and re-written it test-first. \n       Which of these you choose is a judgement call. \n     \n   \n   A few months ago I wrote an entire 100 line program without any tests.  (GASP!)\n     \n       The program was a one-shot.  It was used once and then discarded it.  (It was for a special effect in one of my videos). \n       The program was all about the screen.  In essence it was a pure GUI app.  So I had to  fiddle  the whole thing into place. \n       I wrote it in Clojure, and so I had the REPL!  I could run the growing program from the REPL, and I could  see  the results of every line of code I wrote instantly.  It wasn’t TDD, it was EDD (Eye Driven Development). \n     \n   \n   I usually don’t write tests for frameworks, databases, web-servers, or other third-party software that is supposed to work.  I mock these things out, and test  my  code, not theirs.\n     \n       Of course I sometimes  do  test the third-party code if:\n         \n           I think it’s broken. \n           The results are fast and predictable enough that a Mock would be overkill. \n         \n       \n     \n   \n \n\n It’s not  all  Dogma. \n This list isn’t complete.  I’m sure I’ll think of some other times that I don’t write tests; but the spirit of this list ought to be evident.  Pragmatics  do  come into play when doing TDD; it’s not all dogma. \n\n However, I respect the dogma; there is a reason for it.  Pragmatics can sometimes override; but:  I will not write any significant production code without making every effort to use TDD. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/03/08/AnOpenAndClosedCase.html", "title": "An Open and Closed Case", "content": "\n       I awoke this morning to see a twitter conversation about the Open-Closed Principle.  The tweeter was complaining that it didn’t make a lot of sense.  He said things like: \n\n \n   “Presumably true OCP fans barely use version control, btw. Only reason to change a source file is for bug fixes, right?” \n \n\n When I read this, I had to scrape my eyebrows off the ceiling.  How could anyone come to that kind of conclusion?  So I dug deeper.  The original tweeter had apparently read  this article  that I wrote back in 1996.  As I re-read the article I began to understand why the original tweeter said what he said.  The article defines modules that are open and closed this way: \n\n \n   \n     They are “Open For Extension”.\nThis means that the behavior of the module can be extended. That we can make \nthe module behave in new and different ways as the requirements of the application change, or to meet the needs of new applications. \n     They are “Closed for Modiﬁcation”.\nThe source code of such a module is inviolate. No one is allowed to make source \ncode changes to it. \n   \n \n\n OK, as an isolated sound-bite, point two is a bit overstated.  I mean: “no one is  allowed …”? I’m not sure why I phrased it that way 17 years ago.  I was young and impressionable back then, a mere 43 years old.  So I can only chalk the stridence of that phrase up to my immaturity. \n\n In my defense, the article  does  go on to explain things in much less strident terms.  In particular it says that no significant program can be 100% closed. It talks about strategic closure, and closing modules against certain kinds of changes.  So the overall picture the article paints of the OCP is more moderate than that extreme sound-bite. \n\n So I opened up my book from 2003  Agile Software Development: Principles, Patterns and Practices  to see what I had written there.  I was glad to see that the intervening years had softened my tone: \n\n \n   \n     “Open for extension.”\nThis means that the behavior of the module can be extended.  As the requirements of the application change, we are able to extend the module with new behaviors that satisfy those changes.  In other words, we are able to change what the module does. \n     “Closed for modification.”\nExtending the behavior of a module does not result in changes to the source or binary code of the module.  The binary executable version of the module, whether in a linkable library, a DLL, or a Java .jar, remains untouched. \n   \n \n\n In my next book,  UML for Java Programmers  I changed my tone completely: \n\n \n   This principle has a high-falutin’ definition, but a simple meaning:  You should be able to change the environment surrounding a module without changing the module itself. \n \n\n In my  Clean Coders Video Series , I devoted episode 10 to a very detailed exposition of this principle.  In that video I referred back to Bertrand Meyer, the creator of the Open Closed Principle, and I paraphrased Meyer’s definition as: \n\n \n   So Meyer wants it to be easy to change the behavior of a module, without having to change the source code of that module! \n \n\n And that’s really the essence of the OCP.   It should be easy  to change the behavior of a module without changing the source code of that module.  This doesn’t mean you will never change the source code, this doesn’t mean you can stop using your version control systems (sheesh!).  What it means is that you should strive to get your code into a position such that, when behavior changes in expected ways, you don’t have to make sweeping changes to all the modules of the system.  Ideally, you will be able to add the new behavior by adding new code, and changing little or no old code. \n\n As I told the original tweeter, the OCP is a  Mom and Apple Pie  principle.  I can’t see why anybody would, or could, disagree with it.  But when I look back at my old writings, I can at least see why someone who was skimming and looking at the sound-bites might walk away confused. \n\n There are two morals to this story: \n\n \n   \n     When you write an article remember that people often skim; so extreme sound-bites used for rhetorical emphasis can wind up creating false impressions. \n   \n   \n     When you skim an article, remember that often the meat of the article is in the text that you have bypassed, so keep your conclusions tentative until you’ve had a chance to dig deeper. \n   \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/02/10/ThePrinciplesOfCraftsmanship.html", "title": "The Principles of Craftsmanship", "content": "\n       It should come as no surprise to anyone that I think 8th Light Inc. is a remarkable company.   The company was founded, and is run, by two of my previous apprentices; one of whom is my son Micah, and the other is Paul Pagel.   From the start they based their ideals on those of software craftsmanship.  They wanted to build a community of professionals who  believed in their core  that the best way to build software was to build it well.  Now, with the better part of a decade behind them, they are one of the most successful software consulting businesses in Chicago; and will soon be able to make that claim about Tampa, Florida. \n\n The company is unique in many ways.  For example: Their employees are called craftsmen, and the recruiting model is based upon apprenticeship.  Craftsmen are not hired in the usual sense that an employee is hired.  Rather, they are brought on as apprentices and gradually inducted into the company through a months long intricate ritual of training, mentoring, challenge, and accomplishment.  It’s a tough gauntlet to walk; and those few who make it through know they have achieved something difficult and important. \n\n Two years ago, at a company meeting, the founders presented a slide deck that they planned to show to customers to communicate just how unique the company was; and how they could partner with their clients to help them succeed.  One of the bullets on one of the slides said:  “We are principled.”  The slide referred to their way of doing business, and their way of writing software.  Those words now appear on the  8th Light website . \n\n After the meeting I said to the founders: “It’s easy to say you are principled; but can you  articulate  what your principles are?” \n\n Thus began an effort that has taken almost two years to complete.  Virtually every craftsman at 8th Light has been engaged in the effort to discuss, debate, codify, and ratify those principles.  The document grew, then shrank, then grew and shrank again.  At times it languished; and at other times it was energetically engaged.  But it was never allowed to sputter and die. \n\n The result is quite amazing. It is simple, brief, and powerful. It has been crafted to be almost a poem based on the Software Craftsmanship Manifesto.  Each of the four conclusions of the manifesto is clarified and amplified by three principles of behavior.  Each of these principles is a complimentary pair of statements.  The first statement in each pair describes what a craftsman  will  do; the second: what a craftsman  will not . \n\n Here  are the principles of Software Craftsmanship according to 8th Light. \n\n \n\n At 8th Light, we are principled. These are things that we will and will not do.  Each principle ties to a specific value in the Manifesto for Software Craftsmanship \n\n ###Well-Crafted Software \n\n \n   We humbly demonstrate our expertise by delivering quality software.\n     \n       We do not inflate our abilities or claim expertise where we have none. \n     \n   \n   We continually master a variety of technologies and techniques.\n     \n       We do not let unfamiliarity dissuade us from using the best tools. \n     \n   \n   We take responsibility for the correctness of our code by testing it thoroughly.\n     \n       We do not tolerate preventable defects. \n     \n   \n \n\n ###Steadily Adding Value \n \n   We estimate with diligence.\n     \n       We do not let fear or pressure make us promise what we can’t deliver. \n     \n   \n   We always apply our best efforts to complete our work.\n     \n       We do not make excuses. \n     \n   \n   We work at a sustainable pace.\n     \n       We do not burn out. \n     \n   \n \n\n ###A Community of Professionals \n \n   We embrace differences of opinion and personality.\n     \n       We do not allow our current practice to impede improvement. \n     \n   \n   We prefer open source tools that we can inspect, evaluate, and improve.\n     \n       We avoid proprietary products that lack transparency. \n     \n   \n   We teach anyone with a willingness to learn.\n     \n       We do not hoard our knowledge or practices. \n     \n   \n \n\n ###Productive Partnerships \n \n   We show respect for our customers and fellow craftsmen.\n     \n       We do not act unprofessionally or unethically. \n     \n   \n   We communicate our progress honestly and openly with our customers.\n     \n       We do not conceal or embellish. \n     \n   \n   We partner with our customers to understand their business.\n     \n       We do not propose solutions until we are sure we have found the right problem. \n     \n   \n \n\n \n Over the next several weeks you will see, on  the 8th Light blog site , all twelve of these principles revealed, exposed, and expounded.  We, the craftsmen at 8th Light, will endeavor to explain what these principles mean to us, and why we have chosen them as ours. \n\n ##### We at 8th Light are principled; and  these  are the principles we follow. \n\n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/03/05/TheStartUpTrap.html", "title": "The Start-Up Trap", "content": "\n       \n   You  have joined a new startup. \n   You  are a multi-talented mega-being. \n   You  can work 60, 70, 80 hours per week to get the job done. \n   You  are a  top-notch  coder and designer. \n   You  won’t fall into the traps that others have fallen into. \n   You  will make sure that  this  time will be different. \n   You  are  so  good that the rules don’t apply to  you . \n   You  are  fucked . \n \n\n ###The Start-Up Trap.\nIt’s a sad story that we’ve seen over and over again.  A young programmer begins with all the best intensions, learns all the right disciplines, develops all the right skills, and then falls prey to  The Start-Up Trap .   The Start-Up Trap is the idea that  your  situation is different – that everything you’ve learned about how to do software well somehow doesn’t apply to  this  particular job.  You think it will apply later, once you’ve succeeded.  But not now.  Not yet.  Not while you are in a race to succeed. \n\n The Start-Up trap is the thought that the start-up phase is  different ; and that while you are in that phase success depends upon  breaking  the rules.  This is  stupid .  The start-up phase is  not  different.  The start-up phase is simply the first of many phases, and it sets the tone for all those other phases.  Come back to that company in five years and (if they’ve managed to survive) they’ll still have the same attitude towards the rules that they had in the first phase – except, perhaps, for the overtime. (giggle). \n\n Here’s a little tip:  The disciplines that lead to successful software are always valid, no matter what phase the company is in.  It is laughable to think that good disciplines are less important during the start-up phase.  The truth is that, during the start-up phase, those disciplines are just as critical as they are at any other time. \n\n Of course one of the disciplines I’m talking about is TDD.  Anybody who thinks they can go faster by  not  writing tests is smoking some pretty serious shit.  Oh, I know you are a warrior-god.  I know you can write the code perfectly every time.  I know that the deadline looms and you  just don’t have time to write tests .  – I’m sorry for your impending failures.  I’m sorry that you’re going slow and just don’t know it yet.  And I’m  very  sorry that when you finally brute-force your way to some modicum of success that you will credit your bad behavior, and recommend it to others.  God help us all, because you won’t. \n\n Ask yourself this:  How does the accounting officer of a start-up behave?  This person is responsible for managing the money of the investors.  Do you think that accountant has deadlines?  Do you think he’s under pressure to deliver projections, forecasts, cash-flow reports, etc?  Do you think his bosses tolerate schedule slips in his duties?  I’ll tell you now that the guy managing the investors’ money is under a hell of a lot more pressure than any software developer is. \n\n So how does this accountant behave?  Does he double check his work?  Does he practice double-entry bookkeeping?  Does he follow all his rules and disciplines?  Or are the rules different because he’s in the start-up phase? \n\n What if it was  your  company, and  your  money.  What would  you  think of a start-up accountant who didn’t check his sums; who neglected the debit side of the books and trusted the health and future of  your  company to the single unchecked sums of the credit side? \n\n You’d fire his ass!  You’d fire it so fast that the rest of his worthless carcass would be left outside the door wondering where his ass went! \n\n Is your code somehow less important than that account’s spreadsheets?  Are errors in the code somehow more tolerable than errors in those spreadsheets?  Can errors in the code take the company down and ruin it’s reputation with it’s customers, and investors?  You know the answer to these questions.  And you know this:  If accountants can find a way to practice their disciplines in a start-up;  so can you. \n\n Is neglecting TDD going to help you go fast?  To quote Captain Sulu when the Klingon power moon of Praxis exploded and a young Lieutenant asked whether they should notify Star-Fleet:  “Are you kidding?”  ARE YOU KIDDING? \n\n NO, you aren’t going to go fast.  You’re going to go  slow .  And the reasons are simple, and you already know them.  You’re going to go slow because you won’t be able to refactor.  The code will rot – quickly.  It will get harder and harder to manage.  And  you will slow down. \n\n You won’t notice it at first because it still  feels  fast.  You are working hard and spending 60, 70, 80 hours per week on the code.  The sheer effort you are applying is enormous; and that  feels  fast. \n\n But effort and speed are not related.  It is easy to expend a tremendous amount of effort and make no progress at all.   Hell , it’s easy to expend gargantuan effort and make  negative  progress.   Effort equates neither to speed nor direction. \n\n As time passes your estimates will grow.  You’ll find it harder and harder to add new features.  You will find more and more bugs accumulating.  You’ll start to parse the bugs into critical and acceptable (as if any bug is acceptable!)  You’ll create modules that are so fragile you won’t trust yourself, or anyone else, to modify them; so you’ll work around them.  You’ll build a festering pile of code that, with every passing week, requires more and more effort just to keep running. Forward progress will slow and falter.  It may even reverse as each release becomes buggier and buggier, and less and less stable.  Catastrophes will become more and more common as errors, that should never have happened, create corruptions and damage that take huge traunches of time to repair. \n\n You  know  the story.  You  know  this is where others have wound up.  If you are old enough,  you  have probably wound up there once or twice yourself.  And yet that Start-Up Trap still sings it’s siren song and lures you into destructive, slow, catastrophic behaviors. \n\n If you want to go  fast .  If you want the best chance of making all your deadlines.  If you want the  best  chance of success.  Then I can give you no better advice than this:   Follow your disciplines!   Write your tests.  Refactor your code.  Keep things simple and clean.   Do Not Rush!   You hold the life-blood of your start-up in your hands.   Don’t be careless with it! \n\n Remember:  The only way to go fast, is to go well. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/03/11/TheFrenziedPanicOfRushing.html", "title": "The Frenzied Panic of Rushing", "content": "\n       Last week I wrote a blog entitled  The Startup Trap  in which I lamented the unfortunate tendency of developers in a startup to cast their disciplines aside in order to maintain the “high” generated by the illusion that effort is speed.  I specifically mentioned TDD as one of those disciplines that startup developers sometimes eschew. \n\n The response in support of the blog was gratifying.  The response in denial (Yes, that’s the right word) of the blog was expected; though I was somewhat surprised by the vehemence and amplitude of the various pejoratives, barbs, insults, and other ad hominem statements.  I suppose that people who are addicted to a “high” do not take kindly to those who challenge their ability to justify and maintain that “high”. \n\n One of the most common threads, amongst the more reasonable of the complaints, was this: \n\n \n   In a startup, the business model is more important than the code. \n \n\n This is correct;  but the initial predicate is superfluous.  The business model is  always  more important than the code.  Great code, that doesn’t make or save money, isn’t very interesting to businesses. \n\n The business model argument, correct though it may be, is irrelevant to my original point.  Coding disciplines, including TDD, do not impede the discovery of the business model.  Or to say this more succinctly,  TDD does not slow you down. \n\n Indeed, all of the complaints against my blog boiled down to the same invalid assumption: TDD is too slow. \n\n That’s  BS .  At least it’s BS for me.  TDD does not make me slower, it makes me faster.  It makes others I know faster too.  I presume that it could make everyone faster. \n\n I’m not going to explore the reasons for that speedup here.  There are many other articles, books, and blogs that try to explain it.  For now I’ll just fall back on the old saws:  Slow and steady wins the race.   Anything worth doing is worth doing well.   Sat ci sat bene. (It’s done fast enough when it’s done well.)   Or to quote Brian Marick, “In code, it never pays to rush.” \n\n As an example of one of these strange arguments about the business model, consider  Greg Young’s response .  This is a nice story about a business opportunity, the two week effort to make a Minimum Viable Product (MVP), and the subsquent 9 months of effort to re-do that code “right”.  His point was that the code was profitable after two weeks, and scarcely any more profitable after the nine month effort; so the nine months was a waste. \n\n Of course he’s right.  The two weeks were what counted.  The nine months were a giant pud-pull.  Those nine months would have been better spent on something more profitable. \n\n The logical flaw in Greg’s argument is the presumption that it would have taken him longer than two weeks to get the MVP done if he’d been using good disciplines.  I reject that presumption entirely.  I believe Greg could have gotten that MVP done in two weeks or less using good disciplines.  Moreover, the resultant code would likely have been far more amenable to being improved by another two or three weeks of reasonable effort. \n\n Now let me say this more directly.  Good disciplines don’t slow you down.  Indeed  a discipline that slows you down is not a good discipline.   TDD is a good discipline. \n\n TDD does not take a two week MVP effort and turn it into nine months of auto-stimulation.  Nor does refactoring.  Nor does SOLID.  Nor does pair programming.  Nor does continuous integration.  Nor do any of the other good disciplines out there. \n\n Getting done right does not mean getting done slow.   Getting done right means getting done fast.  You will go faster if you do things right.  You will go faster if you come down off the “high” generated by the illusion that effort is speed.  You will go faster if you calm down, follow your disciplines, and refuse to rush. \n\n Story Time \n\n Greg told the story of his two week MVP.  So now let me tell you the story of Langton’s ant. \n\n The year was 2002 or thereabouts.  James Grenning, I, and half a dozen Thoughtworkers were at an airport in New York waiting for a delayed flight back to Chicago.  We’d been attending one of the early XP Universe conferences.  James and I were in our early fifties.  The TWers were all in their late twenties or early thirties. \n\n In that conference, Kent Beck gave the closing keynote on the topic of emergent behavior.  As he spoke the screen behind him showed a series of black and white pixels crawling around in a random way.  At the end of his talk, as he made his final point about emergent behavior, the random nature of the pixels suddenly congealed into a repeating pattern that started at the center of the screen and crawled its way to the upper right corner.  Kent said the algorithm was:   Langton’s Ant , a simple cellular automaton with just two rules: \n\n \n   \n     At a white square, turn 90° right, flip the color of the square, move forward one unit \n     At a black square, turn 90° left, flip the color of the square, move forward one unit \n   \n \n\n James and I had just ordered Gin and Tonics from the airport bar and returned to the waiting area, when we saw a group of the TWers frantically working on their laptops.  We asked someone what was going on, and were told that they were racing to see who could implement Langton’s Ant quickest. \n\n James and I looked at each other, nodded, opened a laptop, and quietly began to pair-program. We followed our disciplines, wrote test-first, kept the code clean, got it working after a few minutes, and then closed the laptop.  We looked up, and the young twenty-thirty-something TWers were still expending copious energies in a fit of frantic coding. The frenzied intensity of their effort, driven by the panic of potential loss, was evident in their gestures and expressions. We, the two fifty-something programmers, smiled, clinked our Gin and Tonics, and sat back to enjoy the show being put on by all those young turks who never even guessed that they had been calmly bested by two of their seniors who had the experience and wisdom to know that the only way to go fast is to go well. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/03/22/There-are-ladies-present.html", "title": "There are Ladies Present", "content": "\n       Oh!  I think I  get it ! \n\n I’m likely to take some heat for what I’m about to write; but I think I finally understand something that’s been bothering me. \n\n Three times in the last few years I’ve been confronted by women who have been offended by something I said during a talk, or a class. \n\n The first time was at a Rails conference a few years ago.  I gave a talk entitled “What killed Smalltalk could kill Ruby”.  In that talk I referred to C++ as a  testosterone  language, and I referred to Java as an  estrogen  language.  I meant no harm by this statement.  I was mostly making a joke about how seriously C++ programmers take themselves compared to Java programmers.  However, some of the women in the audience were offended, and tweeted about their concern.  I quickly, and publicly, apologized because I  do not  want the women in our industry to feel unwelcome.  I think we need them.  More on that later… \n\n Though I apologized, I was also rather puzzled.  My “Tim - the tool man - Taylor” persona was going: “eeeuuuhhh?”  What was it that was offensive about that testosterone/estrogen remark?  I didn’t know it, but I had already answered that question.  I just didn’t get it – then. \n\n A few years later I was giving a talk at ACCU in Oxford.  The name of the talk was “Requiem for C”.  I started talking about COBOL.  And that caused the standard photo image of Admiral Grace Hopper, the inventor of COBOL, to pop into my mind.  In the photo she’s standing tall in full dress uniform; including a navy cap.  Out of my mouth came the words “in that cute little cap”.  I wanted to grab the words back, but… \n\n After the talk Emily Bache came up to me and gently suggested that the statement was not helpful to all the women in the audience.  She reminded me that it’s difficult enough to be a woman programmer without someone like me making jokes about other women.  I completely agreed, and published yet another apology.  I published it because  I want  women in this industry.  I think we  need  them.  More on this later… \n\n Just this week I was teaching a Clean Code course in London.  I had 30 students, four of whom were women.  I was very pleased with that because it’s among the highest ratio of women I’ve had in class. The class went very well, and after three days I dismissed them all and read their evaluations. \n\n One of the women said she really enjoyed the class and learned a lot, but then wrote “somewhat male oriented, which made me feel out of place”.  Tim Taylor: “Eeeuuuhhh?”  What had I done? Even now I don’t know what remark I may have made, or what off hand statement might have slipped.  I don’t know what her trigger was.  But, because of something else that happened this week, I think I get it. \n\n The event I’m referring to is the fiasco at PyCon involving Adria Richards, Alex Reid, Playhaven, and SendGrid.  If you aren’t aware of it, there’s a reasonable history in  Amanda Blum’s blog .  I’m not going to weigh in on the debate about who was right or wrong in this mess, other than to say that the real villains are those jerkwads on the net executing DDOS attacks, and tweeting all manner of obscenities, personal attacks, and illegal threats. \n\n So what is it that I get now?  Well, let me tell you a few stories: \n\n \n   When I was in 8th grade, a new kid came to our school.  He had been kicked out of his last school in a “tougher” part of the district, so he was an instant hero.  A real “hood”.  He would greet you for the first time by giving you a big smile with sparkling eyes while enticingly pointing his thumbs to his crotch.  As you inevitably looked down to follow his pointing gesture he’s say: “Bagged ya!”.  Ha, ha, ha.  Hee, hee, hee. Ho, ho, ho.  Nothing brightens the day like teenage boy humor. \n \n\n \n\n \n   Twenty years ago I worked as a chief software architect at a network management startup.  I was in the mens room one day and two guys in business suits sidled up to the urinals.  As they finished one said to the other: “Careful how you shake the dew off that lilly!”  As if on cue the other responded: “Any more that two shakes and you’re playing with it!”.  Ha, ha, ha.  Hee, hee, hee.  Ho, ho, ho.  Nothing brightens the day like thirty-something urinal humor. \n \n\n \n\n \n   I was watching ER one night a few years back and saw one beautiful young female doctor tell another beautiful young female doctor that she had dropped her recent boyfriend because he was “orally challenged — he wouldn’t go south of the 32nd parallel — he didn’t want to eat at the Y.”  Ha, ha, ha. Hee, hee, hee.  Ho, ho, ho.  Nothing brightens the day like the writers of a once great show succumbing to the temptation to use teenage urinal humor. \n \n\n \n\n \n   My wife has become a fan of Grey’s Anatomy. I’ve watched a few of the shows and don’t get the attraction. It’s all about farting, cheating, screwing on every accommodating surface, pinning panties to bulletin boards, and having your ex-boyfriend teach you how to please your new lesbian lover. Ha, ha, ha. Hee, hee, hee.  Ho, ho, ho. Nothing brightens the day like 8 year-old potty humor. \n \n\n \n\n All these things have one thing in common.  Locker room humor. I found it all offensive to my intelligence, offensive to my values, and just plain offensive –  because of the context .  These things did not belong in the school, in the public men’s room, or on TV in  my  living room.  If they belonged anywhere, they belonged in a locker room. \n\n Have we created a locker room environment in the software industry?  Has it been male dominated for so long that we’ve turned it into a place where men relax and tell fart and dick jokes amongst themselves to tickle their pre-pubescent personas?  When we male programmers are together, do we feel like we’re in a private place where we can drop the rules, pretenses, and manners? \n\n What if the roles were reversed?  What if women had dominated the software industry for years, and we men were the ones who were struggling to break into the industry?  Men, can you imagine how hard it would be if all the women were constantly, and openly, talking about tampons, cramps, yeast infections, cheating, being cheated on, Trichomoniasis, faking-it, etc?  I don’t know about you, but It would make me feel out of place.  And there’d be no place to escape it, because the women would be  everywhere .  I’d want them to save that kind of talk for the ladies room.  I’d want them to remember that  men were present. \n\n Men, perhaps we need to remember that  there are ladies present . \n\n Now that last phrase is going to get me into trouble because too many injustices have been enacted under the excuse that  ladies were present . I learned about this from a movie called  Dangerous Beauty .  The story was of a beautiful young  courtesan  in 16th century Venice.  The important men of the city would take her on all their political and business outings, allowing her to know what their ventures were all about.  These men kept their wives entirely in the dark, because you didn’t discuss things like that  when ladies were present .  So, to find out what was really happening to their husbands and their city, they consulted the  courtesan ; and they used a phrase that has stuck with me ever since.  They said they were consigned to a life of  perpetual inconsequence . \n\n Perpetual Inconsequence .  That sounds like prison to me.  That sounds like  hell  to me.  If that’s what being treated like a lady means, then to hell with that. \n\n I think this explains another experience I had at the Midwestern Ruby conference a couple of years ago.  Before I gave the closing keynote on Architecture, I was in the audience listening to a young man give a very interesting talk about GitHub.  But the talk bothered me greatly because there was an f-bomb in every sentence.  It was effing this and effing that and what the ef here and there and everywhere.  Then to top it off he talked about all the code he had written while being “hammered”. \n\n Then someone got up and gave another talk and mentioned that he was happy there were so many women in the audience, and that we needed more.  (He’s right…  More on that later…). \n\n Then it was my turn.  I started the talk by saying: “If we want more women to come to these events, and to participate in our industry, we need to be careful about dropping F-bombs.”  Several of the men in the room applauded.  But one young woman stood up and yelled: “ I totally fucking disagree! ” \n\n Tim Taylor: “Eeeuuuhh?” \n\n But now I think I get it.  If I may paraphrase: \n\n “Don’t treat me differently in a professional context, because I won’t stand for perpetual inconsequence.  If you drop f bombs in a professional context, don’t stop just because I’m a girl.  But do remember that I’m a woman and there are things that men do that I don’t care to listen to or be part of.  Save that for the men’s room because it’s making me crazy when you do it in front of me.” \n\n Now, to pay off my debt. \n\n I had the pleasure of spending an hour with Desi McAdam (@desi) one evening at a function where she happened to be working.  I told her that, to me, getting a program to work was like a conquest – killing the beast and brining home the meat.  She replied that to her, it was more of a growing, nurturing experience of helping the software come into the world.  Tim Taylor: “Eeeuuuhhh?” \n\n But I’ve had some time to think about this.  When I think about code, I think about it in terms of killing bugs, molding the code into the shape I demand.  I am the master.  It is my slave.  I have the power to make it be what I want.  It’s a huge power trip for me. \n\n I wonder if that isn’t part of the craftsmanship and professionalism problem that our industry has.  Why is so much of our software so bad?  Is it because we treat it as a conquest?  Is it because we don’t nurture it enough? \n\n Don’t get me wrong, for me it’ll always be a conquest.  I’ll always be the master of my code.  That’s who I am.  But perhaps that’s not enough to turn our programming industry into a real profession.  Maybe we need the other attitude as well. \n\n After all, in my personal life, I know that the worst decisions I have ever made are the ones I’ve made without my wife. My wife completes me in many different ways.  Without her I’d be a shadow of a man walking through a two-dimensional space.  My relationship with her transformed me from a boy into a man.  She helped me to grow up. \n\n Our industry needs to grow up too.  Perhaps the reason we’ve had so much trouble maturing is that we don’t have enough women to help us make the transition from child to adult.  Their attitudes, their ways of thinking, different from ours, may be the very things we need to complete us as an industry, and a  profession ,  and as a craft .  It seems to me we need these women in order for our profession to  become  a profession. \n\n So we need to make the women feel welcome.  We need to stop considering that the whole of the software industry is our private locker room.  We need to treat women programmers as professional peers, while also remembering that there are ladies present. \n\n Of course I’m just Tim Taylor, talking over the fence to Wilson.  Women, do I have this right? \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/05/27/FibTPP.html", "title": "Fib. The T-P Premise.", "content": "\n       \n\t \n\t\t \n\t\t\t \n\t\t\t\t February 2 2011 \n\t\t\t \n\t\t \n\t\t \n\t\t\t \n\t\t\t\tGuilherme Silveira wrote a lovely  blog  exploring the  Transformation Priority Premise  using the Fibonacci sequence. He posed a suite of tests similar to these:\n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n @Test \n public   void  fibTest()  throws   Exception  {\n    assertEquals( 0 , of( 0 ));\n    assertEquals( 1 , of( 1 ));\n    assertEquals( 1 , of( 2 ));\n    assertEquals( 2 , of( 3 ));\n    assertEquals( 3 , of( 4 ));\n    assertEquals( 5 , of( 5 ));\n    assertEquals( 8 , of( 6 ));\n}\n \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\tHe found that the proposed list of transformations did not lead him to a good solution. At first he found that the transformations led him to a solution like this:\n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n switch (n) {\n     case   0 :  return   0 ;\n     case   1 :  return   1 ;\n     case   2 :  return   1 ;\n     case   3 :  return   2 ;\n    ...\n}\n \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\tObviously this is the wrong approach, but the priority list presented in my original article did not prevent it. So I’ve added the  (case)  transformation to the very bottom of the list.  This means that using a switch/case or an ‘else if’ is always the last option to choose. \n\t\t\t \n\t\t\t \n\t\t\t\tGuilherme went on to rightly ignore the switch/case solution to see if he could get a good solution for fib by following the other priorities. I suggest you read his blog to see how that turned out. Meanwhile, let’s try it here.\n\t\t\t \n\t\t\t \n\t\t\t\tThe first test leads us to use the  ({}–>nil)  and then the  (nil->constant)  transformations:\n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n public   class   Fibonacci  {\n   public   static   int  of( int  n) {\n     return   0 ;\n  }\n}\n \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\tThe second test forces an  (unconditional->if)  transformation that we can refactor with a  (constant->scalar) . This coincidentally makes the third test pass which is always nice.\n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n public   static   int  of( int  n) {\n   if  (n <= 1 )\n     return  n;\n   return   1 ;\n}\n \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\tThe fourth tests is tricky. How can we transform that ‘1’ into something that maps 1->1, 2->1, and 3->2. We know that fib(n) = fib(n-1)+fib(n-2) so we could use recursion to solve the problem. That’s the  (statement->recursion)  transformation.\n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n public   static   int  of( int  n) {\n   if  (n <= 1 )\n     return  n;\n   return  of(n- 1 ) + of(n- 2 );\n}\n \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\tThis makes all the tests pass. Hallelujah! And look how simple that was! What a pretty sight.\n\t\t\t \n\t\t\t \n\t\t\t\tUnfortunately there are three things wrong with this pretty solution. First, that algorithm has a horrific runtime complexity of something like O(n 2)  or worse. Secondly, the algorithm does not use tail-recursion, and so uses a lot of stack space. Thirdly, Java is not a great language for recursion anyway since the JVM simply  can’t  optimize tail recursion (yet).\n\t\t\t \n\t\t\t \n\t\t\t\tIt’s a great shame that such a simple expression has so many problems! There are  ways  to address that, but they are beyond the scope of this article. For now we’ll focus on the three problems mentioned above.\n\t\t\t \n\t\t\t \n\t\t\t\tLet’s tackle them one at a time. Is there a transformation that will at least get us to tail recursion? Of course there is, but it was missing from my original list. So I’ve modified that list as follows:\n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t ({}–>nil)  no code at all->code that employs nil\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (nil->constant) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (constant->constant+)  a simple constant to a more complex constant\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (constant->scalar)  replacing a constant with a variable or an argument\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (statement->statements)  adding more unconditional statements.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (unconditional->if)  splitting the execution path\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (scalar->array) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (array->container) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (statement->tail-recursion) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (if->while) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (statement->recursion) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (expression->function)  replacing an expression with a function or algorithm\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (variable->assignment)  replacing the value of a variable.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (case)  adding a case (or else) to an existing switch or if\n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\tSo tail recursion is preferred over arbitrary recursion.\n\t\t\t \n\t\t\t \n\t\t\t\tNow, can we use tail recursion to tranform this?\n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n public   static   int  of( int  n) {\n   if  (n <= 1 )\n     return  n;\n   return   1 ;\n}\n \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\tOf course we can. It’s not as pretty as the previous solution, but it captures the same semantics. And it’s not ugly by any means.\n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n public   class   Fibonacci  {\n   public   static   int  of( int  n) {\n     if  (n <= 1 )\n       return  n;\n     return  of( 0 , 1 ,n);\n  }\n\n   private   static   int  of( int  a,  int  b,  int  n) {\n     if  (n ==  0 )\n       return  a;\n     return  of(b, a+b, n- 1 );\n  }\n}\n \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\tOf course we can clean this up by removing the redundant ‘if’.\n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n public   class   Fibonacci  {\n   public   static   int  of( int  n) {\n     return  of( 0 , 1 ,n);\n  }\n\n   private   static   int  of( int  a,  int  b,  int  n) {\n     if  (n ==  0 )\n       return  a;\n     return  of(b, a+b, n- 1 );\n  }\n}\n \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\tBut now, how do we deal with the fact that Java doesn’t do well with recursion? If we thought that n would always stay relatively small, we could just ignore it. But let’s assume that ‘n’ will be large; forcing us to unwind the recursion and replace it with iteration. This requires a  (if->while)  and a few  (variable->assignment)  transformations.\n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n public   class   Fibonacci  {\n   public   static   int  of( int  n) {\n     return  of( 0 , 1 ,n);\n  }\n\n   private   static   int  of( int  a,  int  b,  int  n) {\n     while  (n !=  0 ) {\n       int  s = a+b;\n      a = b;\n      b = s;\n      n--;\n    }\n     return  a;\n  }\n}\n \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\tThe list of priorities prevents this from being the direct outcome of TDD because it prefers the recursive solution. So my list of proposed priorities will necessarily create Java programs that are recursive, and therefore less than optimal for the language.\n\t\t\t \n\t\t\t \n\t\t\t\tThat makes me think that the priority list is language specific. In Java, for example, we might move  (if->while)  and  (variable->assignment)   above   (statement->tail-recursion)  so that iteration is always preferred above recursion, and assignment is preferred above parameter passing.\n\t\t\t \n\t\t\t \n\t\t\t\tThis makes sense because Java is not a functional language, and strongly resists a functional style. So any bias towards functional style will lead to suboptimal implementations.\n\t\t\t \n\t\t\t \n\t\t\t\tIf the priority list is language specific, is it also application specific? Do different problem domains require different priority lists? I strongly doubt this. We are working at a level of detail so far below the problem domain that it is hard for me to see how different problems would require different solution styles.\n\t\t\t \n\t\t\t \n\t\t\t\tWhat about teams? Will teams tweak the priority list to match their styles? I hope not; but I have to say that I think this is not improbable.\n\t\t\t \n\t\t\t \n\t\t\t\tI think what this shows us is that the transformations and their priorities are a way to encode a particular programming style. So long as we have different languages and styles, we’ll likely need different transformations and priorities.\n\t\t\t \n\t\t\t \n\t\t\t\tOn the other hand, if we compare the Java list with the Clojure list (say), the difference is subtle. The recursive transformations would move  slightly  lower in the list relative to the iterative and assignment transformations. The effect is, of course, profound, but the difference in the lists is actually relatively small. All the other transformations seem to hold their positions.\n\t\t\t \n\t\t\t \n\t\t\t\tSo the good news is that, although there may be different styles based on language type, the vast majority of the low level coding decisions remain similar irrespective of those styles.\n\t\t\t \n\t\t \n\t \n \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/05/27/TransformationPriorityAndSorting.html", "title": "Transformation Priority and Sorting", "content": "\n       January  1 2011 \n \n\tIn this post we explore the  Transformation Priority Premise  in the context of building a sort algorithm.\n\t \n\tWe also explore comic books as a pedagogical tool.\n\t \n \n \n \n \n \n \n \n\n\n Comic created with Comic Life from plasq -  http://plasq.com \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/09/23/Test-first.html", "title": "Test First", "content": "\n       I first heard the term “Test First” in 1998.  Back then it was part of the phrase “Test First Design”.  We often shortened it to “Test First”.  Later, Kent Beck, the originator of the concept, changed the name to Test Driven Development; and it has gone by the acronym TDD ever since. But the words “Test First” have a connotation that the words “Test Driven Development” don’t.  That connotation is deeper than most people suspect.  What does “Test First” really mean? \n\n ###Tests are Specs.\nTo answer that we need to look more deeply at our tests.  What are they –  really ? \n\n Users of the RSpec test framework don’t call them tests.  They call them specs.  Why?  Because, in the language of RSpec, the tests read like specifications.  For those of you who’ve never seen an RSpec test, you don’t know what you are missing.  Here’s a simple example: \n\n describe Stack do\n  context \"Upon Creation\" do\n    let(:stack) {Stack.new}\n    it \"will be empty\" do\n      stack.should be_empty\n    end\n    it \"will raise underflow if popped\" do\n      lambda {stack.pop}.should raise_error(Underflow)\n    end\n  end\nend\n \n\n \n\n \n\n Frameworks, like RSpec, that allow you to write tests in a spec-like format are helpful, but not really necessary.  We can easily write spec-like tests in JUnit. \n\n @Test\npublic void empty_stack_will_be_empty() {\n  assertTrue(stack.isEmpty());\n}\n\n@Test(expected=Stack.Underflow.class)\npublic void empty_stack_will_throw_Underflow_when_popped() {\n  stack.pop();\n}\n \n\n \n\n \n\n Indeed, so long as we keep our tests short, well factored, and well named, they ought to read very nicely.  They ought to read like specifications; because they  are  specifications. \n\n One of the goals of TDD is to be able to trust your test suite to the extent that, if it passes, you know you can ship!  If you trust your tests that much, then those tests must describe everything that the system does.  And if the tests describe everything; then the tests are specs. \n\n ###Rotten Tests\nI’m sure you’ve seen tests that don’t look anything like specs.  Perhaps they looked like this (you don’t really have to read this): \n\n public void testJsonResponse() throws Exception {    \n  WikiPage page = WikiPageUtil.addPage(root, PathParser.parse(\"PageOne\"));\n  PageData data = page.getData();\n  data.setContent(\"some content\");\n  WikiPageProperties properties = data.getProperties();\n  properties.set(\"Test\", \"true\");\n  page.commit(data);\n\n  MockRequest request = new MockRequest();\n  request.setResource(\"PageOne\");\n  request.addInput(\"format\", \"json\");\n\n  Responder responder = new PropertiesResponder();\n  SimpleResponse response = (SimpleResponse) responder.makeResponse(context, request);\n  assertEquals(\"text/json\", response.getContentType());\n  String jsonText = response.getContent();\n  JSONObject jsonObject = new JSONObject(jsonText);\n  assertTrue(jsonObject.getBoolean(\"Test\"));\n  assertTrue(jsonObject.getBoolean(\"Search\"));\n  assertTrue(jsonObject.getBoolean(\"Edit\"));\n  assertTrue(jsonObject.getBoolean(\"Properties\"));\n  assertTrue(jsonObject.getBoolean(\"Versions\"));\n  assertTrue(jsonObject.getBoolean(\"Refactor\"));\n  assertTrue(jsonObject.getBoolean(\"WhereUsed\"));\n  assertTrue(jsonObject.getBoolean(\"RecentChanges\"));\n\n  assertFalse(jsonObject.getBoolean(\"Suite\"));\n  assertFalse(jsonObject.getBoolean(\"Prune\"));\n  assertFalse(jsonObject.getBoolean(PageData.PropertySECURE_READ));\n  assertFalse(jsonObject.getBoolean(PageData.PropertySECURE_WRITE));\n  assertFalse(jsonObject.getBoolean(PageData.PropertySECURE_TEST));\n}\n \n\n \n\n \n\n Now that’s a mess.  I’m sure you can figure it out if you try; but why should you have to try when a little refactoring changes that test to this: \n\n public void testJsonResponseForProperties() throws Exception {\n  createTestPage();\n  SimpleResponse jsonResponse = requestPagePropertiesInJason();\n  assertJsonResponseHasDefaultProperties(jsonResponse);\n}\n \n\n \n\n \n\n Now  that’s  a spec! \n\n It doesn’t take much to get tests to read well; just a little refactoring.  And yet we often forget to refactor our tests.  We treat them as some kind of second-class citizen in our systems.  We let them rot because we think our effort is better spent on the production code.  We think that time spent cleaning tests is wasted. \n\n But, of course, that’s nonsense. \n\n ###Tests are part of the system. \n\n We all know that bad code slows us down.  So why do we write it?  Sometimes we write it because we’re in a hurry.  We say to ourselves we’ll go back and clean it later; but we know that’s a lie.  We know we won’t go back and clean the bad code because we know we’ll be afraid of breaking it.  So we simply allow it to persist and grow.  The more it grows, the more it slows us down.  The more it slows us down the more we rush, and the more bad code we write, and the slower we go. \n\n This is the never ending downward spiral that so many systems are trapped in. \n\n The key to breaking this spiral, is to clean the code.  The key to cleaning the code is to have a test suite that you trust with your life; because then you won’t be afraid to clean the code.  You’ll clean a bit of it, and that’ll make you, and the team, go a little bit faster.  With that extra time you’ll clean more, and you’ll go faster still.  This is the upward spiral of cleaning code.  That upward spiral is enabled by tests. \n\n What this means is that your ability to quickly clean, maintain, and modify your system depends critically upon having a good test suite.  Without that test suite your system rots, and the team slows down to near zero.  With that test suite the team can quickly repair and eliminate bad code, and therefore continue to make rapid and regular process.  With that test suite, they won’t slow down! \n\n The tests enable the team to go fast and the system to stay healthy.  Therefore those tests are an integral, and critical, part of the system.  As such, they deserve to be maintained to the same level of quality as the production code.  Indeed, perhaps the tests deserve even more attention than the production code since the quality of the production code depends on the tests; but the quality of the tests does not depend on the production code. \n\n ###Asymmetry \n\n That last statement was asymmetrical.  The quality of the production code depends on the tests; but the quality of the tests is  independent  of the production code.  The reason for this is that there is an asymmetry in the way the two execute.  The tests are a program that verifies that the system works as specified.  But the system is  not  a program that verifies that the tests execute correctly.  You run the tests in order to refactor the system; but you  don’t  run the system in order to refactor the tests.  You refactor the tests by  running the tests! \n\n There’s another asymmetry that’s even more interesting.  You can (and do) create the system from the tests,  but you can’t create the tests from the system . \n\n Consider a comprehensive suite of tests that fully describes a system.  Those tests are individual units that don’t depend upon each other.  Each one is a statement of behavior that is independent of the other behaviors in the system, and of the other tests in the system.  If I gave you such a comprehensive suite of tests you could make those tests pass, one test at a time.  When you were done, you’d’ve written the whole system.  The tests specify the system. \n\n However, if I give you a system without tests, it’s virtually impossible to create the test suite that fully specifies that system.  The production code is a set of interdependent components that have complex and emergent behaviors.  Trying to understand and specify all the implications of those interdependencies, and those behaviors, is very difficult indeed. \n\n So TDD is a trap-door function.  It’s easy to go from tests to production code, but hard to go the other direction.  And that implies something fascinating about the tests:   The tests are the most important component in the system.   They are more important than the production code. \n\n ###The Choice \n\n I know this sounds ridiculous; but consider.  If somehow all your production code got deleted, but you had a backup of your tests, then you’d be able to recreate the production system with a little work.  Indeed, you’d also enjoy the benefit of  The Second System Effect .  The code would be better because it was the second time you’d’ve written it.  So, in the end, you’d wind up with a fully functional and better designed system. \n\n If, however, it was your tests that got deleted, then you’d have no tests to keep the production code clean.  The production code would inevitably rot, slowing you down.  The quality of the code, and the productivity of the team would be caught in the downward spiral towards zero – and there’s nothing they could do to stop it other than trying recreate the test suite.  And, as we noted, recreating the test suite is very hard indeed. \n\n If we lose the production code, we end up with a better designed system that stays clean because it has tests.  If we lose the tests, then the production code rots and the team slows down in a never ending spiral of lost productivity. \n\n So we can conclude that if it became a choice between the tests or the production code, we’d rather preserve the tests.  And this means that the tests are a more important component of the system than the production code is.  Because the tests are the specs. \n\n ###Simple Design \n\n Years ago Ron Jeffries codified Kent Beck’s rules of simple design.  They are, in order of priority: \n\n \n   All the tests pass. \n   There is no duplication \n   The code expresses the intent of the programmer. \n   Classes, and methods are minimized. \n \n\n Over the years we have used this as a guide for writing our code.  Indeed, Kent would often say: \n\n \n   First make it work, then make it right, then make it small and fast. \n \n\n Given these guidelines we’d write a failing test and then focus on getting that test to pass.  Then we’d refactor to remove duplication and clean up the code.  Indeed, we felt the license to make a mess in the production code as we strove to get it to work.  Then we’d immediately clean that mess once the tests passed. \n\n I’ve used Ron’s rules for a long time, and I’ve grown to trust them.  In the production code it is always better to first make it work, and then clean it up.  However, I think this order is dead wrong for the tests. \n\n ###Tests are first in all things. \n\n We have no need to write messy tests.  Indeed, tests can be written cleanly at first.  To prove this to yourself, simply say the three magic words:  Given ,  When , and  Then .  Before you write a test, you should be able to describe the test you are about to write using those three words. \n\n Given that I have an empty stack\nWhen I push something on it.\nThen it will have a size of 1.\n \n\n \n\n \n\n And once you’ve got those three words, you can turn them into three statements.  It might be as simple as this: \n\n Stack stack = makeEmptyStack();\nstack.push(0);\nassertThat(stack.size(), equalTo(1));\n \n\n \n\n \n\n Or consider that test we refactored earlier: \n\n createTestPage();\nSimpleResponse jsonResponse = requestPagePropertiesInJason();\nassertJsonResponseHasDefaultProperties(jsonResponse);\n \n\n \n\n \n\n It turns out that you can  always  reduce a test down to three statements.   No test ever needs to have more than three lines!   What’s more, you know what those three lines are before you begin writing the test! \n\n Oh, that doesn’t mean that some tests don’t have complicated setups and assertions; many do.  It just means that all that complexity can be extracted from the test into utility functions, leaving behind the three critical statements: Given, When, and Then.  And we can do this extraction before we get the production code to work. \n\n So, when we are writing tests, we can invert Kent’s advice. \n\n \n   First make the test right.  Then make the test work. \n \n\n We can change Ron’s rules for Simple Design, into rules for Simple Tests: \n\n \n   The test expresses the intent of the programmer \n   The test passes. \n   The test has no duplication. \n   The test has a minimum of classes and methods. \n \n\n In the TDD cycle, this means that we first write a portion of a test; and before we make it pass, we clean it up to ensure that it says what we mean it to say.  We get that tiny portion of the test to express our intent; and  then  we make it pass. \n\n The red-green-refactor cycle becomes Red -> Clean Test -> Green -> Refactor. \n\n ###Being First. \n\n The bottom line of all this is that we should consider our tests as  being  first.  We already know we should write them first; but we should also clean them first, maintain them first, think of them first, and keep them first.  We should give our tests the highest priority. \n\n That  is what “Test First” really means.  The Tests Come First! \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/05/27/TheTransformationPriorityPremise.html", "title": "The Transformation Priority Premise", "content": "\n      \t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t December 19 2010 \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis blog poses a rather radical premise. It suggests that Refactorings have counterparts called  Transformations . Refactorings are simple operations that change the structure of code without changing it’s behavior.  Transformations  are simple operations that change the behavior of code. Transformations can be used as the sole means for passing the currently failing test in the  red/green/refactor  cycle.  Transformations  have a priority, or a preferred ordering, which if maintained, by the ordering of the tests, will prevent impasses, or long outages in the  red/green/refactor  cycle.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t “As the tests get more specific, the code gets more generic.” \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tRecently this mantra has taken on a new meaning for me.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tI invented it as rule to prevent my TDD students from acquiring the nasty habit of writing production code that mirrored the tests:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  primeFactorsOfFour() {\n  assertEquals(asList(),    PrimeFactors.of( 1 ));\n  assertEquals(asList( 2 ),   PrimeFactors.of( 2 ));\n  assertEquals(asList( 3 ),   PrimeFactors.of( 3 ));\n  assertEquals(asList( 2 , 2 ), PrimeFactors.of( 4 ));\n  ...\n}\n\n public   class   PrimeFactors  {\n   public   static  of( int  n) {\n     if  (n ==  1 )\n       return  asList();\n     else   if  (n ==  2 )\n       return  asList( 2 );\n     else   if  (n ==  3 )\n       return  asList( 3 );\n     else   if  (n ==  4 )\n       return  asList( 2 , 2 );\n    ...\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tNewcomers to TDD often question why TDD does not lead this kind of code. I answer with the above rule. This explanation usually satisfies the students, especially when I demonstrate the idea with the  Prime Factors Kata .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tPrime Factors\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tI invented that Kata ten years ago when my son, Justin, came home from school with homework. He had to calculate the prime factors of several integers. I told him to do his homework, and that I would write a program that allowed him to check his work. (He’d have to enter his answer, and the program would simply tell him whether he was right or wrong).\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tI sat at my kitchen table and, using this new discipline named  TDD , I wrote the algorithm in Ruby. It was one of those eye-opening events for me. As I went from test to test the algorithm assembled itself in a completely unexpected way. I was astounded that I could make the 3 case pass by changing just one character in the code from a ‘2’ to an ‘n’. I was thrilled when the 8 case was solved by changing the word ‘if’ to the word ‘while’. I could  feel  that there was something profound about that, but I couldn’t put my finger on exactly what it was. But now I think I know.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tBrainlessness\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tOver the years I have written lots and lots of tests. What’s more I’ve executed the various Kata many hundreds of times. From time to time I make subtle improvements in a kata. I refine the tests or the code to make them smoother, simpler, and more elegant. With all that repetition and refinement I’ve begun to notice something. It has to do with another complaint that people have about TDD:  the brainlessness .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tConsider how the  Bowling Game Kata  begins with the test for the gutter game:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  gutterGame() {\n   for  ( int  i= 0 ; i< 20 ; i++)\n    game.roll( 0 );\n  assertEquals( 0 , game.score());\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWhen we teach TDD was ask:  “How should we make that pass?”  Newbies are often confused by the question because they are expecting that they have to write the bowling algorithm. But we surprise them by making it pass this way:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   int  score() {\n   return   0 ;\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tAt this point the programmers in the class roll their eyes and groan. They clearly think this is dumb and are frustrated that I would be telling them to write code that is clearly  wrong .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tI used to go along with the gag. I used to agree with them that this was brainless, and that we were just deferring decisions until we have more information. I told them that it was also a good way to test the tests, since by returning zero we can clearly see that the test is passing and that therefore the test is correct.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tA Sequence of Transformations.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWhat I have begun to discover is that returning zero  is not nearly so brainless as it looks . Not when you put it in the appropriate context.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWhen we use TDD, our production code goes through a sequence of transformations. I used to think it was a transformation from stupid to intelligent. But I’ve begun to see that this is not the case at all. Rather, the code goes through a sequence of transformation  from specific to generic .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tReturning zero from the  score  function is a specific case. But the case is in the correct form. It is an integer, and it has the right value. Therefore the  shape  of the algorithm is correct, it’s just hasn’t been generalized yet.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe next test in the bowling game is:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  allOnes() {\n   for  ( int  i= 0 ; i< 20 ; i++)\n    game.roll( 1 );\n  assertEquals( 20 , game.score());\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWe make this pass by adding up all the pins in the  roll  function and storing the sum in a variable named  score . Then we change the  score  function to return that value:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   int  score() {\n   return  score;\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tNotice that we have transformed the constant  0  into the variable  score . The algorithm has the same shape as before, (i.e. it returns an  int ) but it now has a more generic implementation. Why is this a more generic implementation? Because  a variable is a generalization of a constant .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIn other words, the transformation that has taken place is a simple alteration of some part of the solution from a more specific form, to a more generic form!\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tI used to think that this was merely interesting. I was titillated by the fact  sometimes  you could perform these simple transformations from specific to generic. Lately I’ve begun to suspect that it is a  rule , that  every  change to the code is either a behavior changing transformation from specific to generic, or a refactoring. Indeed, I think this rule may provide some guidance in choosing the next test to write, and in the manner in which the production code should be implemented in order to pass that test.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tBut let’s not get ahead of ourselves. What about the next test in the Bowling Game?\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  oneSpare() {\n  game.roll( 5 );\n  game.roll( 5 );  // spare \n  game.roll( 3 );\n  rollMany( 20 , 0 );\n  assertEquals( 16 , g.score());\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis test forces us to abandon the simple implementation of  score  for a much more complex one. The instance variable  score  which was updated in the  roll  function is removed and the  score  function computes the score from an array of rolls.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tOnce again we have transformed a specific implementation (an instance variable that holds a pre-computed score) to a more general form (a loop that computes the score from an array).\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tAnother common transformation can be seen in the prime factors kata where, in order to get the  2  case to pass, we insert an  if  statement into the implementation. The code transforms from\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n List  factors =  new   ArrayList ();\n return  factors;\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tto\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n List  factors =  new   ArrayList ();\n if  (n> 1 )\n  factors.add( 2 );\n return  factors;\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIn this case we are making the code more general by conditionally splitting the execution into two paths. One path makes all the old tests pass, and the new path makes the new test pass.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe prime factors kata is interesting because that transformation happens again in the  4  case where an if statement is added to handle the case where the input variable is divisible by 2;\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n List  factors =  new   ArrayList ();\n if  (n> 1 ) {\n   if  (n% 2  ==  0 ) {\n    factors.add( 2 );\n    n %=  2 ;\n  }\n   if  (n> 1 )\n    factors.add(n);\n}\n return  factors;\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe new pathway handles the the  4  case by detecting that 4 is divisible by 2, adding 2 to  factors , and adjusting  n  so that the paths can rejoin.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tMore interesting still is that at the  8  case, the inner  if  statement is transformed into a  while  statement. And then for the  9  case the outer  if  is transformed into a  while . Clearly  while  is a general form of  if .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe Transformations\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tSo what are these transformations? Perhaps we can make a list of them:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t ({}–>nil)  no code at all->code that employs nil\n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (nil->constant) \n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (constant->constant+)  a simple constant to a more complex constant\n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (constant->scalar)  replacing a constant with a variable or an argument\n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (statement->statements)  adding more unconditional statements.\n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (unconditional->if)  splitting the execution path\n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (scalar->array) \n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (array->container) \n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (statement->recursion) \n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (if->while) \n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (expression->function)  replacing an expression with a function or algorithm\n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t (variable->assignment)  replacing the value of a variable.\n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThere are likely others.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tPerhaps you noticed the resemblance that these transformations have to refactorings. However refactorings are used to transform the  structure  of code without altering its behavior. These transformations are used in order to change the  behavior  of code. In particular, we use these transformations to make failing tests pass.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIt should be clear that each of the transformations has a direction. They all transform the behavior of the code from something specific to something more generic. In some cases it is a constant being transformed into a variable, or a variable being transformed into an array. In others it is an  if  statement being transformed into a  while  loop, or a simple sequence getting transformed into recursion.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIt should also be clear that I have  roughly  ordered the transformations by their complexity. That is, the transformations at the top of the list are simpler, and less risky, than the transformations that are lower in the list.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe Priority Premise\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tSo the thing that has piqued my interest lately is the idea that transformations on the top of the list should be preferred to those that are lower. It is better (or simpler) to change a constant into a variable than it is to add an  if  statement. So when making a test pass, you try to do so with transformations that are simpler (higher on the list) than those that are more complex.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWhat’s more, when you pose a test, you try to pose one that allows simpler transformations rather than complex transformations; since the more complexity required by the test the larger the risk you take to get that test to pass.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe Impasse Problem\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIt was the  word wrap kata  that got me thinking about this. The kata starts out simply, but you quickly face a dilemma. There is one sequence of tests and implementation choices that forces you into an impasse, where there is no way to get the next text to pass without rewriting the whole algorithm. A different sequence of tests allows the algorithm to come together in the stepwise fashion that TDDers prefer. How can you choose the right sequence?\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis is a relatively common problem faced by TDDers. We pose a test only to find that we don’t know how to solve it without changing a large amount of code. The more code we change, the longer it will be before we get back to green; and the  red/green/refactor  cycle breaks down.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tMy premise is that if you choose the tests and implementations that employ transformations that are higher on the list, you will avoid the impasse.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tCase Study: Word Wrap.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tSo let’s walk through the reasoning. First we’ll execute the word wrap kata and choose the path that leads to the impasse. Then we’ll do it again, but take the path that does not. In each case we’ll show the transformations.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe first test in the word wrap kata is pretty obviously the degenerate case. Note that this employs the very first transformation  ({}–>nil) :\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  WrapNullReturnsEmptyString()  throws   Exception  {\n  assertThat(wrap( null ,  10 ), is( \" \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tAs we write this test, we also write the  failing  implementation which also employs  ({}–>nil) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   static   String  wrap( String  s,  int  length) {\n   return   null ;\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWe can make it pass with  (nil->constant) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   static   String  wrap( String  s,  int  length) {\n   return   \" \" ;\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe next test is the empty string case. Notice that this is just  (nil->constant)  applied to the first test. This test passes without any modification to the implementation. I always take that as an indication that things are going well.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  WrapEmptyStringReturnsEmptyString()  throws   Exception  {\n  assertThat(wrap( \" \" ,  10 ), is( \" \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe next test employs  (constant->constant+) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  OneShortWordDoesNotWrap()  throws   Exception  {\n  assertThat(wrap( \" word \" ,  5 ), is( \" word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tMaking this test pass forces us to use  (unconditional->if)  as well as  (constant->scalar) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   static   String  wrap( String  s,  int  length) {\n   if  (s ==  null )\n     return   \" \" ;\n   return  s;\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe Impasse\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tAt this point, if we were paying heed to the priority premise, we might wonder whether this was a wise step. After all,  (unconditional->if)  is pretty far down the list. But in this case I’m going to ignore the priority premise so that I can show you the impasse.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe next test once again employs  (constant->constant+) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  TwoWordsLongerThanLimitShouldWrap()  throws   Exception  {\n  assertThat(wrap( \" word word \" ,  6 ), is( \" word \\n word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWe can make this pass by using  (expression->function) .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   static   String  wrap( String  s,  int  length) {\n   if  (s ==  null )\n     return   \" \" ;\n   return  s.replaceAll( \"   \" ,  \" \\n \" );\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis is one of those moves that feels  clever . We justify it by saying that we are doing the simplest thing that would work. But given the priority premise, this is no longer all that simple. The  (expression->function)  transformation is down at the bottom of the list.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe next test continues to employ  (constant->constant+) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  ThreeWordsJustOverTheLimitShouldWrapAtSecondWord()  throws   Exception  {\n  assertThat(wrap( \" word word word \" ,  9 ), is( \" word word \\n word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tBut how do we make this pass? The current solution does not appear to be easily transformable into something that will pass the new test. If we had a function like  replaceLast(\" \", \"\\n\")  then perhaps it would be simple; but that wouldn’t help us for the next test case  \"word word word word\" .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis is the impasse. Now this is a simple problem, and it’s not really that difficult to find a solution. But that’s not the point. The current situation forces us to take a step that’s larger than we like. We’ve put ourselves in the position where we must now solve a large part of the problem rather than a small incremental part of the problem. We have to take a step that’s uncomfortably large.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tBreaking the Impasse\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tSo not let’s go back to the point where we first ignored the priority premise. We had just posed the following test:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  OneShortWordDoesNotWrap()  throws   Exception  {\n  assertThat(wrap( \" word \" ,  5 ), is( \" word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThere is nothing unusual about this test that would make us think it’s out of order. There is no obviously better test to pose. However, the implementation forces us to use the  (unconditional->if)  transformation, which has a pretty low priority.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   static   String  wrap( String  s,  int  length) {\n   if  (s ==  null )\n     return   \" \" ;\n   return  s;\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tSo now we should ask ourselves whether there is another test we could pose that could be passed with a higher priority transformation. At the moment the implementation is simply  return \"\";  so are there any other inputs that should return an empty string?\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tA column length that’s less than one is kind of nonsensical. We could return an empty string for that, or we could throw an exception. I think the exception is probably more appropriate; but the tests for that would also require the  (unconditional->if)  transformation. Still, it’s probably a good idea to to get all the invalid input cases done first.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test (expected = WordWrapper.InvalidArgument.class)\n public   void  LengthLessThanOneShouldThrowInvalidArgument()  throws   Exception  {\n  wrap( \" xxx \" ,  0 );\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWhich is passed with:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   static   String  wrap( String  s,  int  length) {\n   if  (length <  1 )\n     throw   new  InvalidArgument();\n   return   \" \" ;\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tBut that just leaves us where we were before. So I guess there’s no better test to write:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  OneShortWordDoesNotWrap()  throws   Exception  {\n  assertThat(wrap( \" word \" ,  5 ), is( \" word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tAnd after a  (unconditionsl->if)  and a  (constant->scalar)  the implementation is:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   static   String  wrap( String  s,  int  length) {\n   if  (length <  1 )\n     throw   new  InvalidArgument();\n   if  (s ==  null )\n     return   \" \" ;\n\n   return  s;\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tSo now we pose the  word word  test again. As before this is just a  (constant->constant+)  transformation.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  TwoWordsLongerThanLimitShouldWrap()  throws   Exception  {\n  assertThat(wrap( \" word word \" ,  6 ), is( \" word \\n word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe last time we saw this test we passed it with a  (expression->function) . Can it be solved with a higher priority transformation? I don’t think so. Every solution I can think of involves some kind of algorithm.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIs there a different test we could pose that could be solved with a higher priority transformation? Yes, there is! So let’s  @Ignore  the current test and write one that uses a simpler transformation.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  WordLongerThanLengthBreaksAtLength()  throws   Exception  {\n  assertThat(wrap( \" longword \" ,  4 ), is( \" long \\n word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis test can be passed with a  (unconditional->if) .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   static   String  wrap( String  s,  int  length) {\n   if  (length <  1 )\n     throw   new  InvalidArgument();\n   if  (s ==  null )\n     return   \" \" ;\n\n   if  (s.length() <= length)\n     return  s;\n   else  {\n     return   \" long \\n word \" ;\n  }\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis might look like a cheat; but it’s not. We have split the execution pathways, and the new pathway can be viewed as starting completely empty, and then transformed by  ({}–>null)  and  (null->constant) . We could have written those transformations and seen them fail; but why bother?\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe next test is completely obvious. We’ve got to get rid of that constant. We can do that by adding a new statement to the existing test with the  (statement->statements)  transformation.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  WordLongerThanLengthBreaksAtLength()  throws   Exception  {\n  assertThat(wrap( \" longword \" ,  4 ), is( \" long \\n word \" ));\n  assertThat(wrap( \" longerword \" ,  6 ), is( \" longer \\n word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThat’s going to require a  (expression->function)  to pass. There’s no simpler transformation, and no simpler test.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n else  {\n     return  s.substring( 0 , length) +  \" \\n \"  + s.substring(length);\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe next test is the plural of the last:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  WordLongerThanTwiceLengthShouldBreakTwice()  throws   Exception  {\n  assertThat(wrap( \" verylongword \" ,  4 ), is( \" very \\n long \\n word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWe can pass that with  (statement->recursion) \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n return  s.substring( 0 , length) +  \" \\n \"  + wrap (s.substring(length), length);\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIt is also possible to pass this test with  (if->while) . Indeed, you might question why I put  (statement->recursion)  above  (if->while) . So a bit later in this paper we’ll explore the iterative solution. Comparing the two may convince you that recursion is, in fact, simpler than iteration.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tSo now let’s go back to that  @Ignored  test and turn it back on. How would we pass it now?\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n if  (s.length() <= length)\n     return  s;\n   else  {\n     int  space = s.indexOf( \"   \" );\n     if  (space >=  0 )\n       return   \" word \\n word \" ;\n     else \n       return  s.substring( 0 , length) +  \" \\n \"  + wrap(s.substring(length), length);\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t (unconditional->if)  followed by a  (nil->constant)  does the trick. What’s more there is no simpler test to pass, nor a simpler transformation to use.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tGetting rid of the constant requires an additional test:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  TwoWordsLongerThanLimitShouldWrap()  throws   Exception  {\n  assertThat(wrap( \" word word \" ,  6 ), is( \" word \\n word \" ));\n  assertThat(wrap( \" wrap here \" ,  6 ), is( \" wrap \\n here \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWhich is passed with  (expression->function) . Again, there is no simpler test or transformation. (For the sake of brevity, and to keep this paper from sounding like a broken record, I’m going to stop making that statement. You should assume it.)\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n int  space = s.indexOf( \"   \" );\n     if  (space >=  0 )\n       return  s.substring( 0 , space) +  \" \\n \"  + s.substring(space+ 1 );\n     else \n       return  s.substring( 0 , length) +  \" \\n \"  + wrap(s.substring(length), length);\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWe can see that the new clause needs  (statement->recursion) . So we write a test that forces the issue:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  ThreeWordsEachLongerThanLimitShouldWrap()  throws   Exception  {\n  assertThat(wrap( \" word word word \" ,  6 ), is( \" word \\n word \\n word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tMaking it pass is simple.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n if  (space >=  0 )\n       return  s.substring( 0 , space) +  \" \\n \"  + wrap(s.substring(space+ 1 ), length);\n     else \n       return  s.substring( 0 , length) +  \" \\n \"  + wrap(s.substring(length), length);\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tNow we can refactor to eliminate the duplication.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   class   WordWrapper  {\n   private   int  length;\n\n   public  WordWrapper( int  length) {\n     this .length = length;\n  }\n\n   public   static   String  wrap( String  s,  int  length) {\n     return   new  WordWrapper(length).wrap(s);\n  }\n\n   public   String  wrap( String  s) {\n     if  (length <  1 )\n       throw   new  InvalidArgument();\n     if  (s ==  null )\n       return   \" \" ;\n\n     if  (s.length() <= length)\n       return  s;\n     else  {\n       int  space = s.indexOf( \"   \" );\n       if  (space >=  0 ) \n         return  breakBetween(s, space, space +  1 );\n       else \n         return  breakBetween(s, length, length);\n    }\n  }\n\n   private   String  breakBetween( String  s,  int  start,  int  end) {\n     return  s.substring( 0 , start) + \n       \" \\n \"  + \n      wrap(s.substring(end), length);\n  }\n\n   public   static   class   InvalidArgument   extends   RuntimeException  {\n  }\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe next test makes sure we break on the  last  space before the limit.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  ThreeWordsJustOverTheLimitShouldBreakAtSecond()  throws   Exception  {\n  assertThat(wrap( \" word word word \" ,  11 ), is( \" word word \\n word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis requires a  (expression->function) , but it’s so simple it seems obvious.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n int  space = s.lastIndexOf( \"   \" );\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThough this passes the new test, it breaks the previous test; but we can do one more  (expression->function)  transformation to fix it.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n int  space = s.substring( 0 , length).lastIndexOf( \"   \" );\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWherever limits are used, the law of trichotomy must be considered. All the lengths used in the tests have been unambiguously beyond the position of the breaking space. But what happens if we break right on the space.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  TwoWordsTheFirstEndingAtTheLimit()  throws   Exception  {\n  assertThat(wrap( \" word word \" ,  4 ), is( \" word \\n word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis fails, but can be made to pass with a  (statement->function)  transformation.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n int  space = s.substring( 0 , length+ 1 ).lastIndexOf( \"   \" );\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis may not look like a  (statement->function) , but it is. Adding is a function. We might as well have said  add(length, 1) .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIteration instead of Recursion\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tNow let’s wind back the clock and see how an iterative, rather than a recursive, solution might evolve. Remember that we introduced  (statement->recursion)  while trying to pass the following test:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n public   void  WordLongerThanTwiceLengthShouldBreakTwice()  throws   Exception  {\n  assertThat(wrap( \" verylongword \" ,  4 ), is( \" very \\n long \\n word \" ));\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe failing code looks like this:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n if  (s.length() <= length)\n     return  s;\n   else  {\n     return  s.substring( 0 , length) +  \" \\n \"  + s.substring(length);\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWe can make this pass by using the  (if->while)  transformation. If we are going to use a  while  then we need to invert the condition of the  if . This is a simple refactoring,  not a transformation .\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n if  (s.length() > length) {\n     return  s.substring( 0 , length) +  \" \\n \"  + s.substring(length);\n  }  else  {\n     return  s;\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tNext we need to create a variable to hold the state of the iteration. Once again, this is a refactoring, not a transformation.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n String  result =  \" \" ;\n   if  (s.length() > length) {\n    result = s.substring( 0 , length) +  \" \\n \"  + s.substring(length);\n  }  else  {\n    result = s;\n  }\n   return  result;\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t While  loops can’t have  else  clauses, so we need to eliminate the  else  path by doing less in the  if  path. Again, this is a refactoring.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n String  result =  \" \" ;\n   if  (s.length() > length) {\n    result = s.substring( 0 , length) +  \" \\n \" ;\n    s = s.substring(length);\n  }\n  result += s;\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tAnd now we can employ  (if->while)  to make the test pass.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n String  result =  \" \" ;\n   while  (s.length() > length) {\n    result += s.substring( 0 , length) +  \" \\n \" ;\n    s = s.substring(length);\n  }\n  result += s;\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe Process\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIf we accept the Priority Premise, then we should amend the typical red-green-refactor process of TDD with the following provision:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t When passing a test, prefer higher priority transformations.\n\t\t\t\t\t\t \n\t\t\t\t\t\t When posing a test choose one that can be passed with higher priority transformations.\n\t\t\t\t\t\t \n\t\t\t\t\t\t When an implementation seems to require a low priority transformation, backtrack to see if there is a simpler test to pass.\n\t\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIssues\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThere are a number of problems with this.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t Are there other transformations? (almost certainly)\n\t\t\t\t\t \n\t\t\t\t\t Are these the right transformations? (probably not)\n\t\t\t\t\t \n\t\t\t\t\t Are there better names for the transformations? (almost certainly)\n\t\t\t\t\t \n\t\t\t\t\t Is there really a priority? (I think so, but it might be more complicated than a simple ordinal sequence)\n\t\t\t\t\t \n\t\t\t\t\t If so, what is the principle behind that priority? (some notion of “complexity”)\n\t\t\t\t\t \n\t\t\t\t\t Can it be quantified? (I have no idea)\n\t\t\t\t\t \n\t\t\t\t\t Is the priority order presented in this blog correct? (not likely)\n\t\t\t\t\t \n\t\t\t\t\t The transformations as described are informal at best. Can they be formalized? (That’s the holy grail!)\n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tAs you can see from my parenthetic remarks, I have quibbles with nearly all these questions. What I am certain of is that there is a fundamental principle lurking somewhere in here. I think that there  are  a fixed and simple set of transformations, even if I have not enumerated them well. I  hope  they can be formalized. I also think that there is some criteria for selecting which transformations to employ, even if it is not quite as simple as a priority list.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tImplications\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tIf my suspicions turn out correct, then a number of things become possible.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t Tool support for transformations similar to the current tool support for refactorings.\n\t\t\t\t\t \n\t\t\t\t\t Tool support for  suggesting  transformations that follow priority order.\n\t\t\t\t\t \n\t\t\t\t\t The sequence of tests, transformations, and refactorings may just be a formal proof of correctness.\n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tFormal Proof of Correctness\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThat last point requires a bit more amplification. If you can describe the desired behavior of a program with a suite of tests, and if you can show step by step how each test is passed by using formal transformations and refactorings, then you have created a proof.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tOddly, the proof is attained by  constructing  the algorithm in a stepwise fashion. It is interesting to compare this to Dijkstra’s approach of proving correctness by taking the algorithm apart.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tConclusion\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tGiven the typical TDD  red/green/refactor  process, it appears that the  green  phase can be achieved by employing a fixed set of behavior changing transformations to the code. These changes move the code from a  specific  form to a more  generic  form. It also appears that these transformations have a preferred order based on complexity. This ordering can be used in both the  red  and  green  phases of TDD. During the  green  we prefer simpler transformations. During the  red  phase we prefer tests that can be passed with simpler transformations. It is the premise of this blog that if tests are chosen and implemented in this preferred order of transformations, then TDD impasses will be reduced or eliminated.\n\t\t\t\t \n\t\t\t \n\t\t \n\n\n\n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/09/26/AT-FAIL.html", "title": "A.T. FAIL!", "content": "\n       Recently Kevin Liddle, a colleague of of mine at  8th Light , wrote a blog entitled:  The Case Against Cucumber .  This blog fits in with a trend started by Jim Shore in 2010 and before.  Jim’s position is well expressed in his blog:  The Problems with Acceptance Testing . \n\n My response to this is: \n\n # _WTF?_ \n\n \n\n \n\n OK, before I rant further let me calm down, count to ten, take a few deep breaths and (sigh).  OK.  There.  That’s better. \n\n It’s not that Kevin and James are wrong.  They’re not.  But the implications of their conclusion are horrific!   How did we get here? \n\n So let’s go back to the beginning.  Why did we think that acceptance tests were important in the first place.  Why were tools like  Fit ,  FitNesse ,  JBehave , and  Cucumber  written?  What were we trying to achieve? \n\n Socratic Answer:  What is the single greatest cause of software failure? \n\n ##Requirements! \n\n Right!   Requirements!   The most costly problems in software development are problems with the requirements.  The reason for that is self evident.  If the requirements don’t address the true business need, then a tremendous amount of work will be wasted in building a system that nobody wants. \n\n Raise your hand if you’ve  been there; done that. \n\n So what is it that goes wrong with the requirements that causes systems to fail to meet the true business need?  Is it: \n\n \n   The requirements are wrong. \n   The requirements are badly written. \n   The requirements are incomplete. \n   The programmers and the specifiers understand the requirements differently. \n \n\n If you answered 1, then you are probably thinking of a start-up where people are trying to guess at a profitable business model.  There’s really not much we can do about that on our end, so let’s focus on the remaining answers. \n\n Answers 2, 3, and 4, are really all part of the same thing.  There’s a miscommunication between the specifiers and the developers.   How can we improve that communication?  There are really just two solutions to this problem: \n\n \n   Shorten the feedback loop between specification and execution. \n   Formalize the specification. \n \n\n We addressed point 1 with  Agile .  The reason we develop in short iterations is to make sure that any miscommunication in the requirements is caught before too much development is based on it.  The earlier we catch errors in requirements, the less waste there will be.  In the worst case, the most we’ll lose is one iteration. \n\n ## ONE ITERATION!?!?!?!!! \nYou can do the math.  Losing an iteration is damned expensive.  Ok, it’s a hell of a lot better than losing the whole friggin’ project, but still;  nobody wants to lose a whole iteration just because requirements were communicated badly. \n\n Acceptance tests were meant to prevent this. The idea behind acceptance testing was to create a formal language that specifiers could use to unambiguously define requirements. Such formal and unambiguous requirements could not be misunderstood by developers.  So the goal of acceptance tests was to  eliminate the problem altogether . \n\n ##Inconceivable! \n\n At first this goal sounds hyper-ambitious.  How can anyone specify a system so formally and unambiguously as to avoid all miscommunication?  That sounds impossible. \n\n But of course it’s not. Indeed, we often specify systems so formally and unambiguously that moronic automatons understand them in their most intimate details.  Such specifications are called  programs .  We weren’t asking for  that  much formality.  We just wanted a formalism that would eliminate ambiguity between  humans . \n\n Is such a human to human formalism possible? \n\n A little history. \n\n In the late ’90s and early ’00s, Kent Beck and Ward Cunningham were working on this problem.  Kent had come up with some project specific formalisms for some of the projects he was coaching.  Ward, on the other hand, came up with a formalism that was general enough to apply to a large class of projects – possibly a majority.  He called his idea  FIT ; and he backed it up with an executable framework.  (Framework for Integration Testing). \n\n Ward demonstrated FIT to me in 2002, and I immediately realized that it needed a platform to run on.  So I, and the team at Object Mentor, created FitNesse. \n\n Meanwhile, and I think independently, Dan North started his explorations into BDD.  He and Chris Matts came up with the triplet:  Given, When, Then ; and work on the JBehave framework was started. \n\n David Chelimsky, who was working at Object Mentor at the time, got involved with the RSpec project for Ruby, and bumped into Aslak Hellesoy, who was creating Cucumber, a tool that implemented many of Dan’s and Chris’ ideas in the Ruby (and Rails) environment. \n\n Tables \n All of these tools are based on tables.  Fit (and FitNesse) use HTML tables to contain test data.  Cucumber uses tables of Given/When/Then statements that bear an uncanny resemblance to State Transition Tables.  Of course the idea for using tables to describe requirements is not new.  It was David Lorge Parnas who first described, and used, the  technique  to great advantage. \n\n The Grand Goal \n\n Anyway, these acceptance testing tools were beautiful!  They were!  They still are!  Using these tools it is possible for  non-programmers  to specify the behavior of a system and then automate the verification of that specification.  Using these tools it is possible for programmers to  know  what needs to be done, and  know  when they  are done .  Using these tools it is possible to fully specify a system, and use that specification to automatically determine how much of that specification has been implemented, and how much remains unimplemented.  Using these tools it is possible to  know  that the system is ready to be deployed. \n\n I could go on. \n\n So,  how  can you use these tools to do all those things? \n\n The idea is very simple.  You get the business to write the acceptance tests, and then the programmers can run them.  If they fail, the programmers have work to do.  If they all execute and pass, then the programmers are done. \n\n Excuse me? \n\n Ha haha ha ha hahahahaha!    Get…   ha ha ha ha   ..the b-b-business…   ha ha hoooo hoooo hah hah hah hah   ..to  wr-wr-wri-i-ite…   HA HA HA HA HA HA HA HA HA   …the…the…the…   HO HO ha ha ha ha ha   …Tests?   HAAAAAAH HA HA HA HA HO. AAAAHHHHH, ha ha ha ha ha ha ha,,,   AHHH HA HA HA HA HA HA ha ha ha ha ha ha ha ha. \n\n Yeah.  I know.  What  were  we thinking?  I mean,  really .  Business people are busy.  They’ve neither the time nor the inclination to write a bunch of tests.  No.  Not at all.  No.  What they do instead is… is… is… \n\n Hmmm.  What  do  they do instead?  I mean, to make sure that the system works and can be deployed?  What do they do? \n\n ###Q.A. \n\n They get  QA  to do it!  Right?  I mean, they hire a zillion people in India, or South America, or Eastern Europe, and get  them  to do it. \n\n \n   Read that again, but this time use Gollum’s voice as he’s talking to himself about Shelob the spider, rubbing his hands together with an evil gleam in his eye:  “We’ll get  her  to do it.” \n \n\n OK, so what is it that QA does?  Let’s ask a QA manager? \n\n Me:    “Hello Mr. QA Manager.  What is it that you guys do? \n\n QA Manager:   “Well, we test the system.” \n\n Me:   “I see.  But how?”  I mean, what is the process?” \n\n QA Manager:   “Well, the testers operate the system and observe the results.” \n\n Me:   “OK, that makes sense.  But how do the testers know what to do and what to expect?” \n\n QA Manager:   “They follow the test plan.” \n\n Me:   “The test plan?” \n\n QA Manager:   “Yes, the test plan.” \n\n Me:   “And just what is this test plan?” \n\n QA Manager:   “The test plan is a document that describes how to execute the tests, and what outcomes to expect.” \n\n Me:   “I see.  So, it must be unambiguous enough for humans to understand.” \n\n QA Manager:   “Of course.” \n\n Me:   “And so it must be rather formal.” \n\n QA Manager:   “Naturally!” \n\n Me:   “And so, let me get this straight.  You write a formal, unambiguous suite of tests; and then get  Humans  to execute it?” \n\n QA Manager:   “Precisely!” \n\n ###Absurdities \n\n So it appears that the business already  does  write the tests; they simply use the most inefficient execution platform possible:  Humans. \n\n Meanwhile, the programmers, cowed by the fact that the business didn’t jump at the chance to write even more tests, skulked away into their hidey holes and, in a fit of passive aggression, decided to write the acceptance tests themselves.   “That’ll show ‘em!” \n\n Now this completely misses the point.  I mean, the point was to get the  business  to provide a formal specification of the system so that the programmers could understand it.  What in the name of heaven is the point of having the  programmers  write the formal specification so that the  programmers  can then understand it?  Am I the only one who sees the logical contradiction here? \n\n But then, and for reasons that still completely elude me, the Cucumber folks decided to turn Cucumber into a  programmer tool!   They integrated it with their development environments.  They made it easy to use a text editor.  They made rampant use of regular expressions.  They did everything they could to make it easy for  programmers  to write these tests. \n\n And then they integrated it into the  development  process.  So the process became: \n\n \n   First write a failing acceptance test. \n   Then write failing unit tests that fail for the same reasons and make them pass using a TDD cycle. \n   When the acceptance test passes, you are done. \n \n\n It doesn’t take long for a programmer trapped in that cycle to realize that there are certain inefficiencies and/or redundancies that don’t make a lot of sense.  So most eventually abandon one of those two test streams.  Of course the most common option, and the  worst  option, is to abandon the unit tests. \n\n When you abandon the unit tests in favor of acceptance tests, you are abandoning the tests you are  supposed  to write in favor of tests that you are  not supposed  to write. \n\n The other option, which was chosen by James Shore, and Kevin Liddle, is better; but still bad.  At least the programmers are doing what they are supposed to do; but we still have the problem that requirements aren’t being communicated properly. \n\n ###Design Flaws. \n\n The other major mistake that the programmers made was to somehow conclude that tests aren’t software and don’t need to be designed.  Again, it was the Cucumber community that led the charge.  They integrated the use of  Capybara  with Cucumber so they could tightly couple their systems into zillions of little tiny, impossible to untie, knots. \n\n \n   The First Rule of Software Engineering:  Don’t depend on things that change a lot! \n \n\n What changes more than the UI?  Nothing!  So let’s couple all our tests to the UI.  Oh, yes!  Lets!  What a good idea! \n\n Of course the problem with this is that, when the UI changes, a whole load of tests break.  And then you are stuck fixing a whole bunch of tests just because some marketing weenie decided they didn’t like the arrangement of the buttons on a page. \n\n \n   Hey!  IOS 99 just came out.  They changed all the icons, and buttons, and text, and…  IT’s really cool.  It doesn’t actually do anything new or different, but it’s much more modern now…  It only took the QA team 152 man years to rework all the tests too! \n \n\n ###The Illogical Conclusion. \n\n So, now, what conclusion are programmers drawing from this cacophony of errors?  Simple.   Acceptance Tests Suck! \n\n And my response to that is: \n\n # _WTF?_ \n\n \n\n \n\n Hey!  I’ve got an idea!  Let’s take all the effort that Q.A. is putting in to writing  and executing  the manual test plan, and use that to write an  automated  test plan instead.  Then, instead of being read by the testers, those tests would be executed by the  programmers ; and the  programmers  could make them pass.  And then the  programmers  would know when they were done with a story, or a feature.  And the  businenss  would know, at the push of a button, that the system was ready to deploy.  And… and… and… \n \n   . \n \n\n \n   . \n \n\n \n   . \n \n\n ## Naaahhhhhh! \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/05/27/FlashTpp.html", "title": "Flash - TPP", "content": "\n      \t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\t February 3 2011 \n\t\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t\t \n\t\t\t\t\tI read a nice  blog  today exploring the  Transformation Priority Premise  using the flash card kata. The author did it in both lisp and C#. Let’s try it in Java.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe flash card kata is pretty straightforward. Given a set of questions and answers, ask the user each question and solicit an answer. If the answer is correct, say so and count it. If the answer is wrong show the correct answer and count the error. At the end, print the number of right and wrong answers.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThe first test is wonderfully simple. Given an empty list of questions and answers, end the game immediately with a summary showing nothing right and nothing wrong.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWe begin with an empty test which passes:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   class   flashCardTest  {\n   @Test \n   public   void  degenerateGame()  throws   Exception  {\n  }\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWe get the test to fail by asserting the answer we want and by doing a couple of  ({}–>nil)  transforms. (In this case -1 is the integer equivalent of nil, e.g. a degenerate value)\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   class   FlashCardTest  {\n   @Test \n   public   void  rightAndWrongShouldBeZeroIfGamePlayedWithNoCards() {\n     List  emptyList =  new   ArrayList ();\n    playGame(emptyList);\n    assertEquals( 0 , rightAnswers());\n    assertEquals( 0 , wrongAnswers());\n  }\n\n   private   int  wrongAnswers() {\n     return  - 1 ;\n  }\n\n   private   int  rightAnswers() {\n     return  - 1 ;\n  }\n\n   private   void  playGame( List  flashCards) {\n  }\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tTo get this to pass just need to do some  (nil->constant)  transforms.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n private   int  wrongAnswers() {\n     return   0 ;\n  }\n\n   private   int  rightAnswers() {\n     return   0 ;\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis solution is a bit ugly since it couples the test and the solution. So let’s refactor to create a class.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   class   FlashCardTest  {\n   @Test \n   public   void  rightAndWrongShouldBeZeroIfGamePlayedWithNoCards() {\n    FlashCardGame flashCardGame =  new  FlashCardGame();\n     List  emptyList =  new   ArrayList ();\n\n    flashCardGame.playGame(emptyList);\n\n    assertEquals( 0 , flashCardGame.rightAnswers());\n    assertEquals( 0 , flashCardGame.wrongAnswers());\n  }\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   class   FlashCardGame  {\n   public  FlashCardGame() {\n  }\n\n   int  wrongAnswers() {\n     return   0 ;\n  }\n\n   int  rightAnswers() {\n     return   0 ;\n  }\n\n   void  playGame( List  flashCards) {\n  }\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tFor the next test, lets try a game with a single flash card, that the user gets right.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n   public   void  rightShouldBeOneIfOneRightAnswer() {\n    FlashCard card =  new  FlashCard( \" Q \" ,  \" A \" );\n     List  cards =  new   ArrayList ();\n    cards.add(card);\n\n    flashCardGame.playGame(cards);\n    assertEquals( 1 , flashCardGame.rightAnswers());\n    assertEquals( 0 , flashCardGame.wrongAnswers());\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis fails of course. We can make it pass by simply incrementing the right count in  playGame  if the list of cards is not zero. This is a  (unconditional->if)  transform. That, plus a little refactoring gives us:\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   class   FlashCardGame  {\n   private   int  rightAnswers;\n\n   public  FlashCardGame() {\n  }\n\n   int  getWrongAnswers() {\n     return   0 ;\n  }\n\n   int  getRightAnswers() {\n     return  rightAnswers;\n  }\n\n   void  playGame( List  flashCards, FlashCardTest answerer) {\n     if  (flashCards.size() !=  0 )\n      rightAnswers++;\n  }\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tOK, so let’s try a wrong answer.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n   public   void  wrongShouldBeOneIfOneWrongAnswer() {\n    FlashCard card =  new  FlashCard( \" QW \" ,  \" A \" );\n     List  cards =  new   ArrayList ();\n    cards.add(card);\n\n    flashCardGame.playGame(cards);\n    assertEquals( 0 , flashCardGame.getRightAnswers());\n    assertEquals( 1 , flashCardGame.getWrongAnswers());\n  }\n\n   public   String  answerQuestion( String  question) {\n     if  (question.equals( \" QR \" ))  return   \" A \" ;\n     else   return   \" W \" ;\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis forced us to create the  answerQuestion  function that pretends to be a user answering questions. If you pass in “QR” you get the right answer “A”. If you pass in “QW” you get the wrong answer “W”. To get this test to pass we’re going to have to get this function called by  playGame . We can do this by passing the test along in an argument using the Change Signature refactoring. Then we can use a  (unconditional->if)  transform to check the value of our new function.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   class   FlashCardGame  {\n   private   int  rightAnswers;\n   private   int  wrongAnswers;\n\n   public  FlashCardGame() {\n  }\n\n   int  getWrongAnswers() {\n     return  wrongAnswers;\n  }\n\n   int  getRightAnswers() {\n     return  rightAnswers;\n  }\n\n   void  playGame( List  flashCards, FlashCardTest answerer) {\n     if  (flashCards.size() !=  0 ) {\n       String  question = flashCards.get( 0 ).getQuestion();\n       if  (answerer.answerQuestion(question).equals( \" A \" ))\n        rightAnswers++;\n       else \n        wrongAnswers++;\n    }\n  }\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tOf course this is hideous, so we need to refactor alot.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   interface   User  {\n   String  answerQuestion( String  question);\n}\n\n----\n\n public   class   MockUser   implements  User {\n   public  MockUser() {\n  }\n\n   public   String  answerQuestion( String  question) {\n     if  (question.equals( \" QR \" ))  return   \" A \" ;\n     else   return   \" W \" ;\n  }\n}\n\n----\n\n public   class   FlashCardGame  {\n   private   int  rightAnswers;\n   private   int  wrongAnswers;\n\n   public  FlashCardGame() {\n  }\n\n   int  getWrongAnswers() {\n     return  wrongAnswers;\n  }\n\n   int  getRightAnswers() {\n     return  rightAnswers;\n  }\n\n   void  playGame( List  flashCards, User user) {\n     if  (flashCards.size() !=  0 ) {\n       String  question = flashCards.get( 0 ).getQuestion();\n       String  answer = user.answerQuestion(question);\n       if  (answer.equals( \" A \" ))\n        rightAnswers++;\n       else \n        wrongAnswers++;\n    }\n  }\n}\n\n----\n\n public   class   FlashCardTest  {\n   private  FlashCardGame flashCardGame;\n   private  MockUser user =  new  MockUser();\n\n   @Before \n   public   void  setUp()  throws   Exception  {\n    flashCardGame =  new  FlashCardGame();\n    user =  new  MockUser();\n  }\n\n   @Test \n   public   void  rightAndWrongShouldBeZeroIfGamePlayedWithNoCards() {\n     List  emptyList =  new   ArrayList ();\n\n    flashCardGame.playGame(emptyList, user);\n\n    assertEquals( 0 , flashCardGame.getRightAnswers());\n    assertEquals( 0 , flashCardGame.getWrongAnswers());\n  }\n\n   @Test \n   public   void  rightShouldBeOneIfOneRightAnswer() {\n    FlashCard card =  new  FlashCard( \" QR \" ,  \" A \" );\n     List  cards =  new   ArrayList ();\n    cards.add(card);\n\n    flashCardGame.playGame(cards, user);\n    assertEquals( 1 , flashCardGame.getRightAnswers());\n    assertEquals( 0 , flashCardGame.getWrongAnswers());\n  }\n\n   @Test \n   public   void  wrongShouldBeOneIfOneWrongAnswer() {\n    FlashCard card =  new  FlashCard( \" QW \" ,  \" A \" );\n     List  cards =  new   ArrayList ();\n    cards.add(card);\n\n    flashCardGame.playGame(cards, user);\n    assertEquals( 0 , flashCardGame.getRightAnswers());\n    assertEquals( 1 , flashCardGame.getWrongAnswers());\n  }\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tNow let’s do two questions, one right and one wrong.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n   public   void  countBothOneRightAndOneWrong() {\n     List  cards =  new   ArrayList ();\n    cards.add( new  FlashCard( \" QW \" ,  \" A \" ));\n    cards.add( new  FlashCard( \" QR \" ,  \" A \" ));\n\n    flashCardGame.playGame(cards, user);\n    assertEquals( 1 , flashCardGame.getRightAnswers());\n    assertEquals( 1 , flashCardGame.getWrongAnswers());\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThis fails, but we can make it pass with a  (if->while)  transform.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n void  playGame( List  flashCards, User user) {\n     for  (FlashCard card : flashCards) {\n       String  question = card.getQuestion();\n       String  answer = user.answerQuestion(question);\n       if  (answer.equals( \" A \" ))\n        rightAnswers++;\n       else \n        wrongAnswers++;\n    }\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tOne thing left. We need to actually compare the answer in the flashcard to the response from the user.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n @Test \n   public   void  countThreeNewQuestionsTwoRightOneWrong() {\n     List  cards =  new   ArrayList ();\n    cards.add( new  FlashCard( \" Q1 \" ,  \" 1 \" ));\n    cards.add( new  FlashCard( \" Q2 \" ,  \" 2 \" ));\n    cards.add( new  FlashCard( \" Q3 \" ,  \" wrong \" ));\n\n    flashCardGame.playGame(cards, user);\n    assertEquals( 2 , flashCardGame.getRightAnswers());\n    assertEquals( 1 , flashCardGame.getWrongAnswers());\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tWe need to make a small change to the Mock.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n public   class   MockUser   implements  User {\n   public   String  answerQuestion( String  question) {\n     if  (question.equals( \" QR \" ))  return   \" A \" ;\n     else  {\n       return  question.substring( 1 );\n    }\n  }\n}\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tAnd now we can make this pass with a simple  (expression->function)  transform.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\t\t \n void  playGame( List  flashCards, User user) {\n     for  (FlashCard card : flashCards) {\n       String  question = card.getQuestion();\n       String  answer = user.answerQuestion(question);\n       if  (answer.equals(card.getAnswer()))\n        rightAnswers++;\n       else \n        wrongAnswers++;\n    }\n  }\n \n\t\t\t\t\t \n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThere’s more to do, of course, but the plumbing is all set up, and the algorithm looks right. There were several cases where we  could  have used a lower transform such as  (variable->assignment)  but there was no need, and the algorithm came out nicely.\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t\tThere is just the slightest chance that the one use of  (if->while)  could have been done with  (statement->tail-recursion) , but since this is Java, that’s probably not the best choice.\n\t\t\t\t \n\t\t\t \n\t\t \n\n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/10/01/Dance-You-Imps.html", "title": "Dance you Imps!", "content": "\n       There are several tools out there that promise to bridge the divide between objects and relational tables.  Many of these tools are of high quality, and are very useful.  They are collectively known as ORMs, which stands for  Object Relational Mappers .  There’s just one problem.  There ain’t no such beast! \n\n The Impedance Mismatch \n\n It all started long ago, in the hinter-times of the 1980s.  Relational databases were the big kids on the block.  They had grown from their humble beginnings, into adventurers and conquerors.  They had not yet learned to be:  THE POWER THAT DRIVES THE INTERNET (TM) ; but they were well on their way.  All new applications, no matter how humble,  had to have  a relational database inside them.  The marketing people didn’t know  why  this was true; but that knew that is  was  true. \n\n And then OO burst forth upon the land.  It came as Smalltalk.  It came as Objective-C, and it came, most importantly, as C++, and then Java and C#.  OO was new.  OO was shiny.  OO was a mystery. \n\n Most creatures at the time feared new things.  So they hid in caves and chanted spells of protection and exorcism.  But there were imps in the land, who thrived by embracing change.  The imps grabbed for the shiny new OO bauble and made it their own.  They could  feel  its power.  They could  sense  that it was good – very, very good; but they couldn’t explain  why .  So they invented reasons.  Reasons like:  It’s a closer way to model the real world!  (What manager could say no to  that ?) \n\n Then one day, when the OO imps were just minding their own business, doing their little dance of passing messages back and forth to each other, the RDBMS gang (pronounced  Rude Bums ) walked up to them and said: “If you imps want to do your impy little dance of passing your stupid messages back and forth in  our  neighborhood, then your bits are ours.  You’re going to store them with us.  Capisce?” \n\n What could the OO imps say?  The  Rude Bums  ruled the roost; so, of course, they agreed.  But that left them with a problem.  The  Rude Bums  had some very strict,  and very weird , rules about how bits were supposed to be stored. \n\n These rules didn’t sit well with the imps.  When the imps did their impy message passing dance, they just sort of threw the bits back and forth to each other.  Sometimes they’d hold the bits in their pockets for awhile, and sometimes they’d just toss them over to another impy dancer. In a full fledged impy dance, the bits just zoomed around from dancer to dancer in a frenzy of messages. \n\n But the  Rude Bums  demanded that the bits be stacked up on strictly controlled tables, all kept in one big room.  They imposed very strict rules about how those bits could be arranged, and where they had to be placed.  In fact, there were forms to fill out, and statements to make, and transactions to execute.  And it all had to be done using this new street banter named SQL (pronounced  squeal ). \n\n So all the OO imps had to learn  squeal , and had to stack their bits on the  Rude Bums  tables, and had to fill out the forms and do the transactions, and that just didn’t match their dancing style.  It’s  hard  to throw your bits around in the impy dance when you’ve got to stack your bits on tables while speaking  squeal ! \n\n This was the beginning of the Impy Dance Mismatch between OO and the  Rude Bums . \n\n ORMs to the rescue,  Not! \n\n The next decade saw a huge increase in the political power of the  Rude Bums .  They grabbed more and more territory, and ruled it with an iron fist.  The imps also gained territory; possibly more than the  Rude Bums ; but they never managed to break free of the  Rude Bums’  rules.  And so they forgot how to dance.  The free and lively OO dance they had done at the very beginning faded from their memory.  It was replaced by a lock-step march around the  Rude Bum’s  tables. \n\n Then, one day, some strangers appeared.  They called themselves ORM (the  OutRaged Mongrels ).  The  Mongrels  had seen the free OO dancing of the imps before the  Rude Bums’  took control of them; and the  Mongrels  longed to see the imps dance free again. \n\n So they came up with a plan.   They  would do the  squealing !   They  would arrange the bits on the tables.   They  would fill out the forms and execute the transactions.   They , the  Mongrels , would stand between the imps and the  Rude Bums  and free the imps to dance once again. \n\n “Oh  dance free  you imps,  dance free! ” \n\n But the imps  didn’t  dance free. They kept right on doing their lock-step march.  Oh they were happy to have someone else take care of the nasty job of speaking  squeal  and arranging bits on the tables.  They were happy that they didn’t have to deal directly with the  Rude Bums .  But now, instead of marching around the  Rude Bum’s  tables, they marched around the “Mongrels’” cabinets (which looked an awful lot like the  Rude Bums’  tables). \n\n The Impy Dance Mismatch between OO and the  Rude Bums  had simply changed to the Impy Dance Mismatch between OO and ORMs. \n\n Their ain’t no such mapping. \n\n An object is not a data structure.  Repeat after me:  An Object Is Not A Data Structure.   OK, you keep repeating that while I keep talking. \n\n An object is not a data structure.  In fact, if you are the consumer of an object, you aren’t allowed to see any data that might be inside it.  And, in fact, the object might have no data inside it at all. \n\n What do you see when you look at an object from the outside?  You see  methods!   You don’t see any data; because the data (if any) is kept private.  All you are allowed to see, from the outside looking in, are methods.  So, from the outside looking in, an object is an abstraction of behavior, not an abstraction of data. \n\n How do you store an abstraction of behavior in a database?  Answer:  You don’t!   You can’t store behavior in a database.  And that means  you can’t store objects in a database.   And that means there’s no Object to Relational mapping! \n\n OK, now wait! \n\n Yeah?  Wait for what?  You want to argue that point?  Really?  You say you’ve got an  account  object stored in the database, and you use hibernate to bring it into memory and turn it into an object with methods? \n\n Balderdash!   Hibernate doesn’t turn  anything  into an object.  All Hibernate does is to migrate data structures that are stored on a disk (a what?  You still using disks?  No wonder you’re confused.) to data structures stored in RAM.  That’s all.  Hibernate changes the form and location of data structures.  Data structures, not objects! \n\n What is a data structure?  It’s a bunch of public data with no methods.  Compare that to an object which is a bunch of public methods with no visible data.  These two things are the exact opposites of each other! \n\n I could get into a whole theoretical lecture on the nature of data structures and objects, and polymorphism, and switch statements, and…   But I won’t.  Because the point of this article is simply to demonstrate that ORMs aren’t ORMs. \n\n ###What are ORMs? \n\n ORMs are data structure migrators.  That’s all.  They move data from one place to another while making non-semantic changes to the form of that data.  They do  NOT  create objects out of relational tables. \n\n Why is this important? \n\n It’s important because the imps aren’t dancing free!  Too many applications are designed such that the relational schema is bound, in lock-step, to the business objects.  The methods on the business objects are partitioned according to the relational schema. \n\n Think about that for a minute.  Think about that, and then weep.  Why should the message pathways of an application be bound to the lines on a E-R diagram?  What does the behavior of the application have to do with the structure of the data? \n\n Try this thought experiment.  Assume that there is no database.  None at all.  There are no tables.  No schema.  No rows.  No SQL.  Nothing. \n\n Now think about your application.  Think about the way it behaves.  Group similar behaviors together by responsibility.  Draw lines between behaviors that depend on each other.  Do you know what you’ll wind up with?  You’ll wind up with an object model.  And do you know what else?  It won’t look much like a relational schema. \n\n Tables are not business objects!   Tables aren’t objects at all.  Tables are just data structures that the true business objects use as a resource. \n\n ###Moral \n\n So, designers, feel free to use ORMs to bring data structures from the disk (the disk?  You still using one?) into memory.  But please don’t think of those data structures as your business objects.  What’s more, please design your business objects without consideration for the relational schema.  Design your applications to  behave  first.   Then  figure out a way to bind those behaviors to the data brought into memory by your ORM. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/11/12/Healthcare-gov.html", "title": "Healthcare.gov", "content": "\n       This is not a political blog.  This is a blog about the Software Industry; and the profound effect that its failure is having upon our society. \n\n The Software Industry failed to deliver  healthcare.gov .  As a result, millions of people are being hurt.   We  did that, folks.  It was  our  industry,  our  failure. \n\n The Affordable Care Act  (ACA) is one of the most massive public policy initiatives in American history.  It was passed through both houses of Congress, and signed into law by the President of the United States.  It is  The Law of the Land ; and it is being  thwarted  by the incompetence of the Software Industry. \n\n Let me repeat that.  A critical public policy is being thwarted, not by the failure of the policy, but by the failure of a  web site .  That’s a software disaster, folks.  A  big  one. \n\n If the media are to be believed, there are  four million people  whose health plans were cancelled when the ACA went into effect on October 1st.  The Software Industry was supposed to have built a web site to help those people find new health insurance.  By failing to get that web site working, the Software Industry has harmed those four million people; and the repercussions of that failure will be felt for decades. \n\n Already, I have spoken to business people who, in the light of this failure, are showing reluctance to build new software.  They say: “Will this be like healthcare.gov?”  What they are really saying is: “Can the Software Industry be trusted to do  anything ?” \n\n Perhaps you disagree.  Perhaps you think this was a failure of government, or of management.  Of course I agree.  Government failed and management failed.  But government and management don’t  know  how to build software.   We  do.  We were  hired  because of that knowledge.  And we are expected to use that knowledge to  communicate  to the managers and administrators who don’t have it. \n\n Challenger \n\n In the hours before the Space Shuttle Challenger blew up, killing seven brave astronauts, a battle was being fought to save their lives.  The engineers who  knew  that it was too risky to launch worked through every official channel to stop that launch.  They wrote memos.  They held meetings.  They raised red flags.  They even refused to watch the launch. \n\n But the managers and administrators overrode them, and launched anyway – for political reasons: Vice President George H. W. Bush was going to be there to observe the launch.  By their horrific incompetence, Christa McAuliffe, a school teacher who was poised to conduct the first grade-school lesson from space, was killed before the eyes of millions of school children who were waiting for that lesson. \n\n It’s easy to blame the managers.  It’s  appropriate  to blame the managers.  But it was the engineers who  knew .  On paper, the engineers did everything right.  But they  knew .  They  knew .  And they failed to stop the launch. \n\n They failed to say:  NO! , loud enough for the right people to hear. \n\n Over the cliff. \n On the  healthcare.gov  project, there must have been some programmers who  knew  that the October 1st launch was going to be a disaster.  There must have been some QA people who  knew .  There must have been architects, project managers, technical leads, and group leaders who  knew  that we were on the verge of one of the largest, and most public, software failures in the history of the world.    Somebody knew! \n\n Why didn’t they say  NO! ? Why didn’t they say it loud and clear?  Why wasn’t the train stopped before it ran off the tracks and plunged over the cliff?  Why didn’t somebody pull the emergency cord?  Why wasn’t there somebody on Fox News, ABC, MSNBC, or CNN, telling us that nobody had tested the site, that they weren’t ready, that it was going to be a disaster? \n\n Just following orders. \n Perhaps you think I’m being unfair.  After all, these folks were just doing their jobs.  Their managers told them what to do, and they did it.  Some of them may have told their managers that they were uncomfortable with the October 1st launch.  Some of them may even have banged on desks, published memos, and raised red flags; only to be overridden by their managers.  And so they launched.  They were just following orders. \n\n In the U.S. military, you can be sentenced to death for disobeying a direct order. ( Article 90 ).  But if you read that article closely you’ll notice a critical word:  Lawful .  You are  not  required to obey an  unlawful  order.  Indeed, if you  do  obey an unlawful order, you can,  and should , be prosecuted. \n\n Never do Harm. \n So what is  our  law?  To what principles do  we  adhere?  What orders must we refuse to follow?  It seems to me that the  Hippocratic Oath  is a good starting place for us.  Allow me to slightly reword one stanza of that Oath to fit our purposes: \n\n \n   I will create software for the good of my users according to my ability and my judgement  and never do harm to anyone . \n \n\n Harm has been done to millions of people.  We of the Software Industry allowed that harm to take place by following unlawful orders. \n\n Professionalism \n If I had to define professionalism in once sentence, I’d say: \n\n \n   Professionalism is the willingness to refuse to obey orders that do harm to others. \n \n\n Or, perhaps this is a better wording: \n\n \n   A professional has the knowledge  and responsibility  to say: “ No! ” as loudly as necessary to prevent harm. \n \n\n Repercussions \n If I were in government right now, I would be leery of starting another big software project.  I’d also know that big software projects are going to be necessary as our civilization gets more and more complex.  So, if I were in government right now, I’d be thinking about laws to regulate the Software Industry.  I’d be thinking about what languages and processes we should force them to use, what auditing should be done, what schooling is necessary, etc. etc.  I’d be thinking about passing laws to get this unruly and chaotic industry under some kind of control. \n\n If I were the President right now, I might even be thinking about creating a new Czar or Cabinet position:   The Secretary of Software Quality.   Someone who could regulate this misbehaving industry upon which so much of our future depends. \n\n This is a warning, not a recommendation!  I don’t think a czar or secretary is the right solution.  And maybe nobody in government has thought of this yet.  Maybe.  But how many more  healthcare.gov  debacles will it take before it does? \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/10/24/The-Careless-Ones.html", "title": "The Careless Ones", "content": "\n       Two days ago my wife placed an on-line order at Walmart for a metal-frame bunk bed for our grandchildren to sleep on when they stay over at our house.  Yesterday it arrived.  Wow!  One day delivery to my front door.  Someone cared. \n\n The metal-frame parts for the bunk bed came in a box that was, perhaps, 4ft X 18in X 6in, or 3 cu ft.  It weight about 100 lbs.  I looked at this carton with a growing sense of despair, knowing that the next several hours of my life would be in the hell of unpacking and assembly.  I’ve been in that hell before.  I had no desire to return to it.  But I love my grand children and I wanted them to have a nice place to sleep. So… \n\n I set my jaw and opened that box. \n\n Have you ever seen the movie “Pulp Fiction”?  Do you remember the briefcase?  The golden glow?  Well, it wasn’t quite like that; but it was close.  The innards of that box were packed – perfectly.  I had expected chaos.  I had expected peanuts! (No!  NO!  Not PEANUTS!….   Run Away!….)  Instead, I found a perfect rectangular prism of parts and well formed packing material. \n\n The packing material was not that horrible, nails-on-chalkboard, land-fill-fodder, phosgene generating styrofoam.  No, this packing material was blocks of thick corrugated cardboard.  Tough.  Recyclable.  Form Fitting.  Cardboard.  And these cardboard blocks were perfectly shaped to fill any empty space in that carton. \n\n It was a masterpiece of packing.  I stood in awe of it.  From the moment I opened that box, it was clear to me that someone cared. \n\n What’s more, as I gradually took the pieces out of the box I found that each piece was nicely wrapped in clear plastic, with little cardboard end caps for those pieces with sharp or oddly shaped ends. The parts were laid in the box in an orderly fashion that was very easy to understand.  Removing the parts was trivial, and did not result in any clanging or banging or bumping.  It just all came apart with a trivial amount of effort. \n\n \n   To the packaging engineers at Dorel:  Nicely Done!  It was clear to me that you guys cared about that packaging.  You  thought  about it.  You must have tried many different combinations before you settled on the final strategy.  I want you to know I  appreciate  that. \n \n\n You know how most kits come with a bag of parts?  This bag is usually filled with screws, washers, nuts, bushings, etc.  It often looks like some dufus in the warehouse reached into several dozen bins, grabbed a handful (or a fingerful) from each without bothering to count, dumped them into the bag, taped the bag shut, and tossed it into the box.  Well,  get this : The parts for this bed were  shrink-wrapped  onto a cardboard backing, that labeled each part.  I could tell, at a glance, what all the parts were, and that all the parts were there. \n\n I won’t bore you with the story of the assembly of the bed.  Suffice it to say that the bed went together with a minimum of issues.  The instructions, if not perfect, were very good, and appropriately detailed.  Overall, I think I made two assembly errors that I could blame on ambiguity in the instructions.  Both of these errors were trivial to fix. \n\n In short, I left that project feeling like I had been supported by a team of engineers who had  thought  about, and  cared  about, how to make the job of assembling that bed as easy as possible.  Again, Dorel, Nice work! \n\n And what about Walmart and Fedex?  They fulfilled, shipped, and delivered that order in less than 24 hours.  Nice going guys!  Thanks for caring! \n\n \n\n I upgraded to OSX 10.9 this morning.  I sat down at my desk, ready to begin the day.  My plan was to write this article.  I’ve been thinking about it for several days.  But up on my screen was the announcement of “OSX Mavericks” with a friendly button that said “Install Now”. \n\n I should have known.  I should have known.  But it was early, and I was sipping my coffee, and…  Well…  I hit that button. \n\n The download began, and that was no big deal.  The download was several gigabytes, so I expected it would take a few minutes.  So, in the mean time I decided to create an account on healthcare.gov. \n\n I had heard that healthcare.gov was having some trouble.  But, since Mavericks was downloading, I figured I’d give it a try.  After all, maybe I could find a better health insurance deal there. \n\n The experience began well.  The first few pages were nicely designed, and easy to read.  They were informational and cheery.  Then it asked me for my state, and I selected Illinois.  The site came back right away with some more useful information about Illinois applications, and then presented me with a big friendly button that said: “Apply Now”.  I hit that button and then… \n\n Well, the little spinner spun.  And spun.  And spun.  So I checked on the OSX download.  It was done and ready to install, but I didn’t want to interrupt my application at healthcare.gov, so I decided to read some email and twitter and… \n\n Healthcare.gov came back in a few minutes with a traditional account creation page.  It wanted a user name and password.  I could rant about the idiocy of websites that force you to put numbers and punctuation in your user names and password.  I could.  I could rant about that.  Oh, yes, I could.  But I won’t.  No.  No I won’t. \n\n The Hell I Won’t! \n\n \n   Dear website creators.  Upper and lower case, numbers and punctuation, DO NOT INCREASE SECURITY.  What number do people use?  They use the number 1.  What punctuation do they use?  They use a period.  Where do they put the number?  At the end.  Where do they put the period?  At the end.  What do they capitalize?  Words!  Do you really think theres a big difference between “unclebobmartin” and “UncleBobMartin1.”  Actually, there is!  The difference is in my frustration level with your stupid website. \n \n\n Anyway…  I filled out the form with my appropriately numbered, punctuated, and capitalized user name, and my appropriately numbered, punctuated, and capitalized password.  And then clicked the “Create Account” button. \n\n I was quickly rewarded with another page that asked me for security questions.  You know the kind.  What is your mothers maiden name?  What is your eldest sibling’s height?  How many times have you been arrested in Nevada?  That kind of thing. \n\n I chose three questions that were easy for me to answer.  One of them was:  “A personally significant date”.  That was easy.  I was married on July 21, 1973.  So I entered  7/21/73 . \n\n A little red sentence appeared beneath my cursor.  It said:  That entry is invalid! .  My mistake was immediately evident.  We don’t use two digit years anymore, do we?  Not since Y2K.  Oh no.  Now we use 4 digit dates.  And we will, I suppose, until Y10K.  So I entered:  7/21/1973 .  But, again, I got the little warning:  That entry is invalid! \n\n Hmmm.  What could be wrong?  Given that I am a programmer, I started thinking like a programmer.  Perhaps the programmer was matching a regular expression like:  \\d{2}/\\d{2}/\\d{4} .  So I tried:  07/21/1973 .  No dice.  So I tried a number of other variations.  No cigar.  I spent 10 minutes or so trying to figure out what the demented programmer who wrote this code was expecting me to enter. \n\n And then it occurred to me.   The programmer did not write the questions!   Some  bureaucrat  wrote the questions.  The programmer never talked to that bureaucrat.  The programmer never read the questions that the bureaucrat wrote. The programmer was simply told to display a set of questions from a database table, and to store the responses in the user’s account.  The programmer had no idea that this particular question was asking for a date!  So the programmer was not trying to match a date!  The programmer was just accepting any string. \n\n Well, not  any  string.  After all, I had been typing strings for the last ten minutes.  No, someone had told the programmer (or the programmer simply decided on his own) that certain characters would not be appropriate in the answers to the questions.  One of those inappropriate characters was probably:  \"/\" .  I think numbers must also have been considered to be inappropriate since I had tried:  17 July, 1973 .  Or perhaps it was the comma.  Who knows?  Who cares?  (Apparently not the bureaucrat, the programmers, or the people who tested this system.) \n\n So I typed:  My Wedding Day .  And all was well! \n\n I was quickly sent to a new page that told me my account had been created and that an email was being sent to me to confirm that I was who I said I was.  I expected this.  It’s gotten to be pretty normal step nowadays. \n\n So I went to my inbox, and there was the letter.  And the letter had the expected confirmation link.  So I clicked on that link, and my browser reacted immediately with a new tab on the healthcare.gov site. \n\n “Wow!” I thought.  That was pretty fast.  Then I read the notice on the page in that new tab.  It said:  OOPS, you didn't check your email in time.   Uhhh.  Huh?  I clicked on the link within 10 seconds of the email’s arrival.  Was I supposed to be faster than that?  Should I have been poised with my finger on the mouse button just waiting for that email to show up so I could click it faster than lightning? \n\n But then I noticed that the page also told me that If I had already confirmed my account, I could just log in using another link.  So I tried that link, but got nowhere with that either.  In the end I concluded that my account had not been created, and that the entire process would have to be repeated.  (Sigh).  Someone didn’t care about my account.  Perhaps lots of people didn’t care about my account.  I wonder why? \n\n \n\n But then I looked over at the OSX installer and I thought, “Well, let’s get on with this.”  So I clicked the install button.  Up popped a warning box telling me that I needed to close all the other applications that were running.  It gave me a button that would do that for me, and I dutifully clicked it. \n\n One by one my applications melted away.  Windows closed.  Warning dialogs popped up requesting permission to close their respective applications, which I dutifully accepted.  Click, click, click, down, down, down.  Until there were just two left.  The OSX installer window, and the Software Updater window that had told me about the new OSX version. \n\n And that’s where the process stalled.  When I clicked on the installer’s  Continue  button, it told me to close all the other applications.  When I tried to close the only remaining application, the Updater, it told me it could not close until the current installation was complete. \n\n This is a classic deadlock!  Both processes were waiting for the other to complete.  Neither could continue until the other finished.   WHO TESTED THIS?  WHO CARED ABOUT IT? \n\n So, using my great powers as a software super hero, I managed to convince the Software Updater that it should close.  And then the OSX installer took hold, restarted my computer, and entered that quasi-state, neither rebooted, nor halted, in which it does it’s installs.  This is the state you don’t want to interrupt.  Indeed, you can’t interrupt it without powering down – and powering down while it’s in the midst of rewriting your operating system is seldom advisable. \n\n And  that’s  when it informed me that this process was going to take 34 minutes. \n\n \n   Note to Apple:  Please have the courtesy to tell me, in advance, if something is going to take a long time.  Don’t let me start an irreversible operation without letting me know that my computer will be out of commission for the better part of an hour? \n \n\n So I went upstairs and took a shower.  When I returned the install process was nearly complete; and I happily watched my computer reboot. \n\n Up came the windows, one by one.  Email.  Calendar.  Chrome.  Finder.  UP, up, up.  What a nice sight.  And so I began to do my daily work. \n\n And while I was typing away, up comes the Calendar app – right on top of my current work.  Calendar throws itself in front of me, grabs the keyboard focus  while I am typing  and then pops up a warning dialog:  A new event has been added.  Please choose the calendar for this event. \n\n What event?  You aren’t showing me the event.  I can’t select a calendar unless I know what event you are talking about.  What event it is? All I see is an entire month on my screen, a month full of events.  Are any highlighted?  No.  Does the dialog name the event? No.  What event it is?  I can’t tell.  Ugh. \n\n So I choose a random calendar to get the annoying dialog off my screen.  I go back to the article I am writing.  I read the last paragraph I wrote in order to reengage the flow of my writing and… \n\n Up comes the Calendar app, right on top of my article.  It’s got another event to add.  And, same as before, it doesn’t tell me what event it is?  So, once again I click on a random calendar to make the ridiculous app go away.  I focus again on my article, reading – again – that last paragraph.  Up comes the calendar app, right on top of what I am reading.  It’s got another event to add. \n\n WTF?  OK, I guess this upgrade of the OS is going to walk through all old events and add them all over again.  Indeed, as I am reasoning this out, several dozen more warning boxes pop up telling me of new events, and demanding I assign a calendar. \n\n So now, for the next twenty minutes, I am a slave to the calendar app, as it requires calendars that I have no context to supply.  I simply hit the buttons by rote, assigning the unspecified events to whatever calendar is on top, hoping (against hope) that this does not destroy my carefully constructed calendar. \n\n Of course the Calendar finally settled down – though I expect it will rudely inject itself any time a new event is added by my assistant, or my wife, or…  And, as far as I can tell, no damage was done to the events that were in my calendar, so the reasons for that furious storm of warning dialogs remains a complete mystery. \n\n \n\n So just what is the moral of this Halloween Horror Story?  The moral is that some people care, and some don’t.  Walmart cared about my bunk bed order.  Fedex cared about delivering that bunk bed promptly.  Dorel cared about packaging that bunk bed, and about guiding me to assemble it, and about it’s overall structure and integrity. \n\n Because these people cared, my grandchildren will have a place to sleep at my home. \n\n Healthcare.gov did not care about my account, did not care about the answers to the security questions, did not care about response times.  And, in the end, did not care about providing me with healthcare insurance. \n\n Apple did not care about the deadly embrace between the Updater and the Installer, did not care to inform me about the installation time, did not care about the rudeness of it’s Calendar application. \n\n What did healthcare.gov and Apple care about?  Schedule. Not me.  Not my needs.  They cared about  their  schedule. \n\n I didnt’ need a new OSX update  today .  It could have waited for a week, or two, or ten. \n\n I  do  need health insurance.  But healthcare.gov cared more about their schedule than about being the place where I buy my health insurance. \n\n I find it sad that the careless people in this story are so obviously software people.  Perhaps that’s not fair since the Walmart website worked perfectly.  Still, the carelessness was all on the software side. \n\n Is that who we are?  Is that how we want our industry to be viewed.  Are we:   The Careless Ones ? \n\n Now perhaps you think I’m being too critical.  After all healthcare.gov is new, and OSX10.9 is new.  Shouldn’t we cut them some slack because of their newness?  I mean, they’ll get the problems ironed out eventually.  So, shouldn’t I just lighten up? \n\n They don’t have to iron out their problems on my back, thank you very much. If they cared, they could have prevented the trouble they caused me.  They didn’t care.  They released software that they knew did not work properly and had not been tested enough.  Nobody at healthcare.gov tested that damned date question, or if they did, they didn’t care.  The people at Apple  had  to know about the incredible rudeness of their calendar app; but they didn’t care.   They cared about their schedule not about me. \n\n And that leads me to my question.  What kind of organization do you want to be associated with?  One that cares?  Or one that is careless.  And if you aren’t in the right kind of organization, what are you going to do about it? \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/11/25/Novices-Coda.html", "title": "Novices. A Coda", "content": "\n       There has been some confusion about my recent post:  Hordes of Novices .  Many people who aspire to become craftsmen took the article to mean that I don’t want new people to enter the craft.  Nothing could be farther from the truth.  We do, in fact, need a growing stream of newcomers entering our craft.  Some folks who run code academies or boot camps took the article to mean that I didn’t think those schools were useful.  Again, nothing could be farther from the truth.  We need a growing and effective supply of accessible software education. \n\n What we  don’t  need is to throw masses of newly trained novices into mission critical projects without careful supervision, monitoring, and continuing education.  What we  don’t  need is to expect novices to behave like professionals.  What we  don’t  need is to continue in the absurd belief that a degree in computer science, or the completion of a boot camp, is sufficient to produce a professional software developer. \n\n The fact is that gaining competence in software development requires several years of supervised probationary experience. Novices should not be able to check code into the main line.  Their coding behavior should be carefully supervised and their code should be just as carefully reviewed by senior craftsmen.  This period of probation should last several months, after which the novices should gradually and incrementally gain greater and greater trust and responsibility. \n\n This is the way any reasonable trade or craft works.  Interns don’t do heart surgery upon getting their medical degree.  Lawyers don’t litigate supreme court cases upon getting their law degree.  Electricians don’t allow novices to wire up houses without supervision.  Plumbers don’t allow novices to connect all the water pipes in a house without supervision. \n\n Demographics. \n\n Our problem is one of demographics.  Our industry is young, and it is young in two dimensions.  The first dimension is that the entire software industry is barely 60 years old.  There hasn’t been a lot of time to learn the lessons that other industries and crafts and learned.   There also haven’t been enough programmers to learn those lessons. \n\n I got my first job as a programmer when I was 17.  That was in 1969.  How many programmers do you think were in the US in 1969?  I don’t know the answer, but I’d be willing to wager that the number was below ten thousand, perhaps below five thousand.  Now the number is orders of magnitude larger.  There are no less than ten million IT professionals in the US alone.  Worldwide the number must approach, or exceed, a hundred million. \n\n And that leads us to the second dimension of youth in our industry.   Most programmers are young . \n\n How many of those hundred million are under thirty?  Again, I don’t know the answer, but the fraction is likely to be very large, because most of the hiring has been in the last decade.  Indeed, the median age of programmers has  gone down  in the last decade. \n\n How many old guys, with 30+ years experience, are available to show these youngsters the ropes?  Not very many.  Firstly there just weren’t that many programmers 30 years ago.  Secondly, most of those programmers stopped coding 20 years ago in order to go into management or marketing.  This might have been good for them, but it hasn’t been good for our industry; because we sorely need some battle hardened leaders to guide the novices. \n\n A Structural Change \n\n The fix for these problems has already begun.  Several software firms are now employing good solid apprenticeship programs that produce competent software engineers and then turn them into competent leaders.  Other companies are sure to follow. In the coming decades all those companies will have a distinct competitive advantage: They’ll be the companies who know how to get software projects done, and done well. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/11/19/HoardsOfNovices.html", "title": "Hordes Of Novices", "content": "\n       Is the software industry trying to write the script for Hamlet by hiring a million monkeys to bang on keyboards?  Perhaps we should rethink that strategy and hire one bard instead.  Perhaps, instead of hordes of novices, we need a small team of professionals. \n\n Demand. \n Do we need more software developers?  The answer to that would seem to be obvious.  The demand for software continues to rise at an exponential pace.  We need software for our laptops, for our smart phones, for our tablets, for our thermostats, automobiles, televisions, toasters, watches, and even our doorbells.  We need software to handle the clerical burden of massive government programs.  We need software to sell our products, to buy our goods, to auction our junk.  We need software to book our travel, to sort our music, to manage our inventories, to pay our employees, to keep our accounts, to guide us to the store, and to alert us to a fire. \n\n We use software every day of our lives – every hour – every minute – probably every second.  We’ve come to depend upon it.  We need software.  We need lots of software.  Indeed, software has become so critical to our society that it may well be that our continued survival as a civilization depends upon it. \n\n So, then, clearly we need more software developers.  Right?  I mean who’s going to write all this code if we don’t get more and more and more programmers to write it?  It seems obvious.  And so companies continue to hire, more and more developers.  Entrepreneurs keep thinking up new and interesting applications and need more and more developers.  Governments keep on thinking up new massive policies that need websites and backend systems, and so they hire more and more contractors that hire more and more developers.  More and more and more and more and more. \n\n In response to this demand we see the foundations of our university system beginning to crumble.  Colleges can’t supply enough developers.  What’s more, the software industry has developed a healthy distrust in the competence of college graduates.  Interviewing bachelor trained computer science graduates can be a disheartening experience.  When it’s possible to get a degree in computer science without writing any code, the quality of the graduates is questionable at best. \n\n So up have sprung the code academies, the programming boot camps, the on-line programming courses.  There are more of them every year.  It’s a growth industry.  And they promise to turn you into a competent programmer in N weeks.  Some charge per lesson.  Some charge a tuition.  Some force you to promise a percentage of your first year’s salary.  Some make deals with employers and act like recruiters.  Some are even free. \n\n These programs are attracting people to them, and employers, desperate to add developers, are hiring them.  This trend can only continue to grow. \n\n Nine Women. One month. \n But is this what we need?  Do we need more hordes of novices?  Do you really get software built faster and better by throwing ever more barely competent bodies at it?  Is the software problem really a raw manpower problem?  Is coding the same as bricklaying?  More bricklayers means more bricks and more coders mean more code; but is more code what we want? \n\n Or do we want less code?  Less code that does more.  Much less code, written much better, doing much, much more? \n\n If one doctor can transplant a heart in ten hours, can ten nurses transplant that heart in one hour?  Can a hundred nursing assistants transplant that heart in six minutes?  Can six hundred hospital receptionists transplant that heart in one minute? \n\n Your first time. \n Do you remember that first line of code you wrote when you were very young.  Do you remember the thrill it gave you to see that line of code actually execute.  Do you remember the feeling of  power  it gave you.  You were the master.  The machine was your slave, and you could make that machine to  anything ! \n\n Many of us became programmers because of a moment like that.  Many of us quickly learned a language and then started writing dozens of lines of code, even hundreds.  We thought we were programmers. \n\n I started taking flying lessons a few months ago.  The first lesson was free!  The instructor took me up in a lovely little Piper Warrior, and then handed me the yoke.  With no instruction at all,  I was flying !  Whoo! Hoo!  I can fly!    I can do this!    I’m a pilot! \n\n See the parallel? \n\n We landed, and I signed up for lessons.  And then the instructor gave me twenty pounds of books to read.  He signed me up for hours and hours of ground school lessons.  I’ve been reading and studying like a college senior the day before finals ever since then.  I do homework.  I take quizzes.  I attend lectures.  And how much flying do you think I’ve done?  I just checked my log.  I’ve been in the air for just over three hours, including that first free flight. \n\n I’m not allowed to just hop into the airplane and do what I want.  My instructor  must  be with me every moment that I am near an airplane. \n\n At first he guided me through the pre-flight inspection, over and over.  He showed me how to crawl around the airplane checking the wheels, the prop, the engine, the wings, the fuel, the control surfaces, the oil, the air intake, the pitot, the stall warning, the…  Then he let me do it and he watched every move I made.  He corrected the slightest error and oversight.  When he was satisfied that I could inspect the plane competently, he signed my log giving me permission to do the pre-flight inspection alone. \n\n He signed my log!   If at some future time there is an incident, and it is discovered that something should have been found in the preflight inspection,  his  name will be found on my log.   He  will bear some of the responsibility! \n\n That’s Just Software \n Is our industry doing the equivalent of offering free rides to hopeful software developers, calling them pilots, and throwing them by the thousands into airplanes just to watch them crash and burn?  The evidence is pretty compelling.  There’s a lot of crashing and burning out there.  Is that because nobody is signing the log?  Is that because we haven’t really been training them to be pilots. \n\n Like the frog in boiling water, have we become so inured to the fact that software systems crash and burn that we shrug it off and say: “That’s just the way software always goes.” \n\n Cannon Fodder \n Do we really need to keep on recruiting and training cannon fodder to throw at software projects?  Or should we rethink this.  Perhaps, instead of throwing hordes of novices into the air to crash and burn, we just need a few good pilots who can carefully and competently complete the missions. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/01/20/Marion_Correctional.html", "title": "Coding in the Clink (9)", "content": "\n       My son, Micah  (@slagyr) , and I had planned to fly his  Archer  on Saturday morning; but the weather said “No.”.  So, we fell into the far-too-familiar routine of leaving the night before,  airports, commercial flights, rental cars, and hotel beds.  Sigh. \n\n In the morning the two of us drive to Marion Correctional Institution; a medium security prison run by the State of Ohio.  We pull into the visitor’s parking lot and call the organizer, Dan Wiebe  (@dnwiebe) .  He says he’s already there waiting inside with several others.  I ask whether I should leave my cell phone in the car.  He says to leave  everything  in the car except our driver’s licenses and the car keys. We comply. \n\n We walk into building “A”.  There’s a desk manned by two officers.  They are polite but there’s a no-nonsense air.  Dan is there with several others.  We introduce ourselves and then wait in a small lobby.  We’re 20 minutes early. \n\n As we stand there chatting other people walk in.  A young mother with an infant in tow.  Another with a toddler. An older woman. They all wait for visiting hours to begin. \n\n And then it’s time.  We sign in.  There’s a metal detector.  We have to walk through with our pockets turned inside out.  Mine don’t turn that way, but the officer waves me in anyway.  Coats are searched.  Car keys are examined.  Drivers licenses are placed into plastic badges and worn around our neck.  The picture in the DL is visible at all times. \n\n Then there’s a buzz, and a sturdy barred door opens.  We pass into an outdoor cage with another door closed in front of us.  That door won’t open until the door behind us closes.  Which it does moments later. \n\n We walk across the yard to another sturdy door which buzzes open as we approach.  We file in and gather before a checkpoint, where an officer behind glass carefully looks at all our badges.  The officer is not happy.  He wasn’t expecting a dozen and a half people.  He gets on a phone, clearly annoyed. \n\n While we wait in the hallway, I see a locked door with a big window.  There are wide windows on either side of the door.  All these windows expose every part of the room inside.  It’s the visiting room. It’s large, with tables, chairs, and vending machines.  It looks like a high school lunch room.  I see one of the young mothers, and the old woman, waiting there, alone.  A man and woman are sitting at a table, caressing each other’s hands, smiling, gazing at each other, making happy smalltalk.  He wears the prison blue. \n\n The officer waves us through and we walk into the prison.  It is austere, like a factory from the 1940s.  It is cloistered, like a boarding school.  The ceilings are high, and utilitarian.  The smell is oil and antiseptic, as if we were in a hospital for auto-mechanics.  There are old-fashioned radiators high on the walls, out of reach.  There are old-fashioned fans up there too – not running today.  There are stark signs on the walls, with instruction about what  not  to do.  Other signs are in friendly cursive font, telling us of up-coming events, or programs to participate in. \n\n We turn left and walk a hundred feet down a hall to another checkpoint.  We sign in again.  Farther down the hall there’s a bank of windows on the right surrounding a windowed door.  Anything going on inside that room can be seen from the outside. Fishbowl. \n\n We enter.  Facing us are a dozen and a half men wearing blue.  They are smiling some of the biggest smiles I’ve seen.  It is immediately clear they are happy we are here.  From my vantage, they are young.  Twenties.  Thirties.  Perhaps one or two are past 40.  Half are caucasian.  Others are hispanic. Two are black.  Most are tattooed. \n\n We sit in a circle.  The circle isn’t well mixed.  We outsiders are mostly on one side, the inmates mostly on the other. \n\n The outsiders range in age from young twenties to early 60s.  The distribution is only slightly weighted towards the young.  There are three young women in our group.  One of us is black.  The rest, by my lights, are Caucasian; though there are a couple of us for whom English is a second language.   We are all geeks.  We are here to write code. \n\n Dan suggests we each say our names.  Ezra, a black inmate in his thirties, with bright engaging eyes, and a wide open smile that is only slightly diminished by a missing tooth, interrupts him and suggests a name-game that they play in some of their other internal programs.  The game takes 30 minutes to play, but is a remarkable success.  At it’s conclusion, all thirty-seven of us knows each other’s names. \n\n Dan explains that some of the inmates have been coding for awhile; others only for a few weeks.  The inmates are allowed 6 hours per week at the computer.  There is no internet.  No google.  No open source downloads.  No StackOverflow. \n\n There are computers.  Donated.  A few years old, but workable.  Dual monitors!  Eclipse.  We will code in Java. \n\n We are to pair with them in 40 minute sessions, on a simple Kata which facilitates a game of Dungeons and Dragons. \n\n My first partner, Nick, is a young hispanic in his early 20s.  He is focussed.  He wants to learn.  We pair on the first part of the problem.  He’s only been coding for a few weeks; but he knows plenty.  I let him drive.  We talk over design issues.  I give him occasional guidance this way or that; which he takes gratefully and enthusiastically.  But there is little for me to correct.  He’s bright.  He’s been studying hard.  He’s doing well.  I forget where I am.  We are coding.  Life is good. \n\n We break for a retrospective, and all gather back into our circle.  Everyone is cheerful.  We talk over some design options.  Inmates listen carefully, but also share their opinions.  We are eager for the next session. \n\n My next partner is Dean.  He’s an energetic man in his early thirties.  He’s been coding for awhile.  He’s like any journeyman programmer, eager to try the next new thing.  He’s all about services.  He want’s to re-organize the code around services that manipulate immutable data.  He’s obviously been reading about functional programming. \n\n As I would with any enthusiastic journeyman, I cautiously try to redirect his enthusiasm toward the problem at hand.  I explain that reorganizing an existing system around new ideas must be done gradually and carefully, and only after one has attained great familiarity with that system.  He graciously accepts my advice; though I can feel him straining against the reins.  He’s a young man on top of his game.  He wants do to things right, and he wants to do them right now. \n\n At our next break the circle has changed.  The inmates and outsiders are randomly distributed around the periphery.  The conversation is technical and happy.  We’ve been coding for two hours and we’re full of the energy that brings. \n\n An officer interrupts the lively banter.  Roll call.  We outsiders fall silent.  Reminded.  Each inmate quickly engages the ancient routine, reciting name, number, and cell block. \n\n Another coding session.  His name is Will.  Smart.  Works hard.  Tests fail.  Tests pass.  Code is refactored.  Time flies. \n\n I need a bathroom break.  Can’t go alone.  Some others want to go too.  We are escorted down the hall.  To my right is the prisoner’s bathroom.  Fishbowl!  Ten yards later we come to a sign with big red letters.   STOP.  YOU WILL BE WRITTEN.   That sounds bad, but our escort leads us past it.  Then, on the right, there’s a door,  without a window .   MENS ROOM STAFF ONLY .  A perfectly normal men’s room like you’d find in any industrial setting. \n\n Lunch.  Pizza.  Lots and lots of Pizza and pop.  Some of the guys from PNN (The Prison News Network) want to interview us. They take us across the hall to their studio. Micah and I sit at a green screen.  Wired for sound.  A cameraman and an interviewer ask us questions about the Agile movement and Software Craftsmanship. \n\n Then it’s time for the next coding session.  My partner is Mark.  Mark is a lanky energetic man in is late 30s or early 40s.  He is smart.  Enthusiastic.  Engaging.  It was Mark who interviewed us at lunch.  It is Mark who keeps all the computers running.  Mark has been in this program the longest.  He’s been coding for a long time.  We code together, and he has ideas.  He also has questions - good questions.  The time flies.  We have fun.  Were this an interview, he’d have scored very high.  Mark has served 20 of a life sentence. \n\n Another session.  Another partner.  Gareth.  We’re talking, and coding.  We’re writing tests, and making them pass.  He knows the rules.  He’s eager to learn.  As he drives, I look at his hands on the keyboard.   P A I N  is tattooed across the knuckles of one hand. \n\n It’s time for Micah and I to leave.  We have a commercial flight to catch, and a long drive to the airport.  They give us a cheerful goodbye.  They give us gifts: a T shirt and a calendar to commemorate the event.  We are escorted out of the room, down the hall, past the checkpoints, through doors.  Doors.  Doors that open easily – for Micah and me. \n\n \n\n For another take on this day see:  Dan’s blog . \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2013/12/10/Thankyou-Kent.html", "title": "Extreme Programming, a Reflection", "content": "\n       In my hand I am holding a little white book that, fourteen years ago, changed the software world forever.   The title of that book is:  Extreme Programming Explained ; and the subtitle is:  Embrace Change .   The author is Kent Beck, and the copyright date is 1999. \n\n The book is small, less than 200 pages.  The print is large and widely spaced.  The writing style is casual and accessible.  The chapters are short.  The concepts are simple. \n\n The implications were an Earthquake whose tremors haven’t even begun to die down. \n\n Chapter 10, which begins on page 53, lays out the 12 practices that caused our industry to erupt into controversy; and spawned a revolution that has changed everything about the way we write software.  Those practices are: \n\n \n   \n     The Planning Game :\nNowadays known as SCRUM.  The idea that software is produced in short increments from a prioritized list of work items. \n   \n   \n     Small Releases :\nThe notion that deployments should be frequent and incremental. \n   \n   \n     Metaphor :\nFinally crystalized by Eric Evans in his book  Domain Driven Design .  The notion that the structure of the system is based upon a simple mental model of the problem domain. \n   \n   \n     Simple Design :\nThe notion that it is best to keep the system as simple as possible at all times regardless of what we fear about the future. \n   \n   \n     Testing :\nThe notion that programmers, and customers,  write automated tests that verify that the production code actually does what they think it should.  Nowadays we call this Test Driven Development (TDD) and Acceptance Test Driven Development (ATDD). \n   \n   \n     Refactoring :\nThe notion that the internal structure of software can, and should, be continuously improved. \n   \n   \n     Pair Programming :\nThe notion that members of a team cannot  be  a team if they work separately.  To be a team they must regularly collaborate,  at the keyboard .  In so doing they share knowledge sufficient to cover for each other as team members should. \n   \n   \n     Collective Ownership :\nThe notion that the code belongs to the team, not to the individual. \n   \n   \n     40 Hour week :\nThe notion that teams who consistently work overtime are failing. \n   \n   \n     On Site Customer :\nThe notion that someone from the business, who is responsible for requirements, must be readily and consistently available to the programming team. \n   \n   \n     Coding Standards :\nThe notion that the team adopts a consistent style in their code emphasizing cleanliness and communication. \n   \n \n\n Controversial? \n Strange isn’t it?   This doesn’t seem all that controversial does it?  But fourteen years ago it was  wildly  controversial.  Indeed, it was  so  controversial that whole books were published describing how this couldn’t possibly work, and how all the proponents were knuckle-dragging, money-grubbing, nitwits who never wrote a line of code in their lives and…. \n\n Ah, but I shouldn’t let those old feelings overtake me…  Because, after all, they’re gone – and we’re still here. \n\n Look at those twelve practices.  Which ones  don’t  you do?  Most of you, my gentle readers, likely do most of these practices on a regular basis.  While it’s certainly a stretch to say that they have become universal, it is by no means a stretch to say that they are now considered main-stream.  What’s more, those teams that don’t do all these practices today,  are trying to move towards them .  These practices have become an ideal, a goal to be achieved as opposed to a heresy to be reviled. \n\n The Churn \n The last fourteen years have been strange.  The Agile movement, which was spawned out of the controversy over Extreme Programming, skyrocketed into success, and was subsequently taken over by the project managers who all but pushed the programmers out.  We’ve seen the creation, the wild success, and the corresponding (and predictable) impotence, of certifications.  We saw the adoption of the planning game (i.e. SCRUM) without the other eleven practices; and we saw that strategy fail – becoming what Martin Fowler called:  Flaccid Scrum .  We’ve experienced continuous and vocal process churn as consultants and authors split and competed over Kanban, Lean, and every new project-management prefix-of-the-day.  We’ve seen the growth of the software craftsmanship movement, and the slow degradation and dilution of the Agile meme. \n\n But in the midst of all that hype and churn, those twelve practices have remained.  Some of their names have changed a bit.   40 Hour Week  became  Sustainable Rate .   Testing  became  TDD .   Metaphor  became  DDD .   Small Releases  became  Continuous Integration  and  Continuous Deployment .  But despite these changes the practices remain very much as they were described fourteen years ago. \n\n We also saw the name  Extreme Programming  fade almost entirely out of use.  Very few people use that term nowadays.  Some still use the abbreviation  XP ; but for the most part the name has evaporated.  It is very rare for me to hear a team describe what they do as  Extreme Programming , even when they are practicing all twelve practices as described.  The names change.  The practices remain.   The practices are persistent . \n\n Amidst the churn, the hype, the controversy, the bluster and blather.  Amidst all the chaos of humans jockeying for position over one-another.  Amidst all the messiness of human avarice, passion, and pride.  Amidst all that politics,  the practices persist . \n\n Stable Values \n I believe the practices persist because they are based on a firm foundation of stable values.  Values that Kent Beck described in Chapter 7 on page 29 of his book: \n\n \n   Communication \n   Simplicity \n   Feedback \n   Courage. \n \n\n I could try to argue why these are the right values; but I think they speak for themselves.  What software craftsman would reject any one of those values?  What software craftsman would not strive to ensure that each one of those values were represented in their work?  These values  are  values of software craftsmanship. \n\n I could try to argue that the twelve practices embrace and exemplify these values, but their persistence – despite the churn and dissolution of the names and movements that surrounded them, is evidence enough. \n\n Success \n Extreme Programming  succeeded!  It succeeded beyond the wildest dreams of its proponents.  It succeeded because it survived the controversy of its birth and the subsequent, and inevitable, churn of its advocacy.  It succeeded because it outlived even its own name! \n\n Extreme Programming  has succeeded in the way that  Structured Programming  succeeded.  Nobody even thinks about structured programming any more – they just do it.  Nobody even thinks about  Extreme Programming  any more, we are all just trying to do it. \n\n That’s success!  An idea succeeds when it outlives the movement that spawns it and simply becomes part of our everyday lives.  That’s  SUCCESS ! \n\n Looking Back \n So today, in these last weeks of 2013, take a moment to reflect back on 1999.  A time when Kent Beck wrote a ground-breaking book.  A book that changed everything.  Look back and remember:  Extreme Programming ; and recognize it as the core of what we, today, simply think of as: \n\n Good Software Practice. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/02/21/WhereIsTheForeman.html", "title": "Where is the Foreman?", "content": "\n       The foreman on a construction site is the guy who is responsible for making sure all the workers do things right.  He’s the guy with the tape-measure that goes around making sure all the walls are placed properly.  He’s the guy who examines all the struts, joists, and beams to make sure they are installed correctly, and don’t have any significant defects.  He’s the guy who counts the screws in the flooring to make sure it won’t squeak when you walk on it.  He’s the guy – the guy who takes responsibility – the guy who makes sure everything is done  right . \n\n Where is the foreman on our software projects?  Where’s the guy who makes sure all the tests are written.  Where’s the guy who makes sure that all the exceptions are caught.  Where’s the guy who makes sure all the errors are checked, and that references can’t be null, and that variables are thread-safe?  Where’s the guy who makes sure that the programmers are pairing enough, talking enough, planning enough? Where’s the guy who keeps the floors from squeaking? \n\n Without a good foreman, a construction site would fall apart into chaos.  The walls wouldn’t line up. The doors would hang askew. The cold water would come out the hot faucet, and the hot out the cold.  Without a good foreman the basement and the roof would both leak, and the fireplace would spew smoke into the living room.  Without a good foreman the construction would be delivered very late, way over budget, and have abysmal quality. \n\n Without a foreman, the floors would squeak. \n\n What would the foreman do on software project?  He’d do the same thing he does on a construction project.  He’d make sure everything was done, done right, and done on time.  He’d be the only one with commit rights.  Everybody else would send him pull requests.  He’d review each request in turn and reject those that didn’t have sufficient test coverage, or that had dirty code, or bad variable names, or functions that were too long.  He’d reject those that, in his opinion, did not meet the level of quality he demands for the project. \n\n I imagine that many programmers recoil in horror from the idea that someone else would have the power to judge their code and reject their commits.  After all, how can you get done on time if the code has to be  right ?  How can you possibly meet your schedule if you have to write all those tests?   I mean, if there’s a guy who’s actually going to  look  at the code, then there’s no way to make yourself look good by saying that the code is done when it’s not.  It’d be awful. \n\n Awful or not, it’s what most industries do.  If you want to get a project done, done right, and done on time, you need a foreman.  And that foreman has to be so technically astute that he can check the work of all the workers.  He has to have the authority to reject any work he considers sub-standard.  And he also has to have the power to say “No” to the unreasonable demands of the customers and managers. \n\n Where is the foreman on our software projects?  Where is the guy with the commit rights?  Where is the guy who makes sure all the tests are written, and all the concerns are separated, and all the right dependencies are inverted? \n\n Why don’t we have this guy? \n\n Is it any wonder that our floors squeak? \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/02/23/OhForemanWhereArtThou.html", "title": "Oh Foreman,  Where art Thou?", "content": "\n       The response to my previous blog:  Where’s the Foreman  has been mixed.  While the vast majority of folks seemed to agree; there was a vocal minority of people, whom I respect, who were quite negative.  The thing that most of these people hated the most was my insistence that the foreman be the only person with commit rights. \n\n The complaints were all based on the notion of  team  vs.  foreman .  Those who disagreed with my blog seem to feel that a software team is based on egalitarian rules, where all are peers, and none has authority over others.  Nearly all of these people call themselves  coaches ; which is odd because, of all the roles in a team, the role of  coach  is the  least  egalitarian.  The coach is  special . \n\n Some of these coaches pointed out to me that the role of a coach (as opposed to a foreman) is to teach, to anticipate problems and set up “learning moments”, to help the team “grow”.  Actually, quite a few people used the term “grow”.  Is there any way to be  less  of a peer, than to be the agent of “growth”? \n\n My point here is that  a coach is not a peer .  A coach takes  responsibilities  that the other team members do not.  And, therefore, the coach has  authority  that the other team members do not. \n\n The Perfect Team \n\n Now, for a moment, let’s imagine the perfect team.  All the team members are enthusiastic, knowledgeable, and compatible.  They write tests.  They pair.  Their code is clean.  They work from iteration to iteration, getting lots done.  The customer is happy.  The coach has nothing to do except watch, and plan for future growth.  All is well. \n\n In such a team, who has commit privileges?  Obviously everyone does; because everyone works well with each other and everyone trusts each other.  No one in this team commits bad code.  No one on this team commits code without tests.  So all have commit privileges. \n\n Do such teams exist?  For brief moments, yes.  But no team stays in this state forever.  Humans are messy.  Things happen.  And corrective action must sometimes be taken. \n\n The Story of Ron \n\n Last week Ron was a functioning member of our “perfect team”; but Ron just screwed up big time.  For the last few days he didn’t come in to work at all.  He worked from home instead.  He had a task to complete, and he committed it last night at 3AM.   Clearly he wasn’t pairing.  The code is crap.  There are no tests.  And nobody but Ron knows this because nobody expects the problem. \n\n Of course the coach calls Ron and asks if everything is OK.  Ron says he’s a little under the weather, but should be back in the office soon.  Satisfied, the coach reports to the team; and everyone remains confident, enthusiastic, and ignorant of the time bomb that has been put into the commit stream. \n\n As promised, Ron returns a few days later.  But something is different about him.  Oh, the old Ron shows through from time to time.  Some days are perfectly normal.  Other days, however, Ron is withdrawn, depressed.  It’s obvious that something is wrong; and it’s obvious that Ron is trying to overcome it. \n\n The coach talks with Ron about it.  Ron simply says there are some personal problems that he’d rather not share; and that things are going to be OK soon. \n\n When Ron pairs with others, he’s a bit more passive than usual; but is still helpful.  He sometimes gets very engaged, just like he used to.  Everyone expects that his troubles will pass. \n\n What they don’t know is that when Ron works alone, he cannot muster the will to write tests, to refactor, to clean the code.  What they don’t know is that all his mental energy is being expended in keeping up appearances.  He’s barely got enough left to get his own tasks  working , let alone clean and tested.  Nobody sees the stream of commits that is reducing the code coverage.  Nobody sees the crappy code that is coupling the system.  Nobody sees the disease that’s beginning to eat at the structure of their system. \n\n Because of their trust in their “perfect team”, they are all but blind to the suffering of their team member.  You see, what they don’t know, is that Ron’s wife has been diagnosed with Cancer. \n\n Clearly Ron’s troubles are going to continue for some time.  Unfortunately the team, in it’s blind faith in it’s own perfection, will not detect the corruption in the code for some time.  Gradually they will begin to see their coverage numbers decline.  They’ll see strange and intermittent failures of the integration test suite.  They’ll note that certain modules are becoming more and more error prone, and harder and harder to change. \n\n And then it will be discovered.  Someone on the team will trace back the commits that are causing the troubles, and they’ll realize that they are all coming from Ron.  They also realize that repairing the mess will require weeks; and that they can’t trust Ron to do it.  Indeed, they can’t trust Ron at all.  Schedules have to be revised.  Customers must be notified.  There is anger.  There is recrimination. \n\n In disgrace, Ron resigns. \n\n Reviso \n\n Now, let’s play this out again.  But this time, let’s imagine that the coach is just a little smarter, and a little less trusting than before.  Let’s say that the coach takes seriously the responsibility for technical quality, and understands that good people sometimes do bad things.  So our coach, let’s call her Jessica, reviews every commit.  This isn’t a public thing.  Jessica just does this as part of her job.  There’s no formal review, no document trail, no daily report.  Jessica just looks at every single commit, looking for problems. \n\n And, of course, Jessica finds Ron’s first bad commit within hours.  So she’s on the phone to him, asking him why he committed code without tests, code that had not been refactored.  Ron says he’s a bit under the weather but that everything will be fine soon. He promises to fix the commit before the end of the day. \n\n Jessica accepts this, but starts to pay closer attention to Ron’s commits.  Ron, aware that Jessica is watching, tries his best; but can’t muster the emotional energy to keep up appearances  and  keep the code clean.  He never does fix that commit.  He avoids pairing.  He starts to miss deadlines.  There’s no place for him to hide. \n\n The whole team can now see that something is very wrong with Ron.  Jessica confronts Ron with the evidence.  Bad commits.  Missed deadlines.  “What’s going on, Ron?” \n\n The truth about Ron’s wife comes out.  The team rallies around Ron.  Tasks are redistributed.  Ron’s load is lightened.  The team survives. \n\n Thank goodness Jessica was  watching ! \n\n Coach or Foreman? \n\n Are these two really the same role?  Of course they are!  After all, the coach of a sports team is also the foreman of that team.  The coach sets up training and practices schedules.  The coach designs the playbook.  The coach chooses the menu of plays for a particular game.  The coach chooses which players are on the field, and when.  And the coach can bench a player for infractions.  The coach has commit rights! \n\n Now, perhaps you are a coach, but you aren’t technical.  That’s OK, coaches often have assistant coaches to help them.  And the coach can delegate responsibilities and authorities to those assistants.  So if you are one of those coaches who is a process expert, but not a coding expert;  you’re going to need a coding expert to act as foreman . \n\n Commit Rights?  Really? \n\n Of course!  But look, in a well functioning team, the foreman/coach doesn’t have to withhold commit privileges.  In a well functioning team the foreman  allows  everyone to commit, and then simply, and silently, reviews the work.  If someone does a great job, an attaboy is appropriate.  If someone does a poor job, a private conversation, followed by remedial action, is appropriate.  In the normal case, everyone on the team can commit. \n\n But the foreman is the only one with  the right  to commit.  What that means is that if Angela is the foreman, she can revoke your permission to commit and reduce you to issuing pull requests.   This would, of course, be a rare occurrence; based on extreme misbehavior or malfeasance.  A good foreman would rarely use that power; but the power must be there. \n\n New team members ought not be granted commit privilege right away.  It would be wise to have them earn that privilege by demonstrating their good work through pull requests for the first few iterations.  Once earned, and appropriately celebrated, the new team member knows they are truly part of the team. \n\n In large teams, a foreman will have to find assistants to help.  Those assistants will review every commit.  The foreman will do spot checks. \n\n Is this really so strange? \n\n No, actually, it’s not.  It’s the way good work gets done.  Would you sail on a ship, or fly in an airline, without a captain?  Would you build a house without a general contractor?  Would you run a sports team without a coach who had the power to bench the players?  Would you create an orchestra without a conductor?  Would you produce a movie without a director?  Would you fight a war without a general? \n\n No, of course not.  We’ve learned that lesson the hard way too many times.  The truth about teams is that teams only function well when there is a competent leader that holds the commit rights. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/02/27/TheTrustSpectrum.html", "title": "A Spectrum of Trust", "content": "\n       The response to my two previous blogs:  Where’s the Foreman  and  Oh Foreman Where Art Thou  continues to be mixed; and has gotten quite loud.  That’s a good thing; because we need to have this discussion. \n\n ###The Perfect Agile Team \n\n At one end of the spectrum of trust within a team is the  perfect  agile team. Such a team is composed of 6-12 experienced developers with good design sense and a deep commitment to craftsmanship. They all sit around a single table.  They all start and end work at the same time.  They all pair with each other 100% of the time; changing pair partners several times per day.  They have an on-site customer who perfectly balances the backlog.  Their velocity is consistent and flat from iteration to iteration.  They all practice TDD perfectly.  Their code coverage is 100%.  They all refactor mercilessly.  Their code is clean.  They are the  perfect  agile team. \n\n Does this team need a foreman[1]?  The question is meaningless because the team is composed of  nothing but  foremen.  Everyone on the team is trusted; and every line of code is seen by a trusted pair partner over and over again.  No single individual needs to take on a special inspection role, because all members are taking that role. \n\n This team also does not need a coach or a scrum master (certified or otherwise).  The team is perfect.  No coaching is necessary.  They are  all  coaches. \n\n There are likely some teams out there that approach this ideal; but in my experience they are few and far between. \n\n ###Open-Source Single-Committer \n\n At the other end of the trust spectrum is the lone programmer who creates an successful open source project.  Let’s call her Rachel.  Rachel finds herself at the focal point of a community of willing and eager programmers, all grimly determined to be of service to her.  She is inundated by a steady stream of pull requests and patches.  Of course Rachel welcomes the help; but the contributions are all over the map as far as quality is concerned.  Some of the contributors share her values, and have done a nice job.  She has no problem committing their code.  Other contributors, however, have done all the wrong things, and have made a mess.  She can’t have those contributions contaminating her code base. \n\n So Rachel cannot afford to trust all the contributing programmers because they don’t all share her values.  And so Rachel reviews the pull requests she thinks are important; and only commits those that rise to her values and meet her standards of quality. \n\n Rachel is the foreman. \n\n The more success Rachel’s open source project enjoys, the more help she needs managing the backlog, pull requests, and patches.  To avoid being overwhelmed she chooses a trusted ally from among the most prolific and astute of the contributors.  We’ll call her Betty. \n\n In contribution after contribution Betty has proven that she shares Rachel’s values.  So Rachel gives Betty commit rights and the two of them work together on the backlog, and on reviewing the stream of contributions from the user community.  They are now sharing the foreman role. \n\n Of course this process will continue until the core team of highly trusted individuals rises to a level that can deal with both the backlog and the contributions.  This core team has shared values and mutual trust.  They all act as foremen over the contributions. \n\n We could view the combination of the core team and the community of contributors as an extended team. The trust level in this extended team is low because the contributors don’t necessarily share the values of the core team.  Rachel, Betty, and the rest of the core team trust each other well enough; but the community at large is another matter.  In order to maintain the quality of the project, and keep the standards of excellence high, the core team must review each request and reject all those that don’t meet their standards. \n\n There are many projects that use a model like this.  Junit, FitNesse, NUnit, Linux, git, etc.  Some of the very best software out there uses this approach.  Indeed, much of the software revered within the Agile community, was produced, and is maintained, using a foreman model. \n\n ###How do I convince the rest of my team?\nIn my travels I encounter many different software teams. Very few are high functioning Agile teams. Most fall pretty low on the trust scale.  To one extent or another, most of these teams pay lip-service to Agile principles; but do not pair, do not consistently write tests, do not refactor, do not have an on-site customer, etc..  They are, in fact, regular old dysfunctional software teams with all the problems you’d expect. \n\n In most cases I encounter these teams because a few members attend a course of mine; or convince their management to bring me in an teach some part of the team.   I teach them TDD.  I teach them Refactoring.  I teach them Craftsmanship.  I teach them design principles.  I teach them professionalism.  I teach them why it is important to care.  I teach them why it is important to never rush.  I teach them that the only way to go fast is to go well. \n\n And then, at the end; when I am done teaching and about to leave them, they ask me  the most common question I hear  : \n\n \n   How do I convince the rest of my team to do the things you’ve taught? \n \n\n And there you go.  That’s the same trust issue that Rachel and Betty had, isn’t it?  The members of the team don’t value the same things; and so they don’t trust each other to write the tests, refactor the code, and keep things clean. \n\n ###Enter the Coach\nLots of people try to solve this misalignment of values through coaching.  The team hires a coach, and the coach works with the team, addressing fears, allaying suspicions, guiding, teaching, cajoling, and gradually, softly, moving the team towards trust. \n\n When this works it is a beautiful thing. \n\n It doesn’t work very often.  The track record for coaching dysfunctional teams is less than stellar.  The landscape is littered with some pretty spectacular failures; while the notable successes are few and far between. \n\n This is not to say that coaching is a bad idea; it is not.  Coaching is a very good idea  when the values of the team members are aligned .  When you have a team that is willing to give it a try; because they truly do value the same things; then a coach can make a huge difference.  But a coach is not the answer when you have a fundamental disagreement of values inside the team. \n\n That’s why the coaches of sports teams are also foremen.  Not only do such coaches teach, guide, advise, and cajole; they also have the power to say “no”.  A coach, faced with a disagreement of values, must have the power to draw a line in the sand and say: “Only things on this side of the line may pass”. \n\n ###My Answer to That Question.\nSo, at the end of my talks, or courses, when the team members ask me how to convince their other team mates; I tell them this: \n\n Most often you  can’t  convince people to change their behavior.  At least not by talking.  Telling people that they are doing things wrong, and that  you  know how they should do them right, is not a good way to convince someone.  What you  can  do, is be a role model.  You can stick to your disciplines and values, and show the others, by example, how to behave.  Perhaps one or two others will notice what you are doing, and decide to emulate you. \n\n And then you’ll have a problem.  Because in a team where some follow TDD (for example) and others don’t, the level of conflict will rise until there is a divorce.  Usually that divorce involves developers leaving the team to find another team that they are more aligned with. \n\n That period of conflict can be long and painful; and the quality of the software will suffer throughout that period leaving a legacy of troubles. \n\n ###Foremen\nIf, however, the person asking me that question happens to be someone who has the appropriate responsibility and authority I tell them to look at the open-source model; because that model has proven effective in teams that have significant trust issues. \n\n I tell them to form the people they trust into a core team; and give that core team the power to draw a line in the sand, the responsibility to declare standards of quality and behavior, and the authority to enforce those standards.  i.e. a team of foremen. \n\n ###Precipitate the Crisis\nSeveral people who read my previous articles have blogged that foremen will cause distrust and destroy the team.  However, the team I am referring to is already in a state of distrust, and is already on a slow and painful course to destruction.  The bloggers are right; the foremen will accelerate that.  They’ll bring the issue to the fore and precipitate a crisis. \n\n And that’s a good thing.  I want the crisis to come quickly.  I want the period of divisiveness to be short.  I want those who are going to change to change sooner rather than later.  I want those who are going to leave, to leave sooner rather than later.  I want to get a team with shared values and mutual trust in place as quickly as possible.  And while that process is going on, I want the code to be protected by the people I trust. \n\n ###End Game\nThe goal, of course, is to increase the ranks of the trusted team members and decrease the ranks of those that aren’t trusted.  The goal is to create a high functioning agile team in which everyone shares the same values.  The goal is to obviate the foremen because  everyone  has become a foreman. \n\n But most teams aren’t high functioning Agile teams. Most teams are caught in a divisive struggle over disciplines, quality, and professionalism. Most teams need to draw a line in the sand and decide what their values are, and what disciplines, behaviors, and standards they will use to support those values.  Most teams need to set, and then enforce, quality standards. \n\n The open-source committer model (i.e. using foremen) has been shown very effective at achieving those ends. \n\n \n [1] In this paper the term ‘foreman’ is used in it’s gender neutral form.  No gender bias is intended by the author; nor should any be imputed by the reader. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/01/27/TheChickenOrTheRoad.html", "title": "The Domain Discontinuity", "content": "\n       What came first, the chicken or the road? \n\n I read, with interest, Justin Searls’  Post #13 .  First I am going to express my deep disagreement with the Author.  I will refute his arguments and utterly destroy his conclusions.  And then, once I’m done salting the ground where he used to live, I’ll tell you why I completely agree with him. \n\n Destruction. \n\n The author bemoans the failures in the way many of us teach Test Driven Development.  He enumerates a set of woes encountered by students as a result of that teaching method.  He then describes a different technique, for teaching and executing TDD, that he believes will eliminate those woes. \n\n I agree that many people have encountered the woes he describes.  Indeed,  I  encountered those woes when I first learned TDD.  I became adept at TDD by  solving  those woes.  And therein lies my problem with the Author’s thesis.  You don’t learn complex things by having the solutions given to you.  You learn complex things by solving the problems that those complexities present.  Solving the issues of TDD is the  homework  that all developers must do if they are to become adept. \n\n Here are the woes that the Author presented. \n\n Large Units. \n\n Given that you are following the discipline of TDD, it is true that when you have a big feature to  write, you’re going to write a lot of little tests.  It is also true that programmers who are new to TDD will tend to make those tests pass inside a single large function.  That function will probably grow, and grow, and become a mess. \n\n The Author rightly says that TDD instructors warn their students about this mess and instruct them to refactor frequently.  The Author also rightly says that many students won’t do that refactoring because they are under pressure to get features done.  The Author finally points out that an appeal to professionalism is inadequate to get these programmers to refactor frequently enough.  All true. \n\n However, while the Author describes this as a failure, I describe it as necessary.  You don’t learn to refactor by being told.  You learn to refactor by experiencing the alternative –  and then remembering what you were told . \n\n Learning to refactor is a hill that everyone has to climb for themselves.  We, instructors, can do little more than make sure the walking sticks are in your backpack.  We can’t make you take them out and use them. \n\n A good TDD instructor gives everyone a stern warning, and then gives them a few exercises that are designed to make them stumble if they don’t refactor.  In the courses I teach, I watch this with a certain amount of glee.  I see the students make the same mistakes I made when I was learning; but in a much safer environment.  As they fail, I see “the lights go on”. \n\n To get to the other egg. \n\n The Author continues with a causal chain that I don’t really understand.  Here is the essence of it: \n\n Extraction is Costly? \n\n The Author argues that the large units created by passing many tests must be broken up by extracting smaller units from them,  and that extracting such units is costly.   That is not my experience at all.  Done early and frequently, extraction is  trivial .  Most of the time it amounts to nothing more than selecting a few lines of code, and then hitting  ctrl-alt-M . \n\n Tests must be rewritten? \n\n Next the Author argues that the new smaller units need new unit tests.  That’s news to me.  I certainly don’t rewrite my  tests just because I extracted some functions or classes. \n\n It is a common misconception that the design of the tests must mirror the design of the production code.  TDD  does not  require, as the Author suggests, “that every unit in your system is paired with a well-designed […] unit test.”  Indeed, that’s one of the reasons that many of us have stopped calling them “unit” tests. \n\n Let me stress this more.  I  do not  create a test for every method or every class.  I create tests that define  behaviors , and then I create the methods and classes that implement those behaviors. \n\n At the start, when there are just a few tests, I might have only one simple method.  But as more and more tests are added, that one simple method grows into something too large.  So I extract functions and classes from it –  without changing the tests .  I generally wind up with a few public methods that are called by my tests, and a large number of private methods and private classes that those public methods call;  and that the tests are utterly ignorant of . \n\n By the way, this is an essential part of good test design.  We  don’t  want the tests coupled to the code; and so we restrict the tests to operate through a small set of public methods. \n\n An Effect without a Cause. \n\n The Author continues his argument by asserting all the various woes that come about because of all those new tests.  He suggests that the new tests are redundant, messy, written after the fact, over mocked, etc. etc.  He describes a nightmare scenario.  Fortunately, that’s all it is, since in the waking world we don’t write those new tests. \n\n Get it right the first time. \n\n The solution that the Author presents is, in short,  Waterfall. \n\n Actually, he proposes a set of teeny-weeny waterfalls.  His recommendation is: \n\n \n   Start writing a unit test for the entry point, but instead of immediately trying to solve the problem, intentionally defer writing any implementation logic! Instead, break down the problem by dreaming up all of the objects you wish you had at your disposal… \n \n\n The Author’s goal seems to be the elimination of refactoring.  He seems to be saying: “Let’s just design it correctly up front.”  His goal is apparently to create the same structure that refactoring would have created; but without the refactoring step. \n\n The Author has apparently forgotten that up-front design is a risky proposition.  Often when you “dream up the objects you wish you had” you find those dreams were actually nightmares.  To correct those nightmares, you have to:  refactor . \n\n And so, up front design does  not  eliminate the refactor step; it just postpones it until it is very expensive. \n\n Salt the Earth \n In short, the Author recommends that we teach students to do TDD by having them do Waterfall.  (Cue: Nuclear Explosion). \n\n I Completely Agree \n\n If you step back from the Author’s actual examples, and view his post in terms of system architecture, then  everything changes . \n\n \n   Refactoring across architectural boundaries is  costly . \n   Behaviors extracted across architectural boundaries need  newly rewritten  tests. \n   Architecture is an  up-front  activity. \n \n\n As an example, let’s just say that you wrote some tests for a feature, and you implemented that feature in a way that passed the tests, but was an architectural nightmare.  Let’s say that you’ve got SQL in your GUI, you’ve got business rules in your SQL, and you’ve got formatting code in JSPs.  You know, like 90% of the code out there… \n\n Separating the GUI, the business rules, and the database from such a mess is very costly.  What’s more, since the tests run through the GUI and use the Database, they are slow; and must be  rewritten  to be fast by using mocks and respecting boundaries.  You know, all that good test design stuff we’ve been yelling about for the last decade. \n\n The solution to  that  problem is to  know in advance  where you are going to put certain behaviors.  You need to know, in advance, that there will be a boundary between the GUI, the business rules, and the database.  You have to know, in advance, that the features of your system have to be broken up into those areas.  In short, before you write your first test, you have to “dream up the [boundaries] that you wish you had”. \n\n The Domain Discontinuity \n\n What’s going on here?  First I say you  don’t  want to do your design up front; then I say that you  do  want to do your design up front. Which is it? \n\n Both.  Or rather, it depends what kind of design it is.  So let’s look deeper. \n\n When we follow the TDD discipline, we write tests and get them to pass.  What do those tests describe?  They describe features requested by the customer.  The tests are a kind of  formal statement of the requirements .  They describe the problem domain. \n\n The design of the code that passes those tests is closely associated with the problem domain that the tests describe.  In the red-green-refactor cycle, we extract methods and classes in order to better fit within the problem domain described by the tests. \n\n But there is another level of software design – a level that has nothing to do with the business rules, or the problem domain.  This is the level of system architecture. \n\n Architecture Classifications \n\n Applications can be mapped into several broad classifications.  For example, our application might be a request/response system, like nearly all websites.  Or it might be an event driven system, like most computer games.  Or it might be a batch processing system, like most old banking and manufacturing applications. \n\n These classifications have nothing to do with the business rules, nor the problem domain.  And yet these classifications have a profound effect on the shape and design of the software.  That shape, that design, is architecture. \n\n If you were to look at two completely different event driven applications, say Quicken and Minecraft, the odds are that you would find a lot of similarities at the highest levels.  The same should be true of any two request/response systems, or two batch systems.  At their highest levels, there will be similarities in shape, flow, and structure. \n\n This shape is the system architecture, and it is dictated by the desired user experience –  not  by the problem domain.  I could, for example, write an accounting application, like Quicken, in a batch style.  I could, though it would be a deep shame, write an adventure program like Minecraft, in a request/response style.  So architecture and problem domain are discontinuous – they do  not  form a continuum – they differ in  kind  not just in level. \n\n This discontinuity means that, although we can use TDD to help us with the design of the problem domain,  we cannot use it to help us with the architecture.  TDD can’t even be  begun  until we know the shape of the system that is to be created. \n\n So we have to decide the architecture up front, based on our desired user experience, and then we can use TDD to help us design a problem domain that lives  within  that architecture. \n\n Fortunately, the number of system architectures is relatively small; and there are a number of well-conceived patterns that they fall into.  What’s more, the architecture of the system is almost entirely a function of the desired user experience. \n\n So our up front decisions can be limited to choosing a user experience, and choosing the architectural pattern that is most consistent with that user experience.  Once those choices are made, we can TDD the problem domain into existence. \n\n Conclusion \n\n And so the status of our misbegotten Post #13 is a quantum-bit.  Like Shrodinger’s cat, it lives in a superposition of states.  It is right after all; except that it is dead wrong.  And in being wrong, it is certainly correct.  Which state it finally collapses into depends on which side of the Domain Discontinuity you stand. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/03/28/The-Corruption-of-Agile.html", "title": "The ", "content": "\n       Andrew Binstock recently wrote a blog entitled  The Corruption of Agile , which built upon another blog by Allen Holub entitled  The Agile Holocracy .  Allow me to summarize: \n\n Holub says: \n \n   “…agile is a culture, not a set of practices…” \n \n\n Binstock amplifies: \n \n   “Whether a site is Agile or not depends on its culture. Does the culture support the personal values of the manifesto? If so, it’s Agile, if not, then it’s doing something else. So, indeed you could have a fully Agile site without TDD, continuous integration, or scrum. Likewise, you can have a site that uses all three practices, but cannot adapt to changes and is wholly inflexible in its work — so, not at all Agile.” \n \n\n At first blush these statements seem reasonable.  After all, a culture is an expression of values.  However, Holub’s and Binstock’s statements carry with them the implication that culture and practice are separate.  They are not.  Indeed, culture is  defined  by practice. \n\n You know the culture you are in by observing the practices of the people around you.  If you see a woman in a burka, you can guess her culture.  If you see a bride and groom breaking a glass under a canopy, you can guess the culture.  If you see a batch of children swinging a stick at a paper mache donkey hanging from a tree, you can guess the culture.[1] \n\n You can’t have a culture without practices; and the practices you follow identify your culture. \n\n ###Example\nAs an example of this I was once a member of a Jiu Jistsu dojo:  Hakko Ryu, the School of the Eight Light .  We adhered to the practice that, upon entering or leaving the dojo, we would bow to the dojo.  This was an expression of respect for the place in which we learned, and for the knowledge and skills that we gained.  If a new student joined, they would quickly observe and emuluate this practice.  No one had to tell them to do it.  The practice was contagious. \n\n One day, our Sensei proposed a new practice.  If you were late to class, instead of simply bowing, you would also drop down and do 10 pushups.  He asked us if we would accept this new practice.  This created an immediate schism.  The younger folks were all in favor of this new practice because it was an expression of respect for the value of punctuality and also respect for those students who were on time.  Others of us were opposed because this new practice represented a disrespect for those of us who had complex schedules that made regular punctuality impractical.  We valued our professions and our marriages above the dojo and did not want that value punished. \n\n The Sensei had made his proposal at the end of a class, and asked us how we felt.  I blurted out that I was “fundamentally opposed”.  When asked why, I could not articulate the reason.  The practice struck at a value that was so deep and intrinsic, I could not find the words.  Indeed, even though this event happened over fifteen years ago, I only just found the words as I wrote this blog.  At the time I simply responded by saying I needed more time to process. \n\n The class ended with no decision.  The subsequent scene in the locker room was grim.  Those of us who didn’t like the proposed rule eyed each other and shook our heads.  One member even vocalized his frustration by saying “things are changing for the worse around here.” \n\n Fortunately, before this negativity could get out of hand,  the Sensei walked into the locker room and said: “Forget I said anything about it.  Please act as though the proposal had never been made.”  This was a very wise move.  The negativity and suspicion had not had time to take root, and by the next class it was gone. \n\n This is an example of how deeply entangled practices are with values and with culture.  Cultures express their values through their practices.  It is absurd to imply, as Holub and Binstock do, that practice is irrelevant to culture. \n\n Despite Binstock’s assertion, if you find yourself in a team who practice continuous integration, short iterations, pair programming, and test driven development,  it is a powerful indication you are in a team who shares the values of the agile manifesto .  If they did not share those values, they would not follow those practices. \n\n ###Ritualism\nOf course it’s possible for a team to be so entrenched in practice that, over time, they forget the values expressed by those practices.  This is a common failing of bureaucracies and religions.  They become so strongly defined by their practices, that the practices become rituals, the original values are forgotten, and the rituals  become  the values. \n\n The fear of ritualism is appropriate.  In 1999, when Kent Beck and I decided to put our energies into the promotion of Extreme Programming, we feared that we could be starting a religion instead of a movement, and vowed to fight ritualism when it arose.  This concern and vow was expressed again in the 2001 meeting that produced the Agile Manifesto. \n\n But in the years since, ritualism has not been the problem.  We don’t see lots of people ritualistically practicing pair programming, test driven development, small releases, on-site customer, etc.  We  do  see people adopting these practices out of enthusiasm.  But enthusiasm should not be mistaken for religion or ritualism.  Enthusiasm implies a change to the status quo; ritualism implies the exact opposite. \n\n Perhaps it was the fear of ritualism that motivated Holub and Binstock to suggest the separation of practice from culture.  Perhaps they fear that the emphasis upon practice is necessarily a deemphasis of value.  But this is entirely incorrect.  The current enthusiasm for TDD, for example, is a very deep expression of value. \n\n In any case, if you separate the practices from a culture, as Holub and Binstock suggest, then you corrupt the culture.  You simply cannot have a culture devoid of practice. \n\n ###TDD\nI raised the specter of TDD because Binstock railed against it rather loudly in his blog.  For example: \n \n   “It will pain some readers to know that the vast, error-free Internet predates Agile and even predates TDD. Crazy, right?” \n \n\n And again: \n \n   “When I speak with adherents of test-driven development (TDD) in particular, there is a seeming non-comprehension that truly excellent, reliable code was ever developed prior to the advent of this one practice. I sense their view that the long history of code that put man on the moon, ran phone switches, airline reservation systems, and electric grids was all the result of luck or unique talents, rather than the a function of careful discipline and development rigor.” \n \n\n These rather snide complaints are disappointing.  Is a practice like TDD, or the enthusiasm for that practice, so threatening that it should be answered with derision? \n\n Of course good software was built before TDD.  Good software is being built  today  without TDD.   So What?  Those facts imply nothing at all about the effectiveness of TDD.  After all, many doctors saved lives before the practice of sterile procedure, and many accountants managed to keep reasonable accounts before the practice of double entry bookkeeping.   So what?   Past success does not imply the ineffectiveness of new practices; nor does past success imply that the enthusiasm for new practices is inappropriate. \n\n ###The Tension of Adoption\nI understand why people might look at new practices with skepticism.  The enthusiastic adoption of a new practice by one group creates tension with others who do not adopt the practice.  The adopters can make the non-adopters feel slighted and invalidated; as if everything they had ever done in the past was wrong.  The adopters, in their enthusiasm, may claim that adoption is a new requirement of professionalism.  The non-adopters may claim that the adopters are fanatics who are detached from reality and ignorant of the past. \n\n Certainly this happened with sterile procedure in medicine.  The adopters were derided and dismissed by the medical establishment for sixty years before the adopters eventually outnumbered the non-adopters.  Doctors at the time did not like to think that they were  causing  disease by failing to wash their hands.  They also expressed their concern about the time such washing procedures would require.  Then, as now, doctors were busy people.  Hand-washing rituals would reduce the number of patients that could be treated. \n\n Double entry bookkeeping had an even more checkered history, being adopted, forgotten, re-adopted, decreed, and ignored, for three centuries.  The fight to make that practice mainstream was long and difficult. \n\n Nowadays these practices are ingrained in the cultures of medicine and accounting.  It is hard to imagine a doctor who fails to scrub before surgery, or an accountant who uses single-entry bookkeeping for corporate accounts. \n\n The new practices won out over the old ones because the population of those who did not wish to change their practices gradually declined while the population of those who were enthusiastic about the new practices grew. The new practices expressed a  shift  in the values of the cultures that adopted them.  Those practices cannot nowadays be separated from those shifted cultures. \n\n ###The True Corruption of Agile\nThe biggest problem I have seen within the Agile movement is the  elimination of the practices .  One by one, over the years, the practices have been de-emphasized, or even stripped away.  This loss of practice has diluted and changed the Agile culture into something that I don’t recognize as Agile any more.  It has been a shift away from excellence towards mediocrity, away from hard realities, towards feel-good platitudes. \n\n It began with the notion that anyone could become a “master” of anything by sitting in a two day class and getting a piece of paper.  Soon to follow was the dilution and eventual loss of the technical practices. This prompted Martin Fowler to publish his classic and definitive blog:  Flaccid Scrum .  Then came the emphasis of project management over craftsmanship and the rise of the soft skills (attitudes) over the hard skills (practices). \n\n At that 2001 meeting in Snowbird where we wrote the Agile Manifesto, Kent Beck stated one of our goals: “..to heal the divide between development and business.”  Unfortunately the deemphasis of practices within the Agile movement has only served to widen that divide.  While project managers have flocked into the Agile movement, developers have fled out of it.  The original movement has fractured into two movements.  The  Software Craftsmanship  movement has preserved the coupling between practice and culture; whereas the Agile movement has shifted away from it. \n\n So, to my mind, the true corruption of Agile is Holub’s statement: \n \n   “…agile is a culture, not a set of practices…” \n \n\n No.  Agile is a culture  expressed through a set of practices . \n\n Are those practices set in stone?  Are the original 13 practices of XP the holy writ?  Are you an apostate if you don’t practice TDD?  Of course not.  But if you don’t use those particular practices, you’d better use some that are as good or better.  And the practices you use will define your culture and be an expression of your values. \n\n If your values are those of the Agile Manifesto, then your practices will likely look a  lot  like those original 13; because to a large degree it was those 13 practices that drove us to write that manifesto. \n\n If you’ve got better practices, I’d love to see them.  If I believe they are better, I’ll adopt them in a heartbeat.  Because, under no circumstances, will I become the doctor who gets in the way of hand-washing. \n\n \n\n \n   [1] If your political correctness alarm just started to ring, you can shut if off.  Yes, I’m profiling.  I’m profiling because  cultures define profiles .  If a culture did not define stereotypes and profiles it would not be a culture.  There is no such thing as a culture without profiles. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/04/03/Code-Hoarders.html", "title": "Code Hoarders", "content": "\n       Have you ever watched an episode of  Hoarders , or a documentary about a hoarder?  Shows like this were popular a few years back.  They showcased the tragic, and all-too-common, phenomenon of people who have lost control over their living space by filling it with so much junk that there’s no place for them to live. \n\n These people fill their homes so full of stuff that all they have left are dark and narrow passageways winding through floor to ceiling towers of junk, trash, old food, pet droppings, dirty laundry, and other less mentionable things.  The volume of each room is taken up by junk.  The counters, the tables, and the furniture are invisible, buried, beneath junk.  The bedrooms and bathrooms are piled high.  There is no room to move.  No room to eat.  No room to sleep.  No room to live. There have been cases in which dead bodies have been found beneath the piles of rubbish and detritus. \n\n And yet, every day, these pitiful hoarders bring another new load of junk into their homes.  And they pile it on top of the towers of older junk.  Sometimes those towers wiggle and wobble, and then fall – blocking the narrow paths that have been carved through the mountains of trash. \n\n Google for “Hoarders” if you’ve never seen one of these shows; and be prepared to be disgusted. \n\n If you have watched a show like this, then perhaps you have wondered how anyone could live like that – how anyone could  choose  to live like that.  But think about it.  It’s not that hard to understand.  It’s easy to rationalize that the “stuff” you are bringing into your house is adding value to your life.  After all, it’s new stuff.  New clothes.  New appliances.  New furniture.  New books.  It’s  property !  And property has value.  So it’s easy to think: “The more stuff, the better.” \n\n It’s also easy to believe that  acquiring  new stuff is more important than  organizing  old stuff.  After all, organizing old stuff does not add value.  It’s acquiring new stuff that counts.  So if there’s any spare time, or spare effort, or spare energy, it should be directed towards acquiring new stuff.  Organizing the old stuff is a waste. \n\n Finally, once the mess starts to grow, it’s easy to rationalize away any hope that it  can  be cleaned.  You quickly conclude that there’s no point in cleaning  anything  because a day’s effort, a week’s effort, even a month’s effort  wouldn’t make a dent .  To quote Dr. Seuss from  The Cat in the Hat : “…this mess is so big, And so deep and so tall, We cannot pick it up. There is no way at all!” \n\n ###Code \n\n Now of course you realize that I’m talking about code.  I’m suggesting that the mentality that creates crufty tangled systems is the mentality of a hoarder.  I’m also suggesting that the result is the same: an unhealthy, unworkable, and unlivable environment.  But let’s walk step by step through the reasoning. \n\n Have you ever been a  Code Hoarder ?  Have you ever worked on a project that was built by  Code Hoarders ?  Do you wander through dark passageways of floor to ceiling cruft as you attempt to add some new feature or fix some old bug?  Do the towers of junk wiggle and wobble and threaten to fall and block your path?   Are there whole rooms of code that you dare not visit?  Is the structure of the system invisible and buried under piles of new features and dead code? \n\n Do you believe that adding value means adding new features.  Do you believe that acquiring new features is more important than organizing old features?  Do you believe that effort applied to cleaning up the old system is wasted effort?  Have you given up on any effort of cleaning because you know that you don’t have the time to make a dent in the mess?  If you have, you are a Code Hoarder; and your work life is a deep and overwhelming mess. \n\n We have a name for the result.  We call it  Legacy Code .  The very term fills us with disgust and despair. \n\n ###Legacies \n\n Who gets to clean up that horrific mess when the Hoarders die?  Who hires the garbage trucks to haul away the hundreds of tons of junk?  Who hires the HAZMAT team to disfinfect the home?  Who hires the cleanup crew to scrub and wash, and paint, and repair the home?  The children, of course.  Or if not the Children, then the Community.  The legacy that a Hoarder leaves is a legacy of junk, trash, filth, and the huge effort required to get it all cleaned up.  The legacy of a Hoarder gets worse and worse so long as the Hoarder is alive. \n\n But legacies don’t have to be that way.  Indeed, most people strive to leave a legacy of  improvement .  Most people want to leave something behind that makes life better for those who follow.  Indeed, it is the work of most people’s lives to continuously improve their legacy. \n\n A  true  legacy gets  better  with age. \n\n ###Better with Age? \n\n And isn’t that what humans do?  Don’t humans make things better with time?  If you’ve got an old car in the garage that you are refurbishing, don’t you expect that car to get better and better and better with each passing day?  If you are painting a picture, don’t you expect that picture to get better and better with age? \n\n Can you imagine hiring an artist, a mechanic, an engineer, a doctor, a lawyer, or anyone else, whose work products get  worse  the longer they work on them?  How can making things worse, day after day, be the behavior of a professional?  How can it be the behavior of a programmer? \n\n A professional team of programmers make their code better each day.  The quality of the code  improves  with time.  That’s how you know they are professionals.  Professionals make things better with time.  The Legacy code left by professionals will be cleaner and cleaner the older it is because it has enjoyed the long attention of those professionals. \n\n Do you make the code better each day?  Or does each day leave the code a little bit worse than the day before? \n\n Is your work leaving a legacy of value, or are you just building a monstrous hoard for someone else to clean up? \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/04/25/MonogamousTDD.html", "title": "Monogamous TDD", "content": "\n       When a  blog  begins like this… \n\n \n   “Test-first fundamentalism is like abstinence-only sex ed: An unrealistic, ineffective morality campaign for self-loathing and shaming.” \n \n\n … you have to wonder if the rest of the post can recover its credibility, or whether it will continue as an unreasoned rant. \n\n Take the first two words: “ Test-first fundamentalism ”.  Fundamentalism is a term that  used  to mean: “back to basics”; but since 911 has taken on the connotation of violent extremism.  I have yet to see any test-driven developers flying airplanes into buildings while repeatedly hollering: “Kent Beck is great!”, so I must entirely reject the connotation. \n\n Are there test driven developers who are passionate about their discipline?  Absolutely, myself included; but I have yet to hear about hordes of TDDers rampaging through the countryside, conquering by the sword.  So, I must conclude that the author used the word “fundamentalism” as a snide way of casting those of us who practice Test Driven Development in a deeply pejorative light. \n\n Take the next sentence fragment: “ abstinence only sex ed ”.  To my knowledge no one seriously teaches abstinence only sex education.  What  some  folks teach is monogamy: the practice of keeping sexual contact within the context of a committed relationship e.g. marriage. \n\n Now it’s true that in certain religious groups monogamy is coupled to morality.  However, there are secular schools of thought (my own) in which monogamy is simply thought of as a good personal strategy without deep moral consequence.  Given the Herpes epidemic of the 70s that was overshadowed by the ongoing HIV epidemic that began in the 80s and 90s; and given that single motherhood is among the strongest factors correlated with poverty, perhaps the strategy of monogamy should not be used as a pejorative adjective. \n\n Of course people might think that, given the divorce rate of 50%, the strategy of monogamy is less than optimal. However, when one considers that 70% of first marriages last until the death of one partner; and that a similar fraction of second marriages last almost as long, the strategy of monogamy starts to look a bit better. \n\n And, after all, from a purely practical point of view, there is simply no better  personal  strategy for preventing disease and unwanted children.  (And, at least in my experience, a happy and rewarding life.) \n\n In that light, the second half of the author’s statement: “ An unrealistic, ineffective morality campaign for self-loathing and shaming. ” starts to look pretty, well… ignorant. \n\n Of course I understand what the author was trying to say.  There is a stridence in the preaching of TDD that makes him uncomfortable.  I have used that stridence myself; and I believe the stridence is called for.  The reason is simple.  As an industry, we suck.  If you aren’t doing TDD, or something as effective as TDD, then you  should  feel bad. \n\n Why do we do TDD?  We do TDD for one overriding reason and several less important reasons.  The less important reasons are: \n\n \n   We spend less time debugging. \n   The tests act as accurate, precise, and unambiguous documentation at the lowest level of the system. \n   Writing tests first requires decoupling that other testing strategies do not; and we believe that such decoupling is beneficial. \n \n\n Those are ancillary benefits of TDD; and they are debatable.  There is, however, one benefit that, given certain conditions are met, cannot be debated: \n\n \n   If you have a test suite that you trust so much that you are willing to deploy the system based solely on those tests passing; and if that test suite can be executed in seconds, or minutes, then you can quickly and easily clean the code without fear. \n \n\n Now there are two predicates in that statement, and they are  big  predicates.  But, given those predicates are met, then developers can quickly and easily clean the code without fear of breaking anything.  And  that  is power.  Because if you can clean the code, you can keep the development team from bogging down into the typical  Big Ball of Mud .  You can keep the team moving fast. \n\n Indeed, the benefit of keeping the code clean, and keeping the team moving fast, is so great, that those two predicates begin to pale in comparison.   Yes!  If I can keep the team moving fast, then I  will  find a way to trust my test suite, and I  will  keep those tests running fast. \n\n Anyway, that’s where the stridence comes from.  Those of us who have experienced a fast and trustworthy test suite, and have thereby kept a large code base clean enough to keep development going fast, are  very  enthusiastic.  So enthusiastic, in fact, that we exhibit a stridence that the author has unfortunately, and inaccurately, dubbed  as “fundamentalism”; claiming it to be ineffective and unrealistic. \n\n What does the author suggest as an alternative?  As someone who writes systems in Rails he suggests integration tests that use the database and operate through the GUI (using Capybara). \n\n My response to this is: If you can meet my two predicates of trustworthiness and speed, go for it!  If you trust those integration tests so much that you are willing to deploy when they pass; and if they execute so quickly that you can continuously and effectively refactor and clean the code, then you aren’t doing any better than me.  Do it. \n\n But (and this is a big “but”), it seems to me that integration tests have very little chance of meeting my two predicates. \n\n First I doubt they can attain the necessary trustworthiness because they operate through the GUI; and you can’t reach all the code from the GUI.  There’s lots of code in a normal system that deals with exceptions, errors, and odd corner cases that cannot be reached through the normal user interface.  Indeed, I reckon you can only cover a bit more than half the code that way.  It seems unlikely to me that anyone would be willing to deploy a system based on tests that leave such a large fraction of the code uncovered. \n\n Second, it seems very unlikely to me, despite the ability to spin up hundreds of servers in the cloud, that you can get those tests executed in anything like the speed you’d need to effectively and continuously refactor the code. Databases and GUIs are  slow . \n\n Now, I could be wrong.  I’d love to be proven wrong.  And perhaps, despite the author’s poor reasoning at the start of his blog, he  really can  trust his tests enough, and execute them quickly enough, to make them effective for keeping the code clean.  If so, then I’ll holler: “Amen, Brother, and Hallelujah!” and will become a strident convert to his particular brand of fundamentalist polygamy. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/03/11/when-to-think.html", "title": "When Should You Think?", "content": "\n       In the last few weeks there have been a spate of blogs, from various sources, that have suggested that TDD could be done better if people would just think before they code.  These blogs have suggested that some people leap into code too quickly, and would be better served by thinking about the problem first. \n\n In some ways these blogs remind me of Rich Hickey’s now famous talk on  Hammock Driven Developmen ; which I enthusiastically support. \n\n Of course I completely agree that you should think before you code.  It is  never  a bad idea to think through a problem or how to solve it.  Please, please,  think .  Lay in the Hammock.  Take some time.  Think.  Think.  Think. \n\n My problem with the recent blogs is that some readers may infer two things that are, in my view, incorrect. \n\n \n   TDD means don’t think ahead of time. \n   Thinking ahead of time is better than thinking at any other time. \n \n\n The first point is a meme that has made the rounds many times; and is often mentioned by both critics and fanatics of TDD.  It is, however, patently false.  Forethought is  in no way  excluded by the rules of TDD.  I, as an avid TDDer,  strongly  encourage you to think ahead. \n\n Let me state that even more forcefully.  If you want to draw UML diagrams because they help you think,  then draw the diagrams !  If you want to sketch out your thoughts about a problem-solution pair, you should  of course  do so.  You should take  every  opportunity to think.  Coding is not the only, nor always the best, way to think. \n\n The second point is also false, and this is critically important to understand.  Your early thoughts are not better than your latter thoughts!  Indeed, quite to the contrary, your latter thoughts are almost always better. \n\n What this means is that no matter how much effort you put into thinking ahead, once you start to code, you’ll have better thoughts.  While coding you’ll very probably discover things your forethoughts missed.  Indeed, you may even discover that your some of your forethoughts were just plain wrong. \n\n It has happened to me more than once, while coding, that I’ve found my forethoughts to be  completely  wrong.  In those instances I’ve had to throw away my forethoughts and start over. \n\n This happens because thinking without coding involves inadequate negative feedback. There are no reliable tests you can run that can tell you if your thinking is staying close to reality. Without that negative feedback, it’s hard to know if you’re thoughts are practical, or if they’ve gone off the rails into La La land. \n\n Having been to La La land a few times in my career, I’ve learned a healthy distrust of too much forethought.  So, nowadays, I bring my forethoughts back to ground by keeping the forethoughts relatively short, and driving them to code before they can get too crazy. \n\n If I keep my fore-thinking episodes short I find that they usually send me in a direction that, while imperfect, still leads to a successful outcome.  The outcome doesn’t often look a lot like the forethoughts; but the forethoughts are a step in the right direction. \n\n So, should you think ahead?  Of course; but don’t give those forethoughts special status or special trust.  In fact, I think healthy skepticism is the best way to treat them.  After all, those forethoughts are the  least  reliable thoughts you’ll have. The  most  reliable thoughts you’ll have will come long after you are done with the project. \n\n So, yes, think before you code.  Then think  as  you code.  Then think after you code. \n\n Or, to be terse, just:   THINK!   – Because  there is no preferred time to think! \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/05/01/Design-Damage.html", "title": "Test Induced Design Damage?", "content": "\n       I miss Jim Weirich.  I miss his laugh.  I miss his good nature.  Most of all, I miss what he would have taught me next year, and in the years to follow.  I  feel  that loss. \n\n Last October Jim gave a talk named  Decoupling from Rails  in which he showed how to refactor a rails app in order to decouple business logic from rails framework code.  The talk is  spectacular .  The hour you spend watching it will pay you back many times over. \n\n At the end of his talk Jim stated his motivation for giving the talk.  He said: \n \n   “The thing I want to stress is that: I don’t think Rails is evil.  I don’t think it’s a bad framework.  I think that as applications grow what it gives you by default is not good for growth.” \n \n\n What Rails programmer of a growing system hasn’t discovered  that ? \n\n Recently, I read  Test Induced Design Damage  by DHH.  In it he refers to Jim’s talk, and then asserts that Jim was damaging the design of his application. \n\n That’s not what I saw.  Not at all.  What I saw was a tightly interwoven mass of rails and business logic being teased apart by a master of his craft.  The result was, frankly,  beautiful .  At the end of his talk, the students began to realize all the options this new structure gave them.  You can see Jim’s eyes light up as he sees that his message has gotten through, and as he amplifies their observations with even more of his own. \n\n What Jim did was to, very elegantly, separate concerns.  The separation of concerns is an old design principle, first explained to us by David Parnas in his 1972 paper:  On the Criteria To Be Used in Decomposing Systems into Modules .  I recommend giving that paper a good read.  If you do you’ll note that the  modularization scheme that Parnas recommended is quite consistent with Jim’s refactoring. \n\n As Parnas notes, one of the primary benefits of separating concerns is changeability.  Listen to the students at the end of Jim’s talk as they remark about how the decoupled business logic could now be invoked through a service instead of over the web or how the data could be fetched from a source other than the DB.  They are talking about changeability. \n\n How do you separate concerns?  You separate behaviors that change at different times for different reasons.  Things that change together you keep together.  Things that change apart you keep apart. \n\n The code in a Rails app that binds the business rules to the Rails framework changes for different reasons, and at different rates, than the business rules themselves.  Putting those business rules into Rails controllers or Rails models therefore violates Parnas’ principle. \n\n GUIs change at a very different rate, and for very different reasons, than business rules.  Database schemas change for very different reasons, and at very different rates than business rules.  Keeping these concerns separate is  good  design. \n\n How does testing play into this?  Jim noted several times in his talk that once he had separated a concern, he could test that concern more easily, and the test would run faster because it wasn’t coupled to the Rails framework.  DHH contends that by focusing on test speed Jim was damaging his design.  But Jim would have done this separation no matter how fast the tests were running.  Jim would have done it because it enhanced changeability, and made the business rules much clearer than they were before.  Jim’s separation  improved , it did not damage, the design of the code. \n\n The primary thesis of DHHs paper is that programmers who “faithfully” practice TDD will create code that is “warped out of shape solely to accommodate testing objectives”.  In his paper, DHH does not actually show any examples of this.  Rather he refers to Jim’s talk.  Yet Jim’s talk does not show someone who is warping their code out of shape.  On the contrary it shows a faithful TDDer vastly improving the shape of his code. \n\n Now of course tests do run faster when you separate concerns.  It’s easy to see why.  If you aren’t coupled to a spinning disk, you’ll run faster.  If you aren’t coupled to an SQL interpreter you’ll run faster.  If you don’t have to send data over a web socket you’ll run faster.  If you aren’t coupled to a framework that has a long load time you’ll run faster.  You pick it.  If you aren’t coupled to it, you’ll run faster.  So if your tests run slowly, it is an indication that you have not separated concerns, and that therefore your design is lacking. \n\n Is it possible to warp your code out of shape solely to increase test speed?  I suppose it might be.  I don’t know what that might look like, and I don’t want to know.  Perhaps that was what DHH was talking about, and not the separation of concerns.  However, DHH pointed specifically at Jim’s talk, and described Jim’s refactorings as: “needless indirection and conceptual overhead”.  What he’s referring to is Jim’s definition of architectural boundaries, and his discipline of managing the dependencies that cross those boundaries.  In other words: The separation of concerns.  I find it hard to accept Jim’s separations as “warping”. \n\n To conclude: It seems to me that using good design principles that make your tests run faster is a noble goal. It also seems to me that decoupling from frameworks such as Rails, as your applications grow, is a wise action.  I believe these things to be evidence that professionals, like Jim Weirich, are at work. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/05/02/ProfessionalismAndTDD.html", "title": "Professionalism and TDD (Reprise)", "content": "\n       Lately I have been criticized, both directly and indirectly, for associating TDD with professionalism.  Indeed, I believe much of the recent “true believer” rhetoric we have been subjected to comes from that association. \n\n I plead guilty to claiming that the association exists.  I wrote extensively about it in the article  Professionalism and Test-Driven-Development, IEEE Software, 06/2007 \n\n Now, let me preface this by saying that there are a number of programmers whom I respect and honor who either do not practice TDD or do not consider it to be associated with professionalism.   Jim Coplien (Cope), Rich Hickey, and David Heinemeier Hansson (DHH) to name a few. \n\n \n   As I have said many times before, Cope is a hero of mine.  His writings in the 90s and since have had a  huge  impact on my way of thinking and my career. \n   Rich Hickey is the author of Clojure, and someone with whom I have had several discussions back in the 90s, when he was a C++ programmer.  I currently use Clojure as my primary language, and I try to read as much of Hickey’s writings, and listen to as many of his talks, as I can. \n   DHH is the author of Rails; a framework that has done more for the Ruby community, the web community, and the whole software industry than any other I can think of.  His influence on me, and on the industry is incalculable. \n \n\n These are good, solid, honorable, professional programmers.  I use their work.  I trust their integrity.  I am privileged to have met and learned so much from them.  They’ve proven their professionalism.  They don’t need me to ratify it for them. \n\n So how can I believe that TDD is associated with professionalism when there are professional programmers, whom I hold in high regard, who don’t agree with that association? \n\n First of all, if you read that article from 2007, you’ll see that I don’t believe that TDD is a  prerequisite  to professionalism.  What I believe is that it currently plays a significant role in professional behavior.  I also believe it will play a much greater role as we look into the future. \n\n In that article I briefly referred to the story of Ignaz Semmelweis who in 1847 achieved a six-fold drop in his maternity ward’s mortality rate by simply having doctors wash their hands before examining pregnant women.  Semmelwies tried to convince his peers to adopt hand-washing as a discipline.  They resisted – for over 60 years.  The reasons for their resistance sound much like the articles we’ve seen of late claiming that TDD is dead. \n\n Doctors at that time did not wash their hands.  They saw no reason to wash their hands.  To them, cleanliness and disease were utterly unrelated.  To us, in the 21st century, that’s difficult to believe; but just 167 years ago it was just as hard to believe that washing hands was anything but a fools errand. \n\n Were these doctors unprofessional?  Of course not!  They were doing the best they could with what they knew.  Their distrust of Semmelweis seems unreasonable now,  because Semmelweis was right .  But it’s hard to fault those doctors for not following every fool fad that came along.  It took time, and unfortunately it took a lot of misery and death, before hand-washing was adopted as a medical discipline. \n\n If any doctors today failed to wash their hands, we’d call them unprofessional and drum them out of the profession.  Such actions would be intolerable.  But back then, those doctors who rejected Semmelweis where honorable, respectable, and professional.  They weren’t evil.  They weren’t even stupid.  Their only fault was that they were human. \n\n Now I realize that I’m casting myself in the role of Ignaz Semmelweis.  It is not my intention to claim such iconic status.  After all, I may be dead wrong.  TDD  may not be  the equivalent of hand-washing, no matter how much I think it is.  If that is the case, then Uncle Bob’s writings and preaching on the topic will fade into a humorous footnote in the history of software.  A footnote that will be deleted after a decade or so.  If that is the case, so be it. \n\n But patients are dying out there!   Healthcare.gov.  Knight Capital.  Toyota.  The list goes on; and the casualties are mounting at an accelerating rate.  Our society as a whole is becoming more and more dependent on the software we produce; and the failures grow in significance with every passing year. \n\n Something must change  … or we are headed for a catastrophe of existential magnitude. \n\n If I am right… If TDD is as significant to software as hand-washing was to medicine and is instrumental in pulling us back from the brink of that looming catastrophe, then Kent Beck will be hailed a hero, and TDD will carry the full weight of professionalism.  After that, those who refuse to practice TDD will be excused from the ranks of professional programmers.  It would not surprise me if, one day, TDD had the force of law behind it. \n\n Now, you may disagree with me about TDD.  That’s fine, and I’m willing to have the debate with you.  But I won’t call you unprofessional, and I won’t  think  you are unprofessional.  Because  today , at this moment in our history, TDD is not the prerequisite of professionalism that I believe it will become. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/04/30/When-tdd-does-not-work.html", "title": "When TDD doesn't work.", "content": "\n       Over the years many people have complained about the so-called “religiosity” of some of the proponents of Test Driven Development.  The  recent brouhaha  over TDD has, once again, brought these complaints to the fore.  So I thought it would be a good idea to talk about when TDD does not work. \n\n I have often compared TDD to double-entry bookkeeping.  The act of stating every bit of logic twice, once in a test, and once in the production code, is very similar to the accounting practice of entering every transaction twice, once on the asset side, and once on the liability side.  The running of the tests is very similar to the creation of the balance sheet.  If the balance of assets and liabilities isn’t zero, somebody made a mistake somewhere. \n\n So stating that there are places that TDD doesn’t work may seem like stating that there are places where double entry bookkeeping doesn’t work.  However, software is different from accounting in one critical way: software controls machines that physically interact with the world. \n\n For example, let’s say that I am writing a program that controls a machine that has a bell.  The software must ring the bell when certain events occur.  How can I test that my software rings the bell? \n\n The only way to actually test that the software rings the bell is to connect a microphone to my computer and write some code that can detect the ringing of a bell. \n\n Well, no, that’s not right.  I could test that the software rings the bell by listening.  In other words, I can test that manually. \n\n Now, I can write unit tests that mock out the bell driver, and I can test that my software sends the appropriate signals to that driver at the appropriate times.  I can write unit tests that prove that the software  should  ring the bell.  But if I want to be sure that the bell rings when the proper signals are sent to the driver, I either have to set up that microphone or just listen to the bell. \n\n How can I test that the right stuff is drawn on the screen?  Either I set up  a camera and write code that can interpret what the camera sees, or I look at the screen while running manual tests. \n\n Now, I can mock out the screen and test that my software sends the right signals to the screen driver.  I can test that my software  should  put the right stuff on the screen.  But if I want to be absolutely sure, I have to either set up that camera, or look at the screen. \n\n You can see where I’m going with this, can’t you.  It’s the stuff out at the boundaries of the system.  It’s the IO devices that require manual testing.  At the moment the software controls something that physically interacts with the world, automated tests become so impractical that manual tests are the best option. \n\n But what about the layer just before the physical world?  Can you write automated tests for that layer? \n\n Consider CSS.  Can you write a test that ensures that the CSS for a page is correct?  Yes, you can, but it’s almost certainly a waste of time.  The reason is that in order to write that test you have to know the contents of the CSS.  If you want to test that the width for a certain element is 5px, then 5px must appear both in the CSS and the test. \n\n Remember the TDD rule:  As the tests get more specific, the code gets more generic.   Every new test case makes the test suite more constrained and more specific.  To make the new test case pass, the programmer strives to make the production code more general, not more specific.  We don’t pass tests by adding if statements that correspond to each test.  We pass tests by innovating general algorithms. \n\n But CSS doesn’t work like that.  There is no general algorithm for CSS.  The CSS is just as specific as any test you could write.  Indeed, you could write a program that reads the CSS and writes the tests.  Such tests add very little value, and they certainly aren’t written first. \n\n Besides, how do you know if the CSS is correct?  Remember we are doing TDD.  We are writing our tests first.  How do you know, in advance, what the CSS should be?  The answer is that usually you don’t.  Usually you write some initial CSS, and then you look at the screen and  fiddle  with the CSS until it looks right.  Your eyes, and your mind, are the actual test.  Once the CSS is right, there’s no point in writing a test for it. \n\n So near the physical boundary of the system there is a layer that requires  fiddling .  It is useless to try to write tests first (or tests at all) for this layer.  The only way to get it right is to use human interaction; and once it’s right there’s no point in writing a test. \n\n So the code that sets up the the panels, and windows, and widgets in Swing, or the view files written in yaml, or hiccup, or jsp, or the code that sets up the configuration of a framework, or the code that initializes devices, or…  Well you get the idea.  Anything that requires human interaction and  fiddling  to get correct is not in the domain of TDD, and doesn’t require automated tests. \n\n So, now we have two places where TDD is impractical or inappropriate.  The physical boundary, and the layer just in front of that boundary that requires human interaction to  fiddle  with the results.  Are there any other areas where tests aren’t appropriate? \n\n Yes.  The test code itself.  I don’t mean the actual unit tests.  I mean the support code for those unit tests.  The FitNesse fixtures, or the cucumber steps, or the Object Mothers, or the Test Doubles.  You don’t have to write tests for those because the actual unit tests and the production code  are  the tests for those pieces of code. \n\n That’s really about it.  For pretty much everything else you can write tests, and you can write them first.  For pretty much everything else, you can use TDD. \n\n However, there’s one other rule.  It’s not fair to load those layers with logic just so you can avoid writing tests for that logic.  Indeed, it is imperative to denude these layers of logic, and export that logic to modules that you can test. \n\n This exporting of logic from the boundaries of the system, and from the fiddling layers next to those boundaries has a name.  It’s called  Humility .   We keep these layers humble by moving all the logic associated with them out into other modules for which we can easily write tests. \n\n This means you don’t put any unnecessary logic into your JSP files, or you Swing setup code, or your yaml files.  You keep that code humble by moving logic into other modules that can be tested. \n\n It has been claimed that this exporting of logic is damaging to the design of the application.  I disagree.  From my point of view, exporting logic is nothing more than separating concerns.  In this case we separate the code that must be  fiddled  from the code that can be tested.  Those two domains of code will change for very different reasons and at very different rates; so it is wise to separate them.  Separating them is  good  design. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/05/08/SingleReponsibilityPrinciple.html", "title": "The Single Responsibility Principle", "content": "\n       In 1972 David L. Parnas published a classic paper entitled  On the Criteria To Be Used in Decomposing Systems into Modules .  It appeared in the December issue of the Communications of the ACM, Volume 15, Number 12. \n\n In this paper, Parnas compared two different strategies for decomposing and separating the logic in a simple algorithm.  The paper is fascinating reading, and I strongly urge you to study it.  His conclusion, in part, is as follows: \n\n \n   “We have tried to demonstrate by these examples that \nit is almost always incorrect to begin the decomposition \nof a system into modules on the basis of a flowchart. \nWe propose instead that one begins with a list of \ndifficult design decisions or design decisions  which are \nlikely to change . Each module is then designed to hide \nsuch a decision from the others.” \n \n\n I added the emphasis in the second to last sentence.  Parnas’ conclusion was that modules should be separated based, at lease in part, on the way that they might change. \n\n Two years later, Edsger Dijkstra wrote another classic paper entitled  On the role of scientific thought.  in which he introduced the term:  The Separation of Concerns . \n\n The 1970s and 1980s were a fertile time for principles of software architecture.   Structured Programming and Design  were all the rage.  During that time the notions of  Coupling  and  Cohesion  were introduced by Larry Constantine, and amplified by Tom DeMarco, Meilir Page-Jones and many others. \n\n In the late 1990s I tried to consolidate these notions into a principle, which I called:  The Single Responsibility Principle .  (I have this vague feeling that I stole the name of this principle from Bertrand Meyer, but I have not been able to confirm that.) \n\n The Single Responsibility Principle (SRP) states that each software module should have one and only one reason to change.  This sounds good, and seems to align with Parnas’ formulation.  However it begs the question:  What defines a reason to change? \n\n Some folks have wondered whether a bug-fix qualifies as a reason to change.  Others have wondered whether refactorings are reasons to change.  These questions can be answered by pointing out the coupling between the term “reason to change” and “responsibility”. \n\n Certainly the code is not responsible for bug fixes or refactoring.  Those things are the responsibility of the programmer, not of the program.  But if that is the case, what is the program responsible for?  Or, perhaps a better question is:  who  is the program responsible to?  Better yet:  who  must the design of the program  respond  to? \n\n Imagine a typical business organization.  There is a CEO at the top.  Reporting to that CEO are the C-level executives: the CFO, COO, and CTO among others.  The CFO is responsible for controlling the finances of the company.  The COO is responsible for managing the operations of the company.  And the CTO is responsible for the technology infrastructure and development within the company. \n\n Now consider this bit of Java code: \n\n public class Employee {\n  public Money calculatePay();\n  public void save();\n  public String reportHours();\n}\n \n\n \n   The  calculatePay  method implements the algorithms that determine how much a particular employee should be paid, based on that employee’s contract, status, hours worked, etc. \n   The ‘save’ method stores the data managed by the  Employee  object onto the enterprise database. \n   The  reportHours  method returns a string which is appended to a report that auditors use to ensure that employees are working the appropriate number of hours and are being paid the appropriate compensation. \n \n\n Now, which of those C-Level executives reporting to the CEO is responsible for specifying the behavior of the  calculatePay  method?  Which of them would be fired[1] by the CEO if that method were catastrophically mis-specified?  Clearly the answer is the CFO.  Specifying the pay of employees is a financial responsibility.  If all the employees were paid double for a year because someone in the CFOs organization mis-specified the rules for calculating pay, the CFO would likely be fired. \n\n A different C-Level executive is responsible for specifying the format and content of the string returned from the  reportHours  method.  That executive manages the auditors and reviewers, and that’s an operations responsibility.  So if there were a catastrophic mis-specification of that report, the COO would be fired. \n\n Finally, it should be obvious which of the C-Level executives would be fired if there were a catastrophic mis-specification of the  save  method.  If the enterprise database were to be corrupted by such a horrific mis-specification, the CTO would likely be fired. \n\n So it stands to reason that when changes are made to the algorithm within the  calculatePay  method, the request for those changes will originate from the organization headed by the CFO.  Similarly it will be the COO’s organization that will request changes to the  reportHours  method, and the CTOs organization that will request changes to the  save  method. \n\n And this gets to the crux of the Single Responsibility Principle.   This principle is about people. \n\n When you write a software module, you want to make sure that when changes are requested, those changes can only originate from a single person, or rather, a single tightly coupled group of people representing a single narrowly defined business function.  You want to isolate your modules from the complexities of the organization as a whole, and design your systems such that each module is responsible (responds to) the needs of just that one business function. \n\n Why?  Because we don’t want to get the COO fired because we made a change requested by the CTO. Nothing terrifies our customers and managers more that discovering that a program malfunctioned in a way that was, from their point of view, completely unrelated to the changes they requested.  If you change the  calculatePay  method, and inadvertently break the  reportHours  method; then the COO will start demanding that you  never change the  calculatePay  method again. \n\n Imagine you took your car to a mechanic in order to fix a broken electric window.  He calls you the next day saying it’s all fixed.  When you pick up your car, you find the window works fine; but the car won’t start.  It’s not likely you will return to that mechanic because he’s clearly an idiot. \n\n That’s how customers and managers feel when we break things they care about that they did  not  ask us to change. \n\n This is the reason we do not put SQL in JSPs.  This is the reason we do not generate HTML in the modules that compute results.  This is the reason that business rules should not know the database schema.  This is the reason we  separate concerns . \n\n Another wording for the Single Responsibility Principle is: \n \n   Gather together the things that change for the same reasons.  Separate those things that change for different reasons. \n \n\n If you think about this you’ll realize that this is just another way to define cohesion and coupling.  We want to increase the cohesion between things that change for the same reasons, and we want to decrease the coupling between those things that change for different reasons. \n\n However, as you think about this principle, remember that the reasons for change are  people .  It is  people  who request changes.  And you don’t want to confuse those people, or yourself, by mixing together the code that many different people care about for different reasons. \n\n \n\n [1] I am indulging in a bit of hyperbole here.  C-level executives don’t usually get fired for small mis-specifications.  Still, it’s not outside the realm of possibility, and it does emphasize that the organizations reporting to these executives care about different concerns. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/05/10/WhenToMock.html", "title": "When to Mock", "content": "\n       A mock object is a very powerful tool, providing two major benefits: isolation and introspection.  But like all power tools, mocks come with a cost.  So where and when should you use them?  What is the cost/benefit trade off?  Let’s look at the extremes. \n\n ##No Mocks.\nConsider the test suite for a large web application.  Let’s assume that test suite uses  no  mocks at all.  What problems will it face? \n\n \n   \n     The execution of the test suite will probably be very slow: dozens of minutes – perhaps hours.  Web servers, databases, and services over the network run thousands of times slower than computer instructions; thus impeding the speed of the tests.  The cost of testing just one  if  statement within a business rule may require many database queries and many web server round-trips. \n   \n   \n     The coverage provided by that test suite will likely be low.  Error conditions and exceptions are nearly impossible to test without mocks that can simulate those errors.  Functions that perform dangerous tasks, such as deleting files, or deleting database tables, are difficult to safely test without mocks.   Reaching every state and transition of coupled finite state machines, such as communication protocols, is hard to achieve without mocks. \n   \n   \n     The tests are sensitive to faults in parts of the system that are not related to what is being tested. For example: Network timings can be thrown off by unexpected computer load.  Databases may contain extra or missing rows.  Configuration files may have been modified.  Memory might be consumed by some other process.  The test suite may require special network connections that are down.  The test suite may require a special execution platform, similar to the production system. \n   \n \n\n So, without mocks, tests tend to be slow, incomplete, and fragile.  That sounds like a strong case for using mocks.  But mocks have costs too. \n\n ##Too Many Mocks\nConsider that same large web application, but this time with a test suite that imposes mocks between all the classes.  What problems will it face? \n\n \n   \n     Ironically, some mocking systems depend strongly on  reflection , and are therefore very slow.  When you mock out the interaction between two classes with something slower than those two classes, you can pay a pretty hefty price. \n   \n   \n     Mocking the interactions between all classes forces you to create mocks that return other mocks (that might return yet other mocks).  You have to mock out all the data pathways in the interaction; and that can be a complex task.  This creates two problems. \n     \n       The setup code can get extremely complicated. \n       The mocking structure become tightly coupled to implementation details causing many tests to break when those details are modified. \n     \n   \n \n\n \n \n   The need to mock every class interaction forces an explosion of polymorphic interfaces.  In statically typed languages like Java, that means the creation of lots of extra  interface  classes whose sole purpose is to allow mocking.   This is over-abstraction and the dreaded “design damage”. \n \n\n So if you mock too much you may wind up with test suites that are slow, fragile, and complicated; and you may also damage the design of your application. \n\n ##Goldilocks Mocks\nClearly the answer is somewhere in between these two extremes.  But where?  Here are the heuristics that I have chosen: \n\n \n   Mock across architecturally significant boundaries, but not within those boundaries. \n \n\n For example, mock out the database, web server, and any external service.  This provides many benefits: \n\n \n   The tests run much faster. \n   The tests are not sensitive to failures and configurations of the mocked out components. \n   It is easy to test all the failure scenarios generated by the mocked out components. \n   Every pathway of coupled finite state machines across that boundary can be tested. \n   You generally don’t create mocks that return other mocks, so your setup code stays much cleaner. \n \n\n Another big benefit of this approach is that it forces you to think through what your significant architectural boundaries are; and enforce them with polymorphic interfaces.  This allows you to manage the dependencies across those boundaries so that you can independently deploy (and develop) the components on either side of the boundary. \n\n This separation of architectural concerns has been a mainstay of good software design for the last four decades at least.  Good software developers pursued such separation long before Test Driven Development became popular.  So it is ironic that striking the right balance of isolation and speed in our tests is so strongly related to this separation.  The implication is that good architectures are inherently testable. \n\n \n   Write your own mocks . \n \n\n I don’t often use mocking tools.  I find that if I restrict my mocking to architectural boundaries, I rarely need them. \n\n Mocking tools are very powerful, and there are times when they can be quite useful.  For example, they can override sealed or final interfaces.  However, that power is seldom necessary; and comes at a significant cost. \n\n Most mocking tools have their own domain specific language that you must learn in order to use them.  These languages are usually some melange of dots and parentheses that look like gibberish to the uninitiated.  I prefer to limit the number of languages in my systems, so I avoid their use. \n\n Mocking across architectural boundaries is  easy .  Writing those mocks is  trivial .  What’s more, the IDE does nearly all the work for you.  You simply point it at an interface and tell it to implement that interface and, voila!, you have the skeleton of a mock. \n\n Writing your own mocks forces you to give those mocks  names , and put them in special directories.  You’ll find this useful because you are very likely to reuse them from test to test. \n\n Writing your own mocks means you have to  design  your mocking structure.  And that’s never a bad idea. \n\n When you write your own mocks, you aren’t using reflection, so  your mocks will almost always be extremely fast. \n\n ##Conclusion \n\n Of course your mileage may vary.  These are  my  heuristics, not yours.  You may wish to adopt them to one extent or another; but remember that heuristics are just guidelines, not rules.  I violate my own heuristics when given sufficient reason. \n\n In short, however, I recommend that you  mock sparingly .  Find a way to test –  design  a way to test – your code so that it doesn’t require a mock.  Reserve mocking for architecturally significant boundaries; and then be ruthless about it.  Those are the important boundaries of your system and  they need to be managed , not just for tests, but for everything. \n\n And write your own mocks.  Try to depend on mocking tools as little as possible.  Or, if you decide to use a mocking tool,  use it with a very light touch . \n\n If you follow these heuristics you’ll find that your test suites run faster, are less fragile, have higher coverage, and that the designs of your applications are better. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/05/11/FrameworkBound.html", "title": "Framework Bound[2]", "content": "\n       Frameworks are powerful tools.  We’d be lost without them.  But there’s a cost to using them. \n\n Think of Rails, or Spring, or JSF, or Hibernate.  Think about what writing a web system would be like without these frameworks to help you.  The idea is disheartening.  There’d be so many little piddling details to deal with.  It’d be like endeavoring to construct a mnemonic memory circuit using stone knives and bearskins[1]. \n\n And so we happily couple our code to those frameworks in anticipation of all the benefits they promise.  We make the mistake that so many programmers have made before us.  We bind ourselves to the framework. \n\n Using a framework requires a significant commitment.  By accepting the framework into your code, you surrender your control over the details that the framework manages.  Of course this seems like a good thing; and it usually is.  However, there’s a trap waiting around the corner; and it’s hard to see it coming. Before you know it you find yourself engaged in all manner of unnatural acts, inheriting from it’s base classes, relinquishing more and more of the flow of control, bowing ever deeper to the framework’s conventions, quirks, and idiosyncrasies. \n\n And yet despite the huge commitment you’ve made to the framework, the framework has made no reciprocal commitment to you at all.  That framework is free to evolve in any direction that pleases its author.  When it does, you realize that that you are simply going to have to follow along like a faithful puppy. \n\n Getting bound to frameworks is an all too common occurrence in software teams.  They begin with rampant enthusiasm and willingly couple their code to the framework, only to find, much later, that as the project matures, the framework gets more and more in the way. \n\n This shouldn’t be surprising.  Frameworks are written by people to solve certain problems that  they  have.  Those problems may be similar to yours,  but they are not yours .  You have different problems.  To the extent that your problems overlap, the framework can be enormously helpful.  To the extent that your problems conflict, the framework can be a huge impediment. \n\n TANSTAAFL! \n\n ###Framework Authors\nRemember that frameworks are written by people, and people have their own agendas.  One of the items on such an agenda is to get you to use their framework.  As an author of past frameworks I can tell you that the more people who used my frameworks, the better I felt about  myself .  A lot of my self-worth got tied up into the acceptance of my frameworks. \n\n Now I don’t want to get too deep into the psycho-analysis of framework authors.  Framework authors are not bad people.  Indeed, for the most part, they are heroes.  Many unselfishly release their code to the open-source community.  Were it not for these people, our programming lives would be far less enjoyable and productive. \n\n I’ve made this point about authors because it helps me understand some of the mechanics of the relationship between framework authors and framework users. \n\n Framework authors will go to great lengths to entice you into the fold.  They’ll write papers and give talks.  They’ll provide examples showing you how to bind tightly, ever tightly to their code.  They’ll demonstrate all the benefits that tight binding gives you.  They are convinced that their code can help you, and they work hard to convince you too. \n\n That’s perfectly normal, and not at all dishonorable.  They want you in the fold. \n\n Once you are in the fold, however, their interest in you might change somewhat.  Here’s a picture of one famous framework author telling his users what he thinks about some of their concerns.  (R rated) \n\n ###Arm’s Length.\nOver the years, I’ve adopted a healthy skepticism about frameworks.  While I acknowledge that they can be extremely useful, and save a boatload of time; I also realize that there are costs.  Sometimes those costs can mount very high indeed. \n\n So my strategy is to keep frameworks like Spring, Hibernate, and Rails at arm’s length; behind architectural boundaries.  I get most of the benefit from them that way; and I can take ruthless advantage of them. \n\n But I don’t let those frameworks get too close.  I surrender none of my autonomy to them.  I don’t allow the tendrils of their code to intermingle with the high level policy of my systems.  They can touch my peripheral subsystems; but I keep them away from the core business logic.  The high level policies of my systems shall never be touched by frameworks. \n\n \n [1] Star Trek:  The City on the Edge of Forever , Harlan Ellison, 1967 \n[2]  Apology \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/05/12/TheOpenClosedPrinciple.html", "title": "The Open Closed Principle", "content": "\n       In 1988 Bertrand Meyer defined one of the most important principles of software engineering.  The Open Closed Principle (OCP).  In his book  Object Oriented Software Construction [1] he said: \n\n \n   A satisfactory modular decomposition technique must satisfy one more requirement: It should yield modules that are  both  open and closed. \n    A module will be said to be open if it is available for extension.  For example, it should be possible to add fields to the data structures it contains, or new elements to the set of functions it performs. \n A module will be said to be closed if is available for use by other modules.  This assumes that the module has been given a well-defined, stable description (the interface in the sense of information hiding).  In the case of a programming language module, a closed module is one that may be compiled and stored in a library, for others to use.  In the case of a design or specification module, closing a module simply means having it approved by management, adding it to the project's official repository of accepted software items (often called the project _baseline_), and publishing its interface for the benefit of other module designers. \n \n\n This definition is obviously dated. Nowadays many languages don’t require that modules be compiled.  And getting module specifications approved by management smacks of a waterfall mentality.  Still, the essence of a great principle still shines through those old words.  To wit: \n\n \n   You should be able to extend the behavior of a system  without having to modify that system. \n \n\n Think about that very carefully.  If the behaviors of all the modules in your system could be extended, without modifying them, then you could add new features to that system  without modifying any old code .  The features would be added solely by writing new code. \n\n What’s more, since none of the old code had changed, it would not need to be recompiled, and therefore it would not need to be redeployed.  Adding a new feature would involve leaving the old code in place and  only deploying the new code , perhaps in a new  jar  or  dll  or  gem . \n\n And this ought to give you a hint about what a  jar ,  dll , or  gem  really ought to be.  They ought to be isolatable  features ! \n\n Is this Absurd? \n At first reading the open closed principle may seem to be nonsensical.  Our languages and our designs do not usually allow new features to be written, compiled, and deployed separately from the rest of the system.  We seldom find ourselves in a place where the current system is closed for modification, and yet can be extended with new features. \n\n Indeed, most commonly we add new features by making a vast number of changes throughout the body of the existing code.  We’ve known this was bad long before Martin Fowler wrote the book[2] that labeled it  Shotgun Surgery ; but we still do it. \n\n Ah, but then there’s Eclipse, or IntelliJ, or Visual Studio, or Vim, or Text Mate, or Minecraft or…  Well, you get my point.  There is a vast plethora of tools that can be easily extended without modifying or redeploying them.  We extend them by writing  plugins . \n\n Plugin systems are the ultimate consummation, the apotheosis, of the Open-Closed Principle. They are proof positive that open-closed systems are possible, useful, and immensely powerful. \n\n How did these systems manage to close their primary business rules to modification, and yet leave the whole application open to be extended?   Simple.  They believed in the OCP, and they used the tools of object oriented design to separate high level policy from low level detail.  They carefully managed their dependencies, inverting those that crossed architecturally significant boundaries in the wrong direction. \n\n After all, the way you get a plugin architecture is to make sure that all dependencies inside the plugin, point at the system; and that nothing in the system points out towards the plugins.  The system doesn’t know about the plugins.  The plugins know about the system. \n\n Plugin Architecture \n What if the design of your systems was based around plugins, like Vim, Emacs, Minecraft, or Eclipse?  What if you could plug in the Database, or the GUI.  What if you could plug in new features, and unplug old ones.  What if the behavior of your system was largely controlled by the configuration of its plugins?  What power would that give you?  How easy would it be to add new features, or new user interfaces, or new machine/machine interfaces?  How easy would it be to add, or remove, SOA.  How easy would it be to add or remove REST?  How easy would it be to add or remove Spring, or Rails, or Hibernate, or Oracle, or… \n\n Well I think you get my meaning.  When your fundamental business rules are the core of a plugin architecture, then you are never bound to a particular feature set, interface, database, framework, or anything else. \n\n Conclusion \n I’ve heard it said that the OCP is wrong, unworkable, impractical, and not for real programmers with real work to do.  The rise of plugin architectures makes it plain that these views are utter nonsense.  On the contrary, a strong plugin architecture is likely to be  the most important aspect  of future software systems. \n\n \n [1]  Object Oriented Software Construction , 1st. ed. Bertrand Meyer, Prentice Hall, 1988.\n[2]  Refactoring , Martin Fowler, Addison Wesley, 1999 \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/05/19/First.html", "title": "First", "content": "\n       In the first  Is TDD Dead?  hangout , at time 30:25  @dhh  makes a remarkable statement: \n \n   “…you’re not done until you also have tests for a piece of functionality – I’m completely on board with that.” \n \n\n I think we can extrapolate from  @dhh ’s statement that he believes that having tests for a piece of functionality is a matter of professionalism. \n\n It’s not hard to understand why.  The benefits provided by a good test suite are enormous.  Let’s just consider a few. \n\n \n   Well designed tests are small isolated snippets of code that call into the system being tested, expecting certain results. A programmer can read the tests to understand what the tested code is supposed to do.  So the tests are  documents .  They are written in a language you understand. They are utterly unambiguous.  They are so formal that they  execute .  And they cannot get out of sync with the application. \n \n\n That’s pretty close to documentation nirvana.  I’ve certainly seen my share of documents that were hard to read, ambiguous, informal, and out of sync with the application.  That fact that a good suite of tests cures those ills makes the tests pretty important. \n\n Another benefit is design: \n\n \n   Well designed tests force a certain degree of decoupling.  Often that decoupling is beneficial to the design of the system. \n \n\n @dhh  has rightly suggested that too much indiscriminate and gratuitous decoupling is deleterious the the design of the code.  On the other hand, no one can doubt that well-considered decoupling is beneficial.  Tests provide an opportunity for that consideration; and that adds to the importance of the tests. \n\n But without a doubt the most important benefit of a good test suite is: \n\n \n   Confidence.  A well designed test suite with a high degree of coverage eliminates, or at least strongly mitigates the fear of change.  And when you aren’t afraid to change your code, you will  clean  it.  And if you clean it, it won’t rot.  And if it doesn’t rot, then the software team can go  fast . \n \n\n Whenever I teach a class, no matter the topic, I always ask my students this question: \n \n   “Have you ever been significantly slowed down by bad code?” \n \n\n The vast majority of programmers say that they  have indeed  been significantly slowed down by bad code.  I mean, honestly,  who hasn’t ? \n\n So it stands to reason that if we keep the code clean, we won’t be slowed down by bad code.  And that means a suite of tests is a key to going fast. \n\n Let me state that more strongly.  If you have a suite of tests that you can execute  quickly , and if you trust that suite of tests enough, then you will not be afraid to change the code.  That makes the code  flexible . \n\n For years we’ve thought that flexibility of code was a function of it’s design.  We thought that poorly designed code was rigid and hard to change; and that well designed code was flexible and easy to change.  And, for what it’s worth, this is true.  But  nothing  makes code easier to change than a quickly executing suite of tests that you trust –  nothing . \n\n How important is that?  How important is it that, at all times, you have the confidence that changes to your code haven’t broken anything?  How important is it that you keep enough control over your code so that you aren’t afraid to  clean  it?  And how  irresponsible  is it to have lost that control and to be  afraid  to make changes – afraid to  clean ? \n\n It seems to me; and apparently it seems to  @dhh , that this is pretty important.  Indeed, I think it’s  critically  important.  So linking professionalism to a quickly executing suite of trustworthy tests is probably not out of line. \n\n But that brings us to an issue:   Order .  When something is critically important, when do you do it?  The answer to that is simple and obvious.  When something is critically important, you do it  first . \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/05/14/TheLittleMocker.html", "title": "The Little Mocker", "content": "\n       The following is a conversation around mocking: \n\n What is this?: \n\n interface Authorizer {\n  public Boolean authorize(String username, String password);\n} >_An interface._\n \n\n So what then, is this? \n\n public class DummyAuthorizer implements Authorizer {\n  public Boolean authorize(String username, String password) {\n\treturn null;\n  \t  }\n} >_That's a Dummy._\n \n\n And what do you do with a Dummy? \n \n   You pass it into something when you don’t care how it’s used. \n \n\n Such as? \n \n   As part of a test, when you must pass an argument, but you know the argument will never be used. \n \n\n Can you show an example? \n \n   Sure. \n \n\n   public class System {\n\tpublic System(Authorizer authorizer) {\n\t\tthis.authorizer = authorizer;\n\t}\n\n\tpublic int loginCount() {\n\t\t//returns number of logged in users.\n\t}\n  }\n\n  @Test\n  public void newlyCreatedSystem_hasNoLoggedInUsers() {\n\tSystem system = new System(new DummyAuthorizer());\n\tassertThat(system.loginCount(), is(0));\n  }\n \n\n I see.  In order to construct the  System  an  Authorizer  must be passed to the constructor; but the  authorize  method of that  Authorizer  will never be called since, in this test, no one will log in. \n \n   You got it. \n \n\n And so the fact that the  authorize  method of the  DummyAuthorizer  returns a  null  is not an error. \n \n   Indeed not.  In fact, it’s the best thing a Dummy can return. \n \n\n Why is that? \n \n   Because if anybody tried to use that Dummy, they’d get a  NullPointerException . \n \n\n Ah, and you don’t want the Dummy to be used. \n \n   Right!  It’s a dummy. \n \n\n But isn’t this a mock?  I thought these test objects were called mocks. \n \n   They are; but that’s slang. \n \n\n Slang? \n \n   Yes, the word “mock” is sometimes used in an informal way to refer to the whole family of objects that are used in tests. \n \n\n Is there a formal name for these test objects? \n \n   Yes, they are called “Test Doubles”[1]. \n \n\n You mean like “Stunt Doubles” in the movies? \n \n   Exactly. \n \n\n So then the word “mock” is just colloquial slang? \n \n   No, it has a formal meaning too; but when we are speaking informally the word mock is a synonym for Test Double. \n \n\n Why do we have two words?  Why don’t we just use Test Double instead of Mock? \n \n   History. \n \n\n History? \n \n   Yes, long ago some very smart people wrote a  paper  that introduced and defined the term Mock Object.  Lots of other people read it and started using that term.  Other people, who hadn’t read the paper, heard the term and started using it with a broader meaning.  They even turned the word into a *verb*.  They’d say, “Let’s mock that object out.”, or “We’ve got a lot of mocking to do.” \n \n\n That kind of thing happens a lot with words, doesn’t it? \n \n   Yes it does.  Especially when a word has just one syllable, and is easy to say. \n \n\n Yeah, I guess it’s easier to say: “Let’s mock that.” instead of: “Let’s make a test double for that.” \n\n \n   Right.  Colloquialisms are a fact of life. \n \n\n OK, but when we need to speak precisely… \n \n   You should use the formal language. Yes. \n \n\n So then what is a Mock? \n \n   Before we get to that, we should look at other kinds of Test Doubles. \n \n\n Like what? \n \n   Let’s look at Stubs. \n \n\n What’s a stub? \n \n   This, is a stub: \n \n\n public class AcceptingAuthorizerStub implements Authorizer {\n  public Boolean authorize(String username, String password) {\n\treturn true;\n  \t  }\n}\n \n\n It returns  true . \n \n   That’s right. \n \n\n Why? \n \n   Well,  suppose you want to test a part of your system that requires you to be logged in. \n \n\n I’d just log in. \n \n   But you already know that login works,  You’ve tested it a different way.  Why test it again? \n \n\n Because it’s easy? \n \n   But it takes time.  And it requires setup.  And if there’s a bug in login, your test will break.  And, after all, it’s an unnecessary coupling. \n \n\n Hmmm.  Well, for the sake of argument, let’s say I agree.  What then? \n \n   You simply inject the  AcceptingAuthorizerStub  into your system for that test. \n \n\n And it will authorize the user without question. \n \n   Right. \n \n\n And if I want to test the part of the system that handles unauthorized users, I could use a stub that returns  false . \n \n   Right again. \n \n\n OK, so what else is there? \n \n   There’s this: \n \n\n public class AcceptingAuthorizerSpy implements Authorizer {\n  public boolean authorizeWasCalled = false;\n\n  public Boolean authorize(String username, String password) {\n\tauthorizeWasCalled = true;\n\treturn true;\n  }\n}\n \n\n I suppose that’s called a Spy. \n \n   That’s right. \n \n\n So why would I use it? \n \n   You’d use this when you wanted to be sure that the  authorize  method was called by your system. \n \n\n Ah, I see.  In my test I’d inject it like a stub, but then at the end of my test I’d check the  authorizerWasCalled  variable to make sure my system actually called  authorize . \n \n   Absolutely. \n \n\n So a Spy, spies on the caller.  I suppose it could record all kinds of things. \n \n   Indeed it could.  For example, it could count the number of invocations. \n \n\n Yeah, or it could keep a list of the arguments passed in each time. \n \n   Yes.  You can use Spies to see inside the workings of the algorithms you are testing. \n \n\n That sounds like coupling. \n \n   It is!  You have to be careful.  The more you spy, the tighter you couple your tests to the implementation of your system.  And that leads to fragile tests. \n \n\n What’s a fragile test? \n \n   A test that breaks for reasons that shouldn’t break a test. \n \n\n Well if you change the code in the system, some tests are going to break. \n \n   Yes, but well designed tests minimize that breakage.  Spies can work against that. \n \n\n OK, I get that.  What other kinds of test doubles are there? \n \n   Two more.  Here’s the first: \n \n\n public class AcceptingAuthorizerVerificationMock implements Authorizer {\n  public boolean authorizeWasCalled = false;\n\n  public Boolean authorize(String username, String password) {\n\tauthorizeWasCalled = true;\n\treturn true;\n  }\n\n  public boolean verify() {\n\treturn authorizedWasCalled;\n  }\n}\n \n\n And, of course, this is a mock. \n \n   A  True Mock . Yes. \n \n\n True? \n \n   Yes, this is a formal mock object according to the original meaning of the word. \n \n\n I see.  And it looks like you moved the assertion from the test, into the  verify  method of the, uh,  true  mock. \n \n   Right.  Mocks know what they are testing. \n \n\n So that’s it?  You just put the assertion into the mock? \n \n   Not quite.  Yes, the assertion goes into the mock.  However, what the mock is testing is  behavior . \n \n\n Behavior? \n \n   Yes.  The mock is not so interested in the return values of functions.  It’s more interested in what function were called, with what arguments, when, and how often. \n \n\n So a mock is always a spy? \n \n   Yes.  A mock spies on the behavior of the module being tested.  And the mock knows what behavior to expect. \n \n\n Hmmm.  Moving the expectation into the mock feels like a coupling. \n \n   It is. \n \n\n So why do it? \n \n   It makes it a lot easier to write a mocking tool. \n \n\n A mocking tool? \n \n   Yes, like JMock, or EasyMock, or Mockito.  These tools let you build mock objects on the fly. \n \n\n That sounds complicated. \n \n   It’s not.   here  is a famous paper by Martin Fowler that expains it well. \n \n\n And there’s a book too, isn’t there? \n \n   Yes.   Growing Object Oriented Software, Guided by Tests  is a great book about a popular design philosophy driven by mocks. \n \n\n OK, so then are we done?  You said there was still another kind of test double. \n \n   Yes, one more.  Fakes. \n \n\n   public class AcceptingAuthorizerFake implements Authorizer {\n\t  public Boolean authorize(String username, String password) {\n\t\treturn username.equals(\"Bob\");\n\t  }\n  }\n \n\n OK, that’s strange.  Everybody named “Bob” will be authorized. \n \n   Right.  a Fake has business behavior.  You can drive a fake to behave in different ways by giving it different data. \n \n\n It’s kind of like a simulator. \n \n   Yes, simulators are fakes. \n \n\n Fakes aren’t stubs are they? \n \n   No, fakes have real business behavior; stubs do not.  Indeed, none of the other test doubles we’ve talked about have real business behavior. \n \n\n So fakes are different at a fundamental level. \n \n   Indeed they are.  We can say that a Mock is a kind of spy, a spy is a kind of stub, and a stub is a kind of dummy.  But a fake isn’t a kind of any of them.  It’s a completely different kind of test double. \n \n\n I imagine Fakes could get complicated. \n \n   They can get  extremely  complicated.  So complicated they need unit tests of their own.  At the extremes the fake becomes the real system. \n \n\n Hmmm. \n \n   Yes, Hmmm.  I don’t often write fakes.  Indeed, I haven’t written one for over thirty years. \n \n\n Wow!  So what do you write?  Do you use all these other test doubles? \n \n   Mostly I use stubs and spies.  And I write my own, I don’t often use mocking tools. \n \n\n Do you use Dummies? \n \n   Yes, but rarely. \n \n\n What about mocks? \n \n   Only when I use a mocking tool. \n \n\n But you said you don’t use mocking tools. \n \n   That’s right, I usually don’t. \n \n\n Why not? \n \n   Because stubs and spies are very easy to write.  My IDE makes it trivial.  I just point at the interface and tell the IDE to implement it.  Voila!  It gives me a dummy.  Then I just make a simple modification and turn it into a stub or a spy.  So I seldom need the mocking tool. \n \n\n So it’s just a matter of convenience? \n \n   Yes, and the fact that I don’t like the strange syntax of mocking tools, and the complications they add to my setups.  I find writing my own test doubles to be simpler in most cases. \n \n\n OK, well, thank you for the conversation. \n \n   Any time. \n \n\n \n [1]  xUnit Test Patterns \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/06/20/MyLawn.html", "title": "My Lawn", "content": "\n       Look at this graph, which I took from this  blog  by Peter Knego. \n\n \n\n Peter was showing the relationship between age and reputation on stack overflow.  The correlation is unmistakable.  The older the programmer, the better the reputation, by far.  He also shows, in the following graph, that the reason for this correlation is that the older programmers answer far more questions than the younger ones. \n\n \n\n So it would appear that the older the programers are, the more knowledge they have.  Moreover, that knowledge does not seem to be out-of-date.  So these programmers were not getting the same year of experience over and over again. \n\n This is interesting; but what caught my attention was the shape of the age distribution in the first graph.  If this graph is correct, then most programmers are under 28.  Most programmers have less than 6 years experience!  The implications of this are hair raising!  Who teaches the young programmers just entering the field?  Where are the seniors who can mentor the juniors?  Is the programming field the logical equivalent of  Lord of the Flies ?  All kids.  No adults? \n\n Maybe this is a good thing.  After all, it is said that Mark Zuckerberg said[1]: “Young programmers are superior.”   Perhaps he thought this because young programmers can work crazy hours.  Young programmers have oodles of high-intensity, frenzied, energy that causes high volumes of code to spew out of every orifice of their bodies.  So maybe there are more young programmers because young programmers are better, and old programmers’ skills and energy fade away after age 30.  Maybe programming is a young person’s game that older people just can’t play all that well. \n\n Peter’s graphs suggests otherwise.  The older the programmer, the more questions the programmer can answer –  and can take the time to answer .   The implication is that older programmers are less rushed because they have more knowledge and skill.  Code doesn’t have to pour out of all their orifices because they can solve problems calmly, quickly, and easily with much less code. \n\n But if older programmers are so much better, where did they all go?  Why are half the programmers in the world under 30?  Do older programmers quit?  Do they move on to management?  Do they burn out and become chicken farmers?  Why does the age distribution fall off so rapidly after age 30? \n\n Actually, most of the old programmers are still here and are still coding.  The problem isn’t that the old programmers are fading away.  The problem is that the number of programmers is growing by a huge factor every year. \n\n I’m 61 years old.  I still write code.  I love writing code. I’m very, very, good at it.  So why aren’t there more 61 year old programmers?  Because, 40 years ago, in 1974, when I was 21,  there weren’t very many programmers at all . \n\n I estimate that the world, today, has twenty-two million programmers[2].  One of of every 300 people is a programmer.  In the US it’s closer to 1%.  But in 1974 the number of programmers worldwide was vastly smaller, and probably numbered less than 100,000[3].  That implies that in 40 years our ranks have increased by a factor of 220.  That’s a growth rate of 14.5% per year, or a doubling rate of five years. \n\n If the ranks of programmers has doubled every five years, then it stands to reason that most programmers were hired within the last five years, and so about half the programmers would be under 28.  Half of those over 28 would be less than 33.  Half of those over 33 would be less than 38, and so on.  Less than 0.5% of programmers would be 60 or over.  So most of us old programmers are still around writing code.  It’s just that there never were very many of us. \n\n What does this imply for our industry? \n\n Maybe it’s not as bad as  Lord of the Flies , but the fact that juniors exponentially outnumbers seniors is concerning.  As long as that growth curve continues[4] there will not be enough teachers, role models, and leaders.  It means that most software teams will remain relatively unguided, unsupervised, and inexperienced.  It means that most software organizations will have to endlessly relearn the lessons they learned the five years before.  It means that the industry as a whole will remain dominated by novices, and exist in a state of perpetual immaturity. \n\n And, again,  this will remain true as long as the exponential growth continues .  Time can’t heal this problem. \n\n Come to My Lawn \n I know.  I know.  By now many of you are rolling your eyes and whispering to yourselves: “Hey, you kids, get off my lawn.”  But, no.  I want the kids on my grass.  I want to teach as many young developers as I can.  I want to help them avoid the traps and pitfalls that I fell in to 35 years ago.  And I want all programmers in their 60s, 50s, and 40s to be teaching and mentoring as much as possible.  These older folks should be taking the newer programmers under their wings and teaching them how to really fly. \n\n If I’m right about the number of programmers doubling every five years, then one in sixteen developers are 40 or older.  That’s a pretty high student to teacher ratio; but it’s not unworkable.  That small older fraction should be  pairing , not programming alone.  They should be taking fewer personal tasks, so that they can assist others with their tasks.  They should be acting as role models and as team leaders, but not as managers. Think of them as Sergeants not Lieutenants; squad leaders, not company commanders.  They should be leading teams in the trenches and on the front lines. \n\n And they should be paid very well because they have a huge leverage. \n\n One seasoned programmer in their 40s can have a profound effect on a team of a dozen or so twenty-somethings.  As a leader, that programmer can teach the team about principles, patterns, practices, and  ethics .  That leader can temper and curb the youthful enthusiasm that leads to premature decisions about frameworks and architectures.  That leader can help to instill the value of refactoring and clean code, as a counterweight to the youthful thrill of  gettingittowork .  That leader can encourage the team to work hard for eight hours, and then to leave work so that they are fresh the next day. This would prevent burnout, resentment, the false sense that hours equals work, and the  insidious  dependence upon, and value of, heroics.  That leader can look down the road a few months, see the dangers and traps that lie ahead, and correct the trajectory of the team to avoid those problems. \n\n But most important of all, that leader will be creating more leaders; faster, and with less collateral damage, than the school of hard-knocks. \n\n So yes, those leaders should be very well paid.  They should be paid as much as a director, because that’s how much value they can add. \n\n Ideal Staffing Strategy \n There is a meme in the software industry that says it is better to hire young programmers because they are cheap, energetic, and produce more code than older programmers.  But as the above analysis showed, that’s a very foolish perspective.  I believe that meme is responsible for many software failures – especially in the startup realm. \n\n To my mind the ideal ratio of juniors to seniors is about five to one.  Given the demographics, that’s very hard to achieve.  Most software companies and departments won’t manage better than 16:1.  Still, software companies and departments,  especially startups , should be striving to get that ratio as high as they possibly can. \n\n \n [1] I was able to find several references to this quote, but no authoritative source; so this may be an Urban Legend.\n \n[2] Estimate based on  this study .\n \n[3] This is my guess.  I haven’t been able to verify it.  I could be off by a factor of 10 in either direction.\n \n[4] It could be slowing.  Current projections for growth are  8% per year .  But then the economy has been sluggish for the last five years.  A roaring economy could easily push that ratio a  lot  higher. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/06/30/ALittleAboutPatterns.html", "title": "A Little About Patterns.", "content": "\n       \n   Can you name a Design Pattern? \n \n\n A Design Pattern?  You mean from the ’90s? \n\n \n   Yes, they were made popular in 1995.  Though they are much older than that. \n \n\n There was a book wasn’t there? \n\n \n   Yes, it was named Design Patterns: Elements of Reusable Object-Oriented Software. \n \n\n Written by four guys. \n\n \n   Correct, “The Gang of Four”: Eric Gamma, Richard Helm, Ralph Johnson, and John Vlissides.[1] \n \n\n It’s kind of old and out of date now, isn’t it? \n\n \n   What makes you say that? \n \n\n Well, there are newer books. \n\n \n   Indeed there are many newer books on the topic, and some are quite good; but none are quite so impactful and insightful as the original. \n \n\n Anyway, the whole idea is old and broken. \n\n \n   Broken? \n \n\n Yeah, like, I’ve heard people say that Design Patterns were invented to fix some problems in Object-Oriented Programming. \n\n \n   Some problems?  What problems? \n \n\n I dunno.  That’s just what I’ve heard.  Like, Design Patterns were really just workarounds for bad languages. \n\n \n   Bad Languages?  Like what? \n \n\n I Dunno – like, maybe C++ and Java.  You know.  Static languages. \n\n \n   What a strange thing to say.  Smalltalk, a dynamic language, played a significant role. \n \n\n Well, and anyway, they don’t really apply to the future, with Functional Programming and all? \n\n \n   My Goodness!  What Nonsense! \n \n\n Well, that’s just what I’ve heard. \n\n \n   I think you’ve been listening to the wrong people. \n \n\n It was just twitter and facebook and stuff. \n\n \n   Ah, I see.  Social Networks:  The font of all wisdom and knowledge. \n \n\n Yeah, OK, I get your point. \n\n \n   I hope so; but just in case let me be perfectly clear… \n \n\n That’s what politicians say when they’re about to lie to you. \n\n \n   … (sigh)…Listen … Carefully.  The Design Patterns book is one of the most important books, if not THE most important book, written about software within the last 20 years. \n \n\n Oh, come on now.  It’s twenty years old.  I mean maybe it was great way back in the nineteen hundreds; but this is like the twenty-first century.  We’ve got Ruby.  We’ve got Clojure.  We’ve got Node, and Angular, and Rails and IOS and Mobile and … I mean … Software’s changed a lot!  We’ve outgrown all that old stuff. \n\n \n   You think software has changed in the last 20 years? \n \n\n Well, of course!  I mean…   Duh. \n\n \n   You have much to learn about what the word “Change” means.  In fact, the fundamentals of software have changed very little in the last four decades. \n \n\n How can you say that?  I mean, look at all this new stuff! \n\n \n   You mean the glitter? \n \n\n Glitter!!  Are you calling Rails, and Angular, and Node and…  Glitter? \n\n \n   Yes.  Glitter.  It sparkles for awhile but it doesn’t last.  There’s always more glitter to follow. \n \n\n But there are some really great ideas in those frameworks.  How can you call that glitter? \n\n \n   The ideas aren’t glitter.   The frameworks are.  Most of the ideas in those frameworks are old.  They’ve been around for decades.  The ideas are … Design Patterns. \n \n\n Wait.  Are you saying that Active Record is a Design Pattern? \n\n \n   Of course it is.  Or rather there are several Design Patterns within Active Record.  Active Record is a unique combination of many very old, tried and true, patterns of software design. \n \n\n OK, now hold on.  Just what  is  a design pattern? \n\n \n   A Design Pattern is a named canonical form for a combination of software structures and procedures that have proven to be useful over the years. \n \n\n Named?  Canonical?  Huh? \n\n \n   Yes, every Design Pattern has a name and a canonical form.  Professional software designers use those names and forms to communicate with each other.  When they see the name of a pattern in software, or if they recognize the form, then they immediately know the designer’s intent. \n \n\n Can you give me an example? \n\n \n   Sure, here are two that are not from the Design Patterns book.  Model-View-Controller and Dependency-Injection. \n \n\n Those are Design Patterns? \n\n \n   Of course.  They have a well-recognized name, and they have a canonical form. \n \n\n I get the name part; but what’s the form? \n\n \n   Well, take MVC.  You’d expect there to be three objects, a model object, a controller object, and a view object.  The Model handles the business rules.  The Controller handles input.  The View handles output. \n \n\n And I put the models, views, and controllers in different directories. \n\n \n   Uh, well, Yes!  In certain frameworks that has become part of the canonical form. \n \n\n What do you mean in  certain  frameworks. \n\n \n   Just that in the original specification of MVC, directories weren’t mentioned.  Indeed, the original form of MVC is considerably more involved than what I just described; but that’s a story for a different time. \n \n\n You mean there’s more to MVC than just three directories and three types of objects? \n\n \n   (chuckle) Oh, yes.  Much, much, more.  But that’s for later. \n \n\n OK, so I’m starting to get the idea.  If I say “Dependency Injection”, everybody knows exactly what I mean.  So I don’t have to explain myself. \n\n \n   Right.  It saves a lot of time when designers all know the Design Patterns.  They can just talk to each other using those names. \n \n\n OK, and if follow the form of the pattern, then everybody will know what to expect in the code. \n\n \n   Right!  By following the canonical form, you relieve everyone else of the work of having to decode what you have done.  They recognize the form and can follow it easily. \n \n\n And you say there are a lot of these patterns? \n\n \n   The Design Patterns book gives names and forms to twenty-three tried and true solutions to common software problems. \n \n\n Hmmm.  And they’re all as useful as MVC and DI? \n\n \n   Some are much more useful than that! \n \n\n Hmmm.  I should probably learn them then. \n\n \n   I would say so!  Can you name one now? \n \n\n Uh… \n\n \n   Just one. \n \n\n Uh… \n\n \n   Let me help you: Abstract Factory, Builder, Factory Method, Prototype, Singleton, Adapter, Bridge, Composite, Decorator, Facade, Flyweight, Proxy, Chain of Responsibility, Command, Interpreter, Iterator, Mediator, Memento, Observer, State, Strategy, Template Method, and Visitor. \n \n\n I think I’ve got some studying to do. \n\n \n   Yes, I think you do. \n \n\n \n [1] John Matthew Vlissides passed away on the 24th of November, 2005.  He was a colleague and a friend.  He came to my aid more than once.  He is missed. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/06/17/IsTddDeadFinalThoughts.html", "title": "Is TDD Dead?", "content": "\n       The five episodes of the  Is TDD Dead?  hangout , are now over.  The chatter has died down.  David, Kent, and Martin have had their says. The audience has asked their questions and gotten some answers.  Now we can put the whole thing to bed  and get on about our business. \n\n Seldom has a keynote talk created such a big splash.  I don’t ever recall a keynote, and a simple blog, generating such a loud fuss.  Indeed, beyond any other argument, I think the volume of that fuss exposed the flaw in the original thesis.  If TDD were a dead topic, the brouhaha that we’ve all watched would not have happened. \n\n The conclusion of the hangout was amicable, respectful, and agreeable.  Martin’s and Kent’s position was that TDD worked for them in many circumstances; but not all.  David’s position was that TDD worked for him in fewer circumstances but still some.  If there was a disagreement, it was simply a matter of degree.  All parties agreed that programmers should try TDD and then tune their use of it to what works  for them . \n\n ###Individuals.\nThose last two words: “ for them ”, suggest that Martin, Kent, and David were thinking of TDD as an  individual  practice that some individuals may find more useful than others.  Indeed, the notion of  individual  was prominent throughout all the episodes.  It seems pretty clear that David, Kent, and Martin tend to work individually on software and not as part of long term teams.  At no point in the hangouts did any of them talk for long about TDD in the context of a  team . \n\n To be fair, I’m sure that all three of them have worked on teams before.  I’m sure they all  interact  with teams now.  Still, the fact that teams were barely mentioned in the episodes is striking.  The overriding message of the hangouts was that programmers should do what is right in their own eyes  as individuals ; with nary a word about how they should behave in teams. \n\n ###Teams.\nYet most software is built by teams.  Well functioning teams are essential if large software projects are to be successful.  Indeed, one of the founding goals of the Agile movement is to enable the creation of high-functioning teams.  And high-functioning teams must have a shared set of values. \n\n Teams that don’t enjoy a shared set of values are unstable.  If each member of the team does what is right in their own eyes, without considering the values of the team; then they don’t actually comprise a team.  Instead they will behave chaotically, and work at cross purposes to each other. \n\n A Team Divided. \n Imagine a team of programmers working together on a project.  Half of them (call them the “K” faction) value TDD the way Kent and Martin do.  They create many small unit tests in very short cycles.  The other half (The “D” faction) use David’s approach of relying on integration tests in long cycles and very few unit tests.   The “K” faction’s tests run fast.  The “D” faction’s tests run slow.  The “K” faction has very high test coverage.  The “D” faction has lower test coverage.  The “K” faction isolates themselves from peripherals by using mocks across significant architectural boundaries.  The “D” faction binds more tightly to those peripherals, and considers the isolation to be “design damage”. \n\n How can such a team work together?  How can such a team  stay  together? \n\n The fast suite of tests that the “K” faction depends upon in order to refactor is poisoned by the “D” faction’s slower integration tests.  The coverage that the “K” faction relies upon for the confidence to refactor is denied to them by the “D” faction.  And the “D” faction’s vision of design is distorted by all the isolation and mocking created by the “K” faction. \n\n This team is divided; and unless they can somehow come to terms they will continue to work at cross purposes.  Eventually they are bound for divorce. \n\n Divorce. \n The divorce isn’t a fast process.  Frustration builds amongst the individuals until they start to look for other teams to join; teams that share their values.  Bit by bit one of the factions will grow to dominate.  The other faction will shrink by attrition. \n\n I’ve seen this happen through internal transfers within a company; but it is also common for people to leave their current company and find a new company to work for. \n\n The bottom line is simple.  TDD is a team discipline, not simply an individual discipline.  Team members and team leaders need to be very careful to ensure that any new members that they recruit share the values of the team.  Thus TDD teams will grow with more TDDers; and non TDD teams will grow with more non-TDDers.  To the extent that there is a mismatch, attrition will change the compositions of the teams until their values match. \n\n Evolution. \n As time goes on these two values will continue to separate.  They will separate within companies; creating TDD and non-TDD factions within organizations.  As employees move from company to company, they will gravitate towards companies that share their values; creating TDD and Non-TDD companies. \n\n This process is already taking place.  There are now whole companies who declare the TDD value.  As this process continues, and the differences become ever more stark, Natural Selection and Survival of the Fittest, will determine which companies, and which values, thrive. \n\n Of course this is vastly oversimplified.  If the hangout episodes showed anything it was that TDD is not a boolean value.  The disagreement in the hangout was more a disagreement of degree and less a disagreement of kind. \n\n Even so, teams can’t tolerate a huge difference in degree.  So the separation and sorting will continue and the best set of values will eventually prevail. \n\n Perhaps you think this is a prediction without basis.  Perhaps you think I’m just woolgathering and staring at clouds in my coffee.  But consider: \n\n \n   \n     Structured Programming was a huge controversy in the ’70s. The idea that  Goto  was “harmful” caused wars and rumors of wars in the editorial pages of the trade journals.  Nowadays, however, we look askance at any use of  goto .  The factions separated, and the structured programming faction won out. \n   \n   \n     Objects were wildly controversial in the ’80s.  The internet newsgroups were alive with flame wars over the topic.  Proponents and detractors flamed each other with intensity and venom.  Nowadays however, we use objects as a matter of course.  The factions separated, and the object faction won out. \n   \n   \n     Agile was wildly controversial in the late ’90s and early ’00s.  Whole books were published about how Agile could not possibly work.  Nowadays, Agile has become mainstream and is rapidly gaining momentum.  The factions separated and Agile won out. \n   \n \n\n In 2014 I’m betting on the TDD faction. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/09/19/MicroServicesAndJars.html", "title": "Microservices and Jars", "content": "\n       One of my clients recently told me that they were investigating a micro-service-architecture.  My first reaction was: “What’s that?”  So I did a little research and found Martin Fowler’s and James Lewis’  writeup  on the topic. \n\n So what is a micro-service?  It’s just a little stand-alone executable that communicates with other stand-alone executables through some kind of mailbox; like an http socket.  Lots of people like to use REST as the message format between them. \n\n Why is this desirable?  Two words.   Independent Deployability . \n\n You can fire up your little MS and talk with it via REST.  No other part of the system needs to be running.  Nobody can change a source file in a different part of the system, and screw your little MS up.  Your MS is immune to all the other code out there. \n\n You can test your MS with simple REST commands; and you can mock out other MSs in the system with little dummy MSs that do just what your tests need them to do. \n\n Moreover, you can control the deployment.  You don’t have to coordinate with some huge deployment effort, and merge deployment commands into nasty deployment scripts.  You just fire up your little MS and make sure it keeps running. \n\n You can use your own database.  You can use your own webserver.  You can use any language you like.  You can use any framework you like. \n\n Freedom!  Freedom! \n\n Meet the new boss. \n But wait.  Why is this better?  Are the advantages I just listed absent from a normal Java, or Ruby, or .Net system? \n\n What about:  Independent Deployability ? \n\n We have these things called  jar  files.  Or Gems.  Or DLLs.  Or Shared Libraries.  The reason we have these things is so we can have independent deployability. \n\n Most people have forgotten this.  Most people think that  jar  files are just convenient little folders that they can jam their classes into any way they see fit.  They forget that a  jar , or a DLL, or a Gem, or a shared library, is  loaded and linked at runtime .  Indeed, DLL stands for  Dynamically Linked Library . \n\n So if you design your  jar s well, you can make them just as independently deployable as a MS.  Your team can be responsible for your  jar  file.  Your team can deploy your DLL without massive coordination with other teams.  Your team can test your GEM by writing unit tests and mocking out all the other Gems that it communicates with.  You can write a  jar  in Java or Scala, or Clojure, or JRuby, or any other JVM compatible language.  You can even use your own database and wesbserver if you like. \n\n If you’d like proof that  jar s can be independently deployable, just look at the plugins you use for your editor or IDE.  They are deployed entirely independently of their host!  And often these plugins are nothing more than simple  jar  files. \n\n So what have you gained by taking your  jar  file and putting it behind a socket and communicating with REST? \n\n ##Freedom’s just another word for…\nOne thing you lose is time.  It takes  time  to communicate through a socket.  It takes  time  to decode REST messages.  And that time means you cannot use micro-services with the impunity of a  jar .  If I want two  jar s to get into a rapid chat with each other, I can.  But I don’t dare do that with a MS because the communication time will kill me. \n\n On my laptop it takes 50ms to set up a socket connection, and then about 3us per byte transmitted through that connection.  And that’s all in a single process on a single machine.  Imagine the cost when the connection is over the wire! \n\n Another thing you lose (and I hate to say this) is debuggability.  You can’t single step across a REST call, but you can single step across  jar  files.  You can’t follow a stack trace across a REST call.  Exceptions get lost across a REST interface. \n\n ##Backpedal\nAfter reading this you might think I’m totally against the whole notion of Micro-Services.  But, of course, I’m not.  I’ve built applications that way in the past, and I’ll likely build them that way in the future.  It’s just that I don’t want to see a big fad tearing through the industry leaving lots of broken systems in it’s wake. \n\n For most systems the independent deployability of  jar  files (or DLLS, or Gems, or Shared Libraries) is more than adequate.  For most systems the cost of communicating over sockets using REST is quite restrictive; and will force uncomfortable trade-offs. \n\n My advice: \n \n   Don’t leap into microservices just because it sounds cool.  Segregate the system into  jar s using a plugin architecture first.  If that’s not sufficient, then consider introducing service boundaries at strategic points. \n \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/09/03/TestTime.html", "title": "Test Time", "content": "\n       I was working with a client a few weeks ago; and they told me that it took several minutes to run their unit tests.  I suggested that they run a smaller suite; and they responded by telling me that the issue wasn’t the runtime of the tests, but the  build time .  They were using C++, and it took several minutes to build and link an executable that could run even a small suite of tests. \n\n I told them that they should link with smaller libraries, and make sure that the dependencies between those libraries were unidirectional, concrete to abstract, in order to shorten their link times.  “After all,” I said “if you limit the amount you link, the link time can be less than a second.” \n\n They responded by telling me that their unit testing library was one of the reasons that the link time was so slow.  This took me by surprise since I couldn’t imagine a unit testing library that would be large enough to slow down the link time appreciably.  So I told them: “If your unit testing tool is slowing down the tests, you need a new tool.” \n\n Their response was that this tool was  the best tool out there . \n\n Here’s a clue:  If your testing tool is the reason it takes a long time to run your tests, then it’s not the best tool for the job – let alone the best tool out there. \n\n Fast. \n Tests need to run fast.   Anything  that gets in the way of fast test times is forfeit, no matter what other wonderful benefits it may provide.  It may have really super mocking tools; but if it’s slow, drop it!.  It may be endorsed by all the top gurus; but if it’s slow, burn it!.  It may be the tool that ships with your IDE; but if it takes ten seconds just to start testing, toss it!  Allow  nothing  to slow down your tests. \n\n Why?  Simple.  Slow tests aren’t run often enough.  The slower the tests the less frequently they are run.  The less frequently the tests are run, the more invested you get in the code you write between the test runs; and the more you will allow the code to rot just to avoid another expensive test run. \n\n The primary benefit of TDD is the ability to refactor without fear, and without cost.  The slower your tests run, the higher the refactoring cost.  The higher the refactoring cost, the faster your code will rot.  And rotten code slows everything else down.   Don’t let the tests get slow! \n\n A Design Challenge \n Keeping the tests running very fast is a design challenge.  It’s one of the design constraints that well heeled craftsmen put upon themselves.  They purposely design the system so that the test time is fast.  That means choosing fast testing tools and building decoupled architectures.  That means  thinking  about how to keep the tests running fast all the time; and   refactoring  when the tests start getting slow. \n\n Decoupled architectures allow you to build fast test doubles that stub out subsystems that are traditionally slow.  For example, if your system is well decoupled, it is trivial to stub out the database or the web server.  If the system is well decoupled, all slow operations fall on the far side of an architectural boundary that can be stubbed.  And that stubbing can turn a test that takes minutes, into a test that runs in milliseconds! \n\n FitNesse \n I and my associates have been working on FitNesse for well over 10 years.  FitNesse is around 72,000 lines of Java code nowadays; 31,000 of those lines are unit tests.  There are also nearly three hundred acceptance tests (FitNesse tests).  The compile/build/test time for the whole project (on my laptop) is about one minute and 45 seconds.  Typically we don’t run the acceptance tests while we are in the fast red/green/refactor loop.  Indeed, we usually only run a subset of the unit tests in that loop.  So running the tests in the red/green/refactor loop seldom takes more than two or three seconds.   That’s the kind of test speed you are after! \n\n To attain those speeds we stub out all the slow things like the persistence framework, and the web framework.  We stub them in the unit tests.  We stub them in the acceptance tests.   Anything  that runs slow, we stub. \n\n Of course not all the tests stub those things out.  Some of the tests go all the way through from web server to database.  But well over 90% of those tests bypass the slow stuff.  After all, how many times do you need to tests the database to know that it works?  How many times do you need to test the web server to know that it works?  The answer to both questions is pretty close to one.  So testing those slow things much more than once is a waste of time. \n\n ###Conclusion\nProgrammers who care about their systems, care about the tests for those systems.  Programmers who care about the tests, make sure those tests run  fast .  Slow running tests represent a design flaw that reflects on the experience and professionalism of the team.  A slow running test suite is either a freshman error, or just plain carelessness. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/10/01/CleanMicroserviceArchitecture.html", "title": "Clean Micro-service Architecture", "content": "\n       How do you scale a software system?  One thing should be obvious: at some point you need to have more than one computer.  There was a day, and it wasn’t so long ago, that scaling a system could be achieved by waiting.  You simply waited for computers to get faster and more powerful. Every few months you automatically got an increase in scale. \n\n Whether that was a good strategy or not; it doesn’t work anymore.  When the millennium turned, hardware designers stopped trying to increase clock rates and started to proliferate cores instead.  Indeed, in order to achieve that proliferation those hardware designers have been removing the caches and pipelines that used to enhance the speed of single core machines. \n\n So today, scaling a software system means adding more cores, and adding more servers.  There’s no way around that.  So how do you do it?  How do you split your application up so that it can be run on multiple cores and multiple servers? \n\n How do you scale? \n\n Your graphics card uses one approach.  It has many processors that operate in lockstep; performing the same operations on different areas of internal memory.  This form of massive parallelism is ideal for a graphics card since high speed graphics are achieved by performing the same transformations over large arrays of similar data.  Indeed, supercomputers have used this approach for decades to predict weather, and simulate nuclear explosions. \n\n Another technique is the traditional three-tiered approach.  You subdivide your system into a GUI, a middleware, and a database.  You allocate some servers to the GUI, a few more to the middleware, and yet a few more to the database.  You compose a suite of messages (typically involving serialized objects) that can be passed between the layers.  Voila!  Scaling. \n\n Micro-services \n\n Lately we are seeing another kind of scaling strategy.   Micro-services .  I’ve written about them  here , and  here .  Martin Fowler and James Lewis have famously written about them  here . \n\n A micro-service is a small executable running on a server somewhere.  It responds to asynchronous messages.  Typically those messages are delivered over HTTP in REST format; though that’s a detail, not a requirement. \n\n A system has a  micro-service architecture  when that system is composed of many collaborating micro-services; typically without centralized control. \n\n Clean Architecture and Micro-services. \n\n Now consider the so-called  Clean Architecture .  Note that it makes use of many components, including Use-cases, Presenters, and Gateways. Those components receive requests in the form of primitive data structures (POJOs) that arrive from a source that is decoupled from the component via a polymorphic input boundary.  Note also that the components respond to these requests by creating new primitive data structures and sending them to an output sink that is decoupled from the component via a polymorphic output boundary.  Could this structure be used to implement a micro-service architecture? \n\n \n   Of course. \n \n\n Nothing in the Clean Architecture demands that the messages need to be either synchronous or asynchronous.  Nothing prevents those request and response messages from being transmitted to another server.  Nothing about the architecture prevents the components from being little executables communicating over HTTP via REST. \n\n So, a micro-service architecture can nicely conform to the Clean Architecture.  Indeed, were I to build a system using micro-services, I would almost certainly follow this route. \n\n The Component Scalability Scale \n\n A Micro-service is just one way to deploy a software component.  There are others; and they have different scalabilities.  Here is a list of some different deployment options for components, in order of scalability. \n\n \n   Micro-services deployed on lots of servers. \n   A smaller number of servers, each running more than one micro-service. \n   Just one server with a group of micro-services running as simple executables. \n   Services running as threads in a single virtual machine communicating via message queues \n   Dynamically linked components (in jars or DLLs) passing data structure messages through function calls. \n \n\n Again, it should be obvious that the Clean Architecture works just as well at any level on this scale.  The reason is that the Clean Architecture  does not care  how the components are deployed.  Indeed a system with a good Clean Architecture  does not know which deployment option it is using. \n\n Let me say that again.  The code within the components of a Clean Architecture has no idea whether: \n\n \n   it is in a micro-service running on an independent server communicating with other independent servers over the internet, \n   or in a small executable among many running on a single machine communicating over simple sockets, \n   or in a lightweight thread communicating with other lightweight threads in the same virtual machine using mailboxes or queues, \n   or in a simple Jar or DLL communicating with other components using polymorphic function calls. \n \n\n And that should give you a clue as to what this article is really about. \n\n The Deployment Model is a Detail. \n\n If the code of the components can be written so that the communications mechanisms, and process separation mechanisms are irrelevant,  then those mechanisms are details .  And details are  never  part of an architecture. \n\n That means that there is no such thing as a micro-service architecture.  Micro-services are a  deployment option , not an architecture.  And like all options, a good architect keeps them open for as long as possible.  A good architect defers the decision about how the system will be deployed until the last responsible moment. \n\n Forced Ignorance. \n\n Many folks will likely complain about this viewpoint by suggesting that if you don’t design your system for micro-services up front, you won’t be able to shim them in after the fact. \n\n \n   That’s BDUF Baloney. \n \n\n The job of good system architects is to create a structure whereby the components of the system – whether Use-cases, UI components, database components, or what have you – have no idea how they are deployed and how they communicate with the other components in the system.  This forced ignorance allows the architects to choose a deployment model that works for the  current  situation, and to adapt the deployment model as the situation changes.  If the system must massively scale, you deploy it in micro-services.  If the system needs two or three servers only, you deploy it in a combination of processes, threads, and jars.  If you never need more than a single server, you can deploy it in jars alone. \n\n Breaking that forced ignorance is a good way to over-engineer a system.  Too often I have seen systems that have adopted huge three-tiered architectures in anticipation of scaling, only to discover that they never need to be deployed on more than one server.  How much simpler could that software have been if only they had tried the single server option first, and kept the components ignorant of the deployment model? \n\n Other Matters. \n\n Of course there are other matters to consider.  Firstly, if you deploy into micro-services, you have the freedom to choose any language you’d like.  You can write your micro-service in Ruby, Clojure, Java, C#, C++, C, assembler, Pascal, Delphi, PHP, Javascript, or even COBOL.  Secondly, you can use whatever framework you like.  One micro-service could use Rails, another could use Spring, still another could use BOOST.  Similarly, each micro-service may be able to use a different database.  One might use Couch, while another used SqlServer and still another used MySql or Datomic.  Finally, there is the intense isolation that a micro-service implies.  A micro-service boundary is the ultimate form of decoupling. \n\n That last point needs amplification.  If two components communicate over HTTP using REST, then they are  strongly decoupled .  The only thing binding those two components together is the schema of the REST messages; i.e. the interface.  Not only are they decoupled by the interface, they are also decoupled in deployment time.  Those two services do not need to be started at the same time; nor do they need to be shut down at the same time.  It is perfectly possible to reboot a micro-service without affecting those that depend on it.  That’s a lot of decoupling. \n\n Restrictions down the scale. \n\n As you move down the scale from micro-services to processes to threads to jars, you start to lose some of those flexibilities.  The closer you get to jars the less flexibility you have with languages.  You also have less flexibility in terms of frameworks and databases. There is also a greater risk that the interfaces between components will be increasingly coupled.  And, of course, it’s hard to reboot components that live in a single executable. \n\n Or is it?  Actually OSGi has been around in the Java world for some time now.  OSGi allows you to hot-swap jar files.  That’s not quite as flexible as bouncing a micro-service, but it’s not that far from it. \n\n As for languages, it’s true that within a single virtual machine you’ll be restricted.  On the other hand, the JVM would allow you to write in Java, Clojure, Scala, and JRuby, just to name a few. \n\n So, yes, as you go down the scale the restrictions increase; but perhaps not all that much. \n\n As for frameworks and databases, is it really such a bad thing, especially early in a system’s development, to limit their numbers?  Do we really want to start out with one team using JPA and another using Hibernate?  Do we really want one component using Datomic and another using Oracle?  And if we allow that, aren’t we creating a lot of configuration complexity? \n\n And, finally, interface coupling is a matter of discipline and good design.  After all, a plain old Java Object (pojo) passed through a polymorphic interface is no more coupled than REST.  A little bit of care in component design is all it takes to make jars whose interfaces are just as loosely coupled as a micro-service. \n\n TANSTAAFL \n\n As you move up the scale, those restrictions drop away, but new problems start to show up.  In what order to you start up the system?  In what order do you shut it down?  How do you deal with configuration and control of all the services.  What about duplicated code?  How about the versioning of message formats? But rather than me itemizing all the issues here, you can read about some of them   here  and  here .  Suffice it to say that the decision to use micro-services is a trade-off not a free lunch. \n\n Monoliths and Marketeers. \n\n Finally, a word about nomenclature.  Some advocates of micro-services like to classify the alternative as  monolithic .  This is a pejorative term chosen to imply: “Bad”.  The word  monolith  means “one rock”.  The implication is that if you aren’t using micro-services, then you must have a big coupled monstrosity. \n\n \n   That’s Marketing Baloney. \n \n\n A well designed system following the Clean Architecture is about as far from a monolith as you can get.  Rather, it is a set of independently deployable dynamically linked components (jars or DLLs) that are strongly decoupled from each other, can be maintained by different teams, can be written in a multitude of different languages, and can be hot-swapped using something like OSGi.  Hardly monolithic. \n\n Conclusion and Recommendation[1] \n\n From all of this you might be getting the idea that I think micro-services are a bad idea; and that you should not use them.  This is not the case.  Micro-services are a perfectly viable deployment model that you should strive to be compatible with.  If you can’t deploy into micro-services, it means you’ve coupled your architecture to a particular deployment model. \n\n By the same token if you can  only  deploy your system with micro-services, then you have coupled your architecture to  that  particular deployment model; and that’s just as bad. \n\n What I am trying to convince you to do is to  ignore  any particular deployment model.  Treat the deployment model as a detail, and leave the options open.  Build your system so that you can deploy it into jars, or into micro-services, or anywhere in between. \n\n Begin by deploying your system into dynamically linked components (Jars or DLLs), and gradually walk up the scale as the need arises.  Don’t leap to the top of the scale in anticipation of massive scaling.  Keep that option open by conforming to the Clean Architecture. \n\n \n\n [1] Who am I to make this recommendation?  After all, as I said in a previous article, I just encountered the term “Micro-services” a few weeks ago. \n\n I may have just discovered the  term ; but in the last 40 years of my career I have had ample opportunity to design and build systems that deployed components as independent executables communicating through messages.  Micro-services might be a new term; but it’s hardly a new idea. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/10/08/GOML1-ResponsiveDesign.html", "title": "GOML-1, Responsive Design", "content": "\n       Consarned kids, can’t keep the definitions of words straight. \n\n Do you remember when the word “responsive” meant  fast ?  Yeah, that’s what it  used  to mean all right.  We used to say things like: “Boy, that system sure is responsive!” or “We’re going to have to work hard to keep that system responsive.” \n\n But now, all of a sudden, the word means:  compatible with mobile devices.   Huh?  Responsive?  In what way does adapting a GUI to the size of a screen make the system responsive?  What the hell is the system responding to?  The screen?  Crimeny! \n\n I wanna know who hijacked that term, and why?  I’d like to sit down and have a  long  talk with that feller.  Couldn’t he have called it “adaptive”, or maybe “mobile friendly”.  Why in tarnation did he pick a word like “responsive”; a word that already had a perfectly good meaning? \n\n I sat in a meeting the other day and heard someone say: “But when will you make the system responsive.”  My immediate reaction was that the system was too slow.  I  responded [1] by saying that I thought the  response  time was actually pretty good.  Everyone turned and looked at me like I was a neanderthal. \n\n And while I’m on the topic of words that have suddenly changed their meaning for no good reason, who’s the nincompoop who stole the word “Design”?  I mean, do you remember when “Design” was a technical term that referred to the internal software structure of the system?  Software designers  used  to be lead programmers.  Everybody wanted to become a  designer .  Everybody aspired to learn  software design . \n\n But not now.  Oh no!  Now, software design means  Graphic Design .  What would have been wrong with calling Graphic Design, Graphic Design?  What would have been wrong with calling Graphic Designers, Graphic Designers?  Why’d they have to take our title away from us. \n\n I was a  designer  dammit.  I designed software  systems .  Now I tell people that I design software systems and they think I’m a UX guy!  How the hell did  that  happen. \n\n Consarned kids!  Get Off My Lawn. \n\n \n [1] See.  That’s what the word “respond” means.  You respond to a stimulus.  A screen is not a stimulus.  A screen is an IO device.  You don’t respond to the format of an IO device.  You respond to  Users .  Consarned kids. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/09/18/TheMoreThingsChange.html", "title": "The More Things Change...", "content": "\n       In 1984 I was the technical lead on a telephone controlled voice response system called the  Craft Dispatch System . The purpose of CDS was to allow telephone repair craftsmen to call in and get their next repair job; without having to interact with a human. \n\n The craftsperson, using standard touch tone (DTMF) buttons, would log in to CDS and ask for their next job.  CDS would recognize their ID, inspect their job queue, and then read the next job to them.  It would tell them the address, the complaint, the most recent status of the telephone line, and any recent and relevant history on that line. \n\n CDS also allowed the craftsmen to run electronic tests on the phone line and get the results spoken back to them within a few seconds.  To achieve this, CDS communicated directly with 4-TEL, our telephone test system. \n\n Finally, CDS allowed the craft person to close out the repair by entering the new status of the line and any recommendations or comments they might have.  They could even leave a voice message for the dispatcher if they thought it necessary. \n\n We designed and built all the hardware for this system.  The processor was a 80286 with one megabyte of RAM.  We had an ST412 hard drive with a capacity of 10MB.  It held all our voice files, program data, and executables. \n\n Our telephone interface card could detect ringing and DTMF; could answer and hang up; and could emit DTMF tones if it needed to dial.  It could play back and record telephone audio; which was encoded using a one bit CVSD codec that allowed us to hold five minutes of voice per megabyte. \n\n Our operating system was Digital Research’s MP/M; which was a multiprocessing variant of CP/M.  It could run several processes at the same time.  Those processes were usually started from the command line. \n\n Radical \n\n I was responsible for the architecture of the system.  I thought long and hard about how to structure this beast; having built several other voice response systems over the previous five years.  Most of those older systems were compiled and linked into single executables.  But not CDS. For CDS I had a  radical  new idea. \n\n The database for the system was kept in RAM, and written to disk at critical moments.  It was a simple name/value pair data store.  We called it the 3DBB (Remember the old Mr. Wizard cartoon with the “Three Dimensional Black Board”?)  3DBB was fronted by a process that ran continuously.  Other processes would request values by passing messages to the 3DBB process.  The 3DBB would respond with the appropriate value. \n\n The values held in the 3DBB were a special format that I called FLD for “Field Labeled Data”.  Nowadays we’d call this JSON or XML.  It had it’s own text encoding scheme that, quite by accident, looked a  lot  like JSON.  However, FLDs were actually binary tree structures that were only converted to text if they needed to be displayed for some reason – usually for debugging purposes. \n\n The operation of the system was a big finite state machine.  Every input from a craftsperson in the field, every result of a processing a job, and every outcome of communicating with an external system was an event to that FSM. The actions of the FSM were  command lines . \n\n The FSM state was stored in the 3DBB.  The FSM logic was described as a state transition table kept in a text file.  The FSM process would read that file and convert it into a table in RAM.  Then it would accept events from message queues, look up the appropriate transitions in the table, and respond by changing the state in the 3DBB and invoking the appropriate command line action. \n\n We had three telephone interface cards, allowing us to listen at three phone lines at once.  So we had three FSM processes running simultaneously.  Each of these processes would accept events from the phone, or from other processes, and invoke appropriate command line tasks based on the state of the machine. \n\n There were many different command line actions that the FSM could invoke.  One was login.  Another was to fetch a job and read it back.  Still another was to start a test, and another was to read test results back.  All in all there were over a dozen different command line actions that were driven by this finite state machine. \n\n The command line processes were invoked with a session ID on the command line itself.  This allowed them to go to the 3DBB and pull the session FLD to discover what was going on.  Then it would do it’s job and accept inputs from the user.  When it had completed it’s job it would update the session in the 3DBB, would send the next event to it’s FSM, and then terminate.  That next event would depend on the outcome of it’s job, and the input received from the user. \n\n Nice huh?  Radical. \n\n Back to the Future. \n\n If I were to describe CDS in today’s terminology it would sound like this: \n\n \n   We used a Micro-Service Architecture, communicating through a message buss, driven by Business Process Model (BPM) interpreted by a BPEL engine, backed by a NOSQL database holding values in JSON. \n \n\n Which leads me to suggest the following: \n\n \n   The more things change, the more they stay the same. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/10/26/LaughterInTheMaleDominatedRoom.html", "title": "Laughter in the male dominated room.", "content": "\n       Look at this graph.  It should scare the hell out of you.\n \n\n It comes from  Episode 576 of Planet Money on NPR .  In that episode they suggest that the reason women have bailed out of software careers is because marketeers in the ’80s decided to market home computers as toys for boys, sort of like Tonka Trucks.  I’m not sure I agree with their conclusion; but the data they present is still frightening. \n\n The data fits my own experience.  As a young programmer in the early 70s, I worked with a number of women. In my very first job they represented about a quarter of the total programming staff.  Oddly, they were all several years older than I was.  Perhaps that was because I, and my buddies, were barely 20.  Still, perhaps my buddies and I were on the first wave of the male dominated cohort. \n\n As the years went by, the ratio of women I worked with declined significantly.  Why?  What is it about software that shuns women?  Women were prevalent in the field in the ’60s and early ’70s.  What happened? \n\n ###GamerGate\n \nThis is  Anita Sarkeesian .  She produces a youtube video series entitled:  Tropes vs. Women in Video Games .  It’s fascinating.  If you want a taste, I recommend  this one .  The points she makes (and she makes them well) are pretty disturbing. \n\n What’s more disturbing is the   harassment  she’s experienced.  Rants. Raves.  Death Threats.   A computer game  that allows you to beat her face in.  I mean, yeah, that’s pretty sick. \n\n ###Correlation?\nIn what sense are these two things related? If you look back at that graph, there must be some  force  that’s pushing that red line down.  And there must also be some force that is motivating the harassment against outspoken women in software like Sarkeesian.  Is it the same force? \n\n ###Hints in the Stumbles.\nSeveral years ago I gave a talk at a Ruby conference.  In that talk I jokingly compared C++ to testosterone, and Java to estrogen.  This elicited a wave of tweets from some disappointed women who felt that I was demeaning women in general.  That was not my intent; but when it was pointed out I agreed and apologized.  The apology was graciously accepted; and we all moved on.  I felt the issue was closed. \n\n \n\n A couple of years later I was giving a talk to a group of C programmers.  The talk took a comic turn towards COBOL, and I put up this image of Commodore Grace Hopper. \n\n \n\n I said something like: “COBOL was invented by Grace Hopper.  Here she is, in her little hat.” \n\n After the talk, a courageous young woman took me aside and chastised me for belittling Grace Hopper.  She said: “When someone like you says something like that, it doesn’t help women like me.” \n\n Again, she was right, and so I made a public apology and moved on.  Again, I thought the issue had ended. \n\n \n\n A few months ago I wrote a blog about frameworks.  I made the point that the relationship between frameworks and framework users is asymmetric.  The commitment only goes one way.  The user is bound to the framework; but the framework is not bound to the user. \n\n And then I made the comparison to a Sultan and his concubines.  The metaphor was crude, and I wanted it to be crude to drive the point home.  I also wanted it to be funny.  It was a kind of joke. \n\n All hell broke lose on twitter.  Some people said the post was sexist.  Others used the word Misogyny. \n\n I pulled the article within two hours of posting it, rewrote it, and apologized again.  But this time I kept the issue open in my mind.  Three times?   Something  was clearly wrong with aspects of my delivery; but I didn’t know what it was.  I think perhaps now I do. \n\n ###We Laugh\nIn all three cases I was using humor as a vehicle to help me make a point.  In all three cases the humor was at the expense of women.  Why?  What’s funny about that? \n\n What’s funny about a video game in which you beat up the face of an outspoken woman?  What’s funny about a video game that allows you to buy the services of a prostitute and then kill her instead of paying her?  What is funny about that? \n\n Why did I think my “estrogen” joke was funny?  Others did too, because there was laughter in the male dominated room. \n\n Why did I think my “little hat” comment was funny?  Others did too, because there was laugher in that male dominated room. \n\n Why did I think my “framework-users as concubines” joke was funny?  Hell,  I still do !  I can feel the giggle inside me trying to bubble up even now. \n\n Perhaps you got the joke too.  Perhaps you felt a  smirk  trying to drag up a corner of your face.  If so, you’re probably male.  But ask yourself  why  that’s funny.  Ask yourself if your laugh is at someone else’s expense.  Ask yourself if the laugh is truly harmless. \n\n ###The Butt\nOne of the folks who chided me about my “concubine” article said it pretty well.  I’ll paraphrase:  The suffering of women should not be used as the butt of one of your jokes .  It was  that  point, amidst all the other complaints, that made sense to me, and was the reason I rewrote the article and made my apology. \n\n In general, the act of using a class of people as the butt of a joke, is demeaning.  You treat those people as  things  to be  used  in pursuit of a goal.  And that’s not right, even if all you are trying to do is to make a technical point about frameworks, or COBOL, or Java. \n\n ###Another Example\n \n\n Several years ago I sat at a bar with Desi McAdam (@desi) who was a co-founder of DevChix.  I told her that when I get a program to work, I feel like I’ve slain the beast and I’m bringing home meat.  She responded by saying that she felt that she had nurtured something into being. \n\n I love that story because it’s a celebration of the wonderful difference between men and women; a difference I think we need and should cherish.  I have told that story several times to several audiences.  It always elicits a laugh.  Why?  What’s funny about it?  Is it funny because it’s resonates so well with common experience?  Is it funny because it’s an instance of a common stereotype?  And is it funny to the women in the audience; or is the laughter I hear just the laughter of men in a male dominated room. \n\n ###The Force.\nIs that laughter the force that’s pushing the red line in that graph down.  Is it laughter, at the expense of women, that’s causing women to flee from software careers.  Is it that laughter that, in the extreme, is driving that horrible harassment? \n\n Could the women be fleeing from the laughter in our male dominated room? \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/11/24/FPvsOO.html", "title": "OO vs FP", "content": "\n       A friend of mine posted the following on facebook.  He meant it as a troll; and it worked, because it  irked  me.\n \n \n\n There are many programmers who have said similar things over the years.  They consider Object Orientation and Functional Programming to be mutually exclusive forms of programming.  From their ivory towers in the clouds some FP super-beings occasionally look down on the poor naive OO programmers and cluck their tongues. \n\n That clucking is echoed by the OO super-beings in  their  ivory towers, who look askance at the waste and parentheses pollution of functional languages. \n\n These views are based on a deep ignorance of what OO and FP really are. \n\n Let me make a few points: \n\n \n   \n     OO is not about state . \n\n     Objects are not data structures.  Objects may  use  data structures; but the manner in which those data structures are used or contained is hidden.  This is why data fields are private.  From the outside looking in you cannot see any state.  All you can see are functions.  Therefore Objects are about functions not about state. \n\n     When objects are used as data structures it is a  design smell ; and it always has been.  When tools like  Hibernate  call themselves object-relational mappers, they are incorrect.  ORMs don’t map relational data to objects; they map relational data to data structures.  Those data structures  are not objects . \n\n     Objects are bags of functions, not bags of data. \n   \n   \n     Functional Programs, like OO Programs, are composed of functions that operate on data. \n\n     Every functional program ever written is composed of a set of functions that operate on data.  Every OO program ever written is composed of a set of functions that operate on data. \n\n     It is common for OO programmers to define OO as functions and data bound together.  This is true so far as it goes; but then it has  always  been true irrespective of OO.  All programs are functions bound to data. \n\n     You might protest and suggest that it is the  manner  of that binding that matters.  But think about it.  That’s silly.  Is there really so much difference between  f(o) ,  o.f() , and  (f o) ?   Are we really saying that the difference is just about the syntax of a function call?[0] \n   \n \n\n The Differences \n\n So what are the differences between OO and FP?   What does OO have that FP doesn’t, and what does FP have that OO doesn’t? \n\n \n   \n     FP imposes discipline upon assignment. \n\n     A true functional programming language has no assignment operator.  You cannot change the state of a variable.  Indeed, the word “variable” is a misnomer in a functional language because you cannot vary them. \n\n     Yes, I know, Funcitonal Programmers often say hi-falutin’ things like “Functions are first-class objects in functional languages.”  That may be true; but functions are first-class objects in Smalltalk, and Smalltalk is an OO language, not a functional language. \n\n     The overriding difference between a functional language and a non-functional language is that functional languages don’t have assignment statements.[1] \n\n     Does this mean that you can  never  change the state of something in a functional language?  No, not at all.  Functional languages generally offer ceremonies that you can perform in order to change the state of something.  F# allows you to declare “mutable variables” for example.  Clojure allows you to create special, uh, objects who’s values can be changed using various magic incantations. \n\n     The point is that a functional language imposes some kind of ceremony or discipline on changes of state.  You have to jump through the right hoops in order to do it. \n\n     And so, for the most part, you don’t. \n   \n   \n     OO imposes discipline on function pointers. \n\n     “Huh?” you say.  But that, in fact, is what OO comes down to.  For all the hi-falutin’ rhetoric about OO and “real-world objects” and “programming closer to the way we think”, what OO really comes down to is that OO languages replace function pointers with convenient polymorphism. [2] \n\n     How do you implement polymorphism?  You implement it with function pointers.  OO languages simply do that implementation for you, and hide the function pointers from you.  This is nice because function pointers are very difficult to manage well.  Trying to write polymorphic code with function pointers (as in C) depends on complex and inconvenient conventions that everyone must follow  in every case .  This is usually unrealistic. \n\n     In Java, every function you call is polymorphic.  There is no way you can call a function that is  not  polymorphic.  And that means that every java function is called  indirectly  through a pointer to a function.[3] \n\n     If you wanted polymophism in C, you’d have to manage those pointers yourself; and that’s hard.  If you wanted polymorphism in Lisp you’d have to manage those pointers yourself (pass them in as arguments to some higher level algorithm (which, by the way  IS  the  Strategy  pattern.))  But in an OO language, those pointers are managed for you.  The language takes care to initialize them, and marshal them, and call all the functions through them. \n   \n \n\n Mutually Exclusive? \n\n Are these two disciplines mutually exclusive?  Can you have a language that imposes discipline on both assignment and pointers to functions?  Of course you can.  These two things don’t have anything to do with each other.  And that means that OO and FP are  not  mutually exclusive at all.  It means that you can write OO-Functional programs. \n\n It also means that all the design principles, and design patterns, used by OO programmers can be used by functional programmers if they care to accept the discipline that OO imposes on their pointers to functions. \n\n But why would a functional programmer do that?  What benefit does polymorphism have that normal Functional Programs don’t have?  By the same token, what benefit would OO programmers gain from imposing discipline on assignment? \n\n Benefits of Polymorphism. \n\n There really is only one benefit to Polymorphism; but it’s a big one.  It is the inversion of source code and run time dependencies. \n\n In most software systems when one function calls another, the runtime dependency and the source code dependency point in the same direction.  The calling module depends on the called module.  However, when polymorphism is injected between the two there is an inversion of the source code dependency.  The calling module still depends on the called module at run time.  However, the source code of the calling module does not depend upon the source code of the called module.  Rather both modules depend upon a polymorphic interface. \n\n This inversion allows the called module to act like a plugin.  Indeed, this is how all plugins work. \n\n Plugin architectures are very robust because stable high value business rules can be kept from depending upon volatile low value modules such as user interfaces and databases. \n\n The net result is that in order to be robust a system must employ polymorphism across significant architectural boundaries. \n\n Benefits of Immutability \n\n The benefit of not using assignment statements should be obvious.  You can’t have concurrent update problems if you never update anything. \n\n Since functional programming languages do not have assignment statements, programs written in those languages don’t change the state of very many variables.  Mutation is reserved for very specific sections of the system that can tolerate the high ceremony required.  Those sections are inherently safe from multiple threads and multiple cores. \n\n The bottom line is that functional programs are much safer in multiprocessing and multiprocessor environments. \n\n The Deep Philosophies \n\n Of course adherents to both Object Orientation and Functional Programming will protest my reductionist analysis.  They will contend that there are deep philosophical, psychological, and mathematical reasons why their favorite style is better than the other. \n\n My reaction to that is:   Phooey! \n\n Everybody thinks their way is the best.  Everybody is wrong. \n\n What about Design Principles, and Design Patterns? \n\n The passage at the start of this article that  irked  me suggests that all the design principles and design patterns that we’ve identified over the last several decades apply only to OO; and that Functional Programming reduces them all down to:  functions. \n\n Wow!  Talk about being reductionist! \n\n This idea is bonkers in the extreme.  The principles of software design still apply, regardless of your programming style.  The fact that you’ve decided to use a language that doesn’t have an assignment operator does not mean that you can ignore the Single Responsibility Principle; or that the Open Closed Principle is somehow automatic.  The fact that the Strategy pattern makes use of polymorphism does not mean that the pattern cannot be used in a good functional language[4]. \n\n The bottom, bottom line here is simply this.  OO programming is good, when you know what it is.  Functional programming is good when you know what it is.  And functional OO programming is also good once you know what it is. \n\n \n\n \n   [0] I imagine there are a few python programmers who might have something to say about that. \n   [1] This, of course, means that Scala is not a “true” functional language. \n   [2] This, of course, means that C++ is not a “true” OO language. \n   [3] Yeah, don’t say it, I know.  OK, an “analog” to a pointer to a function.  (sigh). \n   [4] A good functional language is one that allows for convenient polymorphism.  Clojure is a good example. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/11/15/WeRuleTheWorld.html", "title": "The Obligation of the Programmer.", "content": "\n       We rule the world. \n\n We programmers.  We rule the world.  We write the rules that make our society work. \n\n Think about it; and think about it carefully.   Nothing  happens in our society without software.   Nothing. \n\n It’s certainly true that the Earth turns, the Sun rises, the rain falls, and the tides recede and advance without the aid of software.  But in our society, virtually nothing happens without the involvement of some kind of computer program. \n\n Without software: Phones don’t ring.  Cars don’t start.  Planes don’t fly.  Bombs don’t explode.  Ships don’t sail.  Ovens don’t bake.  Garage doors don’t open.  Money doesn’t change hands.  Electricity doesn’t get generated. And we can’t find our way to the store.   Nothing  happens without software.  And what is software?   Software is a set of rules . \n\n We rule the world. \n\n We don’t quite understand this yet.  More importantly, the world doesn’t quite understand it yet.  Our civilization doesn’t quite realize how dependent it has become on software – on  us . \n\n Healthcare.gov, Knight Capital, Natwest Bank, and Toyota were wakeup calls; but each time we hit the snooze button and went back to sleep.  But the sleep can’t last much longer.  At some point, probably very soon now, our society will realize, to their horror, just how much responsibility, and just how much power, they have placed in the hands of programmers.  In our hands.  It will slowly dawn on our civilization that  everything  has been handed over to us.  Everything! \n\n And if they are smart, it’ll scare the hell out of them. \n\n And it should scare the hell out of us too. \n\n Imagine what we could do if we were to unite.  If programmers around the world united under a single cause; we could dictate terms to the rest of the world.  And the rest of the world would have no choice but to comply. \n\n If we wanted to, if we were willing to organize and plan, there would be no force on the planet that could stop us.  Anyone who tried to stop us would suddenly find that none of their cell phones worked, none of their gas pumps pumped, none of their credit cards were valid, none of their fighter jets flew, none of their cruise missiles cruised, all of their bank accounts were overdrawn, none of their bills had been paid in a year, there were warrants out for their arrest, and there was no record of them ever being born. \n\n Perhaps you think I’m exaggerating?  Perhaps I am.   Perhaps. \n\n But the fact remains that we programmers are in a position of tremendous power that the people whom we serve do not well understand; and that we hardly understand any better.  And so the time is certainly coming, if it has not already come, for us to make some decisions.  What kind of rulers do we want to be? \n\n Will we ignore the power in our hands and remain a disorganized band of rumpled hapless vagabonds?   Will we continue on our undisciplined course,  blown by the chaotic winds of business and government, until one of us finally blunders badly enough to wake the sleeping giant of government regulation?  Or will we recognize the power we have and decide to use it?  And if the latter, will we use that power for good, or for evil?  Or will we take responsibility for that power and promise only to wield it in service to our society? \n\n With great power comes great responsibility.  We, as programmers, should recognize that responsibility and determine to be conscientious servants of our society.  We should set the limits and standards of our behavior.  We programmers, not our employers, not our governments, should decide what it means to  accept  the responsibility of the power that has been placed in our hands. \n\n Think of the military.  We give weapons of tremendous power to the people in the military.  Those people could turn those weapons upon us and rule us absolutely; and we would have little choice but to surrender and obey.  What keeps that from happening?  Only their code of ethics and their sworn duty to use the power we have given them on our behalf, and at our request, in order to protect and defend us. \n\n So should it be with programmers.  To protect our society from the power they have unwittingly ceded to us, we should adopt a code of ethics that describes our sworn promise and duty to humbly use our power to serve. \n\n What would our code of ethics look like?  No one person is in a position to write such a code; but I think the rough outline would contain the following points: \n\n \n   We will not purposely cause harm. \n \n\n Of course this is nothing more,  and nothing less , than a restatement of the first rule of the Hippocratic oath.  It’s hard to improve upon something that’s been around so long. Each programmer will have to interpret the definition of  harm   according to their own moral code.  Some folks may believe that working on weapon systems is the best way to prevent or minimize harm.  So be it. \n\n \n   Our software will be well-written for its purpose and lifetime. \n \n\n Again, this could be interpreted many different ways.  We could add statements like:  We fully test our software.  or  We practice TDD.  or  We follow SOLID principles.  But the bottom line is simply that we go home every night, look in the mirror, and are proud of what we accomplished that day.  We never feel like we have to run home and take a shower. \n\n \n   We behave with Honor and Discipline. \n \n\n No code of ethics would be complete without a statement about personal behavior, and behavior within a group or team.  We, as programmers, need to earn the respect of our customers and peers; and that requires a faithfulness to the truth, and predictability of behavior.  Honor and discipline. \n\n The Obligation of the Programmer \n\n What would our oath to that code of ethics look like?  What promises would we swear to keep? \n\n Consider this oath which I have adapted from the  Order of the Engineer \n \n   I am a computer programmer, I take deep pride in my profession. \n \n\n \n   \n     To it I owe solemn obligations. \n   \n \n\n \n   \n     All human progress has been spurred by the genius of those who manipulate information. \n   \n \n\n \n   \n     By making it possible to manipulate vastly more information than ever before, programmers have created enormous benefits for human society and greatly accelerated human progress. \n   \n \n\n \n   \n     Were it not for the accumulated knowledge and experience of those programmers, mathematicians, and engineers who came before me, my efforts would be feeble. \n   \n \n\n \n   \n     As a programmer, I pledge to practice integrity and fair dealing, tolerance, and respect, and to uphold devotion to the standards and the dignity of my profession, conscious always that my skill carries with it the obligation to serve humanity by making the best use of the precious resources under our stewardship. \n   \n \n\n \n   \n     As a programmer, in humility and with the need for guidance, I shall participate in none but honest enterprises. \n   \n \n\n \n   \n     When needed, my skill and knowledge shall be given without reservation for the public good. \n   \n \n\n \n   \n     In the performance of duty and in fidelity to my profession, I shall give the utmost. \n   \n \n\n \n   `         - The Obligation of the Programmer` \n \n\n We are the rulers of the world.  It’s not a position we wanted.  It’s not a position we anticipated.  But here we are.  We write the rules.  We hold the strings.  Now we have to decide what to do with them. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/11/19/GoingForTheGold.html", "title": "Thorns around the Gold", "content": "\n       This article was inspired by  The IsNullOrWhiteSpace trap  by Mark Seemann (@ploeh).  Mark’s article is well written and quite brief.  Please read it before continuing. \n\n The trap that Mark points out is a special case of a much more general trap that I call  Grabbing for The Gold .  I can demonstrate this trap by referring back to Mark’s article. \n\n Notice that the first tests that Mark wrote were: \n\n [InlineData(\"Seven Lions Polarized\"  , \"LIONS POLARIZED SEVEN\"  )]\n[InlineData(\"seven lions polarized\"  , \"LIONS POLARIZED SEVEN\"  )]\n[InlineData(\"Polarized seven lions\"  , \"LIONS POLARIZED SEVEN\"  )]\n[InlineData(\"Au5 Crystal Mathematics\", \"AU5 CRYSTAL MATHEMATICS\")]\n[InlineData(\"crystal mathematics au5\", \"AU5 CRYSTAL MATHEMATICS\")]\n \n\n He’s already fallen into the trap.  Why?  Because he grabbed for  The Gold . \n\n Gold and Thorns \n The core functionality that Mark is trying to describe is one in which words are alphabetized.  So, naturally, his tests reflect that core functionality.  The core functionality is  The Gold ; and he grabbed for it. \n\n The problem is that  The Gold  is protected by an invisible hedge of thorns that will entangle any unwitting programmer who, dazzled by  The Gold , incautiously grabs for it.  What is that hedge of thorns?  In Mark’s case it is the  null  and blank input cases. \n\n I have been following the TDD discipline for fifteen years now.  In that time I’ve learned a lot about that invisible hedge of thorns.  I’ve learned that it’s  always  there.  I’ve learned that if you grab for  The Gold  too early, that invisible hedge will thwart your progress and tear your efforts to ribbons[1].  So the strategy I’ve learned to follow is to keep my eyes averted from “The Gold”, while probing for the hedge and clearing it away. \n\n Probing and Clearing \n Before I approach the core functionality, I write as many tests as I can that  ignore  the core functionality and deal instead with exceptional, degenerate, and ancillary behaviors; in that order.  One by one, I write those tests, and then make them pass. \n\n \n   \n     Exceptional Behaviors \n\n     These are behaviors that detect invalid inputs that the core functionality should never see.  These behaviors return error codes, log error messages, and/or throw exceptions. \n\n     In Mark’s case handling the  null  input is the only exceptional behavior.  But in more complex applications detecting exceptional cases can be a lot more challenging.  Of course they include input validations.  But they also include semantic violations such as deleting records that don’t exist; or adding records that already exist. \n   \n   \n     Degenerate Behaviors \n\n     These are inputs that cause the core functionality to do “nothing”.  I put “nothing” in quotes because sometimes “nothing” can be relatively complicated. \n\n     In Mark’s case, the empty and blank strings are degenerate inputs.  He eventually solved the problem of blanks and empties with a complex set of conditions and operations that returned an empty string if a sole blank or empty string was the input, and eliminated all blanks in every other case[2]. \n\n     In general, degenerate conditions are things like blanks, empty strings, empty collections, zero length arrays, etc.  In more complicated applications, a degenerate input can be quite complicated and require a great deal of processing.  Consider, for example, a Java compiler processing a source file that contains thousands of lines containing nothing but semicolons and comments.  What should it’s output be? \n   \n   \n     Ancillary Behaviors \n\n     These are sometimes the hardest to find.  Ancillary behaviors are those that surround and support the core functionality; but are not part of it.  For example, the  getSize()  function of a  Stack .  Reporting the size of a  Stack  is not related to its core LIFO functionality. \n\n     The thing about ancillary behaviors is that they frequently turn out to be useful to the core functionality in some in-obvious way.  For example, it turns out that the size of a  stack  is the array index used for  push  and  pop  operations in fixed-length stacks.  I often find that if all the ancillary behaviors are in place before I approach the core functionality, then the core functionality is much easier to implement. \n   \n \n\n These are the first tests I write, and make pass.  I shy away from any tests that are close to the core functionality until I have completely surrounded the problem with passing tests that describe everything  but  the core functionality.  Then, and only then, do I go get  The Gold . \n\n \n [1] Indeed just the day before yesterday I wasted four hours entangled in thorns that I had missed, and had not cleared away properly.  In the end  git reset --hard  was my only escape. \n\n [2] Did he cover all the possible conditions?  What about tab, newline, backspace, and un-printables? \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/12/17/TheCyclesOfTDD.html", "title": "The Cycles of TDD", "content": "\n       When you first learn Test Driven Development, it sounds simple and easy.  If you learned it in 1999, like I did, the rule was to simply write your unit tests first.  Indeed, we called it Test First Design back then. \n\n I sat with Kent Beck in 1999 and paired with him in order to learn.  What he taught me to do was certainly test first; but it involved a more fine-grained process than I’d ever seen before.  He would write  one line  of a failing test, and then write the corresponding  line  of production code to make it pass.  Sometimes it was slightly more than one line; but the scale he used was was very close to line by line. \n\n Second-by-Second   nano-cycle : The Three Laws of TDD. \n\n A few years later this fine granularity was codified into three rules: the so-called  Three Laws of TDD . \n\n \n   You must write a failing test before you write any production code. \n   You must not write more of a test than is sufficient to fail, or fail to compile. \n   You must not write more production code than is sufficient to make the currently failing test pass. \n \n\n I,  and many others , have written about these three laws over the years.  They now appear in many different styles, formats, and injunctive statements.  But the goal is always to promote the  line by line  granularity that I experienced while working with Kent so long ago. \n\n The three laws  are the  nano-cycle  of TDD.  You follow them on almost a second-by-second basis. You will likely iterate them a dozen or so times before you finish a single unit test. \n\n Minute-by-Minute :  micro-cycle : Red-Green-Refactor \n\n If we pull back to the minute by minute scale we see the  micro-cycle  that experienced TDDers follow.  The  Red/Green/Refactor  cycle .  \n \n \n \nThis cycle is typically executed once for every complete unit test, or once every dozen or so cycles of the three laws.  The rules of this cycle are simple. \n\n \n   Create a unit tests that fails \n   Write production code that makes that test pass. \n   Clean up the mess you just made. \n \n\n The philosophy is based on the idea that our limited minds are not capable of pursuing the two simultaneous goals of all software systems: 1. Correct behavior.  2. Correct structure.  So the RGR cycle tells us to first focus on making the software work correctly; and then,  and only then , to focus on giving that working software a long-term survivable structure. \n\n Again,  many  people have written about this cycle.  Indeed the idea derives from  Kent Beck’s original injunction : \n\n \n   Make it work.  Make it right.  Make it fast. \n \n\n Another way to think about this idea is: \n\n \n   Getting software to work is only half of the job . \n \n\n Customers value  two things  about software.  The way it makes a machine behave; and the ease with which it can be changed.  Compromise either of those two values and the software will diminish in real value to the customer. \n\n Executing the  Red/Green/Refactor  cycle takes on the order of a minute or so.  This is the granularity of refactoring.  Refactoring is not something you do at the end of the project; it’s something you do on a  minute-by-minute  basis.  There is no task on the project plan that says: Refactor.  There is no time reserved at the end of the project, or the iteration, or the day, for refactoring.  Refactoring is a continuous in-process activity, not something that is done late (and therefore optionally). \n\n Decaminute-by-Decaminute :  milli-cycle : Specific/Generic \n\n At the 10 minute level we see the  milli-cycle  in operation.   The Specific/Generic cycle . \n\n \n   As the tests get more specific, the code gets more generic. \n \n\n As a test suite grows, it becomes ever more specific. i.e. it becomes an ever more detailed  specification  of behavior.   Good software developers meet this increase in specification by increasing the  generality  of their code.  To say this differently:  Programmers make specific cases work by writing code that makes the general case work. \n\n As a rule, the production code is getting more and more general if you can think of tests that you have not written; but that the production code will pass anyway.  If the changes you make to the production code, pursuant to a test, make that test pass, but would not make other unwritten tests pass, then you are likely making the production code too specific. \n\n It is often said that the fine grained structure of the three laws and the Red/Green/Refactor cycle  lead to local-optimizations .  Without the “big picture” the developer cannot imbue the software with the correct structure for the overall problem, and instead drives towards a structure that is good for the local case; but not for the general case. \n\n The symptom of the local optimum is “ Getting Stuck .”  In order to make the next test pass you must write a large amount of code  outside  of the nano-cycle of the three laws, and even outside of the micro-cycle of RGR. In other words, you have gone down a path that forces you out of the TDD process. \n\n Once you are stuck, the only solution is to backtrack up through the previous tests, deleting them, until you reach a test from which you can take a different fork in the road. \n\n Why do you get stuck?  Because you were not adding sufficient generality to the production code.  You were making the tests too specific, to quickly.  The solution is to backtrack and then add specificity to the tests more slowly, while adding generality to the production code more quickly.  This frequently forces you to choose a different set of tests to follow. \n\n To avoid getting stuck we evaluate our position every few minutes; looking for specificity in the production code.  Have we taken shortcuts that make the production code resemble the tests in some way? Do the most recent changes to the production code fail to pass more tests than we have written? \n\n This is the cycle in which we apply the  Transformation Priority Premise .  We look for the symptoms of over-specificity by checking the kinds of production code we have written. \n\n Hour-by-Hour :  Primary Cycle : Boundaries. \n\n The final primary cycle of TDD is the cycle that ensures that all the other cycles are driving us towards a  Clean Architecture .  Ever hour or so we stop and check to see whether we have crossed, or are encroaching upon, a significant architectural boundary.  Often these boundaries are difficult to see while in the throes of the nano- and micro-cycles.  You can start to smell them at the decaminute level, but even then our gaze is still too narrowly focused. \n\n So every hour or so we stop and look at the overall system.  We hunt for boundaries that we want to control.  We make decisions about where to draw those boundaries, and which side of those boundaries our current activities should be constrained to.  And then we use those decisions to inform the nano-cycles, micro-cycles, and milli-cycles of the next hour or so – the  primary  cycle – of Test Driven Development. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/02/19/ComputerHarem.html", "title": "They Called them Computers.", "content": "\n       I’ve told this story  before ; but it bears repeating in more detail because of some facts I’ve recently learned. \n\n The year was probably 1967.  I was 15 years old, and a Freshman in High-school.  I was sitting in the lunch room watching a man roll in a machine that looked, to me, like the helm of the Starship Enterprise. \n\n The machine was called ECP-18.  It was the size and shape of an office desk.  Indeed, it  was  a desk.  You could sit in front of it, and it had a large flat writing surface. Protruding up from that surface was a  console  – a fascinating array of dozens and dozens of buttons and lights arranged in parallel rows.  Indeed, the buttons  were  lights.  I watched, enthralled, as the man pushed those buttons making them light up in a kaleidoscope of patterns. \n\n I was already a computer geek by this time.  I had taught myself binary math, and knew the binary representation of the octal digits by heart.  I knew boolean algebra, and had been constructing logic circuits from old telephone relays in my basement.  I had read book after book on computers; and had an inkling of what a computer language, like Basic, was.  But I had never touched a  real  computer until this day. \n\n I watched as the man toggled in the bootstrap loader.  I didn’t know it was the bootstrap loader at the time. He didn’t tell me what he was doing.  I was the annoying kid looking too closely over his shoulder, intently staring at every gesture, listening raptly to every word.  I’m sure I weirded him out. \n\n He would push the button/lights in the row marked  address  and mutter under his breath:  “at address two-zero-zero”.  I could see the octal 0200 in the lights!  I knew what he was saying!  Egad!   I understood! \n\n He would push the button/lights in the row marked  Memory Buffer , and mutter under his breath: “Store in two one five”.  The row of lights read 150215.  I saw the 0215 at the end, and inferred that the ‘15’ meant  store .  OH!  Those numbers are  instructions  stored in the memory! \n\n Bit by bit, as I watched that man toggle in diagnostic programs, and execute them, I picked up enough knowledge to formulate a working hypothesis about how this machine worked.  When he left for the day, I leapt to the machine and toggled in some simple programs.   And, by God, I got them to work! \n\n I cannot recall a moment in my life when I have felt quite so victorious as that moment, when I first touched a real computer, and got that real computer to do my bidding.  I was going to be a programmer.  At that moment, I knew it in my bones.  And from that moment on virtually every thought and every action in my young life was directed towards that goal. \n\n My exposure to that machine was short lived.  A week later it was wheeled away, and I never saw it again.  But the course of my life had been set. \n\n \n\n Judy Allen \n I repeated that story so I could tell you this. \n\n A few months ago a man named Tom Polk wrote to me.  His letter said: \n\n \n   Hi, Bob, Tom Polk here.  I Googled ECP-18 and found you.  Below is a picture of a couple of my classmates in Big Spring, Texas around 1968 or so.  You can see our school’s ECP-18 in it. I’m sorry I don’t have a bigger picture, but you’ll recognize this.    I learned to program on this machine and I really enjoyed your comments. \n \n\n \n\n Tom and I struck up a conversation, and from him I learned much more about that machine, and the people behind it.  One thing he said struck a special chord in me: \n\n \n   If your sales tech was a small woman, it was probably the lady who wrote  this .  She mentions her 40% interest in the company and selling it to a high profile educational company in Texas. \n \n\n No, the tech was male.  I recall nothing more about him.  My focus was on those lights, and on his voice, and on his fingers pushing those buttons.  But I am absolutely certain he was male. \n\n And it is that  certainty  that shall be the topic of the rest of this blog. \n\n Go read Judy Allen’s story at the link Tom shared above.  Read three or four of her short chapters, perhaps as far as “The Perkin’s Pub Protest”.  It won’t take you long.  You’ll know when you can return here.  But I warn you, you won’t want to stop; and you’ll almost certainly later return to her fascinating, and inspiring story. \n\n ###Whoa! \n\n OK, pardon my language, but there’s no other way to say this. \n\n This woman was  Bad ASS !  She wrote a  M_____ F____  two-pass assembler,  in binary machine language , in 1024 words, in a couple of months, using nothing but the front panel switches, and a 10 character per second teletype with a paper tape reader/punch – while taking care of a household, a husband, and a gaggle of young kids.  God  Damn!   I dare  you  to try that! \n\n And she wasn’t just a good/great/radical programmer either.  Did you read the part where she faced down the Union bullies?  Did you read the part where she faced down the executive who couldn’t imagine paying a man’s salary to a woman?  Did you  read  the Perkin’s Pub Story? \n\n No, this was no ordinary woman.  Judy Allen was a force to be reckoned with. \n\n But lest you think she was unique, lest you think she was a fluke, let me tell you about my early career. \n\n ##Almost 50-50\nI got my first job as a programmer in 1970 when I was 18 years old.  I was hired to help write a large on-line time-sharing union accounting system on a Varian 620i minicomputer.  This was a 16 bit machine with 32K x 16 of Core, and two 16 bit registers.  Our team consisted of three teenage boys, two men, and three women.  That was a pretty typical ratio of men to women in this company.  Just under half the programmers were women who were writing COBOL, BAL, PL/1, and minicomputer assembler. \n\n After that time, I watched the ratio of women to men in software plunge.  By 1972 I was working for a large company, and the ratio had already dropped to less than 10%.  By 1977 the ratio was virtually zero.  The women had simply disappeared. \n\n Ironically, in the ’60s it was  common  for women to be programmers.  Indeed, in the early ’60s there could very well have been more female programmers than male programmers.  And the reason for this was simple: To some extent programming was considered to be  Women’s Work . \n\n ##Women’s Work\nMen were engineers.  They conceived of machines and built them with their hands.  They wielded the creative energies.  The drudgery of tabulating and calculating was left to women. \n\n This is a tradition that goes all the way back to Charles Babbage, and probably beyond.   Ada Lovelace  had been commissioned to translate into English, the lecture notes written in French by an Italian student of Babbage’s.  A drudgery if there ever was one.  And yet, in so doing, she conceived of and documented the notion of programming Babbage’s machine with algorithms that could deal with non-mathematical topics.  She may have been the first person to realize that a computer manipulates symbols, not numbers.  She may have been the first person to understand what that implies. \n\n In the 1880s, women were commonly recruited, but seldom paid, to do the painstaking measurements and calculations requires by the male scientists of the day.  This was especially true in Astronomy.  Charles Pickering assembled a rather large squadron of such women (They were called:  Pickering’s Harem ) to analyze the huge quantity of photographic plates being produced by the telescopes. \n\n 'Pickering's Computers' standing in front of Building C at the Harvard College Observatory, 13 May 1913. \n\n The women were not allowed to touch the telescopes.  That was  Man’s Work .  But the women could do the drudgery of the computations.  Computing was  Women’s Work .  Indeed, they called the women:  Computers . \n\n Those “computers” did fantastic work.  It was the deeply insightful work of Women like Annie Jump Cannon, and Henrietta Levitt, that allowed us to understand and measure nothing less than the scale and composition of the universe. \n\n ##Bletchley Park\nThe tradition continued through the early part of the twentieth century, and into the ’30s.  At Bletchley Park, where Alan Turing and his team were breaking the German Enigma ciphers, there were perhaps 10,000 people; two thirds of whom were women.  Teams of these women  were gathered together to do the vital, but painstaking, work of listening, gathering, and collating the German messages.  They learned to operate and “program” the machines that Turing and his team had built. \n\n \n The Women of Bletchley Park \n\n ##Grace Hopper\nAfter the second World War, this role for women continued.   Grace Hopper , for example, worked in the Navy programming the Mark I computer.  Later she was hired by EMCC to work on the software for the UNIVAC.  In 1952 she conceived of, and wrote, the very first compiler,  and coined the term .  She was the first Director of Automatic Programming at Remington Rand, and was the visionary behind the development of COBOL. \n\n Grace Murray Hopper at the UNIVAC keyboard, c. 1960 (Uploaded by Jan Arkesteijn) \n\n There were other women in software in those days.  And all these women did fantastic, ground-breaking, work.  And yet it was  Women’s Work .  It was considered appropriate that Women should deal with the drudgery of programming.   Men  built the  machines ! \n\n ##The Revelation of Symbols.\nWhen did it change?  When did programming become  Man’s  work?  When did Men invade this traditional role for Women, and drive drive the women out? \n\n I think there’s a clue in one of Grace Hopper’s statements.  She had developed the very first compiler but she later said: \n\n \n   “Nobody believed that, I had a running compiler and nobody would touch it. They told me computers could only do arithmetic.” \n \n\n Only Arithmetic?  They didn’t understand, did they?  They didn’t see the implications.  They didn’t see what a computer really was. \n\n \n   Ada Lovelace had seen it.  She had seen that Babbage’s engine, and engines like it were not merely calculators.  Ada Lovelace saw that these machines could manipulate  symbols . \n   Alan Turing certainly saw it.  Indeed, he could be said to have given the notion it’s mathematical foundation. \n   Grace Hopper saw it for sure.  She implemented it by writing the very first compiler. \n   Judy Allen saw it.  She wrote a symbolic assembler in  binary . \n   Heinlein saw it.   The Moon is a Hash Mistress  was as clear as a Clarion. \n   Arthur C. Clarke And Stanley Kubrick saw it.  Boy, did they ever. \n   Gene Roddenberry saw it.  And he popularized it in a way that nobody else ever had. \n   And I saw it, as I stared over that male technician’s shoulder. \n \n\n These machines manipulated  symbols .  And if you can manipulate symbols,  you can do anything!   If you can manipulate symbols,  you have power! \n\n Power. \n\n Power has a gender, and that gender is  Male . \n\n ###Judy? \n\n That’s when it shifted.  At least that’s my hypothesis.  In the late ’60s, and early ’70s the popular culture began to see computers as more than just calculators. On Star Trek, Captan Kirk could  talk  to the computer.  In  2001: a Space Odyssey , a computer was simultaneously sympathetic and malevolent.  As a society we were beginning to see that computers had capabilities that were almost boundless.  And it was becoming clear that it was the software, more than the hardware, from which those boundless possibilities arose. \n\n The change was in the preception of who had the power.  And in the late ’60s we were realizing that it was the programmers, and not the hardware developers, who would wield the power. \n\n When programmers became powerful, programming became  Man’s Work. \n\n I felt that power.  I knew it for what it was, while looking over that  male  technician’s shoulder, watching  him  push those buttons.  I felt that power, and I knew that I would  have  that power.  I absorbed the power from  him  as I watched, and learned. \n\n It  was  as man, who’s shoulder I was looking over, wasn’t it?  It had to be!  You couldn’t absorb that kind of power from a woman, could you?  It couldn’t have been a woman, it couldn’t have been – Judy Allen, – could it? \n\n Oh  Christ Almighty , could it? \n\n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/01/08/InterfaceConsideredHarmful.html", "title": "'Interface' Considered Harmful", "content": "\n       What do you think of  interface s? \n\n \n   You mean a  Java  or  C#  interface? \n \n\n Yes, are  interface s a good language feature? \n\n \n   Of course, they’re great! \n \n\n Really.  Hmmm.  What is an interface?  Is it a class? \n\n \n   No, it’s different from a class. \n \n\n In what way? \n\n \n   None of it’s methods are implemented. \n \n\n Then is this an interface? \n\n public abstract class MyInterface {\n  public abstract void f();\n}\n \n\n \n   No, that’s an abstract class. \n \n\n What is the difference? \n\n \n   Well, an abstract class can have functions that are implemented. \n \n\n Yes, but this one doesn’t.  So why isn’t it an interface? \n\n \n   Well, an abstract class can have non-static variables, and an interface can’t. \n \n\n Yes, but this one doesn’t.  So, again, why isn’t it an interface? \n\n \n   Because it’s not. \n \n\n That’s not a very satisfying answer.  How does it differ from an interface?  What can you do with an interface that you cannot do with that class? \n\n \n   A class that  extend s another, cannot also  implement  your class. \n \n\n Why not? \n\n \n   Because, in Java, you cannot  extend  multiple classes. \n \n\n Why not? \n\n \n   Because the compiler won’t allow you to. \n \n\n That’s odd.  Well, then, why can’t I  implement  that class rather than  extend  it? \n\n \n   Because the compiler will only allow you to  implement  an  interface . \n \n\n My that’s a strange rule. \n\n \n   No, it’s perfectly reasonable.  The compiler will allow you to  implement  many interfaces but only allow you to  extend  one class. \n \n\n Why do you suppose the Java compiler will allow you to  implement  multiple interfaces, but won’t allow you to  extend  multiple classes? \n\n \n   Because multiple inheritance of classes is dangerous. \n \n\n Really?  How so? \n\n \n   Because of the “Deadly Diamond of Death”! \n \n\n My goodness, that sounds scary.  Just what is the Deadly Diamond of Death? \n\n \n   That’s when a class extends two other classes, both of which extend yet another class. \n \n\n You mean like this: \n\n class B {}\nclass D1 extends B {}\t\nclass D2 extends B {}\t\nclass M extends D1, D2 {}\n \n\n \n   Yes!  That’s bad! \n \n\n Why is that bad? \n\n \n   Because class B might have an instance variable! \n \n\n You mean like this? \n\n class B {private int i;}\n \n\n \n   Yes!  And then how many  i  variables would be in an instance of  M ? \n \n\n Ah, I see.  Since both  D1  and  D2  have an  i  variable, and since  M  derives from both  D1  and  D2 , then you might expect  M  to have two separate  i  variables. \n\n \n   Yes!  But since  M  derives from  B  which has only one  i  variable, you might expect  M  to have just one  i  variable too. \n \n\n Ah, so it’s ambiguous. \n\n \n   Yes! \n \n\n So Java (and therefore C#) cannot  extend  multiple classes because someone  might  create a Deadly Diamond of Death? \n\n \n   No, because everyone _would  create a Deadly Diamond of Death since all objects implicitly derive from  Object ._ \n \n\n Ah!  I see.  And the compiler writers couldn’t make  Object  a special case? \n\n \n   Uh… Well, they didn’t. \n \n\n Hmmm.  I wonder why?  Have other compiler writers solved this problem? \n\n \n   Well, C++ allows you to create diamonds. \n \n\n Yes, and I think Eiffel does to. \n\n \n   And, gosh, I think Ruby figured out a way to do it. \n \n\n Yes, and so did CLOS and – well, let’s just say that the deadly diamond of death is a problem that was solved decades ago and it isn’t deadly, and does not lead to death. \n\n \n   Hmmm.  Yeah, I guess that’s true. \n \n\n So then back to my original question.  Why isn’t this an interface? \n\n public abstract class MyInterface {\n  \t  public abstract void f();\n}\n \n\n \n   Because it uses the keyword class; and the language won’t allow you to multiply inherit classes. \n \n\n That’s right.  And so the keyword  interface  was invented as a way to prevent multiple inheritance of classes. \n\n \n   Yeah, that’s probably true. \n \n\n So why didn’t the authors of Java (and by extension C#) use one of the known solutions to implement multiple inheritance? \n\n \n   I don’t know. \n \n\n I don’t know either, but I can guess. \n\n \n   What’s your guess? \n \n\n Laziness. \n\n \n   Laziness? \n \n\n Yeah, they didn’t want to deal with the issue.  So they created a new feature that allowed them to sidestep it.  That feature was the  interface . \n\n \n   You are suggesting that the  interface  feature of Java was a hack that the authors used in order to avoid some work? \n \n\n I can’t explain it any other way. \n\n \n   Well I think that’s kind of rude.  I’m sure their intentions were better than that.  And anyway it’s kind of nice to have  interface s isn’t it?  I mean, what harm do they do? \n \n\n Ask yourself this question:  Why should a class have to  know  that it is  implement ing an interface?  Isn’t that precisely the kind of thing you are supposed to hide? \n\n \n   You mean a derivative has to know in order to use the right keyword,  extends  or  implements , right? \n \n\n Right!  And if you change a class to an interface, how many derivatives have to be modified? \n\n \n   All of them.  At least in  Java .  They solved that problem in  C# . \n \n\n Indeed they did.  The  implements  and  extends  keywords are redundant and damaging.  Java would have been better off using the colon solution of  C#  and  C++ . \n\n \n   OK, OK, but when do you really need multiple inheritance? \n \n\n So, here is what I would like to do: \n\n public class Subject {\n\tprivate List<Observer> observers = new ArrayList<>();\n\tprivate void register(Observer o) {\n\t\tobservers.add(o);\n\t}\n\tprivate void notify() {\n\t\tfor (Observer o : observers)\n\t\t    o.update();\n\t}\n}\n\npublic class MyWidget {...}\n\npublic class MyObservableWidget extends MyWidget, Subject {\n\t...\n}\n \n\n \n   Ah, that’s the Observer pattern! \n \n\n Yes.  That’s the  Observer  pattern – done  correctly . \n\n \n   But it won’t compile because you can’t extend more than one class. \n \n\n Yes, and that’s a tragedy. \n\n \n   A tragedy?  But why?  I mean you could just derive  MyWidget  from  Subject ! \n \n\n But I don’t want  MyWidget  to know anything about being observed.  I want to maintain the separation of concerns.  The concern of being observed is separate from the concern of widgets. \n\n \n   Well then just implement the  register  and  notify  functions in  MyObservableWidget \n \n\n What?  And duplicate that code for every observed class?  I don’t think so! \n\n \n   Well then have  MyObservableWidget  hold a reference to  Subject  and delegate to it? \n \n\n What?  And duplicate the delegation code in every one of my observers?  How crass.  How degenerate.  Ugh. \n\n \n   Well, you’re going to have to do one or the other of those things. \n \n\n I know.  And I hate it. \n\n \n   Yeah, it seems that there’s no escape.  Either you’ll have to violate the separation of concerns, or you’ll have to duplicate code. \n \n\n Yes.  And it’s the language forcing me into that situation. \n\n \n   Yes, that’s unfortunate. \n \n\n And what feature of the language is forcing me into this bad situation? \n\n \n   The  interface  keyword. \n \n\n And so…? \n\n \n   The  interface  keyword is harmful. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/02/21/ModeBImperative.html", "title": "The MODE-B Imperative", "content": "\n       If you follow my  twitter ,  facebook , or  github  feeds, you may have noticed that I’ve been writing a PDP-8 Emulator for the iPad. \n\n Here’s a screenshot: \n\n \n\n My purpose in writing this emulator (other than sheer nostalgia) is to use it as a training tool for new programmers.  I think every new programmer should spend a week or two programming one of these old machines.  It seems to me that there is no better way to understand what a computer really is, than to touch a real computer and program it at the bit level, in machine language.  Once you have done that, all the magic disappears and is replaced with cold, hard reality.  And, let me tell you, programming a PDP-8 is  cold, hard, reality.   Oh boy, is it ever! \n\n I’ve tried to be faithful to the machine and it’s environment.  The front panel is a decent abstract representation of the original PDP8, and the lights blink appropriately, and with the correct data. (though I couldn’t resist making the lights touch sensitive, like the ECP-18). \n\n \n\n The paper tapes in the reader and punch move at appropriate speeds, and the holes represent the true data.  They also make the right kinds of noises.  The teletype prints at the appropriate speed (though you can speed it up if you want (you will)) and behaves very much like an ASR-33, making all the appropriate noises, and responding properly to carriage return and line feed characters, etc. (Yes, you can overprint!) \n\n I found some binary images of old PDP8 paper tapes  here , and managed to get them into my emulator by munging their format with a little C program I wrote, and then transporting them to the iPad using Dropbox.  The results have been both satisfying and heart-wrenching.  That ancient code works! \n\n I sit in front of my half-pound, $600 iPad, in its nice little case, and its bluetooth keyboard, surrounded by programming manuals, and marked up listings of a program I am working on.  And I realize that I’m using code written half a century ago by men and women (probably a lot of women back then) who hauled themselves up by their bootstraps to get that silly little machine working. A machine that weighed 500lbs, was the size of a refrigerator, and cost $20,000 in 1967. \n\n Could those men and women ever have guessed that their code would be running in a hand-held tablet computer and would be used to train programmers in the twenty-first century?  Some – many – must still be alive.  I wonder what they’d think if they knew. \n\n \n\n My emulator is written in Lua, using the  Codea  framework for the iPad.  This is a hugely convenient language for iPad development.  Lua is fast (enough), and Codea has wonderful graphics primitives, and a simple, yet very effective graphics framework for developing highly interactive animated programs. \n\n This made the animation (and sound generation) of the front panel, the teletype, and the paper tape reader/punch a snap. \n\n Emulating the PDP8 internals was a bit of a challenge since Lua only has one numeric type: floating point.  Doing 12-bit logic using floating point math is, uh,  interesting.   On the other hand, I get a huge kick out of watching  FOCAL  (FORmula CALculator: a language similar to Basic) run on my PDP-8, and do it’s own floating point math, using the logical operations that I concocted from Lua’s floating point math.  <grin>   You should see those lights blink! \n\n The execution speed is about 4,000 instructions per second.  While that’s 1/7th the speed of a PDP8/S, it’s pretty impressive for an iPad running a “byte-code interpreted” language like Lua, emulating 12 bit logic using floating point math!  I wasn’t expecting that kind of performance.  It actually runs all that old DEC software at reasonable speed.  Even  FOCAL  runs fast enough to compute square roots in half a second or so. \n\n And, again, the blinking of the lights during a compile is deeply satisfying.  Watch that and you’ll know where all those 1950s sci-fi movies got their ideas from. \n\n ##MODE-B\nGetting the Emulator working was really very easy.  I’ve probably invested 30 hours in it overall; and that includes learning Codea and Lua. \n\n The development process was lickety-split.  Codea’s Lua editor for the iPad is intuitive and powerful (though it has no refactorings  <sob> ).  The edit/test loop was, perhaps, 10 seconds long.  I could add a line or two of code, run the app, see the effect, and then hop back into the editor just like that.  It was a satisfying treat, and I had a ball doing it. \n\n Of course I wrote tests for the tricky bits.  I wrote a little test framework just for that purpose, and I put a  TEST  button on the front panel of the emulator to make it easy for me to run those tests.  The emulation code itself would have been nearly impossible if I hadn’t had unit tests.  And of course, I used the TDD discipline for that code.  In the end, there were over 100 tests for the various instructions and behaviors of the PDP-8. \n\n For the GUI, on the other hand (and there’s a lot of GUI code), tests were unnecessary ( <GASP> ).  My eyes were the tests.  I knew what I wanted to see, and so I spun around the edit/test loop every 10 seconds.  Writing tests in a TDD style would have been horrifically difficult, and an utter waste of time. \n\n On the other hand, I was still using the TDD  rhythm .  I knew what I wanted to see on the screen.  That was my test.  I simply modified the code until that test passed.  So even though I wasn’t writing tests, it felt like I was –  it felt like TDD . \n\n It’s true, of course, that I don’t have automated regression tests for my GUI.  On the other hand, it’s absurdly simple to ensure that everything is working.  So far the lack of automated GUI tests has not impacted my loop time. \n\n Of course without refactoring tools the code got a bit messy.  I refactored when I couldn’t stand it anymore; but the backlog of messiness is larger than I’d like.  I’ll continue to clean that code over time; but it’s much slower going without good refactoring tools. \n\n Anyway, for the sake of this article, let’s call this style of programming:  MODE-B.    MODE-B  is the style that allows you to edit on the screen, and see the results either on the screen, or in a passing test, in seconds.  It’s a hyper-speed development loop that doesn’t require listings, pencils, compile time, setup time, or any other impediment.  The time between editing the code, and seeing it run, is much less than one minute. \n\n ##MODE-A\nHaving gotten the PDP8 Emulator to work.  And having gotten all the old tools, like the paper tape editor, and the Pal-3 assembler, up and running.  I set about to write a simple program.  This program would allow the user to type a simple formula on the keyboard, and then it would print the result.  For example, if the user typed:  25+32 , the computer would print  57 . \n\n On a PDP-8, this is a non-trivial program.  I’ve included it below for those of you who want to see how a pretty poor PDP-8 programmer has written it. \n\n The process was the same as the process I used back in the late ’70s when working on assembly language programs on a Teradyne  M365  (An 18 bit relative of a PDP8).  We had magnetic tape, instead of paper tape; and the computer was a bit more powerful than a PDP-8.  But the process was still the same.  It goes like this: \n\n Assume that you are in the middle of writing the program below.  You’ve already got some of it written, and you are adding to it.  Remember, this computer only has 4K words.  It can’t store much in it’s memory.  Remember, also, that the only mass storage you have is paper tape.  So your source code is on a single long paper tape. \n\n \n   Write the changes you want to make to your program on the current listing.  You’ll have changes on many pages, so put paper clips in those pages if the listing is long. \n   Load the editor from paper tape.  This will take a few minutes so get some coffee. \n   Set the front panel switches to  6003 : to compress spaces, and use the high speed reader/punch.  Run the editor (by toggling  0200  into the PC register and hitting the  RUN  button) \n   Put your source code paper tape into the reader. \n   Read in one “page” of code from paper tape using the  R  command.  (50 lines or less. 1 minute or so). \n   Go to that page in your listing and make any changes using the  I ,  C , and  D  commands.  Remember you don’t have a screen, so you are editing line by line, using line numbers.  Plan to spend some time at this. \n   Print out the current page using the  L  command.  Make sure all your changes are correct. \n   Punch the current page to paper tape using the  P  command. (a minute or so). \n   Punch the current page and read in the next with the  N  command and if that wasn’t the last page, go to step 6. \n   Remove the new source tape from the punch and label it with a title and a version number.   Don’t ever forget the version number! \n   Load the assembler into memory from paper tape (10 minutes or so). \n   Set the front panel switches to  2002 : the “pass one, output to printer” configuration. \n   Load your source tape into the reader. \n   Load  0200  into the PC register, and hit  RUN . \n   Pass one compile will read your whole source tape and then print your symbol table.  (10 minutes or so) \n   After the computer halts, set the front panel switches to  4003 : the “pass two, output to punch” configuration. \n   Load your source tape into the reader. \n   Push  RUN .  Pass two compile will read your whole source tape and punch your binary paper tape.  (15 min or so).  Source code errors will print during this pass. \n   After the computer halts, if there are errors, throw away the paper tape that was punched and go to step 1.  Otherwise remove the binary tape from the punch and label it with a title and version number. (I don’t need to remind you about that version number, right?) \n   Set the front panel switches to  6002 : the “pass three, output to printer” configuration. \n   Load your source tape into the reader. \n   Push  RUN .  Pass three compile will read your whole source tape and print your program listing.  You’ll need this for debugging, so don’t neglect it.  (30 min or so because the printer is very slow.)  Make sure you have enough paper in the printer! \n   Tear off the listing and check it over. \n   Put your binary tape into the reader. \n   Set the PC register to  7777  (the address of the bin loader which is usually kept in core memory) and hit  RUN .  If the bin loader is not in memory for some reason, then you’ll have to toggle in the  RIM  loader and then load the bin loader paper tape before doing this step. \n   When the computer halts, your program has been loaded into memory.  Run it, and see if it works. \n \n\n This process is highly abbreviated.  There are lots of littler steps in there, but you get the idea. \n\n This is  MODE-A .  It’s a very fragile, error-prone process that takes an hour or so to execute.  It could be a lot more for a large-ish program.  A very small program might make it around the loop in 15 minutes.  The program I was writing grew to be about 20-30 minute or so, and I cheated by allowing my “teletype” to run at 10X the normal rate. \n\n To get my silly little program to work, I went around that loop seven times.  It took me about a week, and about five hours total.  A lot of that time was  writing the code with a pencil  because, without a screen editor, there was just no way to avoid hand writing, and using a lot of eraser. \n\n Back in the ’70s I spent days, weeks, and years working in  MODE-A .  All programmers did.  That was what programming was back then. \n\n And here’s the thing about  MODE-A : YOU ARE CAREFUL.  Every mistake costs you an hour or so.  So you spend a lot of time going over the details, making sure your code is right; that you edited it correctly; that the switches are set correctly; that the tapes are labelled correctly; etc. \n\n In  MODE-A  you take  nothing  for granted.  You do everything  deliberately  and  carefully .  Because that’s the only way to go fast. (If “fast” is the word.) \n\n Let’s call this carefulness and deliberateness:  MODE-A  behavior. \n\n ## MODE-A  vs  MODE-B \n MODE-A  is a  lot  slower than  MODE-B .  The loop time is impossibly large, and the amount you can get done in each loop is ridiculously small.  For example, my first loop through this process was to write, and debug, the subroutine that read in a line of text from the keyboard, terminated with a CR (Carriage Return…  Yes, the teletype had a “carriage”, or rather a “print head” that could be “returned”.) \n\n MODE-B  is fast!  Really, really, fast.  The time through the loop is very short, and you can get a lot done in each loop.  For example, it only took me a few loops around to get the paper tape animation through the reader and punch to work correctly.  Every PDP-8 machine instruction took a loop or two.  Getting the scrolling of the TTY paper took two or three loops. \n\n And, of course, I wasn’t using listings.  I didn’t write the code on paper first.  I could go anywhere in the program I wanted and edit any line I wanted in a flash.  I had syntax highlighting, automatic indenting, search and replace, scrolling, tabs, and on-line documentation. \n\n MODE-B  is fast! \n\n ##The  MODE-B  Imperative!\nSo then why do so many programmers still work in  MODE-A ?  They do, you know.  They pile mess upon mess, and framework upon framework, until their loop time grows from seconds to minutes and longer?  They inject so many dependencies that the builds become fragile and error-prone.  They create so many unisolated external dependencies that they might as well be using paper tape.  Why would anybody do  anything  that increased their loop time?  Why wouldn’t everyone  defend their loop time with their lives ? \n\n Isn’t avoiding  MODE-A  a guilt-edged priority?  Shouldn’t we all do  everything we possibly can  to keep our development cycle in  MODE-B ?  Isn’t  MODE-B  an  imperative ? \n\n Do you want to know the secret for staying in  MODE-B ?  I know what it is.  I’ll tell you. \n\n \n   The secret for staying in  MODE-B  is to use  MODE-A  behavior. \n \n\n \n ##HOLY STRUCTURED METHODOLOGY BATMAN! \n\n I just discovered who wrote the PAL III assembler for the PDP-8.  Hold on to your hats.  It was  Ed Yourdon . \n\n \n\n PDP8 Program to accept two numbers and a single operator, and print the result. \n\n             *20\n0020  7563  MCR,    -215\n0021  0212  KLF,    212\n0022  7540  MSPC,   -240\n0023  7520  MZERO,  -260\n0024  7766  M10,    -12\n0025  0276  PROMPT, 276 />\n0026  0215  KCR,    215\n0027  7525  MPLUS,  -253\n0030  7523  MMINUS, -255\n0031  0277  QMARK,  277\n0032  0260  KZERO,  260\n        \n            /WORKING STORAGE\n0033  0000  REM,    0\n        \n            /CALL SUBROUTINE IN ARG\n0034  0000  CALL,   0\n0035  3046          DCA AC\n0036  1434          TAD I CALL\n0037  3047          DCA CALLEE\n0040  1034          TAD CALL\n0041  7001          IAC\n0042  3447          DCA I CALLEE\n0043  2047          ISZ CALLEE\n0044  1046          TAD AC\n0045  5447          JMP I CALLEE\n0046  0000  AC,     0\n0047  0000  CALLEE, 0\n\n----------------\n\n            *200\n            /CALC A+B OR A-B\n            /MAIN LOOP: PROMPT, GET CMD, PRINT RESLT\n            \n0200  6046          TLS\n0201  7200  IDLE,   CLA\n0202  1026          TAD KCR\n0203  4034          JMS CALL\n0204  0425          PRTCHAR\n0205  7200          CLA\n0206  1025          TAD PROMPT\n0207  4034          JMS CALL\n0210  0425          PRTCHAR\n0211  4034          JMS CALL\n0212  0400          RDBUF\n0213  2000          BUF\n0214  4034          JMS CALL\n0215  0462          SKPSPC\n0216  2000          BUF\n0217  3222          DCA .+3\n0220  4034          JMS CALL\n0221  0477          GETNUM\n0222  0000          0\n0223  3261          DCA A\n0224  1622          TAD I .-2\n0225  3263          DCA OP\n0226  1222          TAD .-4\n0227  7001          IAC\n0230  3233          DCA .+3\n0231  4034          JMS CALL\n0232  0477          GETNUM\n0233  0000          0\n0234  3262          DCA B\n0235  1263          TAD OP\n0236  1027          TAD MPLUS\n0237  7650          SNA CLA\n0240  5254          JMP ADD\n0241  1263          TAD OP\n0242  1030          TAD MMINUS\n0243  7650          SNA CLA\n0244  5251          JMP SUB\n0245  1031          TAD QMARK\n0246  4034          JMS CALL\n0247  0425          PRTCHAR\n0250  5201          JMP IDLE\n            \n0251  1262  SUB,    TAD B\n0252  7041          CIA\n0253  7410          SKP\n0254  1262  ADD,    TAD B\n0255  1261          TAD A\n0256  4034          JMS CALL\n0257  0600          PRTNUM\n0260  5201          JMP IDLE\n            \n0261  0000  A,      0\n0262  0000  B,      0\n0263  0000  OP,     0\n\n----------------\n\n            *400\n            /READ A BUFFER UP TO A CR\n0400  0000  RDBUF,  0\n0401  7200          CLA\n0402  1600          TAD I RDBUF\n0403  2200          ISZ RDBUF\n0404  3215          DCA BUFPTR\n0405  4216  RDNXT,  JMS RDCHAR\n0406  3615          DCA I BUFPTR\n0407  1615          TAD I BUFPTR\n0410  1020          TAD MCR\n0411  7450          SNA\n0412  5600          JMP I RDBUF\n0413  2215          ISZ BUFPTR\n0414  5205          JMP RDNXT\n0415  0000  BUFPTR, 0\n            \n            /READ ONE CHAR\n0416  0000  RDCHAR, 0\n0417  7200          CLA\n0420  6031          KSF\n0421  5220          JMP .-1\n0422  6036          KRB\n0423  4225          JMS PRTCHAR\n0424  5616          JMP     I RDCHAR\n            \n            /PRINT ONE CHAR\n0425  0000  PRTCHAR,0\n0426  6041          TSF\n0427  5226          JMP .-1\n0430  6046          TLS\n0431  3245          DCA CH\n0432  1245          TAD CH\n0433  1020          TAD MCR\n0434  7440          SZA\n0435  5242          JMP RETCHR\n0436  1021          TAD KLF\n0437  6041          TSF\n0440  5237          JMP .-1\n0441  6046          TLS\n0442  7200  RETCHR, CLA\n0443  1245          TAD CH\n0444  5625          JMP I PRTCHAR\n0445  0000  CH,     0\n            \n            \n            /PRT A BUFFER\n0446  0000  PRTBUF, 0\n0447  7200          CLA\n0450  1646          TAD I PRTBUF\n0451  2246          ISZ PRTBUF\n0452  3215          DCA BUFPTR\n0453  1615  PRTNXT, TAD I BUFPTR\n0454  4225          JMS PRTCHAR\n0455  2215          ISZ BUFPTR\n0456  1020          TAD MCR\n0457  7640          SZA CLA\n0460  5253          JMP PRTNXT\n0461  5646          JMP I PRTBUF\n\n----------------\n\n            /SKIP SPACES AC= FIRST NON-SPACE\n0462  0000  SKPSPC, 0\n0463  7200          CLA\n0464  1662          TAD I SKPSPC\n0465  2262          ISZ SKPSPC\n0466  3215          DCA BUFPTR\n                        \n0467  1615  NXTCHR, TAD I BUFPTR\n0470  2215          ISZ BUFPTR\n0471  1022          TAD MSPC\n0472  7650          SNA CLA\n0473  5267          JMP NXTCHR\n0474  7240          CLA CMA\n0475  1215          TAD BUFPTR\n0476  5662          JMP I SKPSPC\n            \n            /GET DECIMAL NUMBER\n0477  0000  GETNUM, 0\n0500  7200          CLA\n0501  3335          DCA NUMBER\n0502  1677          TAD I GETNUM\n0503  3215          DCA BUFPTR\n                    \n0504  1615  NXTDIG, TAD I BUFPTR\n0505  1023          TAD MZERO\n0506  3334          DCA DIGIT\n0507  1334          TAD DIGIT\n0510  7710          SPA CLA\n0511  5327          JMP NONUM\n0512  1024          TAD M10\n0513  1334          TAD DIGIT\n0514  7700          SMA CLA\n0515  5327          JMP NONUM\n0516  1335          TAD NUMBER\n0517  7100          CLL\n0520  7006          RTL\n0521  1335          TAD NUMBER\n0522  7004          RAL\n0523  1334          TAD DIGIT\n0524  3335          DCA NUMBER\n0525  2215          ISZ BUFPTR\n0526  5304          JMP NXTDIG\n0527  1215  NONUM,  TAD BUFPTR\n0530  3677          DCA I GETNUM\n0531  2277          ISZ GETNUM\n0532  1335          TAD NUMBER\n0533  5677          JMP I GETNUM\n0534  0000  DIGIT,  0\n0535  0000  NUMBER, 0\n\n----------------\n\n            /DIVIDE AC BY ARG\n            /Q IN AC, R IN REM\n0536  0000  DIV,    0\n0537  3033          DCA REM\n0540  1736          TAD I DIV\n0541  2336          ISZ DIV\n0542  7041          CIA\n0543  3361          DCA MDVSOR\n0544  3362          DCA QUOTNT\n0545  1033          TAD REM\n0546  1361  DIVLUP, TAD MDVSOR\n0547  7510          SPA\n0550  5353          JMP DIVDUN\n0551  2362          ISZ QUOTNT\n0552  5346          JMP DIVLUP\n0553  7041  DIVDUN, CIA\n0554  1361          TAD MDVSOR\n0555  7041          CIA\n0556  3033          DCA REM\n0557  1362          TAD QUOTNT\n0560  5736          JMP I DIV\n0561  0000  MDVSOR, 0\n0562  0000  QUOTNT, 0\n\n----------------\n\n                    *600\n                    /PRINT NUMBER IN DECIMAL\n                    DECIMAL\n0600  0000  PRTNUM, 0\n0601  4034          JMS CALL\n0602  0536          DIV\n0603  1750          1000\n0604  4225          JMS PRTDIG\n0605  7200          CLA\n0606  1033          TAD REM\n0607  4034          JMS CALL\n0610  0536          DIV\n0611  0144          100\n0612  4225          JMS PRTDIG\n0613  7200          CLA\n0614  1033          TAD REM\n0615  4034          JMS CALL\n0616  0536          DIV\n0617  0012          10\n0620  4225          JMS PRTDIG\n0621  7200          CLA\n0622  1033          TAD REM\n0623  4225          JMS PRTDIG\n0624  5600          JMP I PRTNUM\n            \n            /PRINT A DIGIT IN AC\n                    OCTAL\n0625  0000  PRTDIG, 0\n0626  1032          TAD KZERO\n0627  4034          JMS CALL\n0630  0425          PRTCHAR\n0631  5625          JMP I PRTDIG\n\n----------------\n\n            *2000\n2000  0000  BUF,0\n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2014/11/12/PutItInProduction.html", "title": "One Hacker Way!", "content": "\n       I used to think that Erik Meijer (@headinthebox) was a pretty smart guy.  Right?  I mean, this is the guy who gave us LINQ, so, no dummy, right? \n\n But lately…  Well, lately I’ve realized that Dr. Meijer is just totally brilliant.  I mean, out of the box, skyrockets in flight, is it a bird or a plane, brilliant!  And the reason?…the REASON? THE REASON is that he’s just pulled off  the perfect hoax ! \n\n Here, watch  this .  As Ron Jeffries tweeted, after watching it: \n\n \n   “well, that’s 45 minutes i’ll never get back.” \n \n\n Watch as Dr. Meijer rants and raves and pants and sweats and storms around the stage emitting more lunacy than a normal human brain can hold.  He’s the hellfire and brimstone preacher.  He’s the righteously indignant candidate for office.  He’s the morally offended civil rights activist. \n\n Feel the man’s passion and enthusiasm as he exhorts his listeners (at time code 30:00) to avoid testing their code.  He says: \n\n \n   “The only way you can create your software is to just put it in production!  Because it will fail.  And then, when it fails you just roll back to the previous version! … There’s no way you can pretend you can test your software beforehand.  So all this TDD crap?  Forget about it.  If your company does TDD what do you do?  Leave!  You quit!  You hand in your resignation today!  … Writing tests is waste.  TDD is for pussies.” \n \n\n But don’t stop the video!  Keep listening!  Keep watching!  See how Dr. Meijer shows us “the only architecture [he] knows”, the OSI seven layer structures for telecommunications.  Mundane you say?  Oh no!  Because simply by virtue of putting up the hierarchical layered approach of an architecture that nobody actually ever implemented, he’s going to make the brilliant argument that software teams ought to be governed by strict command and control structures like the Catholic Church or the Army! \n\n \n   “The church as been around for 2,000 years.  No company has been around for 2,000 years.  Why can the church be around so long?  Because it’s a layered architecture!” \n \n\n Of course!  Why haven’t we seen that all along!  The church has survived because it’s an early implementation of the OSI stack! \n\n \nAt time code 34:16 he puts up this picture of the typical software developer and claims that software developers are actually warriors who best serve in a military structure like the Army.  He says: \n\n \n   “All our companies should be structured like strict hierarchical [military organizations]. ” \n \n\n Speaking of a paragraph in the  Fleet Marine Force Manual 1: Warfighting  he says: \n\n \n   “If you replace the word ‘war’ with the word ‘software’ it just fits!  Because software is like fighting a war.  So, none of this Agile nonsense, let’s look at the military that has been doing this for thousands of years!” \n \n\n And then.  And Then.  AND THEN… \n\n \n   “What can we learn from this?  That war fighting is not for old people!  Old guys, like me, should not be in this industry.” \n \n\n To prove this point he puts up a graph showing that the average age of world-cup soccer players is 27 +/- 1.  This, of course, has immediate bearing on the software world.  Clearly all software developers are similar, in every way, to world-cup soccer players.  The two industries are nearly identical in their goals, and demography.  Right?  Of course. \n\n \n   “I want to treat development teams like professional sports teams.  Between 22 and 32, you do nothing but code! 24/7 you code. Just like a professional athlete.” \n \n\n Yes, of course, this makes total sense.  Programmers are only valuable in those early years.  After they pass 32, their bodies give out on them. They all get Carpal Tunnel Syndrome and they can’t write code anymore.  Of course! \n\n \n   “You, as developers, should only think about code.  You should dream code, eat, drink code.” \n \n\n And then, the coup de gras.  The master stroke.  The meme that seals the deal: \n\n \n   “But that also means, you should earn as much as a professional soccer player.  Why on Earth does Messi (a world-cup soccer player) make sixteen million dollars a year, and you, that writes code, and you’re a professional software coder, you’re as talented as Messi, what do you get?  Sixty thousand Euros?  Something like that?  That’s ridiculous!  So you should be able to work your ass off for ten years, and make all your money, and then retire.” \n \n\n Well, how could anybody say ‘no’ to that? \n\n Brilliance. \n\n There’s much more in Dr. Meijer’s talk.   Much  more.  For example, in the midst of talking about team structure, and how crappy Agile is, he breaks into an elementary introduction to control theory and Mealy state machines. \n\n At first, as you watch his frenetic antics, you may think he’s a little off.  Then you might think he’s gone stark raving mad.  You’ll likely find yourself awash in the sheer volume of the concatenated illogic, contradiction, and emotion.  But, by the end, if you are astute, you will realize that the man is absolutely  brilliant . \n\n He’s brilliant because he’s pulled off the perfect hoax. \n\n Listen to that crowd.  Listen as they accept what he’s saying.  Listen as their weak minds uncritically seize upon his endless train of ridiculous confusion and nonsense.  They eat it up! \n\n Dr. Meijer filled forty-five minutes with absolute drivel, and made everybody love it.  And that takes brilliance. \n\n I’m certain that he walked off that stage giggling so hard that he wet his pants. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/05/28/TheFirstMicroserviceArchitecture.html", "title": "The First Micro-service Architecture", "content": "\n       In the early 1960s IBM created the  1401 computer .  It was wildly popular and exceeded all revenue projections.  IBM built 2,800 of them in the first year (1960), and built over 15,000 of them overall.  They typically rented for $7,000 per month (~$42,000 in todays dollars) and would have sold (if selling had been an option) for several million dollars. \n\n The 1401 had a clock rate of around 88khz, and as much as 16,000 bytes of core memory.  Not 16,384, mind you,  16,000 .  The machine addressed it’s memory in  decimal .  Some installations had magnetic tape drives; but many had nothing more than a card reader, card punch, and a line printer.   The card reader could read 800 cards per minute.  The punch could punch 250 cards per minute.  The line printer could print 600 lines per minute. \n\n A 1401 with 8,000 bytes, and no tape drive, could run the FORTRAN compiler.  The compiler would read in your FORTRAN program from the card reader, and then punch a self-loading binary executable deck of cards. \n\n If you had 8000 bytes, and had to write a FORTRAN compiler, how would you do it?  Remember that compilers generally have to make multiple passes over the source code in order to do their work.  So how would you structure the program so that it could pass over the code several times? \n\n Typical Multi-pass Compiler \n\n The obvious choice might be to load the whole compiler into memory, leaving just enough space for some working storage for the symbol table and IO buffers.  Thus, if you had a three pass compiler, then on pass one, you would read in the FORTRAN source code, and then punch an intermediate deck that was (hopefully) somewhat smaller than the first deck.  Then you’d start pass two, and read in the intermediate deck, and punch another (hopefully smaller) intermediate deck.  Finally, you’d start pass three, and read in the final intermediate deck in order to punch the loadable binary deck.  …And that’s if there were just three passes.  What if there were 5? or 10? \n\n Think of the limitations of this system.  How big is the compiler?  How much space do you need in order to hold the symbol table?  How many cards (80 bytes each) can you read in to your input buffers.  What size output buffer do you need to punch the intermediate cards?  And how much  time  is going to be spent by the operator, shuttling cards from the punch back to the reader, over and over again, for each pass? \n\n Let’s say you’ve got a thousand line FORTRAN program.  How long is pass 1?  Let’s assume that the intermediate deck is 80% the size of the source deck.  So you’ve got to punch 800 cards at 250 per minute.  You also have to read 1000 cards at 800 per minute.  You can do that math.  It’s just over 4 minutes of IO time.  And remember, with an 88khz clock rate, computation time was likely significant.  Each pass would be a bit shorter than the one before it; but given operator time, and the possibility of a handing error (dropping the cards) we can estimate that our 1,000 line FORTRAN program would require several minutes per pass, keeping the operator busy the whole time.  A five pass compilation might require an hour of dedicated operator time; and a lot of wasted intermediate cards.  Not to mention wear and tear on the card punch. \n\n Microservices \n\n But that’s not what the plucky folks at IBM did.  They used an entirely different approach.  They read the source code in  once , and held the entire source program in memory!  They made  Sixty three passes  over the source code; and they did that by using  microservices .  They called them  phases . \n\n Almost all the memory in the computer was used to store the source code.  They held back a few hundred bytes for the executable phases. If you get rid of all the extra blanks, the comments, and other non-essentials, you can hold a pretty big program in 7000+ bytes. \n\n Each compiler phase averaged 150 instructions.  Each was read in from the card reader (or mag tape if you had it).  Each phase would make a pass over the source code, replacing that source code with smaller intermediate results.  So from phase to phase, as the source code took up ever less space, there was ever more working storage for symbol tables and variables.  When each phase was complete, the next phase would be read in from the card reader (or tape) and executed.  The final phase punched the output result (or wrote it to tape). \n\n What did these phases do?  You can read about the details  here .  In short, these phases swept through the source code, reorganizing it, eliminating redundancy, shortening keywords, replacing variables with addresses, replacing function and subroutine references with addresses, and gradually, inexorably grinding that source code down into binary code. \n\n I have just one word for that.   Incredible. \n\n Imagine breaking down the problem of compilation into 62 different programs, each of which can run only once.  Each of which must run in the memory vacated by the previous program.  Each of which must consume the output of the previous, and prepare the input to the next. \n\n Of course these little programs were microservices, and the compiler used a microservice architecture – in 1960. \n\n Which just goes to show that there’s nothing new under the Sun. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/04/15/DoesOrganizationMatter.html", "title": "Does Organization Matter?", "content": "\n       \n   I wonder if it’s even possible to get all three of readability, hackability and abstraction.  \n   –  Ben Kuhn. from  Readability, hackability, and abstraction \n \n\n The short answer, Ben, is: No.  It’s  not  possible.  But then… I’m not answering the question you  wrote .  I’m answering the question you  actually asked .  That question is: \n\n \n   I wonder if it’s even possible to make a complicated program as readable and changeable as a simple program. \n \n\n And there, of course, the answer is:  No! \n\n So let’s get in to why you asked the question; and why you asked it in that particular way. \n\n You noted that simple programs are easy to read, and easy to change.  That’s perfectly true.  This program is easy to read, and easy to change: \n\n void dial(modem, pno, quiet) {\n\tif (quiet) {\n\t\tmodem.setVolume(0);\n\t} else {\n\t\tmodem.setVolume(10);\n\t}\n\tmodem.dial(pno);\n}\n \n\n Or is it? \n\n If the whole program is twenty lines long, and you can see all the interactions of all the variables then, yes, this is easy to read and easy to change. \n\n But what if that code up there was part of a ten thousand line program?  What are the implications of that  if  statement?  Will any other function be confused by the fact that the  dial  function changed the  volume  of the  modem ? \n\n As programs grow, parts start to depend on other parts.  The connections become ever more tangled.  And the program begins to turn into  a system .  And  a system  is always harder to understand, harder to read, and harder to change, than a program. \n\n But you were actually asking a very different question.  You were asking whether breaking big functions into small functions, eliminating redundancy, and partitioning the application into polymorphic objects really made things easier to read and change.   You were asking whether it would just be better to leave everything duplicated, in big functions, without any polymorphism.  And the reason you asked this was because you had seen small programs that had some duplications, some big-ish functions, and no polymorphism that  were  easy to read and change. \n\n The question you are asking is an essential one.  The question you are asking is whether organization matters. \n\n To answer that, let me show you a picture of my desk: \n\n \n\n As you can see it’s not real well organized.  Oh, there’s  some  organization.  But there’s a lot of chaos too.  Does this mean that organization doesn’t matter? \n\n Well, the thing about my desk is that it’s relatively simple.  There is a pile on my right.  There is another pile on my left.  The pile on my right has books I’m currently reading or referring to.  The pile on my left is mostly crap that I don’t want to think about today, but that I’ll have to think about tomorrow.  Neither pile is particularly large.  A quick linear search through either pile gets me what I need.  The dependencies are limited. \n\n In other words, my desk is somewhat unorganized, but it’s relatively simple.  And that’s the key here.  Relatively simple things can tolerate a certain level of disorganization.  However, as complexity increases, disorganization becomes suicidal.  Consider trying to find a book in here: \n\n \n\n Don’t you think it would be easier to find it here? \n\n \n\n A  lot  of effort went into organizing that second library.  And consequently, it requires a fair bit of effort to learn the organization scheme.  A newbie can’t just walk into that well organized library and go right to the book they want.  Instead, the newbie is going to have to learn a bit about the  Dewy Decimal System , and about how to use a card catalog, or the automated index system.  It will require a bit of study and thought before the newbie can find the book they want. \n\n But ask one of the librarians to find a book, and they’ll typically have it in their hands in a matter of seconds! \n\n And so this gets to the crux of the question that you were really asking.  You were asking whether the time required to learn the organization scheme of the system is worth the bother.  Learning that organization scheme is  hard .  Becoming proficient at reading and changing the code within that scheme take time, effort, and practice.  And that can feel like a waste when you compare it to how simple life was when you only had 100 lines of code. \n\n And, then, there’s another problem. \n\n The organization structure that works for a thousand line program, does not work for a ten thousand line program.  And the organization structure that works for a ten thousand line program does not work for a hundred thousand line program. \n\n This almost feels intolerable.  Because as the program grows you must invest time, effort, and practice into an organization scheme that is bound to become obsolete. \n\n And so the question you are asking is whether it is worthwhile to invest in  any  organization scheme given that they’ll all become obsolete one day. \n\n The answer to that question should be obvious.  If you decide that organizing your system isn’t worth the effort, you’ll wind up as a  Code Hoarder . \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/09/23/ALittleStructure.html", "title": "A Little Structure", "content": "\n       What is  Structured Programming ? \n\n \n   Ummm?…  Wasn’t it some ancient history having to do with  GOTO ? \n \n\n Ancient.  Hmmm.  Yes, I guess some might consider 1968 to be ancient.  But can you tell me what Structured Programming is? \n\n \n   It was a rule that said not to use  GOTO  statements. \n \n\n Why do you keep using the past tense? \n\n \n   Because nobody cares about Structured Programming anymore. \n \n\n They don’t? \n\n \n   No, I mean, hardly anybody knows what it is; except that it’s got something to do with not using  GOTO . \n \n\n Do you use  GOTO ? \n\n \n   Of course not!  I mean, well…  Hardly ever. \n \n\n Why not? \n\n \n   Well, mostly because the languages I use don’t have  GOTO . \n \n\n Why do you suppose that is? \n\n \n   Because you don’t really need it. \n \n\n How do you know you don’t need  GOTO ? \n\n \n   Well…  I haven’t had to use it …   much. \n \n\n Have you ever heard of Corrado Bohm or Giuseppe Jacopini? \n\n \n   Who? \n \n\n Corrado Bohm and Giuseppe Jacopini.  In 1966 they wrote a  paper  that mathematically proved that  GOTO  was not necessary. \n\n \n   Huh.  That’s cool…  I guess. \n \n\n Actually, yes, it’s very cool.  Because, you see, in 1966 the  GOTO  statement was the primary means by which programmers connected their programs together. \n\n \n   Really? \n \n\n Yes.  For example, here’s an  if  statement in  FORTRAN : \n\n IF (A-10) 22,33,44\n \n\n \n   That looks primitive.  What does it mean? \n \n\n It means, if the value of the variable  A  minus 10 is negative,  GOTO  statement 22.  If zero,  GOTO  statement 33.  Otherwise  GOTO  statement 44. \n\n \n   Wow!  That’s kinda gnarly.  So, like, how did you use that? \n \n\n So in Java I might say: \n\n if (a>10)\n  b++;\nelse\n  b--;\n \n\n In  FORTRAN  that would be: \n\n \tIF (A-10) 20,20,30\n20\tB = B - 1\n\tGOTO 40\n30\tB = B + 1\n40\t...\n \n\n \n   Yuk!  Yuk!  That’s awful. \n \n\n That’s what we were used to.  We’d never even thought it could be different. \n\n \n   And so then those two guys, Bohm and Jacowhatsit… \n \n\n Bohm and Jocopini. \n\n \n   Yeah, they wrote their paper and everybody stopped using  GOTO . \n \n\n No, not quite.  In fact, not at all.  You see their paper was a pretty technical mathematical proof, so hardly anybody read it. \n\n \n   Heh heh, yeah, I get that.  But somebody must have… \n \n\n Oh yes.  Several.  But most notably a man named  Edsger Dijkstra . \n\n \n   Dije…  DIYGE.. \n \n\n You pronounce his last name: DIKEstruh.  In March of 1968 he wrote a  letter  to the ACM. \n\n \n   A letter?  To who? \n \n\n Yes, a very short note.  It was written to the editors of a magazine called  The Communications of the ACM . He titled it  Go To Statement Considered Harmful . \n\n \n   What did the letter say?  Did it convince everybody? \n \n\n No, it really didn’t.  Oh, some people saw the logic right away.  Others were – um – skeptical – for a long time. \n\n \n   So what did the letter say? \n \n\n Well, you should read it.  It’s pretty short.  But I’ll give you the gist. \n\n He made the case that you could restrict your program to three different control structures:  Sequence, Selection, and Iteration. \n\n \n   OK, so – Huh? \n \n\n Sequence is when two statements follow each other in sequence like this: \n\n doStepOne();\ndoStepTwo();\n \n\n Those statements might be simple assignments, or procedure calls, or any other kind of valid statement.  They are executed in sequence.  Right? \n\n \n   OK, Sure.  So then…  what’s the next one? \n \n\n Selection.  One of two statements will be executed based on some boolean value.  Like this: \n\n if (someBooleanValue())\n\tdoThisStep();\nelse\n\tdoOtherStep();\n \n\n \n   Yeah, OK.  So then… that last one… \n \n\n Iteration.  A statement can be repeated until a boolean value becomes false.  Like this: \n\n while(someBooleanValue())\n \tdoThisStep();\n \n\n \n   Yeah, so, sure.  That’s how we write code nowadays.  But you said people didn’t buy into this right away? \n \n\n No, they didn’t.  Dijkstra argued that if you restricted yourself to those three structures then… \n\n \n   Oh!  Structures.  Structured Programming.  I get it! \n \n\n Um.  Yes.  That’s right.  So, if you restrict yourself to those three, um, structures, then you can easily reason about your code.  But if you use unrestricted  GOTO  then you can’t. \n\n \n   Wait.  What?  Whaddya mean, reason? \n \n\n Well, Dijkstra’s argument was that a structured program can be easily analyzed because the state of the system at any line of code, depends only on the boolean values being tested by  selection  and  iteration , and the list of calling procedures on the stack. \n\n \n   Um. sure.  Whatever. \n \n\n (Sigh.)  Look, just read his paper, he makes it pretty clear. \n\n \n   OK, well, so then what happened.  I mean, how did people become convinced? \n \n\n Well, in 1972, Dijkstra wrote a book with  O. J. Dahl , and  C. A. R. Hoare .  It was called  Structured Programming . \n\n \n   Oh!  So that’s what convinced everybody. \n \n\n Well, no.  Though it did – uh –  elevate  the controversy. \n\n \n   You mean like you guys were having flame wars over this? \n \n\n No, we didn’t have Facebook.  We didn’t even have the internet.  But we could write letters to the editors of the various trade journals.  And, let me tell you, some of those letters were  scathing . \n\n \n   Ha ha.  Sort of like snail mail flames. \n \n\n Indeed.  The more things change, the more they stay the same. \n\n Anyway, the good thing was that the book got lots of people talking, and trying things out, and even convinced some people. \n\n \n   But not everyone. \n \n\n No, not everyone.  Many people continued to hold on to their  GOTO  statements; and would not give them up. \n\n \n   So then when did that end? \n \n\n It ended when people stopped making and using languages that had  GOTO  statements, and started using languages that didn’t. \n\n \n   You mean like Java? \n \n\n Yes.  Like Java.  Nowadays the majority of programmers use a language that has no  GOTO .  And an even larger majority avoid using  GOTO  even if their language has one.  So, for the most part, Dijsktra’s war has been won.  Structured Programming is the norm today. \n\n \n   Wow! So, Hurray for Dijsktra for giving us this new technology…  back in the olden days… \n \n\n New Technology?  No, no, you misunderstand. \n\n \n   Why?  I mean, this structured programming thingie was like his invention, right? \n \n\n Oh, no.  He didn’t invent anything.  What he did was to identify something we  shouldn’t do .  That’s not a technology.  That’s a  discipline . \n\n \n   Huh? I thought Structured Programming made things better. \n \n\n Oh, it did.  But not by giving us some new tools or technologies.  It made things better by taking away a damaging tool. \n\n \n   Hmmm.  OK.  Yeah, I guess that’s right.  He took  GOTO  away from us. \n \n\n It might be better to say that  Structured Programming imposes discipline upon direct transfer of control. \n\n \n   That sound like gobeltygoop. \n \n\n Yes, I suppose it does. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/08/06/LetTheMagicDie.html", "title": "Make the Magic go away.", "content": "\n       I’ve been looking at  rxJava .  It’s a nice little framework that helps you to create and manage observers.  The design philosophy seems to be that everything can be observed and therefore everything ought to be managed by callback. \n\n Of course this is an old idea that dates back to data flow languages, functional languages, and other declarative languages.  The idea even had echoes in the late ’90s when the GOF book was first published.  Those of you who were programming back then may remember that, for a few months, everybody thought that the Observer pattern was  so cool .  We saw lots of observer based designs.  Then that stopped because those designs were too indirect; making them too hard to trace and debug.   (Tests anyone?) \n\n I’m not saying that rxJava is a bad idea.  Like I said, it looks pretty cool.  It’s just that it’s not a really a hot new idea.  And, after all, what is? \n\n The Never Ending Quest \n The authors of rxJava, and of Spring, and JSF, and JPA, and Struts, and [put your favorite framework here] are all searching for the same thing.  These frameworks are born out of frustration with the language; and are an attempt to improve upon that language. \n\n Every framework you’ve ever seen is really just an echo of this statement: \n\n \n   My language sucks! \n \n\n And so we write frameworks to compensate for the lack of features that we wish were in our language.  And if that doesn’t work; then, like James Gosling, Bjarne Stroustrup, Alan Kay, Brad Cox, Dennis Ritchie, Rich Hickey, and so many, many, others, we write a new language. \n\n A new language!  Golden!  Pure!  Perfect!  A new language to solve all ills.  A new language that supersedes all the others. A new language that answers all the complaints, addresses all the weaknesses, and settles all the disputes.  A new magical language that is expressive, safe, dense, flexible, disciplined, and, and, and, — perfect! \n\n Bzzzt!  Wrong answer! \n Of course there is no such beast.  There is no perfect language.  And, lately, all the “new” languages we have seen, are just rehashes of the same old … same old …  same old  stuff.  I honestly don’t think there has been a new idea in computer languages since the late ’70s or early ’80s. \n\n I mean, once you’ve programmed in Assembler, FORTRAN, C, Pascal, C++, Smalltalk, Lisp, Prolog, Erlang, and Forth, you’ve seen it all.  Oh, I suppose you could toss in languages like Snobol, ML, Cobol, and XSLT  (retch, puke) .  But most of their ideas were really covered in the previous list. \n\n The same goes for frameworks.  When is the last time you saw a framework with a truly new idea in it?  For me, it was the  Inside Macintosh  framework, written in Pascal, in the late 70s and very early 80s.  And that was really just a rehash of the Smalltalk framework from a few years before that. \n\n What’s new in software? \n\n \n   For the last thirty years:  Nothing much. \n \n\n Santayana’s Curse \n So why do we keep on writing new languages and new frameworks?  I think the answer to that is very simple: \n\n \n   Those who do not remember the past are condemned to repeat it.   __ –  Jorge Agustin Nicolas Ruiz de Santayana y Borras \n \n\n In other words every new batch of programmers to come along are destined (condemned!) to rewrite the same old languages, and the same old frameworks.  Oh they’ll look a little different, and they’ll have a slightly different twist.  But they won’t be new in any meaningful sense. \n\n And some of those languages and frameworks will gain a certain notoriety and become popular for awhile – as though they were something new and magical; but that’ll just be an illusion based on a near term perspective.  The proponents of those “new” languages and frameworks will make soaring claims about how much faster you can code, and how much easier it is to build systems, and how much better the designs of those systems are.  But in the end, the programs will be written at the same speed as before, the difficulty will be just as great as before, and the designs will still be just as bad. \n\n Magic! \n Why does this happen?  Why are people always hunting for the next new language and the next new framework?  Why do we go around and around on this merry-go-round of frameworks and languages in hopes that we’ll see some different scenery on the next spin?  Why do we hope for the magic? \n\n We hope for the magic, because we believe in magic.  We’ve used languages whose behavior seems magical.  We’ve used frameworks that do magical things.  And, in our naivety, we trust that if we can just muster a little bit more of that magic, then the perfect language, or the perfect framework, will suddenly appear on the next turn of the merry-go-round. \n\n No Magic \n\n But there is no magic.  There are just ones and zeros being manipulated at extraordinary speeds by an absurdly simple machine.  And that machine needs discrete and detailed instructions; that we are obliged to write for it. \n\n I think people should learn an assembly language as early as possible.  I don’t expect them to use that assembler for very long because working in assembly language is slow and painful (and joyous!).  My goal in advocating that everyone learn such a language is to make sure that the magic is destroyed. \n\n If you’ve never worked in machine language, it’s almost impossible for you to really understand what’s going on.  If you program in Java, or C#, or C++, or even C, there is magic.  But after you have written some machine language, the magic goes away.  You realize that you could write a C compiler in machine language.  You realize that you could write a JVM, a C++ compiler, a Ruby interpreter.  It would take a bit of time and effort.  But you could do it.  The magic is gone. \n\n And once the magic is gone, you have a different perspective on things.  You look at a language like C or Java or C# as just another expression of machine language. You look at a line of C code, and you can “see” the machine instructions that it generates.  You look at a line of Java code, and you can visualize the machine instructions being executed by the JVM.  There are no mysteries.  There are no secrets.  There is no magic.  You know, if you had to, you could write it all in machine code. \n\n It’s the same with frameworks.  If you’ve ever written a web server – no matter how simple that web server might be – if you’ve written the code that listens at a socket, unpacks an HTTP Request packet, generates HTML and packs it into an HTTP response packet, and then writes that response back out the socket; then the magic is gone.  You know how to write a web server. And that puts a whole new perspective on any web framework you might be tempted to use. \n\n If you’ve ever written a simple dependency injector, or a simple XML parser, or a simple observer generator, or a simple query generator, then you’ve driven that magic away.  You could write a framework if you needed to.  Or you could just write the code in your application if you needed to.   You don’t need someone else’s framework . \n\n And that puts the framework in a whole new perspective.  You don’t need it.  And if you don’t need it, it can’t have any power over you.  You can look at that framework for what it is:  just plain old code  – and probably a lot more code than you actually need. \n\n Now you can judge whether the cost of all that code is worth the benefit.  Perhaps there’s a simpler framework that will do just as well.  Perhaps you don’t need any framework at all.  Perhaps –  perhaps  – you should just write the little bit of code that you need, instead of importing thousands and thousands of lines into your project.  Lines that you didn’t write.  Lines that you don’t control.  Lines that you probably shouldn’t be putting a whole lot of trust in. \n\n My Advice. \n Never buy magic!  Before you commit to a framework, make sure you could write it.  Do this by actually writing something simple that does the basics that you need.  Make sure the magic all goes away.  And  then  look at the framework again.  Is it worth it?  Can you live without it? \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/07/01/TheLittleSingleton.html", "title": "The Little Singleton", "content": "\n       Do you recognize this: \n\n public class X {\n  private static X instance = null;\n\n  private X() {}\n\n  public static X instance() {\n    if (instance == null)\n      instance = new X();\n    return instance;\n  }\n\n  // more methods...\n}\n \n\n \n   Of course.  That’s the Singleton pattern from the GOF book.  I’ve always heard we shouldn’t use it. \n \n\n Why shouldn’t we use it? \n\n \n   Because it makes our systems hard to test. \n \n\n It does?  Why is that? \n\n \n   Because you can’t mock out a Singleton. \n \n\n You can’t?  Why not? \n\n \n   Well, because, the only class that can touch that private variable is the Singleton itself. \n \n\n Do you know the rule about encapsulation and tests? \n\n \n   Uh, no.  What rule is that? \n \n\n Tests trump Encapsulation. \n\n \n   What does that mean? \n \n\n That means that tests win.  No test can be denied access to a variable simply to maintain encapsulation. \n\n \n   You mean that if a test needs access to a private variable… \n \n\n …the variable shouldn’t be private.  Yes. \n\n \n   That just doesn’t sound right.  I mean, encapsulation is, er, important! \n \n\n Tests are more important. \n\n \n   Wait.  What? \n \n\n What good is encapsulated code if you can’t test it? \n\n \n   OK, OK, but what does this have to do with testing singletons. \n \n\n Look at this code. \n\n public class X {\n  static X instance = null;\n\n  private X() {}\n\n  public static X instance() {\n    if (instance == null)\n      instance = new X();\n    return instance;\n  }\n\n  // methods.\n}\n\nclass TestX {\n  @Before\n  public setup() {\n    X.instance = new XMock();\t\n  }\n}\n\nclass XMock extends X {\n    // overide methods\n}\n \n\n \n   Oh, you made the instance variable “package” scope. \n \n\n Right. \n\n \n   And that allows you to mock the singleton. \n \n\n Right. \n\n \n   And that means that singletons are easy to mock. \n \n\n Right.  Now consider this: \n\n public class X {\n  public static X instance = new X();\n\n  private X() {}\n\n  // methods.\n}\n \n\n \n   Wait!  Where did the instance method go? \n \n\n I don’t need it. \n\n \n   Ah, the instance variable is public.  You can just use it directly. \n \n\n Right. \n\n \n   But… But…  Someone might over-write it? \n \n\n Who would do that? \n\n \n   I dunno.  Uh.  Someone bad. \n \n\n Do you have bad people on your team? \n\n \n   No.   But.   This just doesn’t feel safe. \n \n\n Well, if this were part of a public API, I’d agree with you.  But if this is just code that’s used by our team then… \n\n \n   We trust our team? \n \n\n Of course. \n\n \n   And this is pretty easy to mock, isn’t it? \n \n\n Of course. \n\n \n   So then I guess we could use Singleton if we wanted to. \n \n\n Sure.  Although most of the time I don’t want to. \n\n \n   After all this, and now you’re telling you you don’t want to use Singleton anyway? \n \n\n Well, I think it’s important to understand why. \n\n \n   OK, so why don’t you use Singleton? \n \n\n I do sometimes.  Especially in public APIs. \n\n \n   You mean it’s a trust issue again? \n \n\n Right.  In a public API if I want to ensure that only one instance is being created, then I’ll use a Singleton. \n\n \n   OK, but then what if it’s not in a public API, but you still just want one instance created? \n \n\n Well, then, I simply create one. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/04/27/LanguageLayers.html", "title": "Language Layers", "content": "\n       I’m sitting here, passing time, by playing the old “Lunar Lander” game from 1969.  This game was written by Jim Storer, a high school student.  He wrote it on a PDP-8 in the FOCAL language.  Here’s what a sample run looks like: \n\n \n\n And here’s the source code in FOCAL: \n\n \n\n Jim Storer was a pretty talented High School student.  I mean, look at that code.  He’s got some pretty interesting Taylor expressions in there. \n\n Anyway…, what I’m  actually  doing is conducting a binary search in order to find the value of  K  that, when used consistently, will land the craft perfectly.  So I’ve modified the program to accept a single entry, and then apply that entry repeatedly until the craft either lands or crashes.  As I write this I know that the answer lies between  76.40625  and  76.4453125 , and I am trying  76.4257813 .  I’m beginning to think I’ll run out of precision before I find the answer. \n\n Meanwhile, it occurred to me that I was running this program on the  PDP-8 emulator  that I wrote in Lua for my iPad. \n\n So, OK, let’s think about this. \n\n \n   The iPad has an  A8X  chip, with three cores running at a gigahertz or so. \n   Lua is written in C compiled down to A8X assembler. \n   My PDP8 Emulator is written in Lua, using the  CODEA  package from Two Lives Left. \n   FOCAL was written in the late 1960s in PDP8 assembler. \n   Lunar Lander was written in FOCAL. \n \n\n So that’s A8X, C, Lua, PDP8, and FOCAL. That’s five different languages.  Five different mechanisms for telling a machine what to do; all stacked up on top of each other! \n\n What’s up with that?  Why so many languages? In fact, forget the iPad, the PDP-8, C, Lua, and the rest.  Why are there so many languages? \n\n ##Why are there so many languages? \n\n Think about it!  How many computer languages can you name?  Here, off the top of my head, let me give you a partial list: \n\n \n   FORTRAN \n   ALGOL \n   COBOL \n   SNOBOL \n   LISP \n   BCPL \n   B \n   C \n   SIMULA \n   SMALLTALK \n   EIFFEL \n   C++ \n   JAVA \n   C# \n   PYTHON \n   RUBY \n   LOGO \n   LUA \n   BASIC \n   PL/1 \n   JAVASCRIPT \n   GO \n   DART \n   PROLOG \n   FORTH \n   SWIFT \n   ML \n   OCCAM \n   OCAML \n   ADA \n   ERLANG \n   ELIXER \n   FOCAL \n \n\n You can certainly think of others that I’ve left out.  The question is, why are there so many? \n\n There can really only be one answer to that question.  The reason there are so many computer languages is: \n\n \n   We don’t like any of them. \n \n\n Well, maybe that’s too strong a statement.  Perhaps what I should say is: \n\n \n   We’ve been to Hollywood We’ve been to Redwood We’ve crossed the ocean for the code of gold. We’ve been in our mind, It’s such a fine line That keeps us searching for the code of gold. And we’re getting old. \n \n\n OK, perhaps I should speak for myself…  Anyway, didn’t you always want to just yell at Neil Young that he ought to just stop complaining, find some nice girl, and settle down with her?  Didn’t you want to tell him that the search of the heart of gold was fruitless?  I mean, what would he do with it if he found it? \n\n And what would  we  do with the perfect language if we found it? \n\n \n   We’d write PDP-8 Emulators and run FOCAL so we could play Lunar Lander written by a High School student in 1969! \n \n\n Here’s what I think. \n\n Get over it.  Stop searching.  There is no perfect language.  We’ve searched everywhere.  We’ve looked high and low.  We’ve looked in and out. \n\n \n   We’ve looked at languages from both sides now From in and out and still somehow It’s languages illusions we recall. We really don’t know languages…. ….at all. \n \n\n Yeah, OK, it’s a weird day. \n\n But, still, the point is this: \n\n \n   We don’t need another language. We don’t need to know the way home. All we want is life beyond S.Q.L. \n \n\n Yeah, weird day. \n\n So here’s a thought.  Maybe we need to stop writing new languages and just settle down and pick one or two that work really well.  That would make life a lot simpler for us wouldn’t it? \n\n And, in case you were wondering,  76.4384461  gives you a pretty good landing at 2.23 MPH. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/07/05/PatternPushers.html", "title": "Pattern Pushers", "content": "\n       There were a number of interesting tweets in response to  The Little Singleton .  Here are a few. \n\n Some complained that Singleton was bad because Singletons are global variables. \n\n It is true that Singletons involve using global variables.  However, despite Martin Fowler’s oft referenced imitation of me, global variables are not a sin.  Most systems have a few globals that are used to hold the roots of their data structures, factories, and other important resources.  This is normal, and by no means sinful.  And, if you have a public API to protect, a Singleton can be a useful way to manage a global. \n\n Others complained that Singletons don’t work well in distributed environments.  I Imagine that might be true in some distributed environments.  In others, I think the notion of a singular object is useful.  Of course this depends on what we mean by singular.  Singular to the whole distributed system?  Or singular within each process of the distributed system.  The Singleton pattern works well for the latter.  The former is another matter altogether. \n\n Another complaint was that there is documentation value in the  instance()  method, and the  private  encapsulation of the  instance  variable.  That’s true enough; but I prefer the public  instance  variable when working in a small trusted team. \n\n Someone else was rather upset at my notion that trust is a factor and thought that I was violating  Clean Code  by not being as explicit as possible about all the rules.  But in small trusted teams implicit rules are a huge efficiency gain that I am loathe to abandon. \n\n One of the strangest tweets I saw in response to was the article was: \n\n \n   This is supposed to be clever. Pattern pushers are full of hot air \n \n\n What the devil is a  Pattern Pusher ? \n\n \n   Hey, bud, c’mere.  Ya wanna get high?  I’ve got some el primo Visitor Pattern, direct from Columbia.  Man, this stuff will double dispatch you right to cloud nine!  I’ve got too much of this stuff right now, so you can have this hit for free.  Come back later and maybe I’ll have some real hot Memento pattern for you. \n \n\n So let’s be clear.  It’s a good idea to  learn  patterns.  It is not a good idea to hunt for places to  use  patterns.  Instead, if you  know  the patterns well, then you will find places in your systems where they fit naturally.  Then, if you use the pattern names and canonical forms, you provide a kind of automatic documentation to others on your team who know those patterns. \n\n I mean, if I see a class named  ReportVisitor , I immediately know what the author’s intent was, and what the structure of the code is. \n\n Am I a  Pattern Pusher ?  I suppose I am, inasmuch as I strongly recommend that people learn them.  But that’s no different from an electronics engineer learning the names and forms of common electrical circuits, or a sailor learning the names and forms of common sailing knots. \n\n I’m not sure I’ll ever understand what people have against learning things. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/10/05/WattsLine54.html", "title": "WATS Line 54", "content": "\n       I had an interesting conversation with  Doc Norton  this morning.  And it got me to thinking… \n\n You know what an 800 number is.  Some people call them “toll free”.  The Telephone company call them WATS lines.  Wide Area Telephone Service. \n\n In 1976 I took a job at a company in the suburbs of Chicago.  Teradyne Central it was called.  We made test equipment for the telephone company.  Our product was named 4-Tel.  It tested every telephone line, in a telephone service area, every night.  A telephone service area could have 100,000 lines or more. \n\n A 4-Tel system could have as many as 21 terminals tied to it.  Each terminal could be used by a tester in the service area to test a telephone line anywhere in that service area.  The test would detect and diagnose any problems on those lines; and could determine whether the problem was in the central office, in the lines themselves, or in the customer’s telephone.  This was important because those three diagnoses were handled by different crafts.  Dispatching the right craft to fix the problem saved a lot of money. \n\n 4-Tel had many other cool features.  It was a rich product with a lot of use cases.  And it was installed all over the United States. \n\n When one of our customers had a problem, they would call an 800 number.  This would automatically get routed to one of our two WATS lines.  If it was normal business hours, our receptionist would answer the WATS line.  Once she had ascertained that this was a customer service call, she would put the caller on hold, and then speak over our internal public address system: \n\n \n   Would someone from software please pick up WATS line 54. \n \n\n If it was after hours, we in the lab would simply hear the WATS line ring. \n\n No mater what time it was, we answered. \n\n There were about a dozen of us on the programming staff.  We’d look up at the nearest phone and see the blinking light labeled “ 54 ”.  Whoever was closest to a phone would pick up that line.  If it was me, I’d say: \n\n \n   Teradyne Central Software: This is Bob Martin \n \n\n And then we’d proceed to listen to the issue, and we’d advise the customer what to do. \n\n Sometimes, of course, it was cockpit error, which we could quickly correct.  Sometimes it was a known flaw in our system for which we could communicate a workaround.  And sometimes it was a new defect or problem that we had to diagnose on the spot. \n\n One way or another we stayed on the phone with the customer until the problem was resolved. \n\n Responsible Engineers. \n\n You might be asking yourself why we didn’t have a customer service department handling those calls; and entering defects into a defect tracking system.  The answer to that is simple.  We felt  responsible  for the system.  We wanted to  know  what our customers were experiencing.  We didn’t want a layer of people insulating us from the problems that  we  were creating in the field. \n\n We had a term at Teradyne:  Responsible Engineer .  That was the subhead under the signature line on every Engineering Change Order.  We  signed  for the changes we made.  We were the  Responsible Engineers . \n\n That term had meaning to us.  We  felt  responsible.  And so we did not want anything insulating us from the real world of our customer’s plight. \n\n At Teradyne, we did our own QA.  We did our own devops.  We did our own customer service.  And we frequently traveled to customer sites to work with the Field Service engineers. \n\n In fact, it was common practice for each software developer to spend a day or two riding along with a telephone repairman; just so we could understand what these guys were up against, and how they  really  used our system. \n\n Insulation \n\n Modern software development teams are often highly insulated.  They live in a world free from the distractions of customers and their “petty” problems.  There are whole groups of people who serve to insulate developers from the real world.  Customer Service.  Q/A.  Devops.  You name it.  And why do these groups exist?  They exist because each of these are areas where software developers have failed so badly at that companies have had to defend themselves by creating whole new departments and management structures. \n\n I think that’s a shame.  How can you be a software craftsman if you don’t communicate with your real customer?  How can you be a software craftsman if you don’t directly experience the nightmares you are creating for devops?  How can you be a software craftsman if you leave all your bugs for QA to find? \n\n Software Craftsmanship \n\n It seems to me that a software craftsman is a  Responsible Engineer .  A Software Craftsman should never be insulated from the real world of the customer, of devops, of QA, or of anything else.  The responsibilities of a team of software craftsmen should include QA; should include devops; should include customer service.  And every member of that team should be able to cover for every other member. \n\n There’s nothing wrong with specialization.  There is a  lot  wrong with  insulation . \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/10/14/VW.html", "title": "VW", "content": "\n       Do you know the name  Michael Horn ?  He’s the CEO of  Volkswagen of America .  You know what’s going on with Volkswagen, right?  Dieselgate?  The fact that the software that controls the Diesel engine in some of their cars was specifically written to defeat emissions tests?  Yeah, apparently that software could detect when an emission test was being run, and could put the engine into a mode where it emitted one fortieth of the noxious nitrogen oxides of it’s normal operation. \n\n So a few days ago Michael Horn is testifying before congress about this issue.  Do you know what he said?  He told the lawmakers the following: \n\n \n   _“This was a couple of software engineers who put this in for whatever reason,” _ \n \n\n You can hear him say this at 1:11:40 into  this  recording. \n\n Now, never mind that nobody in their right mind believes this asinine statement.  The effects may be profound. \n\n \n   \n     It is now a matter of congressional record that software developers can be used as scapegoats.  This particular accusation may not be believable; but the next one might be.  Indeed; once executives improve their game a bit; they may be able to set up far more convincing evidence of programmer malfeasance. \n   \n   \n     The public has been made aware that programmers can be culprits.  This will make it more likely that the next time something goes wrong – a plane crash, a fire, a flood – that the public will jump to the conclusion that some programmer caused it.  Yes, this is a stretch; but it wasn’t so long ago that the concept of programmer implication in disasters was non-existent. \n   \n \n\n But that’s just my paranoia talking.  Yeah, I worry that the public, and the politicians, are one day going to realize that they depend upon programmers far too much, and regulate them far too little.  You’ve heard me preach about this before. \n\n The Real Issue. \n\n Never mind all of the above.  There is a much deeper problem here. \n\n Some programmers  did, in fact, do this . \n\n That they did it without authorization is neither likely, nor relevant.   They did it . \n\n It doesn’t matter that their bosses told them to do it.   They did it. \n\n It doesn’t matter that they were paid to do it.   They did it. \n\n I suppose you could make the argument that these programmers did not know what they were doing.  That they were simply given some specs, and they implemented those specs, and didn’t know that they were accomplices in a case of massive fraud. \n\n I think that argument is even more asinine than Michael Horn’s.   They knew .  And if they didn’t know, they  should  have known.  They had a responsibility to know. \n\n If we had a real profession, those programmers would be brought before that profession, investigated, and if found guilty, drummed out of the profession in disgrace. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/10/30/FutureProof.html", "title": "Future Proof", "content": "\n       I recently read Christin Gorman’s blog  Future Proof .  In it she lambastes the idea that you can create code that is protected from future change. \n\n \n   Awesome or not, some things are impossible.  Cordless garden hoses are impossible. So is software that changes without being changed. If you need it to adapt to future requirements, then guess what: that involves adaptation – AKA change. \n \n\n She’s right about that, of course.  When the requirements change, either some code, or some data is going to have to change.  The Open-Closed Principle cannot apply to every element of the system.  No matter how open you make your system to extension, something, somewhere will need modification. \n\n ###External Data \n\n Indeed, Gorman rants rather poetically about the notion that changes to an .xml file, or a database table, are somehow better than changes to source code. \n\n \n   Moving changes outside the code itself does in NO WAY stop you from having to make changes. It does however create extra complexity in the code, while limiting the types of changes you can make. It also makes it much harder to track the changes made, who made them, when were they made and by whom. So if you like the idea of allowing random changes in your production environment, with limited accountability or ability to keep track of what’s going on – by all means move all your settings to your database, or properties files. Have fun. \n \n\n At this point you are probably saying to yourself: “Yeah, but…” \n\n Let’s explore that “but…” \n\n In most systems there are some system parameters that vary between different installations.  Such parameters clearly belong outside the code.  We don’t want to have to compile different versions of the code for different customers[1]. \n\n In fact most systems have certain parameters that the users want to change on a whim without involving the developers who compile the system.  Consider for example, a security system that sends a text message to a security guard whenever there is a security event.  It would be a real shame if the supplier of that security system had to recompile and redeploy their code every time the security guard’s phone number changed. \n\n So, for the sake of argument, let’s assume that Gorman would acknowledge that there are some data elements that belong outside of the application and in a text file, or an xml file or a properties file.  What rule can we use to tell which data elements should be inside the code, and which should be outside the code. \n\n The answer to that is trivial.  Anything that the programmers don’t want to be bothered with on a regular basis should be outside the code.  What kinds of things bother the programmers?  Things that change frequently. \n\n So our rule is:  Anything that changes frequently should be outside the code. \n\n ###Decoupling Modules \n\n Gorman continues. \n\n \n   If you want a future proof system, you don’t want immortal and flexible code. You don’t want the T1000 terminator. You want Southpark’s Kenny. You need code that’s easy and fun to kill. You need to get used to killing it, often, so you can replace it with whatever you end up needing. \n \n\n This is generally very good advice.  It is better to create a  changeable  system than to try to protect all parts of the system from change.  However, there are issues with creating code that’s easy to change (or “kill”). \n\n If killing or modifying a particular module causes many others modules to break, either at compile time, or at test time, then the  cost  of making that change (the impact) is going to be high.  Sometimes it can be  very  high.  I have worked on systems where the impact of certain changes was so prohibitive that they were delayed for years. \n\n What causes this to happen?  Why does changing certain modules affect others?  Dependencies, of course. \n\n When one module depends upon the internals of another; then when those internals change, both modules will require changes.  Dependencies like this can propagate through a system making it very hard to change.  So certainly we’d like to mitigate this by somehow decoupling the modules that need to be changed, from the rest of the system. \n\n Which modules should be decoupled?  I think the rule is similar to the previous rule:   Any module that changes frequently should be decoupled from the rest of the system. \n\n How do you decouple one module from another?  That depends on the level of decoupling you need.  Sometimes simply extracting that code into a separate function is enough.  More often, it’s better to move all the related code into a separate class, and even a separate source file.  And in extreme cases, you want to put those classes behind polymorphic interfaces. \n\n ###Interfaces\nAnd this is where I part company from Gorman to a certain extent.  Because she goes on: \n\n \n   Java interfaces are meant to be used when there are a bunch of implementations available, and your code wants to access them all in the same manner. \n \n\n And again… \n\n \n   Interfaces with only one implementation are the committees of code. If you don’t want to make a decision yourself, if you’re worried about being blamed if it was wrong, you delegate it to a committee. \n \n\n Actually she rants much longer about this; and even refers to the satirical  EnterpriseFizzBuzz  as an example of code that is “not far from the reality out there”. \n\n In general I agree that using polymorphic interfaces without good reason is overkill.  But I am not at all opposed to having interfaces with a single implementation.  Sometimes the decoupling that provides is exactly right for isolating a module that changes frequently.  One should not look at Enterprise Fizz Buzz and conclude that interfaces should be avoided at all costs. \n\n Tests \n\n Finally, as she continues her rant against interfaces, Gorman asserts that interfaces don’t make testing easier. \n\n \n   Some will tell you that interfaces are great for making your code testable.  No they aren’t. They do no harm, but they don’t help either. You don’t need interfaces to create mocks or stubs or spies. Use Mockito or any other sensible mocking framework, and you can easily create mocks for concrete classes. You should also ask yourself why you need those mocks or stubs or spies – with a little rework of your code, you might be able to write tests with very little mocking: \n \n\n I can sympathize with that last point.  I think that many software teams use mocks more than they should.  I use mocks with a certain parsimony.  I will mock; but usually only across significant architectural boundaries.   I don’t mock every class and every function.  But when I am facing a significant boundary, mocks are very useful, and therefore interfaces become essential. \n\n About Gorman’s middle point, that you can always use a mocking tool like Mockito in order to avoid interfaces,  I’ll say two things. \n\n \n   \n     The choice to use a tool, like Mockito, should not be motivated by a resistance to interfaces.  Interfaces should not be actively avoided.  Being restrained is not the same as being repelled. \n   \n   \n     I don’t often use mocking tools because I write my own mocks.  So I find interfaces quite helpful. \n   \n \n\n That last point may strike some of you as odd; but it’s true.  Mocks are very easy to hand write; and hand written mocks can be given nice names, and placed in nicely named packages, and nicely named source files, and nicely named directories.  Hand written mocks don’t pollute your setups with random sequences of dots and parentheses.  So, unless I need a mocking tool’s super powers, I tend not to use them. \n\n So in the end, I don’t completely agree with Gorman’s initial assertion.  Interfaces may not always make testing easier; but at certain boundaries they are absolutely essential. \n\n ###Bottom Line\nFor the most part, I think Gorman made some good points.  The goal is not to “Future Proof” your code.  The future will aways find a way to thwart you. \n\n However, that doesn’t mean that you shouldn’t arrange your code to minimize the impact of frequent change.  And if you can do that by externalizing certain data elements, and putting certain modules behind polymorphic interfaces, there’s no reason you shouldn’t.   Decoupling shouldn’t be gratuitous; but it’s not something to actively avoid.  Indeed, strategic decoupling in moderation is a very, very good thing. \n\n Finally, Gorman herself acknowledges that if you don’t make things easy to change, the cost can be very high. \n\n \n   _ In Norway our parliament voted to change our criminal laws in 2005. But they have only now (2015) been put into full effect, because the police’s computer systems prevented them from applying the new rules._ \n \n\n \n\n [1] Believe it or not we used to do this all the time.  Each installation would have a separately compiled version of the code, complete with their own configuration data that was compiled into the application.  We wound up with an entire group of  configuration engineers  whose job it was to keep the configuration data, software versions, and compiled binaries straight.  It was a nightmare. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/11/18/TheProgrammersOath.html", "title": "The Programmer's Oath", "content": "\n       In order to defend and preserve the honor of the profession of computer programmers, \n\n I Promise that, to the best of my ability and judgement: \n\n \n   \n     I will not produce harmful code. \n   \n   \n     The code that I produce will always be my best work.  I will not knowingly allow code that is defective either in behavior or structure to accumulate. \n   \n   \n     I will produce, with each release, a quick, sure, and repeatable proof that every element of the code works as it should. \n   \n   \n     I will make frequent, small, releases so that I do not impede the progress of others. \n   \n   \n     I will fearlessly and relentlessly improve my creations at every opportunity.  I will never degrade them. \n   \n   \n     I will do all that I can to keep the productivity of myself, and others, as high as possible.  I will do nothing that decreases that productivity. \n   \n   \n     I will continuously ensure that others can cover for me, and that I can cover for them. \n   \n   \n     I will produce estimates that are honest both in magnitude and precision.  I will not make promises without certainty. \n   \n   \n     I will never stop learning and improving my craft. \n   \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/10/16/Agile-And-Waterfall.html", "title": "Agile is not now, nor was it ever, Waterfall.", "content": "\n       I read  Agile is the new Waterfall  at first with disgust, then with horror, and then finally with a meager amount of very qualified approval. \n\n The author makes a reasonable point towards the end; but in getting to that point he states a number of falsehoods and eventually discredits a philosophy and discipline that does not deserve it.  He is close to throwing  everything  out with the bathwater. \n\n He begins by claiming that no sane person advocates waterfall.  I don’t know what universe the author lives in; but in this universe there are quite a few people who advocate waterfall.  Are they sane?  By any legal standard they are.  Anyone who thinks that the battle against waterfall is over simply hasn’t been fighting in the right trenches. \n\n If you want to get a feel for just how wrong the author is about this, just google  “Waterfall Software Teams”  and count the number of articles that talk about striking a balance, or mixing the two processes, etc.   People are not anxious to give up on the past. \n\n Next he quotes MacBeth’s lament over the futility of life, and equates it with the “the promise of Agile” \n\n \n   “…full of sound and fury, signifying nothing.” \n \n\n I take rather profound exception to the idea that the events leading up to the Agile Manifesto, and the Manifesto itself are an example of futility and meaningless noise.  The author wasn’t there.  The author doesn’t know.  While I agree that in certain circles there is more heat than light; to claim that the entire movement is insignificant is to ignore a vast swath of software history. \n\n I’m not going to critique the author point by point.  Suffice it to say that he knows very little of the history, and what little he does know he’s gotten wrong.  In so doing he has cast a pall of disrespect over a large number of people who have made huge contributions to our field.  A pall that discredits an ideology that has had a profoundly positive effect. \n\n He rails against the Agile consultancies who try to help organizations make the shift to Agile.  Some of his complaints are justified; but most are not.  Changing an organization is hard!  Those companies that try to change, and hire help to make that change, are courageous. \n\n Are there Agile consultancies that are better than others?  Yes.  Certainly.  Caveat Emptor!  But denigrating the entire effort is simply ignorant. \n\n The One Point. \n\n The author is wrong about Agile in virtually every regard.  But he does make one good point.  Unfortunately the context in which he makes that point is so wrong that the point is almost lost in the cacophony of blather that surrounds it.  That point is: \n\n \n   “Bring in the bare minimum amount of process.” \n \n\n Yes!  Of course! \n\n Does every software team need the entire suite of agile practices?  Of course not.  But let’s look at them: \n\n \n   \n     The Planning Game.  Over the years it has become very clear that there are many ways to shave this Yak.  Some teams need more process around this than others.  For some, a simple list of features will do. For others, a Kanban board will be sufficient.  Still others will need the full suite of stories, and tasks, and releases, and story points, and…   Well, you know.  Choose wisely! \n   \n   \n     Customer Tests.  Lots of customers don’t want to be bothered with these tests.  That’s a shame, since they are demonstrably the best way to specify requirements.  For those teams that have customers engaged enough to specify the requirements in terms of  Cucumber  tests, or  FitNesse  tests there is no better alternative.  Teams that are not so fortunate are not likely to benefit from this practice.  My personal rule is:  If the customers neither read nor write the tests, then high level unit tests written in code suffice. \n   \n   \n     Small Releases.   It’s hard to imagine a team that would not benefit from this practice.  Keep the releases small.  The more time you wait between exposing the customers to the system, the more can go wrong. \n   \n   \n     Whole Team.   Again, it’s hard to imagine a team that would not benefit from a close relationships between the business people, and the developers.  Not all teams are so fortunate, of course. \n   \n   \n     Collective Ownership.  As far as I’m concerned any team that has individual code ownership is deeply dysfunctional.  If the owner of some part of the code decides to leave, the whole team is left in crisis mode.  There are many ways to achieve collective ownership, but the bottom line is very simple.  No single individual should be able to hold the team hostage.  Every part of the code should be known by more than one person – the more the better. \n   \n   \n     Coding Standard.   This simply goes along with Collective Ownership.  The code should look like the team wrote it, not like one of the individuals wrote it.  The members of the team should agree on the way that their code will appear.  This isn’t rocket science. \n   \n   \n     Sustainable Pace.   This is a real simple idea.  Software projects are marathons, not sprints.  You dare not run at a rate that you cannot sustain for the long term.  Murphy tells us that any team that violates this practice is doomed to flame out at the worst possible moment. \n   \n   \n     Continuous Integration.   Certainly there are teams who’s projects are so small that setting up a CI server is redundant.  However, for most teams this is such a positive win that neglecting it would be immoral, if not insane. \n   \n   \n     Pair Programming.   Some teams benefit greatly by using this practice.  Others do not.  For the latter, some form of code review is likely necessary.  In any case, it is a  very  good idea for every line of code to have been seen by more than one pair of eyes. \n   \n   \n     Simple Design.   If we learned anything in the ’90s it is that over-design is suicide.  The level of design is team dependent, of course; but the simpler the better is simply a good rule of thumb. \n   \n   \n     Refactoring.   Does anybody really want to argue that programmers should not keep their code as clean as possible?  Does anyone want to argue that code should not be improved with time?  Teams may choose different degrees of refactoring; but zero is probably not acceptable. \n   \n   \n     Test Driven Development.   This is certainly the most controversial of all the Agile practices.  But the controversy is not about the word  Test .  Virtually everyone agrees that writing unit tests is important.  Some of us think that the order in which they are written is important too.  Different teams will choose different strategies.  But teams that ignore testing are not destined for rapid success. \n   \n \n\n Conclusion \n Does every team need every one of these practices?  Certainly not.  Do most teams need at least some of them?  Of course they do!  Again, choose wisely! \n\n I believe that the author of the original article was exposed to teams who were doing  Flaccid Scrum  and made the mistake that that’s all there was to Agile.  He is correct that there have been some uninformed consultancies who have taught this poor variant of the Agile practices.  In that sense his diatribe is understandable.  Still, ignorance is no excuse.  If you are going to impugn the character of good people and good ideas, you’d better do your damned homework. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/11/01/PlierForce.html", "title": "The Force of Pliers", "content": "\n       I was sitting on the throne this morning, my iPhone in my hand, idly perusing the stream of random blather that is Facebook; when my eyes landed upon  this post  by Kevlin Henney. \n\n As I followed his link, and then the next, and the next, I began to realize the dreadful truth; the horrible situation we are all in.  The blood drained from my face.  My body shook.  The iPhone nearly fell into – nevermind. \n\n The implications of this are no less than  galactolyptic . \n\n \n   In the statement  5 X 3  we don’t know which number is the multiplier and which is the multiplicand! \n \n\n Holy Mathematical Syntax Batman!  I thought this was solved years ago.  When I was in third grade my teacher, Mrs. Moe, told me that the first number was the multiplicand and the second was the multiplier.  That  5 X 3  meant  5 + 5 + 5  and not  3 + 3 + 3 + 3 + 3 .  In the following 54 years, I have not had any reason to doubt that fact.  Indeed, I have written many computer programs that used that labeling. \n\n \n   int multiply(int multiplicand, int multiplier) \n \n\n Kevlin argues that there is no order since both 5 and 3 are simply factors.  But that flies in the face of history!  I mean, the words  multiplier  and  multiplicand  were chosen for a  reason !  After all, look at the other operators: \n\n \n   1 + 2 :  1  and  2  are both  addends . \n   1 / 2 :  1  is the  dividend  and  2  is the  divisor . \n   1 - 2 :  1  is the  minuend  and  2  is the  subtrahend \n \n\n Notice how those wise men communicated the commutativity of addition by giving both operands the same name!  But they did  not  do that for multiplication because they understood that the two operands of multiplication are not interchangeable, and should be distinguished from one another. \n\n However, not one to rely on past third grade teachers, or Kevlin Henney, for matters of such galactic import, I began to do some research.  I found authoritative source after authoritative source,  Britannica ,  Merriam Webster ,  Wikipedia ,  Dictionary.com ,  Ask.com ,  Wolfram Alpha ,  The Onion , you name it!  Each source confidently declared, asserted, demanded, and bloviated about which operand was the multiplier and which was the multiplicand.  But they all said something different! \n\n Great heavens above,  The Internet did not agree! \n\n In fact I found three completely contradictory rules. \n\n \n   The first number is the multiplicand. \n   The second number is the multiplicand. \n   The larger of the two is the multiplicand. \n \n\n (That last rule is likely written by some old PDP-8 programmer.) \n\n This is hair raising.  Which is it?  No one seems to agree! \n\n But perhaps we can apply logic.  Perhaps, by the force of sheer reason and intellect, we can work out the proper ordering of these two parameters.  If not; then we must henceforth eliminate the two terms and simply call the parameters of multiplication  factors . \n\n Oh what a dry and colorless place this world would be without the joyful melody, the childlike impudence, of “multiplier” and “multiplicand”. \n\n So let us examine this statement in intense detail: \n\n \n   5 X 3 \n \n\n If we say this in  English  we get: \n\n \n   Five times three . \n \n\n Aha!  Now we see the problem.  The associativity is ambiguous.  This might mean: \n\n \n   five (times three) \n \n\n or it might mean: \n\n \n   (five times) three . \n \n\n And as we all know, English verbs need not be associative.  Right?  I mean  (I dug) a hole  parses differently than  I (dug a hole) . \n\n I guess the real question is this.  In the sentence:  Five times three  which is the subject and which is the predicate? \n\n Now, unless you are Yoda, subjects come first.  So we should parse this as “(Five times) three”  Right?  I mean the subject applies the verb to the direct object. \n\n To make this clearer, try saying it the way Yoda would say it.  First say “ I go to the store ” the way Yoda would.  Feel the rhythm of it.  “ To the store I go. ”  Note the odd inflection on the word “ store ”.  In yodese, that inflection is the denotation marker of the direct object.  The nature of that inflection is three tones emitted in rapid succession while vocalizing the vowel.  “ To the sto-o-ore, I go .”  The three tones are connected in an arch that starts at the frequency of ‘the’ then rises a full note, and then descends a half note. \n\n Say it over and over.  “ Sto-o-ore ”.  The intonation is almost like you were in the middle of asking a question, but then realized the answer and finished with a conclusion. \n\n OK, now, try that with  five times three .  You should get  Thre-e-e, five times .  Can you hear the difference? \n\n So  five  must be the multiplier!  My third grade teacher was wrong.  Yoda be praised!  We know the answer! \n\n \n \n\n But wait.  Something just occurred to me.  The word “ times ” is not a verb. \n\n \n   OH!  MY!  GOD! \n \n\n The word “ times ” is not a  verb .  It’s a preposition – a goddam  preposition !  The bane of all English students.  The enigma of the English Language.  The most ambiguous of all the parts of speech.  The part of speech that almost all students are confused by, and go to detention for. \n\n The word itself is an oxymoron.  A  pre   position .  A position before any other positions.  What kind of nonsense is that?  If we took that literally, all prepositions would come before the Big Bang; and we know that  nothing  came before the Big Bang, because that’s when time started; or at least became non-imaginary… \n\n Anyway, the sentence: “Fives times three.” is not a sentence at all; it’s a – I’m not sure I can say it –  prepositional phrase ! \n\n And here’s the thing about prepositions… They aren’t transitive.  They don’t convey action.  They don’t separate a subject from a predicate.  They are (gasp)  order independent …  I mean, FTW (for the win) can be completely inverted! \n\n OK, deep breath.  Is there a way that we can convert  5 x 3  into an English sentence, without the preposition?   Can we create a true sentence with a subject and a predicate, where the predicate has a verb and a direct object?  Let’s try this: \n\n \n   Five is multiplied by 3. \n \n\n Now  that  is a sentence.  “ Five ” is the subject.  “ is Multiplied by three ” is the predicate.  And in that predicate, “ is Multiplied by ” is the verb phrase, and “ three ” is the direct object. \n\n Oh, Hallelujah, we have a sentence.  A parseable, meaningful, unambiguous sentence. \n\n And it even works when Yoda says it: “ By thre-e-e is five multiplied ”. \n\n In fact when Yoda says it, the meaning is even clearer.  Five is the number being multiplied.  Three is the number by which five is multiplied.  It is the multiplication of five, and the number of multiplications is three. \n\n When multiplying five by three, three shalt be the number of thy multiplying, and the number of they multiplying shalt be three.  Thou shalt multiply the number five, and five shalt be the number that thouest multiply – and fill the Earth.    Eayesudominay! \n\n Therefore,   Five is the multiplicand and three is the multiplier .  Mrs. Moe was right after all! \n\n Hurray for third grade teachers! \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2015/11/27/OathDiscussion.html", "title": "Prelude to a Profession", "content": "\n       In my previous blog,  The Programmer’s Oath , I introduced the concept of an ethical oath for programmers.  In this article I want to provide the rationale for this oath.  Why do I think the concept of an oath is important? \n\n Many programmers are likely to respond negatively to the  idea  of an oath.  A few year ago I would have been strongly against it as well.  An oath implies a surrender of freedom.  An oath implies a lack of trust.  An oath implies that programmers must have limits.  And we, being a traditionally independent breed, don’t like the idea of such constraints.  We want to be left alone to do what is right in our own eyes. \n\n And besides, there is no real consensus amongst programmers.  We disagree on languages, on editors, on disciplines, on testing, on documentation on process, and much more.  How can such an unruly, independent bunch ever find enough common ground to define an  oath ? \n\n But the social and political landscape has recently changed very dramatically.  I now believe that we are on the threshold of a crisis of confidence that, if allowed to proceed unchecked, will profoundly and negatively change our industry.  To preserve our freedom of action, our independence, our authority and self direction, there are certain promises we need to make to society at large. \n\n Society knows who we are. \n\n In the 50’s and 60’s society knew very little about us; and we had no impact on daily life.  In those days people knew that computers existed; and that there were people called computer programmers; but we were just egg-heads in laboratories and nothing to be concerned about.  We were part of that mystical thing called “progress” that everyone just knew was going on. \n\n In the ’70s and ’80s everybody probably knew someone who knew a computer programmer.  There were hundreds of thousands of us; perhaps millions.  There were college courses in  Computer Science .  Parents knew enough about us to encourage their children to get into the programming field; but we were still generally irrelevant to daily life. \n\n I remember one commercial for laundry detergent from the era.  A husband and wife were shopping when they met a neighbor.  The husband was a nerd.  Glasses, pocket protector, and calculator, etc.  He was using his calculator to compute price per pound.  The neighbor talked  woman to woman  with the wife explaining that price wasn’t everything, blah, blah.  The programmer was naive in the ways of “the real world”.  He could not stand before the common sense of the neighbor. \n\n By the ’90s computers were showing up in appliances.  Programmers were everywhere.  Everybody knew one or two.  Programmers were part of middle class society.   Indeed, they were well enough known to become villains. \n\n The movie  Jurassic Park  introduced us to the prototypical programmer villain: Denis Nedry. \n\n Programmers were sometimes heroes too.  Sandra Bullock played a meek kind of heroine in  The Net . \n\n Then came  The Matrix . \n\n Then came the  DotCom  financial crisis.  Steve Jobs and Bill Gates were household words.  Everybody knew programmers who had made millions.  Everybody knew programmers who had lost their shirts.  More importantly, everybody knew that computer programmers played a central role in a huge financial boom/bust cycle. \n\n Programmers had become relevant.  Society began to recognize us as a necessary constituent. \n\n Do I need to mention the healthcare.gov debacle?  That was a software failure that nearly toppled a major public policy change.  Programmers have become known to government.  Indeed, for a brief moment, government officials considered creating a cabinet position, reporting to the President, to monitor and regulate software. \n\n I was recently in Stockholm visiting Mojang, the makers of Minecraft.  After a day of lectures we went for a beer.  We sat outside in a pleasant beer garden surrounded by a low fence. \n\n In the midst of our conversation a young American boy, all wide eyed, runs up to the fence, points at  Jens Bergensten , and asks:  “Are you Jeb?”   Jens, and several of the other programmers gave the boy their autographs, and chatted with him a bit.  He went away all starry-eyed. \n\n Programmers are now heroes to young children, who aspire to be like them.  We have become  role models . \n\n But the pinnacle was achieved only a few weeks ago when the CEO of Volkswagen America blamed some programmers for writing the code that allowed certain cars to cheat the EPA emissions standards. \n\n Society now knows that we can be villains, heroes, role models, and scapegoats.  Society knows that we play a profound role.  Society knows that our errors can cause disasters. \n\n A Crisis of Dependence \n\n And yet, given all of the above, Society still does not truly understand just how much it depends upon us.  And we programmers don’t truly understand it either.  But consider this:  People in our society interact with software  every single second of every single day .  There are hundreds of millions of lines of code running in the walls of our homes, in our appliances, in our automobiles, in our watches, in our phones, in our smoke alarms, in our burglar alarms, in our garage door openers, and even in our light switches. \n\n Nothing  happens in our society unless it is mediated by software.  No commercial transaction takes place.  No law gets enacted or enforced. No surgery is performed, No drug is created.  No plane flies.  No car starts.  No alarm clock rings.  No groceries get bought.  No soccer game is played.  No telephone rings.  The lights don’t turn on.  Without software –  nothing works . \n\n The depth of our dependence upon software is complete.  Our society requires software in order to function; even at the most detailed level.  Without software, quite frankly, and quite literally,  we all die . \n\n A Profession. \n\n And so, here we are.  We rule the world.  We write the rules that make the whole world work.  Other people think they write the rules; but then they hand those rules to us, and  we  actually write the rules that make the machines work.   We  rule the world. \n\n With that great power ought to come great responsibility.  And, indeed, society  will  hold us responsible when our actions result in disaster. \n\n And yet nothing binds us together as a profession.  We share no ethics.  We share no discipline.  We share no standards.  We are viewed, by our employers, as laborers.  We are tools for others to command and use.   We have no profession. \n\n This cannot continue.  If we do not form a profession on our own, then society will force it upon us – and define it for us.  And that will be good neither for society, nor for us.  We must get there first. \n\n Michael O. Church recently wrote a  powerful article  describing what such a profession might look like, and why it is important.  I strongly urge you to consider his words. \n\n He makes the point that a profession is based upon a shared standard of skill and ethics.  It is towards that latter point that I have offered  The Programmer’s Oath . \n\n Already the  discussion  about this oath has begun; and I welcome it.  We, as an industry should thoroughly discuss and debate our ethics.  If my meager attempt at an oath helps to spur that interaction, then it has served its purpose. \n\n But keep in mind that an oath is just the first step at defining an ethics; and an ethics is just the first step in defining a profession.  There is much more work to be done. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2016/01/04/ALittleArchitecture.html", "title": "A Little Architecture", "content": "\n       I want to become a Software Architect. \n\n \n   That’s a fine goal for a young software developer. \n \n\n I want to lead a team and make all the important decisions about databases and frameworks and web-servers and all that stuff. \n\n \n   Oh.  Well, then you don’t want to become a Software Architect after all. \n \n\n Of course I do!  I want to be the one who makes all the important decisions. \n\n \n   That’s fine, but you didn’t list the important decisions.  You listed the irrelevant ones. \n \n\n What do you mean?  The Database isn’t an important decision?  Do you know how much money we spend on them? \n\n \n   Too much probably.  And, no; the database is not one of the most important decisions. \n \n\n How can you say that?  The database is the heart of the system!  It’s where all the data is organized, sorted, indexed, and accessed.  Without it there would be no system! \n\n \n   The database is merely an IO device.  It happens to provide some useful tools for sorting, querying, and reporting but those are ancillary to the system architecture. \n \n\n Ancillary?  That’s crazy. \n\n \n   Yes, ancillary.  The business rules of your system may be able to make use of some of those tools; but those tools aren’t intrinsic to those business rules.  If you had to, you could replace those tools with different tools; but your business rules would still be the same. \n \n\n Well, yeah, but I’d have to recode them all since they all used the tools in the original database. \n\n \n   Well, there’s your problem. \n \n\n What do you mean? \n\n \n   Your problem is that you believe the business rules depend upon the tools of the database.  They don’t.  Or at least they shouldn’t if you’ve provided a good architecture. \n \n\n That’s crazy talk.  How can I create business rules that don’t use the tools that they have to use. \n\n \n   I didn’t say they don’t use the tools of the database; I said they shouldn’t depend upon them.  The business rules should not know what specific database you are using. \n \n\n How do you get the business rules to use tools without knowing about them? \n\n \n   You invert the dependency.  You have the database depend upon the business rules.  You make sure the business rules don’t depend on the database. \n \n\n You are talking gibberish. \n\n \n   On the contrary, I am speaking the language of Software Architecture.  This is the Dependency Inversion Principle.  Low level policies should depend upon high level policies. \n \n\n More gibberish!  The high level policies (I presume you mean the business rules) call down to the low level policies (I presume you mean the database).  So the high level policies depend upon the low level policies in the same way that callers depend upon callees.  Everybody knows this! \n\n \n   At runtime this is true.  But at compile time we want the dependencies inverted.  The source code of the high level policies should not mention the source code of the lower level policies. \n \n\n Oh come on!  You can’t call something without mentioning it. \n\n \n   Of course you can.  That’s what Object Orientation is all about. \n \n\n Object Orientation is about creating models of the real world, it’s about combining data and function into cohesive objects.  It’s about organizing code into an intuitive structure. \n\n \n   Is that what they told you? \n \n\n Everybody knows it.  It’s obviously true. \n\n \n   No doubt.  No doubt.  And yet, using the principles of object-orientation you can indeed call something without mentioning it. \n \n\n OK.  How? \n\n \n   You know that in an object-oriented design objects send messages to each other? \n \n\n Yes.  Of course. \n\n \n   And you know that the sender of the message does not know the type of the receiver. \n \n\n That depends on the language.  In Java the sender knows at least the base type of the receiver.  In Ruby the sender at least knows that the receiver can handle the message being sent. \n\n \n   True.  But in either case the sender does not know the exact type of the receiver. \n \n\n Yeah.  OK.  Sure. \n\n \n   Therefore the sender can cause a function to execute in the receiver, without mentioning the exact type of the receiver. \n \n\n Yeah.  Right.  I get that.  But the sender still depends upon the receiver. \n\n \n   At runtime, yes.  But not at compile time.  The source code of the sender does not mention, or depend upon, the source code of the receiver.  In fact the source code of the receiver depends upon the source code of the sender. \n \n\n Nahh!  The sender still depends on the class it’s sending to. \n\n \n   Perhaps some source code would make this clearer.  I’ll write this in Java.  First the package  sender : \n \n\n package sender;\n\npublic class Sender {\n  private Receiver receiver;\n\n  public Sender(Receiver r) {\n    receiver = r;\n  }\n\n  public void doSomething() {\n    receiver.receiveThis();\n  }\n\n  public interface Receiver {\n    void receiveThis();\n  }\n}\n \n\n \n   Next the  receiver  package. \n \n\n package receiver;\n\nimport sender.Sender;\n\npublic class SpecificReceiver implements Sender.Receiver {\n  public void receiveThis() {\n    //do something interesting.\n  }\n}\n \n\n \n   Notice that the  receiver  package depends upon the  sender  package.  Note also that the  SpecificReceiver  depends upon the  Sender .  Notice also that nothing in the  sender  package knows anything at all about the  receiver  package. \n \n\n Yeah, but you cheated.  You put the receiver’s interface in the sender’s class. \n\n \n   You are beginning to understand, grasshopper. \n \n\n Understand what? \n\n \n   The principles of architecture, of course.  Senders own the interfaces that the receivers must implement. \n \n\n Well if that means I have to use nested-classes then… \n\n \n   Nested classes are just one means to achieve an end.  There are others. \n \n\n OK, now wait.  What does all this have to do with databases.  That’s how we started this conversation. \n\n \n   Let’s look at some more code.  First a simple business rule: \n \n\n package businessRules;\n\nimport entities.Something;\n\npublic class BusinessRule {\n  private BusinessRuleGateway gateway;\n\n  public BusinessRule(BusinessRuleGateway gateway) {\n    this.gateway = gateway;\n  }\n\n  public void execute(String id) {\n    gateway.startTransaction();\n    Something thing = gateway.getSomething(id);\n    thing.makeChanges();\n    gateway.saveSomething(thing);\n    gateway.endTransaction();\n  }\n}\n \n\n That business rule doesn’t do much. \n\n \n   It’s just an example.  You’d likely have many classes like this, implementing lots of different business rules. \n \n\n OK, so what’s that  Gateway  thingy? \n\n \n   It supplies all the data access methods used by the business rule.  It’s implemented as follows: \n \n\n package businessRules;\n\nimport entities.Something;\n\npublic interface BusinessRuleGateway {\n  Something getSomething(String id);\n  void startTransaction();\n  void saveSomething(Something thing);\n  void endTransaction();\n}\n \n\n \n   Notice that it’s in the  businessRules  package. \n \n\n Yeah OK.  And what’s that  Something  class? \n\n \n   That represents a simple business object.  I put it in a package named  entities . \n \n\n package entities;\n\npublic class Something {\n  public void makeChanges() {\n    //...\n  }\n}\n \n\n \n   And then finally there is the implementation of the  BusinessRuleGateway .  This is the class that knows about the actual database: \n \n\n package database;\n\nimport businessRules.BusinessRuleGateway;\nimport entities.Something;\n\npublic class MySqlBusinessRuleGateway implements BusinessRuleGateway {\n  public Something getSomething(String id) {\n    // use MySql to get a thing.\n  }\n\n  public void startTransaction() {\n    // start MySql transaction\n  }\n\n  public void saveSomething(Something thing) {\n    // save thing in MySql\n  }\n\n  public void endTransaction() {\n    // end MySql transaction\n  }\n}\n \n\n \n   Again, notice that the business rules call the database at run time; but at compile time it is the  database  package that mentions and depends upon the  businessRules  package. \n \n\n OK, ok, I think I get it.  You’re just using polymorphism to hide the database implementation from the business rules.  But you still have to have an interface that provides all the database tools to the business rules. \n\n \n   No, not at all.  We don’t try to provide all the database tools to the business rules.  Rather, we have the business rules create interfaces for only what they need.  The implementation of those interfaces can call the appropriate tools. \n \n\n Yeah, but if all the business rules need all the tools then you just have to put all the tools in that  gateway  interface. \n\n \n   Ah.  I see that you still do not understand. \n \n\n Understand what?  It seems perfectly clear to me. \n\n \n   Each business rule defines an interface for just the data access facility that it needs. \n \n\n Wait.  What? \n\n \n   This is called the Interface Segregation Principle.  Each business rule class will only use some of the facilities of the database.  And so each business rule provides an interface that gives it access to just those facilities. \n \n\n But that means that you’re going to have lots of interfaces, and lots of little implementation classes that call other database classes. \n\n \n   Ah, good.  I see you are beginning to understand. \n \n\n But that’s a mess, and a waste of time!  Why would I do that? \n\n \n   You would do that in order to be clean, and save time. \n \n\n Oh come on.  That’s just a lot of code for code’s sake. \n\n \n   On the contrary, these are the important architectural decisions that allow you to defer the irrelevant decisions. \n \n\n What do you mean by that? \n\n \n   Remember that you started by saying that you wanted to be a Software Architect?  You wanted to make all the really important decisions? \n \n\n Yes, that’s what I want. \n\n \n   Among those decisions that you wanted to make were the database, and the webserver, and the frameworks. \n \n\n Yeah, and you said those weren’t the important decisions.  You said they were irrelevant. \n\n \n   That’s right.  They are.  The important decisions that a Software Architect makes are the ones that allow you to  NOT  make the decisions about the database, and the webserver, and the frameworks. \n \n\n But you have to make those decisions first! \n\n \n   No, you don’t.  Indeed, you want to be allowed to make them much later in the development cycle – when you have more information. \n \n\n \n\n \n   Woe is the architect who prematurely decides on a database, and then finds that flat files would have been sufficient. \n \n\n \n\n \n   Woe is the architect who prematurely decides upon a web-server, only to find that all the team really needed was a simple socket interface. \n \n\n \n\n \n   Woe is the team whose architects prematurely impose a framework upon them, only to find that the framework provides powers they don’t need and adds constraints they can’t live with. \n \n\n \n\n \n   Blessed is the team whose architects have provided the means by which all these decisions can be deferred until there is enough information to make them. \n \n\n \n\n \n   Blessed is the team whose architects have so isolated them from slow and resource hungry IO devices and frameworks that they can create fast and lightweight test environments. \n \n\n \n\n \n   Blessed is the team whose architects care about what really matters, and defer those things that don’t. \n \n\n Nonsense.  I don’t get you at all. \n\n \n   Well, perhaps you will in a decade or so…  If you haven’t gone into management by then. \n \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2016/01/14/Stabilization.html", "title": "Stabilization Phases", "content": "\n       While sipping my morning coffee, and scrolling through facebook on my phone, I found myself inundated with updates by Tesla owners who were excited that their cars could now drives themselves into a garage.  My response to those facebook updates was perhaps a little cynical; but I told them all: \n\n \n   It will be a long time before I trust software to drive car I am in.  Because I –  know . \n \n\n What is it that I know?  I know just how hard it is to test software systems for every contingency.  And I know just easy it is to fool yourself that you have. \n\n And that started me thinking about how you should test the software for a self-driving car. \n\n And  that  started me thinking about how people  do  test software systems. \n\n And  that  finally started me thinking about  stabilization phases . \n\n You know what a stabilization phase is, don’t you?  A stabilization phase occurs at the end of a release.  Time is set aside to just  let the system run .  For a week, or a month, everybody just watches the system run.  They treat it like a sleeping baby.  They avoid loud noises, slamming doors, and loud conversation.  They tiptoe around, peeking in on it from time to time, hoping that it won’t wake-up and fail. \n\n OK, maybe that’s a little over the top.  – Maybe.  I imagine most teams who run stabilization phases actually work pretty hard to stress their systems.  At least I hope they do.  They should be running lots of data through the system under varying loads; including data that is malformed, and that has caused problems in the past. \n\n But here’s the thing about Stabilization phases: \n\n \n   We run them because we are afraid.  We are afraid because we are uncertain what the system will do. \n \n\n There is a certain dissonance between calling ourselves professionals, and being so uncertain about what we have created that we fear what it might do. One would expect a team of professionals to have a high degree of confidence and certainty. \n\n The longer a team wants a stabilization phase to run, the less certain that team is about the system.  The teams who only need a day are much more certain about their systems than the teams who want a week, or a month. \n\n The logical flaw here is that  execution time indicates quality .  But time is actually unrelated to quality.  Time simply raises  false  confidence. \n\n The behavior of the system in the stabilization phase has little to no bearing on the behavior of the system in production; because the data coming into the system in production is  entirely new data .  That new data may drive the system down pathways that the stabilization phase never executed. \n\n So stabilization phases are really just about creating false confidence.  They are a CYA strategy.  When the system fails in production, you can at least claim that you performed the due diligence of running the system for a month in the stabilization phase; thereby exonerating the team from the culpability of leaving a critical defect in the system. \n\n The crux of the issue is that stabilization phases exist because the development team has produced code that they are not certain of.  So they run the system for a month in order to create enough false confidence to counter that uncertainty. \n\n What the team  really  needs to do is attack their uncertainty directly.  Not by uselessly running the system for a month; but by correcting the deficits in their development process that led to their uncertainty.  Consider the following checklist: \n\n \n   Are you running coverage tools?  Do you check that every  if  statement and  while \tloop are covered?  Is the unit-test coverage close to 100%.  Do you need to drive it a bit higher by writing more unit tests? \n   Do you have automated acceptance tests written by (or at least validated by) the business and QA?  Is the coverage of these tests high enough?  Do you need to drive it higher by asking QA to consider more corner cases? \n   Do you have automated integration tests written by architects and development leads.  Do those tests stress the communications pathways between the components?  Do they check for corner cases, boundary issues, and timeouts?  Do they probe system behavior under varying loads? \n   If you have multiple threads, do you have a strategy for stressing those threads during your unit tests and acceptance tests?  For example, have you implemented tools that introduce random delays and random loads so that the chances of race conditions are magnified.  Better yet, are you gradually eliminating the possibility of race conditions by eliminating mutable state between threads?  Have you drawn all the message sequence charts and examined them for potential races? \n \n\n This checklist is just an example.  I’m sure you can think of more things to put on it.   The point is that it is better to be proactive about your uncertainty, than it is to be passive about it.  And stabilization phases are  passive . \n\n The goal of software teams who are currently using stabilization phases should be to increase their certainty over time, and thereby decrease the duration of their stabilization phases.  Decrease them from a month, to a week.  Then from a week to a day.  Then from a day to an hour. \n\n And then, finally, increase your certainty to the point that you can eliminate the stabilization phase  once and for all . \n\n \n Anecdote: \n \n   I recently test drove a Tesla.  It’s a fun car to drive.  I mean, really fun.  I tried the “auto-steer” feature, which is easily engaged by double clicking a button on the steering column.  The car happily informs you that the car is now driving itself; and warns you to keep your hands on the wheel.   Heed that warning! \n \n\n \n\n \n   The car did tolerably well when the road markings were visible, but seemed quite willing to plow me into a bunch of construction barricades too.  It is  not  safe to take your hands off the wheel or your eyes off the road.  To me, that makes the feature rather less than useless. \n \n\n \n\n \n   The salesman was sitting next to me.  At one point we were going 45mph towards the rear end of a car stopped at a red light.  The salesman said: “Trust the car.”  And I thought: “The hell I will!”  And  I  applied the brakes. \n \n\n \n\n \n   This technology is cute; but dangerous.   NEVER  “trust the car”! \n \n\n \n\n \n   Perhaps you can tell that the prospect of self-driving cars does not fill me with enthusiasm.  I shall continue to wonder just what those cars will do one second after midnight on February 29th. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2016/03/19/GivingUpOnTDD.html", "title": "Giving Up on TDD", "content": "\n       \n   Did you read Ian Sommerville’s  recent blog  about TDD? \n \n\n You mean the one where he says that he tried it for a few months and then gave up?  Yes, I read it. \n\n \n   Well?  What did you think? \n \n\n I think he gave up too quickly and too easily. \n\n \n   Well, he said he tried it for a few months.  Isn’t that long enough? \n \n\n Normally, it should be; but he said he was just using it for some home projects. I doubt he was using it 40 hours per week for a few months.  I suspect the actual number of hours he logged was relatively low; because I recognize the symptoms. \n\n \n   The symptoms?  What do you mean? \n \n\n The symptoms that most TDD novices experience in the first couple of weeks.  Sommerville spells them out pretty clearly.  His first is a classic – it even has a name: \n\n ###The Fragile Test Problem \n * Because you want to ensure that you always pass the majority of tests, you tend to think about this when you change and extend the program. You therefore are more reluctant to make large-scale changes that will lead to the failure of lots of tests. Psychologically, you become conservative to avoid breaking lots of tests. \n\n \n   This is a common problem? \n \n\n Sure.  And a very, very, old one. \n\n Forget about tests.  If you have one part of your system that breaks whenever you change another part of your system, what can you conclude about the design of that system? \n\n \n   Well, you’d probably conclude that the part that breaks is tightly coupled to the part that changes. \n \n\n Right.  And that’s what Sommerville is experiencing with his tests.  His tests are tightly coupled to his production code. \n\n \n   Well, sure, that’s normal isn’t it?  I mean, tests really do have to be coupled to the production code, don’t they? \n \n\n Not so coupled that they break all the time.  If many of your tests break every time you change the production code then you have over-coupled the tests to the code.  You have a  test design problem . \n\n \n   A test design problem? \n \n\n Yes.  You haven’t properly designed the interface between your production code and your tests. \n\n \n   Wait.  What?  There’s an _interface  between the code and the tests?_ \n \n\n Of course there is.  And that interface has to be  designed . \n\n \n   But isn’t the interface between the tests and the code just the low level functions inside the code that the tests are calling? \n \n\n They are if you want to have fragile tests.  But if you want to properly decouple your tests from your production code, you design an API for those tests. \n\n By the way, that API will also be the API that other layers of the system use to communicate. \n\n \n   Other layers? \n \n\n Yes.  The other layers of the system.  You  do  compose your system out of layers don’t you? \n\n \n   um… \n \n\n This is software design 102.  Compose your system out of independent layers that communicate through well defined interfaces. \n\n And this gets into another one of Sommerville’s complaints… \n\n \n   Wait.  Before you rush ahead, I need to understand your current point. \n \n\n OK. \n\n \n   You are saying that when you use TDD, you have to _design  the tests?_ \n \n\n Not just the tests.  You have to  DESIGN  period.  No matter what you are writing; whether a unit test, or an acceptance test, or production code, or a mock, or a stub,  you have to DESIGN. \n\n \n   But I thought TDD meant that you didn’t have to design. \n \n\n Yeah, and: “Love means you never have to say your sorry.”  What a bunch of horse hockey!  We are  programmers !  We  design !  We create structures with high cohesion and low coupling.  We manage dependencies.  We isolate modules.    WE.  DESIGN. \n\n \n   OK, and so what you are saying is that people who start using TDD forget to design? \n \n\n Sometimes they forget.  Sometimes they’ve been wrongly told not to design.  But most of the time they are so focussed on the new discipline that they don’t have room in their brains to think about design. \n\n This happens to all novices, no matter what the new discipline is.  When you first learn to drive you are so focussed on controlling the car that you can’t afford the brain power required to recognize a stop sign or a stop light.  That’s why we learn to drive with an experienced driver in the seat next to us.  It takes time to get comfortable enough with the controls to start engaging the parts of our brain that recognize stop signs and stop lights. \n\n \n   And you think this is what happened to Sommerville? \n \n\n I know it is.  I know this because developers who are experienced with Test Driven Development do not experience the  Fragile Test Problem . \n\n \n   OK, so you said this leads to another of Sommerville’s complaints. \n \n\n Absolutely. \n\n ###The Design Problem \n * The most serious problem for me is that it encourages a focus on sorting out detail to pass tests rather than looking at the program as a whole. I started programming at a time where computer time was limited and you had to spend time looking at and thinking about the program as a whole. I think this leads to more elegant and better structured programs. But, with TDD, you dive into the detail in different parts of the program and rarely step back and look at the big picture. \n\n \n   So what is the connection? \n \n\n It’s really the same issue. \n\n \n   How do you mean? \n \n\n As a novice, when you are focussed on the discipline of TDD, you don’t have room in your brain for a lot of design thinking.  That’s one of the reasons we push the notion of refactoring so much.  The idea that TDD “encourages a focus on sorting out detail to pass tests rather than looking at the program as a whole” is simply an artifact of being a novice. \n\n \n   But wait.  I mean, tests _are  all about detail, aren’t they?_ \n \n\n Sure.   Code  is all about detail.  But that doesn’t mean you aren’t thinking about the problem as a whole.  Nobody said that in order to practice TDD you have to abandon the big picture. \n\n On the contrary, Ron Jeffries, one of the original TDDers has repeatedly stressed:  “Act locally.  Think Globally.”   That’s good advice for any programmer. \n\n \n   So Sommerville was wrong about this too? \n \n\n No!  Not wrong.  This is exactly what anyone would experience as part of the learning curve of TDD.  It takes time and experience with the discipline to get past these hurtles.  More time than I believe Sommerville gave it.  I think he just gave up too soon. \n\n The bottom line is that  you must never abandon the big picture!   Sommerville was right about that. He was just wrong that TDD promotes that abandonment.  It’s being a novice with the discipline that promotes the abandonment of the big picture. \n\n \n   OK, so then what about his other complaints? \n \n\n ##The Testable Design Problem \n * It is easier to test some program designs than others. Sometimes, the best design is one that's hard to test so you are more reluctant to take this approach because you know that you'll spend a lot more time designing and writing tests (which I, for one, quite a boring thing to do) \n \n\n The first part of this complaint has an element of truth to it.  Some things are harder to test than others.  GUIs are hard to test.  Device drivers are hard to test.  Indeed just about anything that interacts with an IO device is hard to test.  So we have developed strategies for dealing with that.  Strategies like  The Humble Object  pattern. \n\n \n   The what?  The Humble what? \n \n\n The Humble Object pattern.  Michael Feathers and Gerard Meszaros wrote about this years ago.  Look it up. \n\n \n   OK, but you said you only agreed with the first part of his complaint. \n \n\n Right.  The second part is nonsense. \n\n \n   Really?  Nonsense?  Isn’t that kind of, um…  harsh? \n \n\n Not at all.  The notion that: “sometimes the best design is one that’s hard to test” is the highest order of drivel. \n\n \n   I can see that you aren’t backing down on your harshness. \n \n\n No, I’m not.  This is a very simple and important point.  Let me state it much more clearly. \n\n Something that is hard to test is badly designed. \n\n \n   Hmmm.  I’m not sure… \n \n\n Look.  Suppose you ask me to write an app to control your grandmother’s pacemaker.  I agree, and a week later I hand you a thumb-drive and tell you to load it into her controller.  Before you do you ask me: “Did you test it?”  And my response is:  “No, I chose a design that was hard to test.” \n\n \n   Hmmm.  Yeah.  OK.  I guess I see… \n \n\n Do you?  Are you sure?  Let me drive that home even more. \n\n Any design that is hard to test is crap.  Pure crap.  Why?  Because if it’s hard to test, you aren’t going to test it well enough.  And if you don’t test it well enough, it’s not going to work when you need it to work.  And if it doesn’t work when you need it to work the design is crap. \n\n ARE WE UNDERSTANDING THIS? \n\n \n   Yes.  I see your point.  Good designs are easy to test. \n \n\n Yeah.  Forget that, and all is lost. \n\n \n   OK, well, Sommerville had one last complaint. \n \n\n ##The Magic Bullet Problem \n * In my experience, lots of program failures arise because the data being processed is not what's expected by the programmer. It's really hard to write 'bad data' tests that accurately reflect the real bad data you will have to process because you have to be a domain expert to understand the data. The 'purist' approach here, of course, is that you design data validation checks so that you never have to process bad data. But the reality is that it's often hard to specify what 'correct data' means and sometimes you have to simply process the data you've got rather than the data that you'd like to have. \n\n Of course he’s absolutely correct.  My problem with this complaint is that I have no idea what it has to do with TDD. \n\n In effect Sommerville is saying:  “TDD doesn’t solve world hunger. So I’m giving up.” \n\n \n   Well, I’m not sure I’d go that far. \n \n\n It’s true that TDD is not going to help you defend against things you didn’t anticipate.  That’s not a failing of TDD because that’s not a promise that anyone has made about TDD. \n\n \n   TDD will not cure cancer. \n   TDD will not bring world peace. \n   TDD will not protect you from drunk drivers. \n   TDD will not bring sanity to American elections. \n \n\n \n   I think you should stop ranting about this. \n \n\n Yeah, OK, I just find it frustrating that anyone would give up on TDD because it doesn’t cure athlete’s foot. \n\n \n   I said stop ranting. \n \n\n OK.  OK.  Sorry.  Urgh. \n\n \n   So then is there no solution to this problem? \n \n\n I never said that.  I just said that TDD never  promised  to solve that problem. \n\n \n   So what can we do?  How can we protect ourselves from unanticipated data. \n \n\n Well, it’s not just unanticipated data.  It’s unanticipated  anything .  And the way to address that is to work hard at anticipating as much as possible.  That’s one of the reasons we have other people, like business analysts and Quality Assurance testers, write acceptance tests. \n\n \n   OK so you are saying that the solution to this problem is to get lots of people involved. \n \n\n Of course.  There really isn’t any other way.  And, by the way, even that will fail. \n\n \n   You aren’t offering a lot of hope. \n \n\n Look.  Apollo 1 caught fire.  Apollo 13 exploded half-way to the Moon.  Challenger blew up just after launch.  Columbia broke apart during reentry.  Why?  Because despite the thousands of brains trying to think of everything, something unanticipated happened. \n\n \n   So you are saying… \n \n\n Deal with it.  There will always be risk.  Don’t blame TDD, and don’t give up. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2016/01/15/Manhandled.html", "title": "Manhandled", "content": "\n       \n   Warning: Possible sexual abuse triggers. \n \n\n One of my regular bike-riding podcasts is  Astronomy Cast , by Dr. Pamela Gay and Fraser Cain.  Indeed, if you go to  cleancodeproject.com  you’ll see that  Astronomy Cast  is one of the charities on my list of favorites.  Make a contribution and I will send you a green  Clean Code  wristband, or coffee cup, or sweatshirt.  If you listen to  Astronomy Cast  you’ll also find that I am a sponsor. \n\n This podcast is always about science; and the science content is quite good.  It’s techie.  It’s geeky.  It’s right up my alley.  I’ve listened to almost every one of the 399 episodes.  If you like science – especially science about space and astronomy, this is a great resource. \n\n But yesterday was different.  Yesterday was  episode 399 ; and it was not about science at all.  It was entitled:  Women in Science ; and it was about –  sexual harassment . \n\n Not the big kind that gets reported.  Not the notorious kind that gets people fired.  Not that kind – though there’s enough of that to go around.  No, this was about  commonplace, everyday, normal  sexual harassment. \n\n Honestly, I didn’t know there was such a thing.  I’ve always thought that sexual harassment was anomalous behavior perpetrated by a few disgusting, arrogant men in positions of power.  It never occurred to me that sexual harassment was an everyday, commonplace, run-of-the-mill, what-else-is-new occurrence.  But I listened, aghast, as I heard Dr. Gay recount tales of it.  Tales of the kind of sexual harassment that women in Science regularly encounter; and have simply come to expect as a normal fact of life. \n\n As just one example,  [Time code 23:10]  Dr. Gay recounts:  “…because this was somebody who, when he was drunk, grabbed my boob at one point, as you do, at a conference, at a bar.” \n\n As you do?  As you do?  As  who  does?  My mind is still reeling from this. She made it sound like it was a regular, normal, occurrence.  Indeed, in a particularly gut-wrenching  blog  she wrote: \n\n \n   “ …we, like every other segment of society, have our share of individuals who, given the right combination of alcohol and proximity will grab tits and ass. I’ve had both body parts randomly and unexpectedly grabbed at in public places…“. \n \n\n Who does something like that?  What kind of low-life, scumbag, knuckle-dragging, troglodyte, behaves that way?  Drunk or not?  Didn’t their fathers ever teach them about respect? \n\n This is a huge disconnect in my mind.  When I go to conferences I do not have people walking up to grope me.  The fear that that might happen never enters my mind.  So, for me, this kind of thing doesn’t exist.  It’s not in my universe of experience. \n\n And yet, Dr. Gay says that when she goes to a conference she has to make sure her rear is against a wall, and her arms are folded in front of her chest, just to ward off the potential gropes from drunken dipshits  that she works with . \n\n I can’t imagine living like that – having to  tolerate  being  manhandled  – having to always be on guard against it.  Having to wonder whether merely  resisting  it was harming my standing in the community; and harming my ability to  earn a living . \n\n And it made me think: Could this be happening in the software community as well?   My community? \n\n Are women programmers being manhandled at their places of employment and at software conferences? Do they have to continually fend off unwanted pats, slaps, pinches, and gropes?  Do they have to worry that their careers could be damaged by speaking out and defending themselves? In order to get along in this community do women have to pretend that such actions are just clumsy,  harmless, fun-and-games instead of the wretched, disgusting actions that they actually are? \n\n So I did some research on this topic.  I was horrified by what I found.  The number of  recorded  events is large, and strikes very close to home.  I was mortified to find that it has taken place at conference that I have attended.  It turns out that women in our field have to deal with being  manhandled , in one form or another, on a regular basis. \n\n Here are just a few of the resources I’ve found: \n\n \n   \n     When Geeks Attack . More on this later. \n   \n   \n     The Ada Initiative . Now defunct, but contains a lot of history about harassment and abuse in the software field. \n   \n   \n     Geek Feminism: Timeline of incidents .  Much more detail about harassment episodes in software.  Hell, I  know  more than one of the harassers mentioned here… \n   \n \n\n Although these lists contains hundreds of events; they list only those that were reported.  If you read through a few of the references you’ll find an ubiquitous thread.  The women interviewed say things like: “Something like this happens to me two or three times a year at a conference.” or “This one was bad, but I’ve had worse.”  The undercurrent of these lists makes it plain that the lists are just the tip of a very large iceberg. \n\n One very disturbing aspect of my research was the finding that many descriptions of harassment are referenced by third parties; but the primary articles by the victims  and witnesses  have been taken down by their authors.  It’s as if the record of such occurrences has a lifetime; after which the information gradually disappears.  Some authors explain the removal of their descriptions by replacing them with a message that says, in effect, that they needed to write it; but that it hurts too much to leave the record intact. \n\n This voluntary deletion implies that the damage done by these reprehensible actions is so significant that the victims, and first hand witnesses, cannot tolerate leaving the record intact. \n\n I am convinced that most programmers do not engage in such overt harassment. I am also convinced, by the research I’ve done, that there are too many programmers and managers who do. Even more concerning is my fear that there are too many more who may turn a blind eye to it. \n\n Some of the descriptions are heart wrenching depictions of women being obviously groped by a co-worker or boss  in public ,  in plain view .  Sometimes someone intervenes; and  cudos to those folks . But it’s only  sometimes . \n\n \n\n There was a time when just under half of all programmers were women.  Today that number is less than 10%.   I’ve puzzled over this for years now, and even  written  about it. Why aren’t women becoming programmers?  But if this kind of stomach turning behavior is common in our community, then the puzzle is solved. No wonder women avoid the field!  No wonder at all.  I imagine that they find out, very early on, that to stay in this field is to have to submit to being regularly  manhandled  by too many of their co-workers and bosses. \n\n \n\n The graphic above shows that women started leaving our field in droves right around 1985.  Why would this be?  If they are being driven out by the bad behavior of their co-workers, did that bad behavior begin just as computers were becoming household appliances, and the demand for programmers shot through the roof?  Did it begin shortly after the average age of programmers began to decline by 10 years? \n\n Along those lines I found this statement from the  When Geeks Attack  article to be insightful. \n\n \n   …the stereotypically introverted programmer – think of the Mark Zuckerberg portrayal in The Social Network – has evolved into a far more cocksure, frat-house kind of figure. These so-called brogrammers have reason to believe they can act with impunity: The demand for computer engineers is outpacing virtually every other industry in the nation, according to the U.S. Bureau of Labor Statistics. That has translated into six-figure salaries and over-the-top perks like concierges, car service, and free gourmet meals. Wall Street’s masters of the universe have been supplanted by Silicon Valley’s big-swinging code jocks. And thanks to the anonymity of the Internet, fueled by a dogmatic belief that all speech is free speech, they have made the very act of being a woman in the industry something of an occupational hazard. \n \n\n Is that it?  Is it really about status and power?  Is it really about the corruption that power breeds?  Are there programmers who are so sure that they are indispensable, they feel immune from reproach?  Do their “incredible brains”, and their “mastery over frameworks”, make them feel entitled to fondle? \n\n If so, why aren’t women programmers sharing in this kind of status and power?  Why is it denied to them?  Is it because they don’t express power in terms of sexual harassment and abuse as much as men do? \n\n As I said of our industry in a recent  blog : \n\n \n   “Power has a gender, and that gender is male.” \n \n\n Is that because men are able to intimidate women through commonplace sexual harassment? \n\n Look back at that red line on the graphic.  Is that line the reciprocal of the amount of sexual harassment in our industry? \n\n Professionalism. \n\n I write a lot about professionalism.  So let me just say this for the record to all of you who want to be professional programmers: \n\n \n   \n     You  never  lay your hands on someone with sexual intent without their explicit permission.  It does not matter how drunk you are.  It does not matter how drunk  they  are.  You  never ,  ever  manhandle someone without their very explicit consent.  And if they work for you, or if you have power over them, then you must  never  make the advance, and must  never accept the consent . \n   \n   \n     What’s more: if you see harassment in progress, or even something you  suspect  is harassment, you intervene!  You stop it!  Even if it means you’ll lose a friend, or your job,  you stop it! \n   \n \n\n One last thing: \n\n To employers, team leads, and managers:   No one  is so indispensable that you should tolerate, or cover up, this kind of behavior.  If it happens, and you can prove it, quick termination should be the result. \n\n To the rest of you: If your employers do not act decisively in such matters,  quit! \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2016/06/10/MutationTesting.html", "title": "Mutation Testing", "content": "\n       At  XP2016  I attended an open-space demonstration of mutation testing.  In particular, an open source tool for the Java space named  pitest .  I came away pretty impressed. \n\n I had heard of mutation testing before.  A decade and a half ago there was an open source tool named  jester .  Nothing much came of Jester back then.  Perhaps we were too focussed upon TDD to think beyond it.  After all, the very notion of rigorous unit testing was very controversial – at the time. \n\n But before I get philosophical, perhaps I should describe what mutation testing is. \n\n The Problem. \n\n What’s the problem with unit tests?  Dijkstra said it long, long ago.  “Testing shows the presence, not the absence of bugs.”  This sentiment has long been used as a complaint against the discipline of TDD.  “Write all the tests you like”, the detractors would say, “it still doesn’t prove your code actually works.” \n\n Dijkstra was right, of course.  However, his complaint could also be raised about the scientific method.  To paraphrase: Experiments can only disprove, never prove, a theory.”  And yet every day we are willing to bet our lives on those unproven theories of Newton, Einstein, Maxwell, and Boltzmann.  If experiments are good enough for science, why aren’t unit tests good enough for software? \n\n There are many answers to that, which I will phrase as questions[1]: \n\n \n   Have you written enough tests? \n   Have you covered every line, every branch, every path? \n   If a semantic change is made to the code, will some test fail? \n \n\n Sufficiency. \n\n The first question is obvious.  If you missed a test, you may have a bug.  There are two kinds of missing tests.  The first is some statements in the code that are not tested by the tests.  The second is some requirements that the developers missed. \n\n There’s not much we can do about that later case other than to carefully review the requirements and to expose the software to customers and users early and often to get their feedback. \n\n The former case is a symptom of test-after.  If you write the code first, and then write the tests for the code, you are very likely to miss some statements or branches.  This is one of the biggest arguments in favor of the test-first strategy of TDD.  If you write your tests first, and if you refuse to write a line of production code unless it is to get a failing test to pass, then you are not likely to leave any code uncovered by the tests. \n\n Coverage. \n\n And that brings us to the second question.  Have you covered all the lines, branches, and paths?  Covering every path is impractical.  The simple explosion in the number of paths makes the testing burden enormous.  We may solve this problem one day; but today is not that day. \n\n However, we have tools that will tell us what lines and branches are covered.  These coverage tools are readily available, easy to run, and generally quite fast.  They aren’t perfect, but overall they’re pretty good. \n\n So what should your goal be?  The question is absurd.  There is no justifiable goal other than 100%.  Every single line, and every single branch, should be tested by your unit tests.  I realize that this goal is not practicably achievable. So I think of it as an asymptotic goal – one that we are always pushing towards, but never quite achieving. \n\n Of course the problem with coverage is that it doesn’t prove what you might think it proves.  It does not prove that you have  tested  every line and every branch.  All it proves is that you have  executed  every line and every branch.  Pull out all the asserts from your tests, and your coverage remains unchanged! \n\n Semantic Stability. \n\n And that leads us to the third, and most important question.  If you make a semantic change to the code – a change that alters the meaning of the code – will one of your tests detect it?  That’s a very high bar for a test suite.  But, again, it is the only justifiable bar to set.   Of course  your test suite should fail if you make a semantic change to your production code.  Does anybody realistically doubt that? \n\n And think about what such semantic stability means.  It means that your test suite  tests  every line and every branch.  It means that your test suite verifies every behavior written into your system. \n\n Well, perhaps not quite.  Remember we aren’t ensuring that all pathways are covered.  Still, if we change the sense of an  if  statement, and some test doesn’t fail, that’s a problem.  If, on the other hand, we walk through the code and, one-by-one, change the sense of every  if  statement to see if a test fails, then we can be pretty sure that all our  if  statements are covered  and  tested.  If we also change, one-by-one, the sense of every  while  statement; and if, one-by-one, we remove every function call; and we ensure that each of those changes causes our test-suite to fail, then we can be pretty sure that those  while  statements and function calls are covered and tested. \n\n Mutation Testing \n This is what mutation testing does.  The  pitest  tool first runs a coverage analysis by executing your test suite and measuring which lines are covered by the tests.  Then, one-by-one, it makes semantic changes to the  Java  byte code, such as inverting the sense of  if  and  while  statements, and removing function calls.  Each one of those changes is called a  mutant . \n\n For each mutant the tool runs the unit test suite; and if that suite  fails , the mutant is said to have been  killed . That’s a  good  thing. \n\n If, on the other hand, a mutant passes the test suite, it is said to have  survived .  This is a bad thing. It means that the tests do not check for that semantic change.  Strangely, the sense for mutant tests is  inverted ; we expect them to fail.  A passing mutant test is bad.  Mutants should all be red! \n\n A surviving (green) mutant might be the result of tests that have been  @ignore d, or commented out, or when asserts have been removed, or  never added.  It can also happen if TDD discipline got a little lax at some point, and some code got added without a corresponding test. \n\n As a side note, I have found  pitest  to be pretty easy to operate, and relatively fast.  It apparently does some smart dependency detection to help it determine what tests need to be run for particular mutants.  Your mileage may vary; and I did have to break the mutation tests up into small parts for one larger project I am working on.  Still, I have found the tool to be quite useful at identifying semantic instabilities in my test suites. \n\n Implications \n A fundamental goal of TDD is to create a test suite that you can trust, so that you can effectively refactor.  We need to be able to refactor in order to keep the code clean enough to modify and enhance without paying huge costs and taking huge risks.  The cleaner the code the longer it’s useful lifetime. \n\n For years the argument has been that test-after simply cannot create such a high reliability test suite.  Only diligent application of the TDD discipline has a chance of creating a test suite that you implicitly trust. \n\n However, with a mutation testing tool like  pitest  I have successfully augmented a test-suite created with lax TDD discipline into one that I can implicitly trust.  The implication of that is significant.  If a development team dedicates itself to creating a test suite that is semantically stable, and verifies that stability by using a mutation tester like  pitest , then does it really matter if the tests were written first? \n\n Oh there are other arguments for test-first, of course.  There’s the design argument, and the cycle time argument, and the fun factor argument among many others. Valid as those arguments may be, they pale in comparison to creating a test suite that guarantees semantic stability. \n\n As hard-nosed as I am about TDD as a necessary discipline; if I saw a team using mutation testing to guarantee the semantic stability of a test-after suite; I would smile, and nod, and consider them to be highly professional.  (I would also suggest that they work test-first in order to streamline their effort.) \n\n \n\n [1] If you think about it, each of those questions has an analog for science.  We don’t trust our lives to theories that have not been subjected to experimental testing that is complete, covered, and controlled (semantically stable). \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2016/05/21/BlueNoYellow.html", "title": "Blue. No! Yellow!", "content": "\n       \n   What language was used to write the very first programs for the very first stored-program computer? \n \n\n Binary machine language, of course. \n\n \n   Why? \n \n\n Well, obviously, because they had no symbolic assembler.  The first programs had to be written in binary. \n\n \n   How much easier is it to write programs in Assembler than binary machine language? \n \n\n It’s  much  easier. \n\n \n   Give me a number.  How many times easier it is? \n \n\n Well, gosh, the assembler does all the horrible clerical work for you.  I mean it calculates all the physical addresses.  It constructs all the physical instructions.  It makes sure you don’t do things that are physically impossible, like addressing out of range.  And then it creates an easily loadable binary output. \n\n The workload savings is  enormous . \n\n \n   How enormous?  Give me a number. \n \n\n OK.  So, if I were to write a simple program, like printing the squares of the first 25 integers, in assembler on an old machine like a PDP-8, it would take me about two hours.  If I had to write that same program in binary machine language, it would probably take me double that. \n\n I say  double  because I would first write the program in a symbolic syntax on paper; and then I’d do the assembly of the machine language by hand, on paper.  And then I’d have to enter that binary into the computer by hand.  And all that extra work would probably take me just as long as it would take to write the program in the first place.  Maybe longer. \n\n \n   Good enough.  So using a symbolic assembler reduces the workload by a factor of two? \n \n\n Actually, I think it’s a lot more than that.  Squares of integers is a pretty simple program.  Bigger programs are a lot harder to hand-assemble and hand-load.  So I think the workload savings is actually a function of the program size.  For big programs it saves a  lot  of time. \n\n \n   Please explain. \n \n\n Well, imagine a one-line change to a symbolic assembler program.  That might take me 20 minutes on an old PDP-8 with paper tape.  But if I were hand-assembling, then I’d have to recalculate all the addresses and re-assemble all the instructions by hand.  It would take me hours and hours depending on how big that program was.  Then hand loading it would take even more hours. \n\n I could save some of that time by segmenting the program into modules that were loaded at fixed addresses that had free gaps between them.  I could same a bit more time by writing a smaller program that helped me load the big program.  But the clerical workload would still be very, very high. \n\n \n   OK.  So give me a number.  On average, how much easier is assembler than binary? \n \n\n OK.  I guess I’d have to say 10 times easier. \n\n \n   So a symbolic assembler allows one programmer to do the work of ten programmers working in binary? \n \n\n Yes, that’s probably about right. \n\n \n   If symbolic assembly reduced the workload by a factor of 10, how much more did Fortran reduce the workload? \n \n\n Well, gosh.  If we’re talking about the 1950s, Fortran was a pretty simple language back then.  I mean, it was hardly more than a symbolic assembler for symbolic assembly – if you catch my meaning. \n\n \n   So, does that mean it reduced the workload by another factor of ten? \n \n\n Oh, gosh no!  The clerical burden of symbolic assembler was nowhere near that high.  I’d say that Fortran decreased the workload by a smallish factor.  Perhaps 30%. \n\n \n   So, 10 Fortran programmers could do the work of 13 assembly programmers? \n \n\n If you want to look at it that way; yes – that’s probably about right. \n\n \n   So how much work does a language like C save over a language like Fortran? \n \n\n Well, ok, um, C does save a bit of clerical work over Fortran.  With that old Fortran you had to remember things like line numbers, and the order of common statements.  You also had rampant  goto  statements all over the place.  C is a much nicer language to program in than Fortran 1.  I’d say it might reduce the workload by 20%. \n\n \n   OK.  So 10 C programmers could do the work of 12 Fortran programmers? \n \n\n Well, this is all guesswork of course; but I’d say that’s a good educated guess. \n\n \n   Good.  So, now:  How much did C++ reduce the workload compared to C? \n \n\n OK, well, now look.  We’re ignoring a much larger effect. \n\n \n   Are we?  What? \n \n\n The development environments.  I mean, in the 1950s we were using punched cards and paper tapes.  Compiling a simple program took half an hour at least.  And that’s only if you could get access to the machine.  But by the late 1980s, when C++ was becoming popular, programmers kept their source code on disks, and could compile a simple program in two or three minutes. \n\n \n   Is that a reduction in workload?  Or is it just a reduction in wait time? \n \n\n Ah.  OK.  I see your point.  Yes, back in those days we spent a lot of time waiting for the machine. \n\n \n   So when you give me your workload estimates, please don’t consider the wait times.  I’m only interested in the savings of the languages themselves. \n \n\n OK.  I get it.  So you asked about C++.  Um.  Frankly, I don’t think C++ saved an awful lot of workload.  Oh  some ; but not any more than, say 5%.  I mean, the clerical overhead of C just isn’t that high, and the comparative savings of C++ is just not that great. \n\n \n   If I use your 5% number, then 100 C++ programmers could do the work of 105 C programmers.  Does that sound right? \n \n\n Well, yes.  But only for small and intermediate programs.  For  big  programs C++ provided some extra benefit. \n\n \n   What might that be? \n \n\n It’s kind of complicated.  But the bottom line is that the object-oriented features of C++, specifically polymorphism, allowed large programs to be separated into independently developable and deployable modules.  And for very large programs that reduces a significant clerical overhead. \n\n \n   I need a number. \n \n\n Well, if you’re going to twist my arm… Given the number of truly big programs that were being created in the 80s and 90s, I’d say that, overall, C++ decreased the workload by, um, maybe 7%? \n\n \n   You don’t seem very confident. \n \n\n I’m not.  But let’s use that number.  7%. \n\n \n   All right.  So then 107 C programmers could do the work of 100 C++ programers? \n \n\n Like I said.  Let’s use that number. \n\n \n   How much work did Java save over C++? \n \n\n Well, ok, um.   Some .  I mean, Java is a simpler language.  It has garbage collection.  It doesn’t have header files.  It runs on a VM.  There are lots of advantages.  (And a few disadvantages.) \n\n \n   The number? \n \n\n We’re kind if in the mud here.  But since you are pressing me, I’d say that, all else being equal (which it never is), you might get a 5% reduction in workload by using Java over C++. \n\n \n   So, 100 Java programmers could do the work of 105 C++ programmers? \n \n\n Well, yeah; but.  No.  That’s not right.  The standard deviation is too high.  If you pick 100 Java programers at random and compare them to 105 C++ programmers at random, I can’t predict the results.  We need much larger numbers to see the real benefit. \n\n \n   How much bigger? \n \n\n Two orders of magnitude at least. \n\n \n   So, 10,000 randomly chosen Java programmers could probably do the work of 10,500 randomly chosen C++ programmers? \n \n\n I’d take that bet. \n\n \n   Very well.  How much does a language like Ruby reduce the workload over Java? \n \n\n Hoo boy!  (sigh).  Really?  Look, Ruby is a really nice language.  It is both simple, and complex; both elegant and quirky.  It’s dog slow compared to Java; but computers are just so cheap that… \n\n \n   That’s not what I’m asking you. \n \n\n Right.  I know.  OK, so the major workload that Ruby reduces over a language like Java is  Types .  In Java you have to create a formal structure of types and keep that structure consistent.  In Ruby, you can play pretty fast and loose with the types. \n\n \n   That sounds like a big reduction in workload. \n \n\n Well, no.  You see that’s offset by the fact that playing fast and loose with the type structure leads to a class of runtime errors that Java programmers don’t experience.  So, Ruby programmers have a bigger test and debug overhead. \n\n \n   Are you saying that the effects cancel out? \n \n\n That depends on who you ask. \n\n \n   I’m asking you. \n \n\n OK then.  I’d say that the effects do not cancel out.  Ruby reduces the workload over Java. \n\n \n   How much?  20%? \n \n\n People used to think so.  In fact, in the 90s people thought that Smalltalk programmers were many times more productive than C++ programmers. \n\n \n   You are confusing me.  Why mention those languages? \n \n\n Well, because C++ is pretty close to Java, and Smalltalk is pretty close to Ruby. \n\n \n   I see.  So then Ruby reduces the workload many times over Java? \n \n\n No, probably not.  You see, back in the 90s, that wait-time issue was still quite pronounced.  The compile time for a typical C++ program was many minutes.  The compile time for Smalltalk was, um,  zero . \n\n \n   Zero? \n \n\n Effectively yes.  The problem is that languages like Java and C++ have a lot of work to do to reconcile all the types.  Languages like Smaltalk and Ruby don’t bother.  So, back in the 90s, it was minutes to milliseconds. \n\n \n   I see.  So this is all just wait-time, and we can ignore it. \n \n\n Not quite.  You see, when the compile time if effectively  zero  it encourages a different programming style and discipline.  You can work in  very  short cycles;  seconds  as opposed to minutes.  This allows  very  rapid feedback.  When compile times are long, that rapid feedback isn’t possible. \n\n \n   Does rapid feedback reduce workload? \n \n\n Yes, in a way.  When your cycles are extremely short, the clerical overhead in each cycle is very small.  Your brain has less to keep track of.  Longer cycles increase clerical overhead – in a non-linear fashion. \n\n \n   Non linear? \n \n\n Yes, clerical overhead grows out of proportion to the cycle time.  It might be as high as  O(N^2) .  I don’t know.  But I’m quite sure it’s not linear. \n\n \n   Well, then, Ruby takes the lead by a mile! \n \n\n No.  That’s the point.  Because our hardware has improved so much in the last twenty years, compile times for Java are effectively  zero .  The cycle time of a Java programmer is no longer (or  need  be no longer) than the cycle time of a Ruby programmer. \n\n \n   What are you saying? \n \n\n I’m saying that programmers who use a short-cycle discipline will see little or no difference in workload between Java and Ruby.  What differences there are will not be enough to measure. \n\n \n   No measurable difference? \n \n\n I think to measure a statistical difference you’d need to run trials with thousands of programmers. \n\n \n   But you said before that Ruby reduces the workload over Java. \n \n\n I think it does; but only if the cycle time is long.  If the edit/compile/test cycle time is kept very short, then the effect is negligible. \n\n \n   Zero? \n \n\n Well, no, probably more like 5%.  But the standard deviation is enormous. \n\n \n   So, it takes 10,500 short-cycle Java programmers to do the work of 10,000 short-cycle Ruby programmers? \n \n\n If you add another order of magnitude to the sample size; then I might take that bet. \n\n \n   Are there any languages that can do better than Ruby? \n \n\n Oh, you might get another 5% out of a language like Clojure, just because it’s so simple, and because it’s functional. \n\n \n   You are giving only 5% to a functional language? \n \n\n No, I’m saying that a short-cycle discipline virtually erases the productivity differences in modern languages. \n\n So long as you work in short cycles, it hardly matters what modern language you use. \n\n \n   So: Swift? Dart? Go? \n \n\n Negligible. \n\n \n   Scala?  F#? \n \n\n Negligible. \n\n \n   You are saying that we’ve reached the pinnacle.  That no future language will be better than what we have now. \n \n\n Not quite.  What I’m saying is that we’ve passed the point of diminishing returns.  No future language will give us the factor of 10 advantage that assembler gave us over binary.  No future language will give us 50%, or 20%, or even 10% reduction in workload over current languages.  The short-cycle discipline has reduced the differences to virtual immeasurability. \n\n \n   So then why are there always new languages being invented? \n \n\n It’s a quest for the  Holy Grail . \n\n \n   Ah, so it’s really just a matter of your favorite color. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2016/07/27/TheChurn.html", "title": "The Churn", "content": "\n       Did you year about the guy who said goodbye to OO? \n\n \n   Oh no.  Not another one.  What did he say? \n \n\n He described all the promises of OO, and how none of them had really been delivered, and that all the features of OO cost more than they were worth, and that functional programming was better and… \n\n \n   Sigh.  Yes, I’ve heard it all before. \n \n\n So, then, OO is finally dead, and we can move on. \n\n \n   Move on to what? \n \n\n Why, to  THE NEXT BIG THING  of course! \n\n \n   Oh. – That.  Do you know what it is yet? \n \n\n I dunno, I’m pretty excited about  micro-services ; and I’m really keen on  Elixr ; and I hear  React  is really cool; and … \n\n \n   Yes.  Yes.  The Churn.  You are caught up in The Churn. \n \n\n Huh?  What do you mean by that.  These are exciting times. \n\n \n   Actually, I find them rather depressing. \n \n\n Why?  I mean, there are new technologies bubbling up every few days!  We are climbing to ever higher heights. \n\n \n   Bah!  All we are really doing is reinventing the wheel, over, and over again.  And we’re wasting massive amounts of time and effort doing it. \n \n\n Oh come on!  We’re making  PROGRESS . \n\n \n   Progress.  Really?  That’s not the way I see it. \n \n\n Well, just what is it that you see? \n\n \n   I see waste.  Massive, incalculable, waste.  Waste, piled upon waste, piled upon even more waste. \n \n\n How can you say that? \n\n \n   _Well, consider this OO issue.  OO isn’t dead.  OO was never alive.  OO is a technique; and a good one.  Claiming it’s dead is like claiming that a perfectly good screwdriver is dead.    Saying goodbye to OO is like saying goodbye to a perfectly good screwdriver.  It’s _waste! \n \n\n But Functional Programming is better! \n\n \n   I’m sorry, but that’s like saying that a hammer is better than a screwdriver.  Functional programming is not “better” than Object Oriented programming.  Functional Programming is a technique, and a good one, that can be used alongside Object Oriented programming. \n \n\n That’s not what I heard.  I heard they were mutually exclusive. \n\n \n   Of course they aren’t.  They address orthogonal concerns.  Concerns that are present in all projects. \n \n\n \n\n \n   Look there are people who think that software is a linear chain of progress.  That we are climbing a ladder one rung at a time; and that every “new” thing is better than the previous “older” thing.  That’s not the way it works. \n \n\n So, how  does  it work – in your opinion? \n\n \n   Progress in software has followed a  logarithmic growth  curve.  In the early years, progress was stark and dramatic.  In later years the progress became much more incremental.  Now, progress is virtually non-existent. \n \n\n \n\n \n   Look: Assembler was massively better than Binary.  Fortran was much better than Assembler.  C was a lot better than Fortran.  C++ was probably better than C.  Java was an improvement over C++.  Ruby is probably a bit better than Java. \n \n\n \n\n \n   Waterfall was a whole lot better than nothing.  Agile was better than waterfall.  Lean was a little better than Agile.  Kanban may have been something of an improvement. \n \n\n \n\n \n   Every year. though we apply massive effort, we make less progress than the year before; because every year we get closer and closer to the asymptote. \n \n\n Asymptote!  You think there’s an upper limit to software technology and progress? \n\n \n   I absolutely do.  What’s more I think we are so close to that limit now, that any further striving is fruitless.  We are _well passed  the point of diminishing returns._ \n \n\n What?  That sounds ludicrous!  That sounds depressing! \n\n \n   I understand.  But that’s because we got used to all that early rapid growth.  Those were heady days; and we want them back again.  But they’re gone; and we have to face the fact that we are wasting time and effort on a massive scale trying to recreate them. \n \n\n But if we don’t push for the future; we’ll never create it! \n\n \n   Believe me, I definitely want us to push for the future.  That’s not what we are doing.  What we are doing is pining for the past. \n \n\n So what future do you think we should be pushing towards? \n\n \n   A productive one.  A future that is not dominated by all this wasteful churn. \n \n\n What’s wasteful about it? \n\n \n   Have you ever used _IntelliJ  or  Eclipse  to program Java?_ \n \n\n Sure. \n\n \n   Those are _incredibly  powerful tools.  A skilled professional can be  wildly  productive with those tools.  The refactorings! The representations! The facilities!  My God; those tools are spectacular!_ \n \n\n \n\n \n   Yet every time a new language comes along we dash away from those powerful tools to use the  NEXT NEW THING .  And the tools for that new language are like programming in the third world.  God, you often don’t even have a reasonable  rename  refactoring! \n \n\n \n\n \n   It takes _time  to build up a reasonable toolset.  If we keep on switching languages, we’ll never be able to tool those languages up._ \n \n\n But the newer languages are better. \n\n \n   Oh bull!  They’re _different ; but they aren’t better.  Or at least not better  enough  to justify throwing our toolset back into the stone age._ \n \n\n \n\n \n   And think of the training costs for adopting a new language.  Think of the cost to the organization of having to use 84 different languages because the programmers get excited about shiny new things every two weeks. \n \n\n Shiny new things?  That’s kind of insulting isn’t it. \n\n \n   I suppose so; but that’s what it comes down to.  New languages aren’t better; they are just shiny.  And the search for the golden fleece of a new language, or a new framework, or a new paradigm, or a new process has reached the point of being  unprofessional. \n \n\n Unprofessional? \n\n \n   Yes! Unprofessional.  We need to realize that we have hit the asymptote.  It’s time to stop the wasteful churning over languages, and frameworks, and paradigms, and processes. \n \n\n \n\n \n   It’s time to simply get down to work. \n \n\n \n\n \n   _We need to choose a language, or two, or three.  A small set of simple frameworks.  Build up our tools.  Solidify our processes.  And become a goddam _profession. \n \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2016/05/01/TypeWars.html", "title": "Type Wars", "content": "\n       My son, Justin, and I have begun a new video series on  cleancoders.com  investigating mobile apps using  swift .  Learning  swift  has been an interesting experience.  The language is  very  opinionated about type safety.  Indeed, I don’t remember using a language that was quite so assiduous about types. \n\n For example, the fact that a variable of type  X  might also be  nil  means you must declare that variable to hold an “optional” value by using a  ? .  [e.g.  myVariable: X? ] The language tightly constrains how that optional value is used. For instance, you cannot reference it without either checking for  nil  with an  if  or asserting with a  !  that it is not  nil .  [e.g.  myVariable!.myFunc() ] \n\n The extreme nature of the type system in  swift  got me thinking about the “Type Wars” that our industry has been fighting for the last six decades.  And that got me to thinking that I should blog about the history of those wars, and then predict the future outcome.  So, here goes:   Don’t Panic . \n\n \n   WARNING: This history has many omissions and contains much that is apocryphal, or at least wildly inaccurate. \n \n\n The issue of types actually predates computers.  In the last half of the nineteenth century, Gottlob Frege developed a logical system with which he hoped to derive all of mathematics from first principles.  His work was the progenitor of predicate calculus; but it was found to have a critical flaw.  On the eve of publication, Bertrand Russel wrote to Frege and pointed out that Frege’s logical system allowed statements that were ambiguous – neither false nor true.  Frege was devastated and included the following remark in an appendix to his work. \n\n \n   “Hardly anything more unfortunate can befall a scientific writer than to have one of the foundations of his edifice shaken after the work is finished. This was the position I was placed in by a letter of Mr. Bertrand Russell, just when the printing of this volume was nearing its completion.” \n \n\n One of the solutions that was proposed to resolve the problem was the notion of  types .  It was hoped that if the types of the parameters within Frege’s logical statements could be constrained, then Russell’s ambiguities could be eliminated.  But in 1931 these hopes were dashed by Kurt Godel’s  incompleteness theorems . \n\n This was the mathematical world that Alan Turing was active in.  He, and other mathematicians like John Von Neumann, and John Backhus, who were steeped in these issues, were also responsible for many of the founding concepts of computer science, and our first languages – like  Fortran . \n\n I first ran into the concept of types in 1966 while learning  Fortran  as a teenager.  My father had managed to acquire some very early manuals on languages like  Fortran ,  Cobol , and  PL/1 ; and I was devouring them.  (Though I didn’t have access to a computer!) \n\n In  Fortran  there were essentially two types.  Fixed Point (integer), and Floating point.  In the original language the type was denoted by the first letter of the variable.  Variables that began with the letters  I  through  N  (the first two letters in the word “integer”) were fixed point. All other variables were floating point. \n\n Expressions in  Fortran  could not “mix modes”.  You could not have integers alongside floating point numbers.  Only certain library functions could translate between the modes. \n\n As a young programmer, without access to a computer, I found this very confusing.  I wondered why such a distinction, with such horrific constraints, was so important. It wasn’t until I learned assembly language that I began to understand. \n\n During my first years as a programmer, I wrote a bit of  Fortran ,  Cobol , and  PL/1 ; but I was much more focussed on assembler.  I felt the “high-level” languages were bloated, slow, cop-outs.   Real  programmers wrote assembler – or so I thought.  But then in 1977 I was introduced to  C . \n\n It was an instant love-affair.  As I sat by the campfire in my back yard, reading a copy of  Kernighan and Ritchie , I cheered this language that was  not  a bloated, slow, cop-out; but was, instead, a simple translation of assembler. \n\n C , of course, had types; but they were not enforced in any way.  You could declare a function to take an  int , but then pass it a  float  or a  char  or a  double .  The language didn’t care.  It would happily push the passed argument on the stack, and then call the function.  The function would happily pop its arguments off the stack, under the assumption that they were the declared type.  If you got the types wrong, you got a crash.  Simple.  Any assembly language programmer would instantly understand and avoid this. \n\n The late 70s was the first time I realized that there was a war going on.  A war about types.  Although I had been completely enthralled by  C  I was aware of another contender for my attention:  Pascal .   Pascal  was everything I hated about high level languages.  I considered it to be a bloated, slow, cop-out.  People who programmed this language were  not  real programmers because they depended on the language to keep them safe.   Pascal , you see, was strongly typed. \n\n In  Pascal  if you declared a function to take certain argument types, then you had to call that function with those types.  The language enforced type safety.  For an assembly language programmer like me, in my twenties, this was just too constraining.   Pascal , I felt, was a language for babies. \n\n Apparently many programmers agreed with me because  C  won that language war; and won it  decisively .  The early ’80s was the heyday of  C .  As mini-computers proliferated, so did  C .   Pascal  survived, but only because Apple decided (wrongly) to use it as the language for the Macintosh – a decision it would eventually reverse. \n\n There were other languages bubbling up from time to time; but they weren’t taken too seriously.  We heard about Smalltalk, and the million dollar machines required to run it.  We knew of Logo, Lisp, and several other languages.  But in my part of the world, we were happy with  C  and couldn’t imagine anything better. \n\n Objective-C  popped up around 1986 or so.  I remember looking pretty seriously at Brad Cox’s odd little hybrid between  C  and  Smalltalk .  But it was  C++  that captured my attention.  It captured my attention because, after the better part of a decade of  C ’s ambivalence towards types, I was ready for a language to enforce strong typing. \n\n You see, I had learned my lesson.  As programs grew ever more complicated in the late 70s and early 80s, the problem of keeping your types straight began to get out of hand.  There were too many times that I debugged problems in the field, only to find that someone had called a function with a  long  that was declared to take an  int . \n\n So when Stroustup published his book on  C++ , I was ready!  (I was also glad that this was a  C  derivative, so that I didn’t have to admit to the  Pascal  weenies that they’d been right all along!) \n\n The pendulum had swung.  The industry adopted  C++  and the era of strong typing had begun.  We  C  programmers, combat hardened and war weary, swore we’d never go back to the careless days of unenforced types. \n\n Meanwhile the  Smalltalk  programmers were scratching their heads wondering what the big deal was.  You see, their language was also strongly typed; but their types were  undeclared .  In  Smalltalk  types were enforced at  runtime .  Type errors in  Smalltalk  did not lead to  undefined  behavior, as they did in  C .  The behavior in  Smalltalk  was  very well  defined.  You got a type exception during execution. \n\n We in the  C++  community felt that was simply the same as dereferencing a  null  pointer.  After all, who cares if the software in the missile fails because of a type exception or a segmentation error?  Either way, the missile fails. \n\n The late ’80s and early ’90s were a kind of “cold-war” between the  static  type-checking of  C++  and the  dynamic  type checking of Smalltalk.  Other languages rose and fell during this time; but those two broad typing categories pretty well defined them. \n\n Then the war got hot again.  But this time it wasn’t just programmers in the streets carrying  C  and  Pascal  signs.  This time it was a real war with heavyweight players.  It was a war between IBM and Sun. \n\n But to set the stage I have to tell you about the research done by Capers-Jones regarding the productivity of programmers using different languages.  His study showed that  Smalltalk  programmers were much more productive than  C++  programmers.  Some people thought the difference was as much as 5X.  Others thought it was more like 2X-3X. But nobody thought  C++  programmers were more productive than Smalltalk programmers. \n\n In light of this, IBM chose  Smalltalk  as the development language for the internet.  They made a huge bet on this by developing IDEs, and frameworks, and all the necessary infra-structure.  Sun, on the other hand, put their weight behind  Java  (which was just  C++ Lite .) \n\n The battle raged.  But the deciding factor was types.  Sun fielded rank upon rank of  Java  (nee  C++ ) programmers and won that battle over IBM on the basis of type safety (the  missile  argument). On that day  Smalltalk  died as a language.  (But don’t worry, the  Smalltalk ers got their revenge.) \n\n And so,  Java , and it’s bastard cousin  C#  became the languages of the internet; and held sway for two decades.  But there was a lot going on behind the scenes. \n\n You see, the  Smalltalk  programmers had solved the missile problem in their own unique way.  They invented a discipline.  Today we call that discipline:  Test Driven Development . \n\n Using TDD, the  Smalltalk  programmers were assured that the missile would reach it’s target.  In fact, they had much more assurance than the type-checking of the  C++  or  Java  compiler could provide.  So when  Smalltalk  died, a fifth column of  Smalltalk  programmers infiltrated the ranks of the  Java  programmers and began to teach TDD.  Their goal:   subversion . \n\n You see, when a  Java  programmer gets used to TDD, they start asking themselves a very important question: “Why am I wasting time satisfying the type constraints of  Java  when my unit tests are already checking everything?”  Those programmers begin to realize that they could become  much  more productive by switching to a dynamically typed language like  Ruby  or  Python . \n\n And that’s exactly what happened in second half of the first decade of the current millennium.  Tons of  Java  programmers jumped ship and became dedicated dynamic typers, and TDDers.  That ship-jumping continues to this day, spurred on by the fact that salaries tend to be higher for programmers of the dynamic languages. \n\n Oh, and I should tell you about one special unit of  Smalltalk  programmers who stayed at IBM planned their revenge against Sun.  They executed that revenge by creating …  Eclipse . \n\n And so here we are.  The pendulum is quickly swinging towards dynamic typing.  Programmers are leaving the statically typed languages like  C++ ,  Java , and  C#  in favor of the dynamically typed languages like  Ruby  and  Python .  And yet, the new languages that are appearing, languages like  go  and  swift  appear to be reasserting static typing?  So is the stage for the next battle being set? \n\n How will this all end? \n\n My own prediction is that TDD is the deciding factor.  You don’t need static type checking if you have 100% unit test coverage.  And, as we have repeatedly seen, unit test coverage close to 100% can, and is, being achieved.  What’s more, the benefits of that achievement are enormous. \n\n Therefore, I predict, that as TDD becomes ever more accepted as a necessary professional discipline, dynamic languages will become the preferred languages.  The  Smalltalk ers will, eventually, win. \n\n So says this old  C++  programmer. \n\n \n Early Fortran Spec:  https://www.fortran.com/FortranForTheIBM704.pdf \n\n Caper-Jones Study: http://www.cs.bsu.edu/homepages/dmz/cs697/langtbl.htm \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2016/09/01/TheLurn.html", "title": "The Lurn", "content": "\n       It has often been said (by me) that a professional software developer never stops learning. \n\n The  Pragmatic Programmer  book said it this way (p. 15): \n\n \n   Learn at least one new language every year. \n   Read a technical book each quarter. \n   Read non-technical books too. \n   Take classes. \n   Participate in local user groups. \n   Experiment with different environments. \n   Stay current. \n   Get Wired (blogs, redit, HN, YC, etc). \n \n\n Good ideas all.  But how does this jibe with the blog I wrote last month:  The Churn  which made the case that there is little to be gained by learning new languages and frameworks because our industry is approaching a kind of asymptote in language and framework technology.  I mean, should we really learn at least one new language every year, if there’s nothing to be gained? \n\n Yes.  Of course.  You should learn at least one new language every year.  And if you do, you’ll eventually come to realize that the languages start to become repetitive.  You’ll learn Lua, and realize it’s really just Javascript (or vice versa).  You’ll learn Ruby and realize that it’s really just Python in different clothes.  You’ll learn Swift and realize it’s just a rehash of Java with strong overtones of Pascal with some bangs ( ! ) thrown in.  You’ll learn GO and realize that it’s an amalgam of C, Java, and Erlang. \n\n You’ll begin to see the patterns behind all these languages, and recognize that those patterns are relatively few in number.  It will eventually occur to you that the endless recombination of those patterns is rather like the search for the holy grail. \n\n So, yes.  Learn at least one new language every year so that you can come to the understanding that we’ve pretty well explored the language domain. \n\n The same is true of frameworks.  Learn a new one every year or so.  And when you do you’ll come to the realization that none of the new frameworks is actually “new” in any real sense.  You’ll realize, as with languages, that the underlying patterns are relatively few in number; and that the endless recombination of those patterns approaches futility. \n\n Is this a depressing viewpoint?   It shouldn’t be.   The fact that we’ve explored one small aspect of our profession does not mean that there aren’t other things to explore and learn.  Yikes!  The list of things to learn and improve upon is huge.  What’s more, we’ve been neglecting those things in our endless churning search for new languages and frameworks. \n\n So, for example, let’s talk about  concurrency .  This is something  we suck at !  Don’t you think it’s time we got better at it?  Wouldn’t it be a good idea for us to explore the realm of concurrency as deeply as we’ve explored languages?  Isn’t this especially true now as our applications are growing ever more dependent on multi-processor platforms, configurations, and deployments?  Doesn’t the exponential advance of cloud based applications demand that we develop a very good understanding of concurrency? \n\n Here, let me test your knowledge a bit: \n\n \n   Do you know the Dining Philosopher’s problem, and what it teaches? \n   Can you define a deadly-embrace and describe how to avoid it? \n   Do you know what the actor model is? \n   Are you familiar with run-to-completion threads? \n   Have you ever written a circular buffer to communicate between an interrupt head and a background application? \n   Do you know what a semaphore is, who invented it, and why? \n   Are you familiar with the rationale behind double-checked locking? \n   What is priority inversion? \n \n\n If you can’t answer a majority of those questions competently (after appropriate googling to disambiguate vocabulary differences) then I’d say you’ve got several glorious years of exploration ahead of you; and I advise you to take advantage of them. \n\n Or, how about communication protocols.  Have we explored this realm sufficiently?  Do you know the domain? \n\n \n   Do you understand how unreliable connections are made to carry reliable communications? \n   Do you know what a sliding window protocol is? \n   How about a CRC? \n   Why is collision detection important to Ethernet? \n   What is exponential backoff? \n   What are the seven layers in the OSI model; and why are they important? \n   What is the difference between BPS and BAUD? \n \n\n How well versed are you in the classics of our industry.  Have you read  and understood  (just to mention a few): \n\n \n   The Art of Computer Programming : Knuth \n   Computer Networks : Tanenbaum \n   The Structure and Interpretation of Computer Programs : Abelson and Sussman \n   Structured Analysis and System Specification : DeMarco \n   Design Patterns : Gamma, Helm, Johnson, Vlissides \n   Analysis Patterns : Fowler \n   Structured Programming : Dijkstra, Dahl, Hoare \n   Object Oriented Software Construction : Meyer \n \n\n Do you understand the difference between  discrete event simulation  and  continuous simulation ?  When would you use them? \n\n How’s your queueing theory?  Do you understand how to organize a series of gates and queues to maximize throughput in various environments? \n\n Are you familiar with graph algorithms?  How would you find the shortest route between two towns?  The fastest? \n\n Could you write a quicksort on demand, without looking it up? \n\n What is DeMorgan’s theorem, and why is it useful to you? \n\n What is the difference between a Mealy and a Moore machine? \n\n How’s your computational geometry?  How would you calculate the area of an arbitrary polygon? \n\n Have you written a genetic algorithm?\nHave you worked with a neural net?\nWhat do you know about Big Data.\nHave you written a floating point package?\nHave you written an IO driver?\nHave you written a file system?\nHave you written a multi-tasking nucleus\nHave you written a compiler? \n\n And then there are languages.  Have you learned the really important ones – the ones that truly  were  innovations – the ones that were the stepping stones that we took to get where we are?  Have you learned Fortran?  Have you learned Cobol?  Have you learned Snobol, and Forth, and Lisp, and Prolog, and C? \n\n Have you written in a machine language, hand assembling your code into binary?  Have you read Alan Turing’s original paper:  On Computable Numbers… ? \n\n *~~~* \n\n So, yes.  Learn at least one new language every year.  And perhaps one of those new languages could be new – to you; but old and significant to our profession.  And perhaps, while you are learning a new language, you could apply that learning to some of the other topics I mentioned here. \n\n Our field is a rich one.  We’ve barely scratched the surface.  There are deep things to learn and understand about computing, and about software.  It would be a shame if we never truly explored those depths because we kept on being distracted by shiny objects. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2016/10/26/DijkstrasAlg.html", "title": "Dijkstra's Algorithm", "content": "\n       I was at  SCNA  the other day, and someone approached me about TDD and Dijkstra’s algorithm.  He wondered if you could find a sequence of tests that would lead you to the algorithm.  This sounded like a fun little exercise, so I decided to give it a try here. \n\n I’ll begin as usual with an executing degenerate test case. \n\n public class MinPathTest {\n  @Test\n  public void nothing() throws Exception {\n  }\n}\n \n\n Dijkstra’s algorithm is a simple technique for finding the minimum path through a graph with edges of arbitrary length.  Given the starting and ending nodes, the algorithm will tell you the minimum path, and the length of that path. \n\n So from the very start there are some interesting decisions to make.  How shall I represent the input graph?  How shall I represent the output of the algorithm?  We can probably decide much of that later.  For now, we should concentrate on the most degenerate of cases: an empty graph. \n\n public void noGraph_NoPathZeroLength() throws Exception {\n  Assert.assertEquals(\"{}0\", minPath(\"\"));\n}\n \n\n The very first test has forced me to make some decisions about the output format.  I will represent the output path as a set of nodes between curly braces.  The length of the path will be an integer following the curly braces. \n\n This notation is only for my tests.  I am using a technique that I call  composed asserts .  I like to compose my assertions into statements that are easy to read.  This often means I must write simple translators in my tests that convert the composed assertions into the true API of the system under test. \n\n Clearly I can make this test case pass with nothing more than this: \n\n private String minPath(String graph) {\n  return \"{}0\";\n}\n \n\n Let’s look at that test case carefully.  That call to  minPath  isn’t right.  Dijkstra’s algorithm finds the minimum path between two specified nodes.  So, assuming the nodes have names, the test should really look something like this: \n\n Assert.assertEquals(\"{}0\", minPath(\"\", \"A\", \"Z\"));\n \n\n It’s also pretty ugly.  We can refactor it to be a bit prettier: \n\n private void assertPath(String graph, String expected) {\n  assertEquals(expected, minPath(graph, \"A\", \"Z\"));\n}\n\n@Test\npublic void noGraph_NoPathZeroLength() throws Exception {\n  assertPath(\"\", \"{}0\");\n}\n \n\n Notice that the  assertPath  method simply assumes that the test cases will all use  \"A\"  and  \"Z\"  for their start and end points. \n\n One last change.  I think the path and the length should be separated. \n\n private void assertMinPath(String graph, \n                           int length, String path) {\n  assertEquals(length, minLength(graph, \"A\", \"Z\"));\n  assertEquals(path, minPath(graph, \"A\", \"Z\"));\n}\n\n@Test\npublic void noGraph_NoPathZeroLength() throws Exception {\n  assertMinPath(\"\", 0, \"{}\");\n}\n\nprivate int minLength(String graph, String begin, String end) {\n  return 0;\n}\n\nprivate String minPath(String graph, String begin, String end) {\n  return \"{}\";\n}\n \n\n Since there are two methods to check in my assertion, I think it’s reasonable to assume that they should be methods of some class that executes Dijkstra’s algorithm.  So let’s use the  extract delegate  refactoring to pull out a new class. \n\n public class MinPathTest {\n  private void assertMinPath(String graph,\n                             int length, String path) {\n    PathFinder pf = new PathFinder(graph);\n    assertEquals(length, pf.minLength(\"A\", \"Z\"));\n    assertEquals(path, pf.minPath(\"A\", \"Z\"));\n  }\n\n  @Test\n  public void noGraph_NoPathZeroLength() throws Exception {\n    assertMinPath(\"\", 0, \"{}\");\n  }\n}\n\nclass PathFinder {\n  public PathFinder(String graph) {\n  }\n\n  public int minLength(String begin, String end) {\n    return 0;\n  }\n\n  public String minPath(String begin, String end) {\n    return \"{}\";\n  }\n}\n \n\n I think it’s interesting how much thought and effort have been put into the structure of this problem; for just the first degenerate test case.  But now I think we can add several more degenerate cases: \n\n @Test\npublic void degenerateCases() throws Exception {\n  assertMinPath(\"\", 0, \"{}\");   //empty graph\n  assertMinPath(\"A\", 0, \"{}\");  //one node\n  assertMinPath(\"B1C\", 0, \"{}\");//no start or end\n  assertMinPath(\"A1C\", 0, \"{}\");//no end\n  assertMinPath(\"B1Z\", 0, \"{}\");//no start\n}\n \n\n These tests forced me to make another decision for the  composed assert .  For our tests, the structure of a graph edge will be  <name>length<name> .  So  B1C  is an edge of length 1 that connects node  B  to node  C . \n\n I think that’s all the degenerate cases.  So let’s ratchet the complexity up one notch and do the first test case that will force us to do something marginally intelligent. \n\n @Test\npublic void oneEdge() throws Exception {\n  assertMinPath(\"A1Z\", 1, \"{AZ}\");\n}\n \n\n This test fails, of course.  It also makes me uncomfortable because I am testing two things, the length and the path, that I could be testing separately.  This makes me grumble because I spent all that time setting up the composed assert; and now I want to break it apart again.  But I think there’s a “clever” way to get around that. \n\n private static String ANY = null;\n\nprivate void assertMinPath(String graph,\n                           Integer length, String path) {\n  PathFinder pf = new PathFinder(graph);\n  if (length != null)\n    assertEquals((int)length, pf.minLength(\"A\", \"Z\"));\n  if (path != null)\n    assertEquals(path, pf.minPath(\"A\", \"Z\"));\n}\n...\n@Test\npublic void oneEdge() throws Exception {\n  assertMinPath(\"A1Z\", 1, ANY);\n}\n \n\n This keeps my composed assert intact, and allows me to ignore either of the two components should I desire.  So now let’s make that test pass. \n\n Now, obviously, I could do something hideous like this: \n\n public int minLength(String begin, String end) {\n  if (graph.equals(\"A1Z\"))\n    return 1;\n  return 0;\n}\n \n\n But this breaks a number of rules.  First, it breaks the  generality  rule which says:  As the tests get more specific, the code gets more generic.   In other words, the production code has to become more general in order to pass a failing test.  You can’t add expressions to the production code that are specific to the failing test. (I say much more about this in  Episode 19: Advanced TDD , at  cleancoders.com ) \n\n The second broken rule is test coupling.  We don’t want the tests to become strongly coupled to the production code.  The more the coupling the more fragile the tests become.  We don’t want to encourage the case that a single change to the production code breaks dozens or hundreds of tests.  In our particular case, we don’t want the composed assertion to leak into the API of the production code. \n\n This means that I should not be passing the  String graph  into the constructor of the  PathFinder .  It also means that the  minPath  function should not return the  String  used by the composed assert. \n\n So, it’s time to begin to decouple the tests.  The  makePathFinder  function below shows how I did that. \n\n public class MinPathTest {\n  private static String ANY = null;\n\n  private void assertMinPath(String graph,\n                             Integer length, String path) {\n    PathFinder pf = makePathFinder(graph);\n    if (length != null)\n      assertEquals((int) length, pf.minLength(\"A\", \"Z\"));\n    if (path != null)\n      assertEquals(path, pf.minPath(\"A\", \"Z\"));\n  }\n\n  private PathFinder makePathFinder(String graph) {\n    PathFinder pf = new PathFinder();\n    Pattern edgePattern = \n            Pattern.compile(\"(\\\\D+)(\\\\d+)(\\\\D+)\");\n    Matcher matcher = edgePattern.matcher(graph);\n    if (matcher.matches()) {\n      String start = matcher.group(1);\n      int length = Integer.parseInt(matcher.group(2));\n      String end = matcher.group(3);\n      pf.addEdge(start, end, length);\n    }\n    return pf;\n  }\n\n  @Test\n  public void degenerateCases() throws Exception {\n    assertMinPath(\"\", 0, \"{}\");   //empty graph\n    assertMinPath(\"A\", 0, \"{}\");  //one node\n    assertMinPath(\"B1C\", 0, \"{}\");//no start or end\n    assertMinPath(\"A1C\", 0, \"{}\");//no end\n    assertMinPath(\"B1Z\", 0, \"{}\");//no start\n  }\n\n  @Test\n  public void oneEdge() throws Exception {\n    assertMinPath(\"A1Z\", 1, ANY);\n  }\n}\n\nclass PathFinder {\n  private List<Edge> edges = new ArrayList<>();\n\n  public PathFinder() {\n  }\n\n  public int minLength(String begin, String end) {\n    int length = 0;\n    for (Edge edge : edges) {\n      if (edge.begin.equals(begin) && edge.end.equals(end))\n        length += edge.length;\n    }\n    return length;\n  }\n\n  public String minPath(String begin, String end) {\n    return \"{}\";\n  }\n\n  public void addEdge(String start, String end, int length) {\n    edges.add(new Edge(start, end, length));\n  }\n\n  private static class Edge {\n    public final String begin;\n    public final String end;\n    public final int length;\n\n    public Edge(String begin, String end, int length) {\n      this.begin = begin;\n      this.end = end;\n      this.length = length;\n    }\n  }\n}\n \n\n Note that all the parsing for the composed assertion remains in the test class.  The  PathFinder  class knows nothing of the funny syntax I’m using in the tests.  Note also, in order to get the tests to pass, the production code must  assume  that the graph has only one edge.  That’s an assumption we’ll be breaking within the next few test cases.  In the mean time we should get rid of that  ANY . \n\n assertMinPath(\"A1Z\", 1, \"{AZ}\");\n \n\n So I’m going to need to build the list of nodes in the path.  List?  Ah, there’s a  toString  syntax for lists.  We should change that test; and all the tests as follows: \n\n @Test\npublic void degenerateCases() throws Exception {\n  assertMinPath(\"\", 0, \"[]\");   //empty graph\n  assertMinPath(\"A\", 0, \"[]\");  //one node\n  assertMinPath(\"B1C\", 0, \"[]\");//no start or end\n  assertMinPath(\"A1C\", 0, \"[]\");//no end\n  assertMinPath(\"B1Z\", 0, \"[]\");//no start\n}\n\n@Test\npublic void oneEdge() throws Exception {\n  assertMinPath(\"A1Z\", 1, \"[A, Z]\");\n}\n \n\n Now to get this to pass we’ll have to change the  assertMinPath  test helper function by adding a  toString  call. \n\n ...\n    if (path != null)\n      assertEquals(path, pf.minPath(\"A\", \"Z\").toString());\n...\n \n\n We add the  path  list to the  PathFinder  and simply load it up in the  minLength  function. \n\n class PathFinder {\n  private List<Edge> edges = new ArrayList<>();\n  ...\n\n  public int minLength(String begin, String end) {\n    int length = 0;\n    for (Edge edge : edges) {\n      if (edge.begin.equals(begin) && edge.end.equals(end)) {\n        length += edge.length;\n        path.add(edge.begin);\n        path.add(edge.end);\n      }\n    }\n    return length;\n  }\n\n  public List<String> minPath(String begin, String end) {\n    return path;\n  }\n...\n \n\n This works.  But I don’t like the fact that  minLength  is also calculating the path.  I think we should separate the calculation from the reporting. \n\n   private void assertMinPath(String graph,\n                             Integer length, String path) {\n    PathFinder pf = makePathFinder(graph);\n    if (length != null)\n      assertEquals((int) length, pf.getLength());\n    if (path != null)\n      assertEquals(path, pf.getPath().toString());\n  }\n\n  private PathFinder makePathFinder(String graph) {\n    PathFinder pf = new PathFinder();\n    ...\n    pf.findPath(\"A\", \"Z\");\n    return pf;\n  }\n\nclass PathFinder {\n  private List<Edge> edges = new ArrayList<>();\n  private List<String> path = new ArrayList<>();\n  private int length;\n\n  ...\n\n  public int getLength() {\n    return length;\n  }\n\n  public List<String> getPath() {\n    return path;\n  }\n\n...\n \n\n OK, that’s better.  Now, let’s make sure we get the length right. \n\n assertMinPath(\"A2Z\", 2, \"[A, Z]\");\n \n\n Yeah, that works just fine.  So let’s try two sequential edges. \n\n @Test\npublic void twoEdges() throws Exception {\n  assertMinPath(\"A1B,B1Z\", 2, ANY);\n}\n \n\n This fails as expected, giving us a zero length.  To make this pass we’re going to have to parse multiple edges in the  makePathFinder  helper.  That’s pretty simple.  Just split the graph on comma, and put the regular expression matcher in a loop. \n\n private PathFinder makePathFinder(String graph) {\n  PathFinder pf = new PathFinder();\n  Pattern edgePattern = Pattern.compile(\"(\\\\D+)(\\\\d+)(\\\\D+)\");\n  String[] edges = graph.split(\",\");\n  for (String edge : edges) {\n    Matcher matcher = edgePattern.matcher(edge);\n    if (matcher.matches()) {\n      String start = matcher.group(1);\n      int length = Integer.parseInt(matcher.group(2));\n      String end = matcher.group(3);\n      pf.addEdge(start, end, length);\n    }\n  }\n  pf.findPath(\"A\", \"Z\");\n  return pf;\n}\n \n\n This still fails the test, of course, so now we’re going to have to connect the two edges.  Let’s assume that the edges are specified in order, so that node A starts the first edge, and node Z ends the second edge.  In that case, we should be able to do the whole connection by changing the  &&  to an  ||  in the  findPath  function: \n\n public void findPath(String begin, String end) {\n  length = 0;\n  for (Edge edge : edges) {\n    if (edge.begin.equals(begin) || edge.end.equals(end)) {\n      length += edge.length;\n      path.add(edge.begin);\n      path.add(edge.end);\n    }\n  }\n}\n \n\n Did you like that change?   &&  to  || .  Yeah, pretty clever.  It’ll only work for two consecutive edges.  The assumptions are mounting to the sky!  And, anyway, it doesn’t work. \n\n Oh, it passes the  twoEdges  test, and the  oneEdge  tests, but it fails the  degenerateCases  tests.  And it’s no wonder, since our the last two degenerate cases match the “A” first, or “Z” last assumption. \n\n To get all these tests to pass, I need an implementation that produces zero length and an empty path if there is no path from A to Z.  Now since I don’t know how many edges there are (it could be zero, one or two) I can’t just grab the two.  Instead, I could do a case analysis for zero, one, or two edges; as follows: \n\n public void findPath(String begin, String end) {\n  if (edges.size() == 0)\n    return;\n\n  else if (edges.size() == 1) {\n    Edge edge = edges.get(0);\n    if (edge.begin.equals(begin) && edge.end.equals(end)) {\n      path.add(edge.begin);\n      path.add(edge.end);\n      length += edge.length;\n    }\n  } else {\n    for (Edge edge : edges) {\n      if (edge.begin.equals(begin) || edge.end.equals(end)) {\n        path.add(edge.begin);\n        path.add(edge.end);\n        length += edge.length;\n      }\n    }\n  }\n}\n \n\n OK, that works, but it’s truly awful.  Not only does it violate the  generality  rule; it’s also just icky. What’s more, it doesn’t correctly assemble the path.  E.g.  assertMinPath(\"A1B,B1Z\", 2, \"[A, B, Z]\");  fails because it produces  [A, B, B, Z] .  I could fix that by adding yet another horrible  if  statement; but I have a better idea.  Let’s walk the graph from start to end. \n\n public void findPath(String begin, String end) {\n  List<String> p = new ArrayList<>();\n  int l = 0;\n  p.add(begin);\n\n  for (Edge e = findEdge(begin); \n       e != null; e = findEdge(e.end)) {\n    p.add(e.end);\n    l += e.length;\n    if (e.end.equals(end)) {\n      length = l;\n      path = p;\n      return;\n    }\n  }\n}\n\nprivate Edge findEdge(String begin) {\n  for (Edge e : edges) {\n    if (e.begin.equals(begin))\n      return e;\n  }\n  return null;\n}  \n \n\n OK, this works.  It’s strange that we had to use temporary length and path variables; but that’s the only way I could think of to ensure that we ignore paths that don’t exist.  I also think this solution gets rid of our order dependency. \n\n @Test\npublic void twoEdges() throws Exception {\n  assertMinPath(\"A1B,B1Z\", 2, \"[A, B, Z]\");\n  assertMinPath(\"B1Z,A1B\", 2, \"[A, B, Z]\");\n  assertMinPath(\"A1X,Y1Z\", 0, \"[]\");\n}\n \n\n Yes.  These all pass.  I also think three or more edges will work.  And so will some graphs with only one complete path, and with other dangling paths. \n\n @Test\npublic void threeEdges() throws Exception {\n  assertMinPath(\"A2B,B3C,C4Z\", 9, \"[A, B, C, Z]\");\n  assertMinPath(\"B3C,C4Z,A2B\", 9, \"[A, B, C, Z]\");\n}\n\n@Test\npublic void OnlyOnePath() throws Exception {\n  assertMinPath(\"A1B,B2C,C3Z,B4D,D6E\", 6, \"[A, B, C, Z]\");\n}\n \n\n But this one fails because the graph walk misses the  C3Z  edge. \n\n assertMinPath(\"A1B,B2C,C3D,C3Z\", 6, \"[A, B, C, Z]\");\n \n\n OK.  So we can’t simply walk the graph in order.  What we’re going to have to do instead is inspect every possible pathway that leads away from the starting node; and we’ll have to record our temporary variables along the way, until we get to the end. \n\n As you’ll see below, this required a fair bit of clerical effort.  I need to keep track of all the nodes, and the paths and lengths associated with those nodes.   But, other than that, the algorithm is about the same. \n\n The loop iteration is different.  It starts at the beginning node, and then walks all the  unvisted  neighbors, and stores the accumulated lengths and paths in those neighbors. \n\n Note that I used  Integer.MAX_VALUE  as a sentinel that means “Unreached from a visited node”.  We limit the search to only those nodes that have been reached, because we are still walking from begin to end.  Any node that hasn’t been reached is clearly not  next  on the path. \n\n class PathFinder {\n  private List<Edge> edges = new ArrayList<>();\n  private Set<String> nodeNames = new HashSet<>();\n  private Map<String, Node> nodes = new HashMap<>();\n  private Node endNode;\n\n  public void findPath(String begin, String end) {\n    List<String> unvisited = initializeSearch(begin, end);\n\n    for (String node = begin; \n\t     node != null; node = getNext(unvisited)) {\n      unvisited.remove(node);\n      visit(node);\n    }\n\n    setupEndNode(end);\n  }\n\n  private List<String> initializeSearch(String begin, \n\t                                    String end) {\n    nodeNames.add(begin);\n    nodeNames.add(end);\n    List<String> unvisited = new ArrayList<>(nodeNames);\n    for (String node : unvisited)\n      nodes.put(node, new Node(Integer.MAX_VALUE));\n\n    nodes.get(begin).length = 0;\n    return unvisited;\n  }\n\n  private void visit(String node) {\n    List<Edge> neighbors = findEdges(node);\n    Node curNode = nodes.get(node);\n    for (Edge e : neighbors) {\n      Node nbr = nodes.get(e.end);\n      nbr.length = curNode.length + e.length;\n      nbr.path = new ArrayList<String>();\n      nbr.path.addAll(curNode.path);\n      nbr.path.add(node);\n    }\n  }\n\n  private void setupEndNode(String end) {\n    endNode = nodes.get(end);\n    if (endNode.length != Integer.MAX_VALUE)\n      endNode.path.add(end);\n    else\n      endNode.length = 0;\n  }\n\n  private String getNext(List<String> unvisited) {\n    for (String name : unvisited) {\n      Node candidate = nodes.get(name);\n      if (candidate.length != Integer.MAX_VALUE)\n        return name;\n    }\n    return null;\n  }\n\n  private List<Edge> findEdges(String begin) {\n    List<Edge> found = new ArrayList<>();\n    for (Edge e : edges) {\n      if (e.begin.equals(begin))\n        found.add(e);\n    }\n    return found;\n  }\n\n  public int getLength() {\n    return endNode.length;\n  }\n\n  public List<String> getPath() {\n    return endNode.path;\n  }\n\n  public void addEdge(String start, String end, int length) {\n    edges.add(new Edge(start, end, length));\n    nodeNames.add(start);\n    nodeNames.add(end);\n  }\n\n  private static class Edge {\n    public final String begin;\n    public final String end;\n    public final int length;\n\n    public Edge(String begin, String end, int length) {\n      this.begin = begin;\n      this.end = end;\n      this.length = length;\n    }\n  }\n\n  private static class Node {\n    public int length;\n    public List<String> path;\n\n    public Node(int l) {\n      this.length = l;\n      this.path = new ArrayList<>();\n    }\n  }\n}\n \n\n This passes.  So now we need to add a test that has parallel paths.  Here’s a simple one that should fail: \n\n assertMinPath(\"A1B,B2Z,A1Z\", 1, \"[A, Z]\");\n \n\n And it does. \n\n To get this to pass we are going to have to detect when two pathways converge.  That’s simple.  If the length of the target node is not  Integer.MAX_VALUE  then another path has reached this node.  Since we are after the minimum, we can simply set the length on that node to the minimum of the converging pathways.   Integer.MAX_VALUE  happens to be a very convenient value for that sentinel since it substitutes for an “infinite” length. \n\n private void visit(String node) {\n  List<Edge> neighbors = findEdges(node);\n  Node curNode = nodes.get(node);\n  for (Edge e : neighbors) {\n    Node nbr = nodes.get(e.end);\n\n    int newLength = curNode.length + e.length;\n    if (nbr.length > newLength) {\n      nbr.length = newLength;\n      nbr.path = new ArrayList<String>();\n      nbr.path.addAll(curNode.path);\n      nbr.path.add(node);\n    }\n  }\n}\n \n\n We can probably speed the algorithm up a bit by terminating the search when we visit the end node. \n\n for (String node = begin; node != null && !node.equals(end); node = getNext(unvisited)) {\n  unvisited.remove(node);\n  visit(node);\n}\n \n\n And we can likely speed the algorithm even more by preferring to search the unvisited nodes that have been reached by the shortest length. \n\n private String getNext(List<String> unvisited) {\n  String minNodeName = null;\n  int minLength = Integer.MAX_VALUE;\n\n  for (String name : unvisited) {\n    Node candidate = nodes.get(name);\n    if (candidate.length < minLength) {\n      minLength = candidate.length;\n      minNodeName = name;\n    }\n  }\n  return minNodeName;\n}\n \n\n This, for all intents and purposes  is  Dijkstra’s algorithm. This implementation is not fast because I used sets and lists, and lots of inefficient structures.  Speeding it up would require a fair bit of work.  Moreover, there are some built in assumptions that would have to be fixed.  Specifically, the input graph is assumed to be directed; whereas the general algorithm makes no such assumption.  Finally, the whole thing could use a bit more refactoring. \n\n But the goal was to see whether it was possible to use TDD to stepwise approach Dijkstra’s algorithm.  I think it is; though I have to say the approach was pretty jerky.  Those first few tests drove me through several tentative algorithms that did not neatly evolve into one another.  Still, each new test exposed weaknesses in previous implementation that could be corrected in a relatively straightforward manner. \n\n Is there a better sequence of tests that would lead more directly to Dijkstra’s algorithm, without the tentative jerkiness?  Perhaps; but if so, I haven’t found them. \n\n Anyway, this was a fun exercise.  Thanks to the attendee of SCNA for recommending it. \n\n The final code, that still needs a bit of cleanup (left to the reader as an exercise ;-) \n\n package dijkstrasAlg;\n\nimport org.junit.Test;\n\nimport java.util.*;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport static org.junit.Assert.assertEquals;\n\npublic class MinPathTest {\n  private static String ANY = null;\n\n  private void assertMinPath(String graph,\n                             Integer length, String path) {\n    PathFinder pf = makePathFinder(graph);\n    if (length != null)\n      assertEquals((int) length, pf.getLength());\n    if (path != null)\n      assertEquals(path, pf.getPath().toString());\n  }\n\n  private PathFinder makePathFinder(String graph) {\n    PathFinder pf = new PathFinder();\n    Pattern edgePattern = \n            Pattern.compile(\"(\\\\D+)(\\\\d+)(\\\\D+)\");\n    String[] edges = graph.split(\",\");\n    for (String edge : edges) {\n      Matcher matcher = edgePattern.matcher(edge);\n      if (matcher.matches()) {\n        String start = matcher.group(1);\n        int length = Integer.parseInt(matcher.group(2));\n        String end = matcher.group(3);\n        pf.addEdge(start, end, length);\n      }\n    }\n    pf.findPath(\"A\", \"Z\");\n    return pf;\n  }\n\n  @Test\n  public void degenerateCases() throws Exception {\n    assertMinPath(\"\", 0, \"[]\");   //empty graph\n    assertMinPath(\"A\", 0, \"[]\");  //one node\n    assertMinPath(\"B1C\", 0, \"[]\");//no start or end\n    assertMinPath(\"A1C\", 0, \"[]\");//no end\n    assertMinPath(\"B1Z\", 0, \"[]\");//no start\n  }\n\n  @Test\n  public void oneEdge() throws Exception {\n    assertMinPath(\"A1Z\", 1, \"[A, Z]\");\n    assertMinPath(\"A2Z\", 2, \"[A, Z]\");\n  }\n\n  @Test\n  public void twoEdges() throws Exception {\n    assertMinPath(\"A1B,B1Z\", 2, \"[A, B, Z]\");\n    assertMinPath(\"B1Z,A1B\", 2, \"[A, B, Z]\");\n    assertMinPath(\"A1X,Y1Z\", 0, \"[]\");\n  }\n\n  @Test\n  public void threeEdges() throws Exception {\n    assertMinPath(\"A2B,B3C,C4Z\", 9, \"[A, B, C, Z]\");\n    assertMinPath(\"B3C,C4Z,A2B\", 9, \"[A, B, C, Z]\");\n  }\n\n  @Test\n  public void OnlyOnePath() throws Exception {\n    assertMinPath(\"A1B,B2C,C3Z,B4D,D6E\", 6, \"[A, B, C, Z]\");\n    assertMinPath(\"A1B,B2C,C3D,C3Z\", 6, \"[A, B, C, Z]\");\n  }\n\n  @Test\n  public void parallelPaths() throws Exception {\n    assertMinPath(\"A1B,B2Z,A1Z\", 1, \"[A, Z]\");\n    assertMinPath(\"A1B,A1C,A2D,C5E,B8E,C1F,D3F,F2G,G3Z,E2G\",\n                   7,\"[A, C, F, G, Z]\");\n  }\n}\n\nclass PathFinder {\n  private List<Edge> edges = new ArrayList<>();\n  private Set<String> nodeNames = new HashSet<>();\n  private Map<String, Node> nodes = new HashMap<>();\n  private Node endNode;\n\n  public void findPath(String begin, String end) {\n    List<String> unvisited = initializeSearch(begin, end);\n\n    for (String node = begin; \n\t     node != null && !node.equals(end); \n\t     node = getNext(unvisited)) {\n      unvisited.remove(node);\n      visit(node);\n    }\n\n    setupEndNode(end);\n  }\n\n  private List<String> initializeSearch(String begin, \n\t                                    String end) {\n    nodeNames.add(begin);\n    nodeNames.add(end);\n    List<String> unvisited = new ArrayList<>(nodeNames);\n    for (String node : unvisited)\n      nodes.put(node, new Node(Integer.MAX_VALUE));\n\n    nodes.get(begin).length = 0;\n    return unvisited;\n  }\n\n  private void visit(String node) {\n    List<Edge> neighbors = findEdges(node);\n    Node curNode = nodes.get(node);\n    for (Edge e : neighbors) {\n      Node nbr = nodes.get(e.end);\n\n      int newLength = curNode.length + e.length;\n      if (nbr.length > newLength) {\n        nbr.length = newLength;\n        nbr.path = new ArrayList<String>();\n        nbr.path.addAll(curNode.path);\n        nbr.path.add(node);\n      }\n    }\n  }\n\n  private void setupEndNode(String end) {\n    endNode = nodes.get(end);\n    if (endNode.length != Integer.MAX_VALUE)\n      endNode.path.add(end);\n    else\n      endNode.length = 0;\n  }\n\n  private String getNext(List<String> unvisited) {\n    String minNodeName = null;\n    int minLength = Integer.MAX_VALUE;\n\n    for (String name : unvisited) {\n      Node candidate = nodes.get(name);\n      if (candidate.length < minLength) {\n        minLength = candidate.length;\n        minNodeName = name;\n      }\n    }\n    return minNodeName;\n  }\n\n  private List<Edge> findEdges(String begin) {\n    List<Edge> found = new ArrayList<>();\n    for (Edge e : edges) {\n      if (e.begin.equals(begin))\n        found.add(e);\n    }\n    return found;\n  }\n\n  public int getLength() {\n    return endNode.length;\n  }\n\n  public List<String> getPath() {\n    return endNode.path;\n  }\n\n  public void addEdge(String start, String end, int length) {\n    edges.add(new Edge(start, end, length));\n    nodeNames.add(start);\n    nodeNames.add(end);\n  }\n\n  private static class Edge {\n    public final String begin;\n    public final String end;\n    public final int length;\n\n    public Edge(String begin, String end, int length) {\n      this.begin = begin;\n      this.end = end;\n      this.length = length;\n    }\n  }\n\n  private static class Node {\n    public int length;\n    public List<String> path;\n\n    public Node(int l) {\n      this.length = l;\n      this.path = new ArrayList<>();\n    }\n  }\n}\n \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/08/28/JustFollowingOders.html", "title": "Just Following Orders", "content": "\n       The year is 2006.  Executives at VW know that their diesel engine can not meet American emissions standards.  So they ask the enginers for a solution that does not require a redesign of the engine. \n\n Imagine the scene in that meeting room.  What was said?  What was agreed to?  We may never know all the details; but it’s clear that the executives asked the engineers to find a way to defeat the emission tests. \n\n Now think of the engineers.  What a cool problem to have to solve?  No, really!  Imagine how much fun it would be to figure out some sneaky way to bypass the emission test.  How would you do it?  Could you do it in hardware?  Could you do it in software?  How could you detect that you were on a test stand? \n\n “Wait!” one of the geeks says.  And he looks around the room, making sure all eyes are on him, playing the timing just right.   “On a test stand, … the wheels are spinning, … but the car is not moving.” \n\n “Oh, wow!” another engineer says.  “Could we use GPS to tell if the car is moving?” \n\n Imagine the brainstorming, the “good” ideas.  The coolness of knowing that there’s a really nifty solution to this problem. \n\n Imagine how pleased the executives would be with this really cool engineering solution.  Imagine how proud the engineer were. \n\n … \n\n And now one of them, James Liang, is going to jail.  There will be others who will follow him there.  Their defense, of course, was that they were just following orders.  They were just protecting their jobs – making sure they could feed their families.  But that defense didn’t fly.  It’s jail for them.  A big time in the big house: \n\n \n   “The 40-month jail sentence was [] at the high end of the maximum allowable five-year term for his crimes. Liang’s lawyer had argued that instead of jail time, he could be sentenced to a period of house arrest, arguing that he was only following orders out of ‘misguided loyalty to his employer.’“ \n \n\n And it’s fines.  Big fines for a big screwup. \n\n \n   “Despite federal prosecutors only asking for a $20,000 fine, Michigan district court Judge Sean Cox decided to make an example of the techie and ordered he pay 10 times that as a deterrent to other auto engineers and executives.” \n \n\n See this article . \n\n Engineers take note:  Your employer can’t cover for you.  Doing your job does not mean that you just follow orders.  The courts are going to hold you to a high ethical standard, even if your employer does not. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/09/29/TheUnscrupulousMeme.html", "title": "The Unscrupulous Meme", "content": "\n       There is a meme running rampant in our society, and our industry, that we need to confront and resist.  It is the unscrupulous meme:  The ends justify the means. \n\n This meme is unscrupulous by definition.  When a person has scruples it means that they choose honorable means to achieve their ends.  A person without scruples will use dishonorable means to achieve their ends.  Therefore the notion that the ends justify the means is a notion without scruples, and therefore without honor. \n\n Let us choose a real example in order to illustrate this point.  Let us assert that there should be more women programmers.  This seems like a worthy goal.  After all, the distribution of the sexes in software favors males by 20:1 or so. \n\n What honorable means could we employ to increase the number of women programmers? \n\n Before we can answer that, we ought to try to determine what the cause of the 20:1 ratio might be.  Allow me to enumerate a few of the many possibilities: \n\n \n   Some in our industry may not view women as programmers. \n   Some in our industry may prefer men programmers over women programmers. \n   Some in our industry may actively discourage women from becoming programmers. \n   Some in our industry may abuse women programmers. \n   Women, in general, may not be as interested in programming as men are. \n \n\n I think it is apparent that there are honorable means to address all of these causes.  These include educational programs that help people see women as programmers; and work to reverse any conscious, or unconscious prejudice against women.  Also included would be disciplinary standards and consequences for the active discouragement and/or abuse of women.  Lastly, but of no lesser importance, would be public relations campaigns to help women become more interested in programming. \n\n What about hiring quotas?  Should we demand that more women be hired?  Is this honorable?  At first blush this might seems like a good tactic.  Upon reflection, however, it must inevitably require that hiring decisions are not to be based upon merit.  No matter how hard a candidate tries, no matter how much a candidate succeeds, that candidate may be passed over for someone of lesser merit solely because of their gender.  It is very difficult to find honor in that.  I would dare to say that the  only  honorable hiring policy is one that is entirely merit based. \n\n What about reverse social pressure?  Could we use that to achieve our ends?  What if we see a speaker at a conference using language and/or metaphors that could be construed to demean women programmers? Should we organize against that speaker? Should we boycott any conferences where that speaker speaks? Should we urge conference organizers to avoid, or even disinvite, such speakers?  Shall we put pressure on that speaker’s employer to terminate their employment?  Should we physically attack that speaker’s servers and websites?  Are these honorable means to achieve our end? \n\n Clearly not.  Such means are not honorable because they are unilateral and asymmetric.  They are done in relative secrecy by a few, but they affect many.  The few set themselves up as the judge, jury, and executioner.  The accused is tried in absentia, and in secret, without the ability to offer a defense; and is then sentenced to be labeled a pariah, and to be ostracized from the industry.  The sentence is enforced throughout the industry by the fear of reprisal.  And the many, who could have benefited from the teachings of that speaker, are denied that benefit. \n\n Not only are such a tactics dishonorable, they are disgusting.  Anyone living in a free society should judge them to be abhorrent and reprehensible.  The honorable way to deal with a bad idea is not to suppress that idea, but to  present a better idea.   If we see someone using rhetoric that we believe demeans women programmers, then we should present better rhetoric, with better ideas, to counter them. \n\n Unfortunately, what we see in our industry (and in our society) today is an ever increasing use of the unscrupulous meme that says:  any  means is acceptable if it pursues the achievement of our ends. \n\n And so we saw Doug Crockford  disinvited from Nodevember  for making a muscle.  We saw  cleancoders.com  endure a Ddos attack[*], because of  this blog . And we saw James Damore fired for suggesting that women may not be as interested in programming as men are. \n\n The firing of Damore was dishonorable. The disinvitation of Doug Crockford was dishonorable.  Any Ddos attack is both dishonorable and criminal. \n\n Here’s a better idea: \n\n \n   Have scruples.  Be Honorable! \n \n\n \n [*]  Thanks to all our supporters who made the two days of that attack among our highest revenue days on record. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/09/26/SierraJulietFoxtrot.html", "title": "Sierra Juliet Foxtrot", "content": "\n       \n   Thanks to John Sonmez for the  title . \n \n\n Clean Architecture  is on the stands!  The response has been terriffic.  Thanks for all your support. \n\n But just you wait until the SJFs read the chapter on  Frameworks .  OMG!  Their poor little snowflake hearts will just melt away.  I mean, I actually had the  chicharrones  – the  horchata  – to use a (gasp)  relationship metaphor !   I mean, this is far worse than the offense that caused  Doug Crockford  to be disinvited from  Nodevember ! \n\n Speaking of Doug Crockford and Nodevember, why do you think he was disinvited?  I mean, Nodevember had offered him the keynote slot last year; and then suddenly revoked it under the banner of  making the conference more comfortable for all .  Could it be that the SJFs skulked around the dark corners of the net and tweeted him down?  Oh, yes, I think it could be.  I mean, my god, the man had used the word “sangrita”!  (or was it “quesadilla”?) \n\n Skulking .  Yeah, that’s the S in SJF.  Little skulkers who slither and slime around the net whispering innuendoes and lies onto people’s screens – making them nervous – making them fear – calling into question their political correctness and their social standing. Making them wonder  who  might be saying  what  about them – and  when . \n\n You wouldn’t think anything quite so  juvenile  (yes, that’s the J in SJF) would actually scare someone – but it does.  Oh yes!  It works because  cowards abound .  The cowards at Nodevember disinvited Crockford because he dared clench his bicep for 500 ms  during a talk, and some SJF was (gasp)  offended . \n\n Oh, and he might have used the word “ranchero” too! \n\n It was too much for the SJFs to tolerate, so they tweeted their nasty little keystrokes and scared the Nodevember organizers shitless.  One of the little SJFs actually threatened to (gasp) not speak at any conference where Crockford spoke.  (I’ve actually had a few SJFs make the same threat about me.  So far I haven’t noticed any dearth in invitations.) \n\n The SJF methods are  fascist  in nature.  (Yes, that’s the F in SJF).  They won’t confront you if you offend them.  Oh no. They have no need to talk, debate, or try to come to an understanding.   They understand already!   So, instead of talking to you about it, they’ll send their slimy little tweets to people who might employ or hire you.  They’ll demand to know the nature of the relationship those employers have with you. \n\n The implications are clear.  You are a  pariah , and all employers, supporters, friends, and everyone else, must immediately declare their distance from you or suffer the –  consequences.  (OH God, NO! Not –   THE CONSEQUENCES!   Brrrrr.) \n\n And so, with  one  exception that I know of, the dirtly little cowards at Nodevember quacked (yes, that’s the right word) in their boots, filled their pants, and then  publicly shamed  a good man who had contributed much to their community; and who had harmed no one.  (Except that he might have used the word “camarones”.) \n\n By that action, the organizers of Nodevember became  SJF meat .  The SJFs ate them for lunch. And then the SJFs turned their pestilential little keyboards towards another target – and another – and another.  Relentlessly pursuing their twisted and perverse agenda for what they call “social justice”.  Two words that they have no right to use, because their methods are both anti-social and unjust. \n\n Illumination is the SJF cure. \n\n If you happen to come accross the slime trail of an SJF, shine a light on it.  If you read a tweet that appears to be a skulking juvenile fascist innuendo, send that tweet to the target of the innuendo, so that they know about it. \n\n For example, if you see a tweet something like: “I didn’t think I could have less respect for Bill Davis; but here I am.”  Send that tweet to Bill Davis directly, and include the name of the original tweeter.  I’m sure Bill Davis might like to respond. \n\n If we shine enough light on the SJFs, perhaps they will see just how anti-social and unjust their ways have become.  Perhaps they will begin to use more honorable methods to pursue the more reasonable points in their agenda. \n\n And then, perhaps, that “F” can be changed back to What is Was. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/10/03/TestContravariance.html", "title": "Test Contra-variance", "content": "\n       Do you write unit tests? \n\n \n   Yes, of course! \n \n\n Do you write them first? \n\n \n   Yes, I follow  the three laws of TDD . \n \n\n What is the difference in module structure between your tests and your code? \n\n \n   I create one test class per production class. \n \n\n So if you have a production class named  User  you will have a test class named  UserTest ? \n\n \n   Yes, almost always. \n \n\n So the structure of your tests, and the structure of your code are  covariant . \n\n \n   Um.  I suppose so, yes. \n \n\n And so you are coupling the structure of your tests to the structure of your production code. \n\n \n   I hadn’t thought about it being a coupling before; but, yes, I suppose I am. \n \n\n So when you refactor the class structure of your production code, without changing any behavior; do your tests all break? \n\n \n   Well, yes.  Of course. \n \n\n And that means you can’t run your tests while you are refactoring, doesn’t it? \n\n \n   Yes, yes, it does. \n \n\n So then you can’t really call it refactoring, can you? \n\n \n   Why not? \n \n\n Because refactoring is defined as a sequence of small changes that keep the tests passing at all times. \n\n \n   Well, OK.  I guess by that definition, the changes aren’t refactoring. \n \n\n Instead, you have to commit yourself to a big change and hope you can put everything back together again – including the tests. \n\n \n   Yes, yes.  What of it? \n \n\n That is an example of the Fragile Test Problem. \n\n \n   The Fragile Test Problem? \n \n\n Yes.  A common complaint amongst people who try TDD for the first time.  They note that small changes to the production code can cause large changes to the tests. \n\n \n   Yeah.  That’s really frustrating.  I almost gave up on TDD when I first encountered it. \n \n\n Unfortunately, that is a common reaction. \n\n \n   OK, but what can be done about it? \n \n\n Design the structure of your tests to be contra-variant with the structure of your production code. \n\n \n   Contra-variant? \n \n\n Yes.  The structure of your tests should not be a mirror of the structure of your code.  The fact that you have a class named  X  should not automatically imply that you have a test class named  XTest . \n\n \n   But wait.  That breaks the rules! \n \n\n What rules? \n\n \n   The rules that say that there should be one test class per class. \n \n\n There is no such rule. \n\n \n   There isn’t?  I’m sure I’ve read it and seen it. \n \n\n Not everything your read and see is a rule. \n\n \n   Fair enough.  But if the structure of the code and the tests must be, um. contra-variant, then how should the tests be structured? \n \n\n First, let’s agree on a basic fact.  If a small change in one module of a system causes large changes in many other modules of the system, the system has a design problem. \n\n \n   Yes, I think that’s obvious – software design 101 so to speak. \n \n\n Then, clearly, if a small change to the production code causes large changes to the tests, there is a design problem. \n\n \n   I see that point, yes. \n \n\n Therefore the tests must have their own design.  They cannot simply follow along with the structure of the production code. \n\n \n   Hmmm.  I see.  If the two designs are the same, then they are coupled; and that coupling causes fragility. \n \n\n Right.  The coupling between the tests and the production code must be minimized. \n\n \n   But wait!  The tests and the code must be coupled because they both describe the same behavior. \n \n\n Correct.  Their behavior is coupled; but their structure need not be coupled.  And even the behavioral coupling need not be as tight as you think. \n\n \n   Can you give me an example? \n \n\n Suppose I begin writing a new class.  Call it  X .  I first write a new test class named  XTest . \n\n \n   But you said we shouldn’t do that. \n \n\n Bear with me.  We’ve just begun.  As I add more and more unit tests to  XTest  I add more and more code to  X . \n\n \n   And you refactor that code! \n \n\n Indeed I do.  I refactor it by extracting private methods from the original functions that are called by  XTest . \n\n \n   And you refactor the tests too, right? \n \n\n Absolutely!  I look at the coupling between  XTest  and  X  and I work to minimize it.  I might do this by adding constructor arguments to  X  or raising the abstraction level of the arguments I pass into  X .  I may even impose a polymorphic interface between  XTest  and  X . \n\n \n   You’d do all that just for a test? \n \n\n Think of it this way.  The  XTest  is just the first client of  X .  I always want to decrease the coupling between clients and servers.  So I used the same techniques I would use in normal production code to reduce the coupling between the  XTest  and  X . \n\n \n   OK, but the structure of the tests is still the same as the structure of the code.  You still have  X  and  XTest . \n \n\n Yes, at the class level they are the same; and that’s about to change.  Before we explore that change, however, I want you to note that there are already profound structural differences at the method level. \n\n \n   Um.  Sure.   XTest  is just using the public methods of  X ; but most of the code is now in the private methods that you extracted. \n \n\n Right!  The structural symmetry is already broken.  But now it’s going to break even more. \n\n \n   How so? \n \n\n As I look at all those private method in  X  I will inevitably see that there are ways to group those methods into different classes.  One group of methods will use a particular subset of the fields of  X .  That group can be extracted as a class. \n\n \n   But you don’t write a new test for that class, do you? \n \n\n No!  Because every bit of the code within that new class is being covered by the tests that are still just using the public API of  X . \n\n \n   And this process continues, doesn’t it? \n \n\n Yes!  More and more functions are extracted.  More and more classes are discovered.  After awhile we have a whole family of classes in the production code that sit behind that simple API of  X . \n\n \n   And they are all tested by  XTest . \n \n\n Right!  The structure has been almost perfectly decoupled.  What’s more the API of  X  has been successively refined to be so narrow and abstract that it is minimally coupled to the clients that use it; including  XTest . \n\n \n   OK, I see that.  I see that the structure of the tests can vary independently from the structure of the production code.  And I agree that that’s a good thing.  But what about the behavior.  They are still strongly coupled by behavior. \n \n\n Think about what’s going while  X  is being developed.  What’s happening to  XTest ? \n\n \n   Well, more and more tests are being added to it; and the interface with  X  is being progressively narrowed and abstracted. \n \n\n Right.  Now say that first part again. \n\n \n   More and more tests are being added? \n \n\n Right.  And each one of those tests is entirely concrete.  Each one of those tests is a small specification of a very particular behavior.  Taken together the sum of all the tests is… \n\n \n   The specification of the behavior of the  X  API. \n \n\n Right!  So as development proceeds the test suite becomes more and more of a specification – it becomes more and more  specific . \n\n \n   Sure.  I see that. \n \n\n But what is happening to the classes behind the  X  API?  What would any good software designer do when confronted with an ever growing list of specifications? \n\n \n   Well, of course, the way we deal with complex specifications is to generalize. \n \n\n Correct!  Instead of writing code for each and every case of every paragraph of every specification, we find ways to make the code  generic . \n\n \n   Why does this matter to the coupling of the behavior? \n \n\n As development proceeds, the behavior of the tests becomes ever more specific.  The behavior of the production code becomes ever more generic.  The two behaviors move in opposite directions along the generalization axis. \n\n \n   And this reduces coupling? \n \n\n Yes.  Because while the behavior of the production code  satisfies  the specifications within the tests, it also has the ability to satisfy a whole spectrum of unspecified behaviors. \n\n And that last bit is absolutely essential, because no test suite can specify every required behavior.  The production code  must  generalize the subset of behaviors specified by the tests to  all  the behaviors required of the system. \n\n \n   So you are saying that the test suite in incomplete? \n \n\n Of course!  It is entirely impractical to specify absolutely everything.  So what happens instead is that we gradually increase the generality of the production code until every test that we could possibly write will pass. \n\n \n   Woah!  We keep writing failing tests in order to drive the generality of the production code to a point where it becomes impossible to write another failing test.  Woah! \n \n\n Woah indeed.  But here’s the thing.  The act of generalizing  is the act of decoupling .  We decouple by generalizing! \n\n \n   Oh, wow!  And so we decouple structure AND behavior.  Wow. \n \n\n Right!  Now, give me a recap. \n\n \n   OK.  Um.  The structure of the tests must not reflect the structure of the production code, because that much coupling makes the system fragile and obstructs refactoring.  Rather, the structure of the tests must be independently designed so as to minimize the coupling to the production code. \n \n\n Good!  And what about behavior? \n\n \n   As the tests get more specific, the production code gets more generic.  The two streams of code move in opposite directions along the generality axis until no new failing test can be written. \n \n\n Good!  I think you’ve got it. \n\n \n   Yeah!  Contra-variance FTW! \n \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/10/04/WomenInDemand.html", "title": "Women In Demand", "content": "\n       The demand for programmers is high, and growing: \n\n \n   “According to the U.S. Bureau of Labor Statistics, software developer jobs are expected to grow 17% from 2014 till 2024. They categorize this growth as “much faster” than the average rate among other professions.”    Forbes 2017 \n \n\n The supply of programmers is well behind the demand. \n\n \n   “In an average month, there were 115,058 unique job postings for software developers and 33,579 actually hired. This means there was approximately 1 hire for every 3 unique job postings.”  –  Emsi 2017 \n \n\n Why then is the industry ignoring nearly 50% of the population? \n\n \n   “92.1 percent of the respondents to Stack Overflow’s 2015 developer survey identified as male; only 5.8 percent identified as female. (The remaining respondents either chose “other” or declined to answer.) The survey was conducted online in February, using ads placed on Stack Overflow, and included respondents from 157 countries.”  –  Splinter 2015 \n \n\n Could this be a clue? \n\n \n   “Computer Science and Engineering majors have stagnated at less than 10% of all degrees conferred [to women] in the U.S. for the past decade, while the demand for employees with programming and engineering skills continue to outpace the supply every year”  –  Randal S. Olson 2014 \n \n\n Or this? \n\n \n   “The share of computer science majors who are women also has been in a near constant decline since the early 1980s, with women representing about 15 percent of computer science majors in 2011.”  –  Inside Higher Ed 2015 \n \n\n This is made all the more poignant given that women outnumber men in college by 2:1. \n\n \n   “Although college enrollment has increased, the ratio of male to female students is nearly 1:2 “  –  Denver Post 2017 \n \n\n So, even though twice and many women enroll in college than men, fifteen times as many men become programmers than women. \n\n This ratio is much worse than the CS graduation rates. \n\n \n   \n     Of the 812,669 bachelors degrees earned by men in 2015, 48,840 (6%) were in Computer science.   NCES \n   \n   \n     Of the 1,082,265 bachelors degrees earned by women in 2015, 10,741 (1%) were in Computer Science  NCES \n   \n \n\n So five times as many men than women graduate with BSCS degrees.  Then, nearly two out of three of those women do not pursue a programming career. \n\n I gave a keynote talk to several hundred programmers recently.  I asked all the programmers to stand up; and then asked all the men to sit down.  A paucity of people remained standing.  Perhaps one in twenty. \n\n I don’t want to overstate it; but that’s a significant signal. The question is, what does it signify and what can be done about it? \n\n The question is not an idle one.  The demand for programmers is not going to abate.  We are going to need more and more programmers over the next several decades.  The lack of supply is going to drive salaries higher and higher. You’d think that massive numbers of young people, both men and women, would be lining up. And yet most women are not.  Why not? \n\n I have pair programmed with women who coded rings around me.  I  know  women can code.  So why do so many who graduate in CS take another direction.  And why do so many entering college fail to consider the career? \n\n Is it possible that women are being actively rejected by the job market?  Given the high demand and the low supply, that possibility seems absurd.  Businesses that are starved for programmers can’t afford to filter based on gender.  They can’t afford to filter based on anything except merit[1], and they’ll have to compete for that.  What’s more, there are many employers who have engaged in outreach programs specifically aimed at recruiting minorities and women.  So, whatever it is that is shunting women away from becoming or remaining programmers, it’s difficult to believe that it’s a hiring policy. \n\n Is it possible that women are being actively resisted by the colleges?  In a student body that is dominated 2:1 by women, you’d think a problem like that would be resolved very quickly.  No institution is going to want to alienate two thirds of it’s paying clients.  Any institution that did, would find itself in deep financial trouble very quickly. \n\n If a plurality women students expressed interest in computer science, the universities would bend over backwards to accommodate them.  In fact, they are bending over backwards to accommodate them.  Or at least  some  are.  The rest are certainly very concerned. \n\n Some researchers think the problem is really in high school. \n\n \n   “Researchers found that computing appeal for girls peaks in middle school, where having an inspiring teacher and thinking that coding is “for girls” are instrumental in sparking interest. The appeal dips in high school in what researchers call the “high school trap” because of a lack of friends in coding classes or the lack of those classes at all.”  –  USNews 2016 \n \n\n We can envision this as a pipeline of potential programmers.  This pipeline begins in primary school or before, and wends its way through middle school, high school, college, early internships and jobs, and finally to careers.  Somewhere along this pipeline, and perhaps at many places, women are shunted aside. \n\n That shunt may be very early indeed.  As Maggie Johnson, Director of Education and University Relations at ( ironically ) Google, says: \n\n \n   “It’s important to note that there are no biological or cognitive reasons that justify a gender disparity in individuals participating in computing (Hyde 2006). With similar training and experience, women perform just as well as men in computer-related activities (Margolis 2003). But there can be important differences in reinforced predilections and interests during childhood that affect the diversity of those choosing to pursue computer science.  \n \n In general, most young boys build and explore; play with blocks, trains, etc.; and engage in activity and movement. For a typical boy, a computer can be the ultimate toy that allows him to pursue his interests, and this can develop into an intense passion early on. Many girls like to build, play with blocks, etc. too. For the most part, however, girls tend to prefer social interaction. Most girls develop an interest in computing later through social media and YouTubers, girl-focused games, or through math, science and computing courses. They typically do not develop the intense interest in computing at an early age like some boys do – they may never experience that level of interest (Margolis 2003).”  –  Google Research Blog 2015 \n \n\n Director Johnson’s notion is, to put it very simply, that women tend not to be as interested in programming as men because, as very young girls, they prefer different forms of play. \n\n Whether she is correct or not is a matter of some debate.  However, it would appear that there are potential shunts all along the pipeline, from the very start to the very end.  How can we remove those shunts? \n\n There seems to me to be little downside in encouraging young girls and young women to consider programming as an entertaining activity, something fun to learn, and eventually a good career.  This could be undertaken by parents, schools, and employers using various educational and public outreach programs.  I also think that the boot-camp idea has merit, especially as an alternative to university.  All these mechanisms help to unclog the upstream portions of the pipeline, where it will do the most good. \n\n Is there a way to increase the demand?  After all, as demand increases so do salaries.  Won’t this attract women into the industry?  Unfortunately this is the strategy we have been using for the last few decades, and it does not seem to be working.  In fact, despite the increase in demand, and in salaries, the trend for women seems to be going in the opposite direction.  HuffPost 2015 \n\n What about hiring quotas?  Will they help to unclog the pipeline?  The theory of a quota program might be that it artificially increases the demand for women.  But, again, demand isn’t the problem.  It’s already sky high and rising. \n\n What quotas will do, however, is to redirect women into those companies that employ quotas, and redirect men into those companies that do not.   Let’s imagine a scenario. \n\n Assume that there are two sets of employers.  Set G represents the big, high profile, companies who hire 10% of all programmers.  They have big beautiful buildings, and lavish work areas.  They have all the glitz and glitter we have come to expect of big software companies.  They attract the best and brightest; and can keep them.  Resumes and candidates flood into G; but only a few make the cut.  If G’s demand is not sated, it is only because G keeps their standards very high. \n\n Set R represents all the rest.  These are small to medium sized employers who need programmers just as much as G does, but have to fight much harder to get the best people.  Resumes and candidates do not flood into R.  Instead, R must pay search firms and employment agencies for resumes.  They seldom see the best of the best. \n\n Now let’s use the numbers from above, rounded for convenience.  In 2015 there were 50,000 men, and 10,000 women who graduated with BSCS degrees.  G will hire 10% or 6,000 of them; 1,000 women, and 5,000 men.  R will hire the remaining 9,000 women, and 45,000 men.  G will choose the top decile from the batch.  R will draw from the rest. \n\n Now clearly I am making a lot of assumptions here.  There are many reasons why top decile programmers may not want to work at G.  I have completely bypassed the effect of startups, culture, and location.  Still, as a general principle, we should expect that G will succeed in hiring a disproportionate number of the top decile. \n\n I am also assuming that programmers can be divided up by a single performance dimension and assigned into deciles.  This is clearly simplistic.  There are many dimensions that an employer will use to select employees to hire.  Still, it should be clear that employers will attempt to rank the acumen of candidates; and choose from among those who rank highest. \n\n The end result of our thought experiment is that G and R will end up with the same ratio of men and women, and that the relative ranking of men and women in each set will be the same.  G gets the 6,000 in the top decile, and R gets the rest. \n\n Now let’s assume that G decides to establish a quota to hire 2,000 women, instead of 1,000.  This necessarily means that they must lower their standards to accept the top two deciles of women, while still maintaining their standards for the top decile of men.  In fact, since they will be hiring fewer than 10% of the available men, they can actually raise their standards for the men.  This means that, on average, men will outperform women in G. \n\n R will hire 8,000 women who rank at the 7th decile and below.  R will also hire 46,000 men who rank from just above the 8th decile and below.  The result is that R will see a lower ratio of women to men (17% instead of 20%), and on average the men will outperform the women. \n\n One argument that can be presented against this analysis is that G will not have to lower it’s standards in order to hire women, because G will be looking at “other factors” that compensate for raw acumen or ability.  Those factors may include culture, personality, etc. \n\n This may indeed be true.  However, regardless of what set of complex criteria G uses to hire more than the normal share of the women, R will have to choose from women that G passed over. \n\n A quota policy may seem to be helping women by offering them a greater chance of employment.  However, at least in this simplistic thought experiment, the net result is to tilt the playing field in favor of men. \n\n \n   \n     No more women get hired overall, because demand is too high to allow any candidate, male or female, to go unhired. \n   \n   \n     In the vast majority of environments (R) women will appear scarcer than they really are, magnifying and reenforcing the male signal. \n   \n   \n     In every environment the women will be working with men who, on average, will outperform them. \n   \n \n\n On top of all that, G will have hired 1,000 employees who fall below their normal standards.  Overall the performance of their staff will be decreased. They must also face the ethical question of bypassing more qualified candidates in favor of gender – which is clearly discriminatory. \n\n So it is difficult to see how a quota program helps anyone at all[2]. \n\n \n [1] I use the term “merit” in the sense of academic or professional achievement. \n\n [2] I am fully aware that this post will generate a fair number of nasty responses.  That’s the way of things in our current polarized environment.  My hope is that it also stimulates some reasoned and civil discourse.  I do not hold that my conclusions are absolutely correct in every regard.  My goal is to learn from others.  So if you can tell me, kindly, and respectfully, where the flaws are in my analysis, I would be grateful. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/10/04/CodeIsNotTheAnswer.html", "title": "Tools are not the Answer", "content": "\n       I just finished reading an extremely depressing  article  in  The Atlantic  entitled:  The Coming Software Apocalypse .  The article does a good job, at first, of describing several terrible software bugs that have harmed, maimed, and killed people.  But then the article veers off in a direction that I found disheartening. \n\n The author of the article interviewed many thought leaders in the industry, but chose only those thought leaders who were inventing new technologies.  Those technologies were things like  Light Table ,  Model Driven Engineering , and  TLA+ . \n\n I have nothing against tools like this.  I’ve even contributed money to the Light Table project.  I think that good software tools make it easier to write good software. However, tools are not the answer to the “Apocalypse”. \n\n Nowhere in the article did the author examine the possibility that programmers are generally undisciplined.  The author did not interview software experts like Kent Beck, or Ward Cunningham, or Martin Fowler.  The author completely avoided the notion that the software apocalypse might be avoided if programmers just did a better job.  Instead, the author seemed convinced that the solution to the software apocalypse – the solution to bad code – is more code. \n\n I disagree.  Tools are fine; but the solution to the software apocalypse is not more tools. The solution is better programming discipline. \n\n Last night I watched the pilot episode of  The Wisdom of the Crowd .  It’s a fun new series on CBS.  The hero of the story is the inventor of a huge social network platform whose daughter is murdered.  He leaves everything to invent a new social network called Sophie which uses crowd sourcing to solve crimes. \n\n They must have a few programmers consulting on the show because there was one scene that made me hang my head in shame.  Someone posts a video of a crime scene on the platform.  It leads the investigators in a certain direction that doesn’t pan out.  Later they find that the video was actually much longer and contained clear evidence; but that the Sophie platform had truncated it to 30 seconds causing the investiators to miss that evidence. \n\n The hero was furious.  Murderers might have escaped.  More people might have died.  So he confronted the lead developer and demanded to know how this could have happened.  He was told that one of the programmers had reused some code from a different platform and had not realized that it had a built-in 30 second truncation.  The hero was livid.  He demanded to know which programmer had been so sloppy.  The lead developer refused to say.  She told him that if he wanted to fire someone, he should fire her.  So the hero backed down. \n\n Later, the guilty programmer thanked the lead developer for protecting him.  He said: \n“I knew I shouldn’t have reused that code, but we were in a rush.”  She smiled at him and told him not to worry about it. \n\n And right there, ladies and gentlemen, you can see both the cause of the apocalypse, and the obvious solution. \n\n The cause: \n\n \n   Too many programmer take sloppy short-cuts under schedule pressure. \n   Too many other programmers think it’s fine, and provide cover. \n \n\n The obvious solution: \n\n \n   Raise the level of software discipline and professionalism. \n   Never make excuses for sloppy work. \n \n\n This is the point that the author of the Atlantic article missed entirely.  The one thing he failed to consider was that the reason we are facing bugs that kill people and lose fortunes, the reason that we are facing a software apocalypse, is that too many programmers think that schedule pressure makes it OK to do a half-assed job. \n\n I found it astounding that in that entire long article, the notion of testing was never examined as a solution – it was only presented as a forlorn and foolish hope.  At one point he said of bugs: \n\n \n   You could do all the testing you wanted and you’d never find them all. \n \n\n True as this may be, it is not a reason to discard, or reduce, or fail to increase testing as a discipline. \n\n Several of the tools that the author presented were simply glorified REPLs.  They allow the programmers to immediately visualize the results of their code. \n\n I think that such rapid feedback is wonderful.  I think that rapid feedback of that kind is extraordinarily valuable.  But REPLs don’t replace tests. \n\n Whenever I hear that a programmer “tested it in the REPL” I cringe.  You don’t  test  things in the REPL; you  try  things in the REPL.  A test is much more formal than a trial.  A test can be repeated.  A test can be enhanced.  You can add to a test.  You can review a test.  You can add a test to a larger suite of tests. A test is a document.  A test is a  program . \n\n Better REPLs are not the answer.  Model Driven Engineering is not the answer.  Tools and platforms are not the answer.  Better languages are not the answer.  Better frameworks are not the answer. \n\n Yes, those things are shiny, and they sparkle and glisten; but… \n\n \n   “The fault, dear Brutus, is not in our stars, But in ourselves…“ \n \n\n I stood before a sea of programmers a few days ago.  I asked them the question I always ask:   “How many of you write unit tests on a regular basis?”   Not one in twenty raised their hands. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/11/18/OnThePlateau.html", "title": "Living on the Plateau", "content": "\n       Languages have evolved quite a bit over the years.  The early evolution from machine language to assembler was necessary and obvious.  The evolution from assembler to Fortran and Basic was also necessary and obvious.  We might even say that the evolution of COBOL was predictable, if not entirely necessary. \n\n Let’s look at just one thread through that evolution: \n\n  1946    1949     1958       1960     1966    1967  1970  1972 1980   2009\nBinary -> Asm -> ALGOL58 -> ALGOL60 -> CPL -> BCPL -> B -> C -> C++ -> Go\n \n\n What drove all these steps Why was Algol58 not good enough?  Or BCPL?  Or B? \n\n One factor, of course, is that we were just starting to learn about computer languages at that time.  It was difficult to separate the language from the architecture of the machine.  You can see the architecture of the hardware peeking out ever more clearly as you go back in time through these languages.  C, as abstract and portable as it was, remained very close to “the metal”.  Even Go admits that below all the turtles there’s some hardware. \n\n Another factor – and probably a much more important factor – is that the machines were getting exponentially more powerful.  In 1958 computers were slow, fragile, had 16K or less memory, and cost many millions of dollars.  By 1972 you could buy a fast 64K PDP-7 for ~$100,000. \n\n C held sway for a good long time.  Not because computers weren’t getting more powerful; but because they were shrinking.  They were shrinking in both size and cost.  In 1980 you could buy an 8085 microcomputer with 64K of RAM for a few thousand dollars.  And so even though the top end computers were getting more and more powerful; the bottom end machines remained the perfect size for C. \n\n But through the 80s the power and capacity of the machines continued to grow exponentially.   By 1990, C was just too small a language for the tasks at hand.  We needed something better.  And C++ was waiting in the wings. \n\n The C++ era was short lived because Java/C# came along in the latter half of the decade.  By this time machines had gotten so vastly powerful that it was possible to implement whole systemes in  virtual  machines.  Just ten years before:  that  would have been  unthinkable . \n\n But the unthinkability wasn’t done with us.  By 2010 PHP, Ruby, and Python –  interpreted languages  – were running some of the biggest, and most profitable, systems on the planet. \n\n I want you to sit back for a second and contemplate just how  crazy  this is.  We, who used to worry about microseconds, now cavalierly push systems to production written in languages that execute in textual interpreters! \n\n So what is it that drove all this language evolution?  It should be quite apparent that the current state of our languages can be traced directly to Moore’s Law: the fact that computer speed, component density, and memory size  doubled  every 18 months; while at the same time, size, cost, and power consumption changed just as dramatically in the opposite direction. \n\n The reason we have Clojure, Elixr, Swift, Dart, F#, Scala, and Go today is the sheer raw power of our machines.  It is that power that enables Eclipse, Vim, EMacs, and IntelliJ.  It is that power that allows us to tolerate garbage collection, and to ignore memory, ignore processor speed, ingore efficiency.  It is that power that allows us to behave as though all resources are infinite – because, for nearly all intents and purposes,  they are . \n\n Most of us work in an environment of virtual infinities.  There may be some contraints out there somewhere, but most of us know we will never bump into them.  That’s what happens when you ride Moore’s trains through twenty-eight doublings. \n\n But one of those trains reached the plateau and pulled into its final station over a decade ago.  Clock rates doubled from a few kilohertz to 2.8ghz and then, simply, stopped. \n\n \n\n There were those amongh us who expected this.  Others found it jarring.  Many people thought we had many more doublings to go.  That’s the way it is with limits – often you don’t see them coming until you hit them. \n\n We took some solace in the fact that although clock rates had plateaued, density was still on the rise.  We took even more solace, along with a fair bit of trepidation, from the fact that processor cores were multiplying.  We saw the first dual core machines in the early years of the new millenium.  Quad core machines followed within a few years.  We were all quite sure that the 8, 16, 32, and 64 core machines were on their way. \n\n Indeed, some of us viewed this as a crisis.  Multiple cores meant multiple threads – and not the kinds of thread we had been used to.  Multi-processor threads are wild beasts compared to the well behaved threads that are administered by an operating system.  So we looked to functional languages for our salvation. \n\n Functional langauges put extreme discipline on the mutation of state (i.e. assignment operations).  Disciplined use of assignment is the key to dealing with multiple threads.  You can’t have concurrent update problems if your threads don’t actually update anything. \n\n And so F# and Scala and Clojure and Elixr and even Erlang started to soar in popularity. \n\n But the 8 core processors never came – at least in the laptop space.  We’re still waiting.  And it’s looking more and more as though Moore’s law for density has hit the wall. \n\n \n\n Now perhaps you want to make the argument that the Apple A10X has 6 processing cores and 12 GPU cores.  Yes, I’ll admit there is still progress being made in density.  However, that progress appears to be incremental rather than exponential. \n\n I think it is quite realistic to expect that the speed and density of computers have reached their practical limits.  It’s just possible that we have been living on the plateau for the last decade; and that we will remain on this plateau for the foreseeable future. \n\n We knew this was going to happen.  We knew it was just a matter of time.  We had hoped that there might be a few more doublings ahead of us – but that appears not to be the case. \n\n As a bit of anecdotal evidence for this I submit my lovely wife’s Macbook air.  This machine was purchased back in 2013.  It works quite well for her.  Every time I encourage her to buy a newer model she rebuffs me by saying that the machine is quite adequate for her needs.  Who among us would have said such a thing about a 5 year old machine back in the 90s?  Back then 5 years was three doublings! \n\n If we aren’t going to see 1024 core processors in the near future, what need have we of functional programming languages?  Oh, don’t get me wrong.  I think functional programming is a good idea.  Every programmer should learn it.  But the crisis that we anticipated – the crisis that drove the current infatuation with functional languages – seems not to be occurring.  And that may well mean that functional languages never achieve the ascendency that we had anticipated for them.  It may just be that Java/C#, and Ruby/Python, and Swift/Dart/Go are, and will be, sufficient. \n\n If Moore’s law was the driver of our language evolution, what will drive it now?  Could it be that we are living at the beginning of a period in which language evolution will slow it’s frenetic pace?  Will we see the number of languages cease it’s relentless rise, and begin to decline?  Will our industry gradually abandon the exuberance of adolescence and settle down into a stable period of adulthood and middle age? \n\n I, for one, hope so.  I, for one, look forward to the end of the barnstorming era and the onset of the era of professional, and ethical craftsmanship.  An era where we discard the detritus of the excesses of our youth, and settle upon a small complement of languages, platforms, and frameworks, with which to carry out the long work that is ahead of us. \n\n I, for one, having started near the bottom, and having climbed the plateau all the way to it’s current dizzying height, look forward to a long and healthy life up here. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/01/15/behindThePowerCurve.html", "title": "Operating Behind the Power Curve", "content": "\n       I promise that this blog is about software.  So bear with me for a bit. \n\n What happens when you increase the throttle in an airplane?  You go faster, right?  More power to the engine means more thrust which means more speed. \n\n Most of the time this is true; but there’s a different mode you can get the aircraft into which reverses this relationship.  It’s called:  the region of reversed command . \n\n Remember the four forces that govern flight: gravity, lift, thrust, and drag.  Lift opposes gravity, and thrust opposes drag.  An airplane, in straight and level flight, balances all these forces perfectly. \n\n Thrust, of course, comes from the engine.  Lift comes from the air flowing around the wings, and drag…?  Well, drag comes from two different sources. \n\n Parasitic drag is simply the cost of plowing through the air.  It is the air resisting the movement of the plane.  But there’s another kind of drag called  induced drag . \n\n Induced drag is caused by the pilot.  It happens when the pilot raises the nose of the aircraft.  Raising the nose, changes the angle of the wings causing the lift vector, which is always perpendicular to the wings, to point a bit  backwards , thereby  opposing  the forward motion of the aircraft. \n\n If you raise the nose just a little, the induced drag is small, and so increased thrust still causes increased speed.  But if you raise the nose a lot, then the induced drag can cancel out the thrust.  This is called getting  behind the power curve . \n\n \n\n The graph[1] shows an airplane in straight and level flight.  To the right, you can see that the speed and power have a positive relationship.  The more power, the more speed.  But to the left, in the region of reversed command,  it takes more and more power to go slower and slower . \n\n In other words, the pilot has the nose so high that the thrust vector is being defeated by the backwards pointing lift vector; and the plane is kind of  mushing  through the air on raw power, barely making any headway. \n\n \n   Can you see where I’m going with this? \n \n\n Except during the final moments of landing, pilots don’t usually operate their aircraft behind the power curve.  It’s a bit dangerous back there.  The slower you go, the more power you need.  If you go slow enough, you’ll max out your power and descend with the stall warning screaming in your ears.  So pilots stay in front of the power curve by watching their airspeed, and keeping it above the inflection point. \n\n So what does this have to do with software?  (As if you haven’t already guessed.) \n\n Too many software teams operate behind the power curve  all the time .  Rotten code is  induced drag .  These teams have created so much induced drag that it takes a huge effort to make any forward progress.  The team mushes forward at full power, barely making any headway.   Indeed, many teams have maxed out their power and are in a slow uncontrollable descent. \n\n How does a pilot get out from behind the power curve?  By lowering the nose.  This brings the lift vector to vertical allowing the thrust vector to dominate, and the plane screams off into the wild blue yonder. \n\n How does a software team get out from behind the power curve?  By lowering their noses, cleaning up the messes, and reducing the induced drag.  With that drag gone, and all the power they have, the wild blue yonder is theirs to explore. \n\n Wouldn’t it be great if we could invent an airspeed indicator and a stall warning horn for software teams?  Oh, yeah, we did!  It’s called the velocity chart.  Good Agile teams operate in front of the power curve, because the velocity chart allows them to see their speed, and keep it in front of the inflection point.  When the velocity starts going down, good agile teams increase their refactoring to eliminate the induced drag. \n\n Startup culture in the U.S.  believes  in operating behind the power curve.  That’s where they think they  want  to be.  They are so focussed on fast progress, and so convinced that high quality means low speed, that they abandon discipline and principles for the sake of the goal. This is a tragedy. \n\n They start out believing that power and speed are related without paying any attention to drag.  So they haul back on the yoke, put their noses into the sky, ram the throttle forward, and then burn fuel madly while going nowhere in a hurry.  They don’t understand that when you make a mess, you induce drag, and you cancel out your power. \n\n \n\n [1] https://i2.wp.com/aviationglossary.com/wp-content/uploads/2015/08/region-of-reversed-command.png?ssl=1 \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/12/18/Excuses.html", "title": "Excuses", "content": "\n       The parallels between double entry bookkeeping and Test Driven Development are deep and plentiful. \n\n \n   \n     Both are disciplines used by experts who carefully manipulate complex documents full of arcane symbols that must, under pain of terrible consequences, be absolutely correct in both type and position. \n   \n   \n     Both involve representing a long sequence of granular gestures in two different forms on two different documents. \n   \n   \n     Both techniques update their documents one granular gesture at a time, and each such update concludes with a check to be sure that the two documents remain in balance with each other. \n   \n \n\n To put this in more concrete terms: \n\n \n   Accountants enter each transaction into two different accounts.  One is a Liability account.  The other is an Asset or Equity account.  These accounts are summed on the balance sheet, and the following relationship must hold:  Assets + Equities = Liabilities. \n   Accountants are trained to enter transaction one at a time, checking the balance sheet after each such entry. \n   Programmers who practice TDD enter each new granule of behavior in two different programs.  One is a test program.  The other is the desired production program.  The two must execute in a complimentary fashion demonstrating that the production code works as the tests describe. \n   Programmers are trained to add one granule at a time to each program, and then to execute the tests after each such addition. \n \n\n As I said, the equivalence between the two disciplines is stark.  They are virtually identical approaches. \n\n And no wonder.  Both disciplines serve the same purpose.  Both allow those of us who maintain complex documents full of arcane symbols that must be absolutely correct under pain of severe consequences, to be confident that those consequences will not be encountered. \n\n Do accountants have deadlines?  Do managers put pressure on them to finish their accounting by a certain date?  Of course!  There’s no difference in the pressure.  Accountants feel the pressure just the same as programmers do. \n\n Are the programs of the programmers somehow less important than the accounts of the accountants?  Of course not!  Those programs are the instruments that make or save the company money.  They are critically important.  They are easily as important as the accountants’ accounts. \n\n So then why are accountants able to maintain their discipline so assiduously and so completely?  Can you imagine a group of accountants saying to each other: “Guys, we’re really under the gun.  We’ve got to finish these accounts by Friday.  So, do all the Assets and Equities.  We’ll come back and do the Liabilities later.” \n\n No.  You  can’t  imagine that.  Or, at least, you shouldn’t.  Accountants can go to jail for doing things like that. \n\n So then why is it that so many programmers wail and moan when confronted with the discipline of TDD?  Why do they present so many reasons why TDD is impractical, or unreasonable, or… \n\n Can you imagine an accountant making excuses like these: (All taken from articles about TDD) \n\n \n   I never do double entry bookkeeping because keeping track of all those accounts takes too much time. \n   Accounts don’t need to be prefect, they only need to be good enough.  Double entry bookkeeping is overkill. \n   All these accountants who practice double entry bookkeeping are just too religious about it.  They’re following an unnecessary dogma. \n   Balancing the Assets and Equities with the Liabilities doesn’t actually prove that the accounts are correct.  So double entry bookkeeping is too much work for too little benefit. \n   Double entry bookkeeping is a scam promoted by consultants who are just trying to make money by selling books and courses and videos. \n   Double entry bookkeeping is dead.  The guys at Andersen consulting wrote a blog about it. \n   I am against double entry because experience shows that it prevents high quality account design from emerging. \n   I thought double entry was an elaborate practical joke when I first heard of it.  It’s monumentally dumb. \n   Double entry doesn’t work.  I don’t care what you’ve heard.  I don’t care how much your manager wants it.  I don’t care how much the stress-spattered eyes of your coworkers gleam in their endorsement.  It. Doesn’t. Work. \n   Double entry promotes account design damage. \n   Not everything is accountable. \n   Double entry locks the account design. \n   Double entry sounds good in theory.  But, in practice, it’s not clear how good it is. \n   Life is short and there are only a finite number of hours in a day.  So we have to make choices about how we spend our time.  If we spend it by making double entries that is time we are not doing something else. \n   Using double entry does not guarantee that you’ll design good accounts. \n   I’m going to go out on a limb here and declare with brutal honesty that it’s  literally  a ritualistic waste of time. \n   Another concern is the debated degree of perfection to which one must do double entry to do it successfully.  Some insist that if double entry isn’t done persistently by everyone on the team from the beginning of the project, you’ll only suffer. \n   Double entry rides on guilt.  It encourages procedure over understanding. It has tons of doctrines and slogans. \n   For me, the double entry zealots are religious loonies knocking on my door, trying to prove that my way of doing things is irreparably broken and the only path to salvation is double entry. \n   What annoys me about double entry is the fact that there are a lot of rules or guidelines about it. \n   On your first double entry project there are two big losses, time and personal freedom. \n \n\n I could go on.  (And on.  And on.) You will find that there is no lack of silly, misinformed, and disgruntled detractors. \n\n The bottom line, for me, is simple.  TDD is a good discipline for ensuring that the complex documents, full of arcane symbols, are crafted in such a way so as to avoid significant negative consequences.  I know of no other discipline that comes close. \n\n Double entry bookkeeping was invented by the Koreans over a thousand years ago.  It was independently reinvented by the Italians over six hundred years ago.  It flourished and spread along with the capitalism and economic prosperity that it helped to secure.  However it’s adoption was not without resistance and delays.  The last countries to standardize on double entry bookkeeping did so in the twentieth century. \n\n Let’s hope it doesn’t take 500 years for a discipline of testing to becomes the standard for software developers. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/04/02/InTheLarge.html", "title": "In The Large", "content": "\n       From the very first moments of the Agile revolution we pondered the question of  Agile in the Large .  How could we take the principles of light-weight, iterative, incremental, high-feedback development and apply them to truly  huge  projects? \n\n At first the answers were things like  Scrum of Scrums .  The idea was to recursively apply the principles of Agile development at ever higher levels of scale.  A project that required more than one team of 5-12 developers could be built by two such teams with a higher level team to “oversee?” them. \n\n Note the question mark.  As we start to consider large projects, we can’t avoid hierarchy; but hierarchy seems anathema to Agile.  After all, Agile is all about egalitarianism.  It is a rejection of command and control.  It is a rejection of plans and schedules and… \n\n \n   Oh Bollocks!  It is not! \n \n\n Agile was a revolution in the sense of “a turning of the wheel”.  In the earliest days of software we wrote code in an agile way.  We wrote little bits, tested them, built them into bigger bits, tested those bits, in a never ending cycle.  If you went back to the late 1960s and watched the way code was written, you’d see bits of Agile peeking out. \n\n Of course we were greatly hampered by the hardware.  Compiles took hours.   Editing was done on teletypes.  Most programmers back then didn’t know how to use a keyboard at all; so they had keypunch operators type in their code for them.  In that kind of environment quick feedback loops were difficult to achieve. \n\n Even so, we did what we could to shorten those feedback loops.  We wrote in assembler so we could sit at the console and debug by patching our software in octal or hex.  We’d test our code by executing it in a debugger, or even by single stepping the computer.[1]  We got pretty good at this.  It was the only way to get things done in anything like a reasonable amount of time. \n\n But the wheel turned.  We started using languages that were not easy to debug at the console.  We started to write bigger and bigger programs.  To make that work in an environment with such long feedback loops we needed  plans . \n\n This is the environment from which waterfall emerged.  When it takes a full day to go around the edit/compile/test loop, lots of planning and checking are necessary.  You can’t to TDD and Refactoring in a 24 hour loop! \n\n But the wheel kept turning.  Moore’s law put us on an exponential curve that most of today’s programmers have no inkling of.  We went from 24 hour turnarounds in 1970, to 60 minute turnarounds in 1980, to ten minute turnarounds in 1990, to ten second turnarounds in 2000.  And by 2005 the turnaround time for most programmers was sub second. \n\n This is the environment in which Agile emerged.  Agile was a return to the quick turnaround, high feedback, development strategies of the 1960s but with much more powerful machines, much better languages and tools, and much larger projects. \n\n But Agile also emerged from the flames.  Waterfall, though necessary in the 70s and 80s, was painful in the extreme.  We learned a lot, during those decades, about what  not  to do.  So when Agile emerged in the late 90s it carried with it the lessons learned through those dark times. \n\n Agile, was not simply a return to short feedback cycles.  Agile imposed  disciplines  on top of those short feedback cycles.  Disciplines like testing, refactoring, pairing, and intense automation.  It’s true that Agile was a turning of the wheel; but when wheels turn, they drag the vehicle forward.  Agile definitely moved us forward from the strategies of the ’60s. \n\n But forward in what?  What was it that the Agile revolution improved? \n\n Agile was a revolution in how relatively  small  teams can develop relatively  small  software projects.  Note the emphasis on the word  small . \n\n An Agile team is great at creating a software system of 100,000 lines or so.  And 100,000 lines can do a hell of a lot.  So for many companies one or two Agile teams is sufficient for just about anything they want to do. \n\n On the other hand, if you need to create a system of ten million lines, a single Agile team isn’t going to cut it.  You need about a hundred teams to build a ten million line system. \n\n But how do you manage a hundred agile teams?  How do you feed stories to them.  How do you coordinate the interfaces between them?  How do you segregate those ten million lines of code so that the teams can work independently of each other? \n\n And how (and this was the real question) do you do that in an “Agile” way? \n\n \n   Answer:  You don’t! \n \n\n Here’s the thing.  We, humans, are actually very good at building big projects.  We’ve known how to do this for a very long time. \n\n \n\n Just think of the  truly huge  projects that we, humans, have accomplished. \n\n \n   Apollo: We put men on the moon! \n   D-Day: We invaded Normandy with 156,000 troops along a 50 mile, heavily fortified, border. \n   We have a world economy that supports 8 billion people. \n   The globe is ensconced in a massive digital network allowing you to read this blog on your phone while hiking in the woods! \n   You want a thingamajig?  You can get it delivered tomorrow, or even today, with a few taps on your phone. \n \n\n I don’t think I need to go on.  We, humans, are really quite good at doing big things.  It’s  who  we are.  It’s  what  we do.  We put red sports cars into solar orbit.  We do big things. \n\n So why are we worried about big software?  We already know how to build big software.  We’ve been doing it for 50 years or more.  The “big” part was never actually the problem.  The problem that we solved with Agile was the  small  part.  What we  didn’t  know how to do well, was build  small  projects. \n\n We’ve always known how to do big projects.  You do big projects by breaking them up into a bunch of  small  projects.  Agile solved the  small  part of that.  Agile really has nothing to do with the big part. \n\n But, but, but, but…  Egalitarianism!  The rejection of plans and of Command and Control!  Agile! \n\n \n   Bollocks! \n \n\n Agile was never about egalitarianism.  Agile was never about the rejection of plans, or  the rejection of command and control.  Indeed, Agile was the  embodiment  of the lowest level unit of command and control:  The squad . \n\n Yes, at some level down the hierarchy command and control stops being effective.  A small squad of individuals can work together in short cycles with lots of feedback and intense communication to achieve an objective.  That’s an Agile team.  At that level strict command and control is intensely harmful.  But above that level command and control begin to become necessary.  The higher you go, the more concrete and obvious that becomes.  You don’t design, build, produce and sell hundreds of millions of iPhones without an awful lot of command and control. \n\n There are quite a few Agile in the Large strategies out there.  Books and blogs have been written about the topic.  Whole consulting companies have been created to transition companies to use Agile in the Large approaches.  There is nothing wrong with this. \n\n There is nothing wrong with the strategies and techniques described by these Agile in the Large approaches.  Except one thing.  They aren’t Agile.  They have nothing to do with Agile.  Rather, they are Agile “flavored” variations on the strategies and techniques that humans have been using for millennia to get big things done. \n\n The flavor comes from the use vocabulary and concepts from Agile.  There’s nothing wrong with that flavor – it’s fine.  If you find that it helps to use the words and concepts from Agile, go for the flavor.  But don’t be overly concerned with the actual “Agile-ness” of it.  Once you are doing something big, you have left the realm of Agile.  Hopefully your development teams are using Agile disciplines; but the overall project is not Agile. \n\n Because Agile is about the  small  things. \n\n \n [1] In those days, computers had switches on the front panel that allowed you to stop execution, and then step through a program one instruction at a time.  The front panel had lights that showed the contents of the various registers.  This allowed you to watch what was happening as your program executed. \n\n Look at the switches at the bottom right in the image below.  You’ll see  Sing Step  and  Sing Inst .  A single  step  was one cycle of the clock.  An instruction often took several clock cycles.  So you could literally watch the internals of how the instructions were executed. \n\n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/02/25/UncleBobFlyIn.html", "title": "Uncle Bob Fly-In.", "content": "\n       I just got my pilot’s license.  That means I can fly!  So now I want to fly –  to you!   I want to fly to anyone within a 500 nautical mile radius of Chicago (see map below), to give talks, conduct workshops, and provide training… \n\n At Half Price! \n\n 500 nautical miles includes cities like: \n\n \n Ann Arbor \n   Cincinnati \n   Cleveland \n   Columbus \n Detroit \n   Duluth \n   Fort Wayne \n   Grand Rapids \n Green Bay\n Indianapolis\n Kansas City\n Lincoln \n Louisville\n Madison\n Milwaukee\n Minneapolis \n Nashville\n Omaha\n Pittsburgh\n Rochester \n Rockford\n Sioux City\n Sioux Falls\n Springfield, IL \n Springfield, Mo\n St. Louis\n Toledo\n Toronto \n \n \n\n Why half price?  Well, because, there are some things you’ll have to be flexible with. \n\n \n   \n     The Date .  Flying an airplane is weather dependent.  I might have to call you up the night before, or the morning of, and reschedule.  Sorry, I’m not flying through thunder storms to get to you.  No.  Not going to happen.  Deal with it! \n   \n   \n     The Duration . One of my goals is to be home every night.  I love my wife!  I love my family!  What can I say?  I want to be home.  So I can fly out in the morning, and fly back in the evening.  An overnight stay is possible if you really want it; but we’d have to talk about the price. [Grin]. \n   \n \n\n Local airports are usually pretty friendly places.  Many have meeting rooms.  So if you host a user group you might want to consider contacting a local airport to see if you can bring 10, or 20, or 30 people in.  I’ll fly there.  We can have a day long workshop. \n\n Or, if your company would like to have me for a day (or maybe two [grin]) I could fly out and conduct a course, or a lecture, or a keynote, or… \n\n Well, the sky’s the limit!  [Big Smile]. \n\n Perhaps you are wondering why I’d want to do this.  Well, first of all, flying is a hoot!  I enjoy the hell out of it. \n\n But secondly, I live smack in the middle of  fly-over country .  This, it seems to me, is an area of the country that is badly underserved by speakers like me.  There are lots of programmers inside that circle who never see authors, or speakers, or industry experts.  I’d like to do my part to change that. \n\n So whaddya say?  Send an email to my diligent and delightful assistant (and Daughter) Angela Brooks (brooks@cleancoder.com), and start the conversation. \n\n Let’s have a fly in! \n\n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/12/03/BobbyTables.html", "title": "Bobby Tables", "content": "\n       \n   SQL  is demon spawn, and no self-respecting software developer should ever use it. \n \n\n OK, that’s a little hyperbolic.  Demons did not create  SQL .  Indeed, the folks who created it were filled with nothing but good intentions. \n\n But you know what they say about the road to hell. \n\n I want you to think about just what a supremely bad idea it is to use a textual data access language.  Such a language can pass through the user interface of a system and provide unauthorized access to all the data contained within. \n\n Now, of course, we all know that we are supposed to scan all our inputs for potential  SQL  injection ( SQLi ) attacks.  And yet, hundreds of thousands, if not millions of users have had their data stolen by just this mechanism.  Why?  Because it is unreasonable to expect that every single user input of every single system is going to have the protections required. \n\n The problem, of course, could have been eliminated at the source, decades ago.  Back in 1998  rain.forest.puppy  described, in  Phrack , how to slip  SQL  statements in through a user interface and execute them.  The instant that article was published  every single programmer in the world should have ceased to use  SQL  on the spot! \n\n I find it absolutely amazing that  SQL  is still used.  Did we learn nothing from Equifax, or Yahoo, or…  Well, I mean, it’s been just about everybody hasn’t it? \n\n And here we all are, comforting ourselves that our current frameworks will handle the issue.  Hibernate will handle the issue.  JPF will handle the issue.  Rails will handle the issue.  Poppycock!  Frameworks don’t handle the issue; programmers do!  And programmers haven’t been handling that issue all that well; have they? \n\n Here’s an idea: \n\n \n   STOP USING SQL! \n \n\n SQL  is the ultimate security breech.   SQL  is a portable, universal, textual language that can be transmitted through the user interface of a system and, if passed to the  SQL  engine, can provide absolute access and control to  all  the data in the system. \n\n The very idea that  SQL  statements might come in through the user interface and be held in RAM ought to fill you with unmitigated  terror !  All it takes is for some poor idiot programmer to fail to remember the exact arcane gestures that offer the appropriate protections. \n\n For example, can you tell which of these statements is vulnerable to a  SQLi  attack?  The language is Ruby, and the framework is Rails (circa 2015). \n\n # Unsafe\nUser.where(\"email = '#{email}'\")\nUser.where(\"email = '%{email}'\" % { email: email })\n\n# Safe\nUser.where(email: email)\nUser.where(\"email = ?\", email)\nUser.where(\"email = :email\", email: email)\n \n\n Can you see the vulnerability?  Do you understand just what combinations of question marks, hash marks, parentheses, and percent signs makes a statement vulnerable? \n\n Do you understand that these kinds of statements appear thousands of times in a typical application?  Do you realize that if even one such statement has the wrong combination of question marks and parentheses it opens the system to a  SQLi  attack?  Isn’t it obvious that, so long as there is a SQL engine in the system, there is simply no reliable way to guarantee that such an attack can be prevented? \n\n Oh, don’t get me wrong.  A good clean architecture can absolutely prevent  SQLi  attacks.  If you put your  SQL  engine below an architectural boundary, and you make absolutely sure that all source code dependencies cross that boundary pointing away from the  SQL  engine.  And if you make absolutely certain that there is no  SQL  above that boundary.  And if you make absolutely certain that no text that crosses that boundary going towards the  SQL  engine has any  SQL  in it.  Then you will absolutely prevent  SQL  attacks. \n\n But there are too many “absolutelys” in that paragraph.  You never know when some 22 year old programmer, working at 3AM under a horrific schedule pressure, will forget to use just the right  ?  and  #  in just the right positions. \n\n The solution.  The only solution.  Is to eliminate  SQL  from the system entirely.  If there is no  SQL  engine, then there can be no  SQLi  attacks. \n\n What would replace  SQL ?  An API of course!  And  NOT  an API that uses a textual language.  Instead, an API that uses an appropriate set of data structures and function calls to access the necessary data. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2017/12/09/Dbtails.html", "title": "Dbtails", "content": "\n       A Bit of History about Bits. \n\n Memory was always the problem.  Once we had established that we could  process  information with relays, or vaccuum tubes, or transistors; the problem was where to keep the information we were processing. \n\n \nRandom Access Memory (RAM) within the computer was tricky during those days.  By the time I became a programmer, in 1968, the move to  core  memory was pretty well established.  But in the years just before that there were many different schemes, including accoustic waves through tubes of mercury, or magnetic drums, or even using electron beams to store charge on the surface of a Cathode Ray Tube. \n\n Indeed, one of the early programmable calculators that I used in High School,  the Olivetti-Underwood Programma 101 , stored its information in accoustic waves travelling down steel wires. \n\n Paper with Holes. \n \nEarly computers used paper tape and punched cards to store the information being processed.  In my earliest years as a programmer, back in 1968 when computers were frightfully expensive, I worked for a company that offered computing services to companies.  Those companies would ship us massive crates full of punched cards containing their business records.  We’d process that data and punch a new massive pile of punched cards that we’d crate up and ship back to them.  It was quite an operation. \n\n \nWhat’s more, we had a whole room full of equipment just for dealing with cards.  We had card sorters, and card duplicators all over the place.  We even had little patch panel machines that could read cards and make small modifications to the information on those cards – like punching new sequence numbers.  Much of the preparation work for a big batch job was done on these machines because they were a lot cheaper than time on the big IBM 360. \n\n Magnetic Tape. \n \nI started working as a programmer just as magnetic tape was replacing punched cards.  As you can imagine, shipping reels of magnetic tape back and forth to the customer was a lot cheaper, and a lot more reliable, than shipping crates of cards.  What’s more, the computer could read those tapes a lot faster than it could read cards, so time on the computer was reduced by a huge margin. \n\n At first, of course, the cards were simply copied to tape.  A card was 80 bytes of data, so the data on tape was simply a linear array of 80 byte records.  But as time went by we realized that there was no need to limit ourselves to 80 byte records on tape.  So tape record size became independent of cards. \n\n \nBut as fast and convenient as tapes were, the information density was still pretty low.  You could store 800 bytes per inch on those tapes.  So a megabyte was a thousand inches.  A gigabyte would have required almost sixteen  miles  of tape.  What’s more, the data on tape was stored  serially .  There was no way to get the data on the end of the tape other than to read the whole tape.  Random access was simply not an option. \n\n Disks. \n \nDisks were invented to solve that problem.  Disks were thin platters of metal with a magnetic coating.  A dozen or so of these platters were stacked up on a spindle and then spun at high speed.  Typically 3600 RPM.  Magnetic read-write heads were mounted on servo motors so they could move radially across the platters. \n\n The data on the disk were written onto circular tracks.  Each platter could have several hundred of these tracks.  The tracks were subdivided into records that could hold a fixed amount of data.  Since the platters were stacked up vertically, we called the vertical group of tracks  cylinders .  In order to read a record from the disk, the programmer would seek the heads to the appropriate cylinder, select the appropriate head, and then wait for the desired record to travel around the spindle until it came under the head.  We called the address of a record a  CHR , or Cylinder-Head-Record. \n\n Programmers could format the disk any way they liked.  They could decide that the first 20 cylinders should hold Employee records that were 250 bytes long; and that the next 50 cylinders should hold item records that were 500 bytes long.  We could arrange the record size and the number of records per track, as part of the design of our application.  We could even split the disk vertically such that the top few platters held index information while the bottom platters held the data.  We could set that disk up any way we liked.  The data on the disk had a  fixed formal arrangement . \n\n Of course that fixed formal arrangement meant that the disks were specific to a particular application.  When it was time to run the Bill of Materials application, the computer operators would load the Bill of Materials disks into the disk drives. Those disks were formatted just for that Bill of Materials application. \n\n File Systems. \n As time went on we realized that we wanted to use the disks in a different way.  We wanted easy access to the capacity and random access that disks afforded without imposing a fixed formal arragnement to our data.  So we invented file systems.  In general, files systems required that we format the disks in a uniform way.  Every track was formatted to hold a relatively small number of records (called sectors).  All tracks were formatted identically.  We invented direcrories, and indexes, and files, and all of the trappings that we have become so used to in our daily work. \n\n File systems allowed us to view the memory on disk as named arrays of bytes.  They imposed no fixed formal arrangement of the data.  Rather, we could have any number of directories, containing any number of files, containing any number of bytes. The actual structure of the disk itself was invisible to this view.  It did not matter to us which cylinder, head, and record we were accessing.  We simply created, wrote, read, and deleted individual files.  The structure of the disk had been entirely abstracted away and decoupled from the structure of the data. \n\n Relational Databases. \n As convenient as file systems were for storing documents and simple linear files, they weren’t all that convenient at accessing vast arrays of fixed-sized, organized records.  Finding the  Employee  Record for  Bob  in the  Employees.dat  file usually implied a linear search, or some kind of ad hoc, nightmarish indexing scheme.  And so other modes of access were invented.  One of those methods was the  Relational Database . \n\n Relational databases abstract away the physical nature of the disk, just as file systems do; but instead of storing informal arrays of bytes, relational databases provide access to sets of fixed sized records.  Each record type is called a table, and each record within a table is called a row.  Each row has a unique identifier called a key. \n\n This is similar to the way we conceived of disks in the early days, except back then we specifically formatted the disk to hold the fixed sized records, and we used the  CHR  as the key. \n\n SSD. \n The disks are going away.  Your laptop or desktop computer almost certainly does not contain a piece of spinning metal.  (If it does, I’m sorry. ;-)  Within the next few years.  all  the disks will go away.  They will have been replaced by solid state memory – RAM.  (We call it SSD.  What could that “D” stand for?  Not “disk”, certainly.  Not “drive”.  What?) \n\n Every time our technology has changed, from Cards, to Tape, to Disk, new access methods were invented.  The older technology fell away, and newer modes of access were adopted. \n\n Will this happen with SSD?  Will file systems and relational databases fall to the wayside?  If so, what will replace them? \n\n With each new technology a fact that we considered essential was abstracted away.  Cards were linear arrays of 80 byte records, period.  There wasn’t any arguing about that.  It was a physical limitation of the medium. \n\n Magnetic tape abstracted away the record size limit; but maintained the linear sequence. \n\n Disk abstracted away the linear sequence, but replaced it with that strange physical  CHR  addressing scheme. \n\n File systems abstracted away the physical structure of the disk but provided no easy access to fixed sized records. \n\n Relational databases (which often ride on top of file systems nowadays) provided random access to fixed sized records. \n\n What does SSD change?  What physical limitation can be abstracted away because of this change? \n\n Well, the one big thing that has changed is  time .  Accessing SSD is  fast .  There is no rotaional latency.  There is no seek time.  The time to access a byte in SSD is the same regardless of where that byte is. \n\n So what does this mean for us? \n\n Tape made cards faster; but changed the way we viewed records.  Disks made tape faster but changed the way we viewed access.  SSD makes  everything  faster but changed the way we viewed _____.  Fill in that blank. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/01/18/TheCitizenshipArgument.html", "title": "The Citizenship Argument", "content": "\n       An old friend of mine recently posted a video blog that I found to be  brilliant . \n\n Mike (GeePaw) Hill presented  Five Underplayed Premises of TDD  on a web page that contains an article and a video.  The video and the article appear to be identical in content; so you can read, or listen, or both.  However, there is much more information in the video, because Mike’s passion is an effective seasoning to the solid content of his article. \n\n The primary thrust of Mike’s message is that TDD helps you to: \n\n \n   Deliver Value Faster. \n   Exercise Good Judgement. \n   Improve Internal Quality. \n   Increase Efficiency. \n   Improve External Quality. \n \n\n Mike makes these points quickly, adeptly, and convincingly.  The video is a joy to watch.  The article is a joy to read.  They make me hope that the GeePaw series of video blogs will continue to produce such high quality content for a very long time. \n\n A Quibble. \n But I have a quibble.  And it’s kind of a big one – at least for me.  And so, for the rest of this article, I shall employ the  Writer’s Workshop  convention of using the phrase “The Author”, instead of the author’s name. \n\n It’s not a quibble with any of the author’s major points.  Those points are perfectly solid.  It’s a quibble with something the author said in an aside. \n\n While discussing that TDD helps us deliver value faster, the author says: \n\n \n   “TDD is not about good citizenship. You are not immoral if you don’t TDD. You’re not not a good looker forwarder or a protector of the future. It’s not about citizenship. TDD isn’t even about raising the quality of your code. Now TDD very often does increase the quality of teams that aren’t doing it to begin with, but that’s actually neither here nor there, because TDD isn’t about that. TDD is about more value faster.\n \nThe other thing it’s not about? It’s not about art and it’s not about craftsmanship. It’s not about the excellence of our high standards. The reason we test drive is by test driving, we go faster. We put more value into the field faster than if we didn’t do TDD. And that is the money premise. We’re in this for the money.” \n \n\n These two paragraphs are entirely inconsistent with the rest of the article in the following ways: \n\n \n   \n     The author asserts that TDD is not about quality; and yet his third and fifth points assert that TDD helps internal and external quality.  If you accept those points then clearly TDD  is  about raising the quality of your code, as well as the quality of your product. \n   \n   \n     The author asserts that TDD is not about art, or craftsmanship, or excellence of our high standards; and yet his five points stress  judgement , and  quality , and  efficiency  which are  the very essence  of craftsmanship, art, and high standards. \n   \n   \n     The author asserts that TDD is not about good citizenship or morality.  Herein lies the crux of my quibble; and so I must explain at some length. \n   \n \n\n By “citizenship” I presume the author means  citizenship in the software community .  By “morality” I presume the author means  professional ethics .  In the following discussion we’ll stick with the author’s words and infer their meanings appropriately. \n\n How can programmers be good moral citizens if they are not using the best means at their disposal to: deliver value faster, use good judgement, improve internal quality, increase efficiency, and improve external quality?  Is the author suggesting that a programmer who, out of negligence, goes slow, uses bad judgement, makes an internal mess, create gross inefficiencies, and delivers bad features is being a good moral citizen?  I think that’s absurd. \n\n Now, to be fair, it may be that the author’s point is that TDD is not the  only  way to achieve these five goals; and that therefore TDD is not the only way to be a good moral citizen of the software community.  Whether that is so, or not, is irrelevant to the point.  The fact that TDD helps with those five goals  means  that TDD is about good moral citizenship.  It may not be the only path to that good moral citizenship; but it is a path. \n\n Let’s say this a different way.  Does the author believe that he would continue to be a good moral citizen of the software community if he abandoned TDD?  Now, since I know the author to be such a good moral citizen, the only possible answer to this question is: Yes!  Of course.  Because the only way the author would abandon TDD is if he found some better means to achieve those five goals. \n\n And so we can deduce that the five goals are the issue; and not TDD.  Striving to achieve those five goals is what makes you a good moral citizen of the software community.  TDD is merely one of the paths that assist in that striving. \n\n That’s all well and good.  However, since TDD is a path towards the goals, it is not fair to say that TDD is not about the goals. TDD  is , in fact, about being a good moral citizen of the software community.  TDD  is  about professional ethics. \n\n Lastly, let me say that I eagerly await a better path to those goals.  So far, I haven’t found one. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/03/29/WeProgrammers.html", "title": "We Programmers", "content": "\n       The Good. \n \n \n\n There is a little red sports car heading out towards the asteroid belt and,  we programmers  put it there.  Oh, I don’t mean to give short shrift to Elon Musk and all the rocket scientists and engineers at SpaceEx.  It was their vision, and their accomplishment.  But they couldn’t have done it without us. \n\n Think, for a moment, about all the software involved in that project.  Think about the automation in the spacecraft itself.  Think about the ability of those boosters to land, in tandem.  Think about the steering vanes, and the engine gimbals, and the throttles.  Think about ground control, and the communication protocols, and… \n\n Think about how the engineers worked.  Think about the CAD/CAM software.  Think about the NC machines, and the 3D modeling software.  Think about the fluid dynamics simulations, the finite element analyses, the orbital calculations, the spreadsheets, the word processors, the email, the text messages, the phone calls… \n\n I think you see where I’m going with this.  Every minute step along the pathway from the dream, to the realization, was lubricated, enabled, enhanced, and simplified by software.  Billions and billions of lines of software that  we programmers  wrote.  [Yes, the Sagan-ism was intentional.] \n\n Now think about what this event means to our civilization.  Yes, it was a token – a gesture – a mere droplet in the sea of potentials.  But what a droplet!  Just think of the sheer chutzpah, the colossal, arrogant, exuberant, joyous wastefulness!  It was the peacock spreading it’s opulent tail feathers.  It was the prong-horn antelope leaping into the air out of sheer enthusiasm.  It was an expression of our rejection of limits, and our willingness to flippantly expend massive resources to achieve a tiny portion of a passionate dream. \n\n It was a message that we sent to ourselves, and to the universe at large, saying that we are coming, and nothing in this universe will stop us.  And it was  we programmers  who, more than anyone else, enabled the sending of that message. This is something that you, and I, and all programmers everywhere should feel very good about. \n\n The Bad. \n \n \n\n Elaine Herzberg is dead.  She was struck by a “self-driving” car while walking her bicycle across the road.  And  we programmers  killed her.  Oh, I don’t mean to say that any programmer maliciously, or even negligently, wrote the code that killed her.  But, make no mistake about it, it was the code that killed her. \n\n Perhaps there was an  IF  statement somewhere in that code that, had the boolean predicate been in the opposite state, would have prevented the collision.  Or perhaps it was a function that generated a number that, had the number been different by a few bits, would have prevented the collision. \n\n We may never be able to identify that  IF  statement, or that function.  Machine learning neural networks are insidiously difficult to understand.  Even if the car’s log files contain all the inputs, and we can replay the event over and over again, we may never really understand, in the maelstrom of weights, and averages, and feedback loops, just why the car behaved the way it did. \n\n But what we  can  say is that  we programmers  wrote the code that killed her.  And this is something that you, and I, and all programmers everywhere should feel very bad about. \n\n The Ugly. \n There is a sentiment amongst programmers that arguments of ethics and morality should play no part in our discussions about disciplines and practices.  Those who hold this sentiment suggest that our practices and disciplines should be a matter of pure logic and economics.  Given the two scenarios above, I find this disturbing.  It seems to me that ethics and morality have become  intrinsic  to everything  we programmers  do; because so very much depends upon the quality of our work. \n\n Our Motto. \n\n It is well past the time that  we programmers  can safely isolate ourselves from the rest of the world.   We programmers  must no longer hide in our little techie bubbles.  The code  we programmers  write  matters .  It  matters  to the hopes and dreams of our society and of our civilization.  It  matters  to people walking their bicycles across the street.  It matters to anyone and everyone because the code  we programmers  write lubricates, enables, enhances, and simplifies virtually every aspect of daily life.  From something as small as a young mother checking her baby monitor, to something as large as international nuclear-weapons policy, and interplanetary travel, our code  matters . \n\n Recently Grady Booch tweeted something that I think  we programmers  should adopt as our motto: \n\n \n   Every line of code represents an ethical and moral decision. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/08/28/CraftsmanshipMovement.html", "title": "The Tragedy of Craftsmanship.", "content": "\n       \n   You look pensive. \n \n\n Yes.  I just read a  transcript  of Martin Fowler’s keynote at Agile Australia, 2018.  He called it  The State of Agile in 2018 . \n\n \n   Ah, yes, a great talk. \n   \n     Beware the Agile-Industrial-Complex. \n     Maintain Technical Excellence. \n     Products over Projects. \n  Great stuff!  So what bothers you? \n   \n \n\n In that talk he said that the formation of the Software Craftsmanship movement was tragic. \n\n \n   Yes.  He’s right. \n \n\n He is?  How could that be?  I thought Software Craftsmanship was a good thing. \n\n \n   Oh, it is.  It’s a very good thing. \n \n\n But then why…? \n\n \n   The tragedy is that the Agile movement was supposed to promote the ideals of Craftsmanship; and it failed.  Horribly. \n \n\n I don’t understand. \n\n \n   The Agile movement got so involved with promoting conferences and with certifying Scrum Masters and Project Managers that they abandoned the programmers, and the values and disciplines of Craftsmanship. \n \n\n But I thought it was programmers who started the Agile movement. \n\n \n   Yes.  It was.  That’s the grand irony.  It was programmers who started the Agile movement as a way to say: “Hey look!  Teams matter. Code should be clean.  We want to collaborate with the customer.  And we want to deliver early and often.” \n \n\n \n   The Agile movement was started by programmers, and software professionals, who held the ideals of Craftsmanship dear.  But then the project managers rushed in and said: “Wow! Agile is a cool new variation on how to manage projects.” \n \n\n \n   There’s an  old song , by Alan Sherman, called J. C. Cohen.  It’s about a subway conductor who did such a great job at pushing people into the train cars, that he pushed the engineer out.  This is what happened to the Agile movement.  They pushed so many project managers in, they pushed the programmers out. \n \n\n That’s not quite the way Martin Fowler described it.  He said that the Craftsmanship movement started because a bunch of programmers said: “Oh, we need to create a whole new world for ourselves […] where we can go away, get away from all of these business experts and project managers and business analysts, and just talk about our technical stuff.” \n\n \n   Oh, no.  Martin got that completely wrong.  It’s very clear from the  Software Craftsmanship Manifesto  that the goal of Craftsmanship is to continue and expand the Agile message.  Software Craftsmanship is not some kind of Techie Nocturnal Emission.  Software Craftsmanship is simply a continuation of the original goals of Agile. \n \n\n \n   Craftsmanship is the Agile, that the Agile movement left behind. \n \n\n Left behind?  Left behind to do what? \n\n \n   To promote conferences, certifications, and fancy new project management strategies. \n \n\n What’s wrong with certifications? \n\n \n   Let me put it this way: Anyone who suggested a two-day course to certify Craftsmanship would be laughed out of the room, laughed out of the town, and laughed out of the state.  The very idea is absurd. \n \n\n OK, but how can you have a movement without hype, certifications, the training, the conferences?  Don’t you need that stuff to get people’s attention? \n\n \n   Perhaps.  But I hope the Craftsmanship movement doesn’t leave it’s original purpose behind the way the Agile Movement did. \n \n\n What purpose is that? \n\n \n   The original Agile purpose.  You see, Craftsmanship is not about new stuff.  Craftsmanship is about old stuff.  It’s about working well, adding value, and doing a good job.  It’s about interacting, communicating, and collaborating.  It’s about productively adapting and responding to change.  It’s about professionalism and ethics.  It’s about the goal that Kent Beck had for Agile. \n \n\n What goal was that? \n\n \n   At the Snowbird conference, in 2001, where the Agile Manifesto was written, Kent Beck said that one of our goals was to heal the divide between programmers and management. \n \n\n \n   The Agile movement abandoned that goal by turning Agile into a business that promotes “new-and-better” ways to manage.  Instead of bringing managers and programmers closer together, the Agile movement focussed almost entirely on project management, and virtually excluded the programmers. \n \n\n And so that’s why the programmers split away? \n\n \n   No!  The programmers did not split away.  The programmers stayed the course!  The programmers continued to pursue Agile as it was originally conceived.  Read the opening line of the  Agile Manifesto : “We are uncovering better ways of developing software by doing it and helping others do it.”  It is Software Crafts-men and -women who are continuing that work.  It’s not the project managers in the Agile movement.  They’re off pursuing something else? \n \n\n What is it that they are pursuing? \n\n \n   Newness and Novelty.  Nowadays the Agile movement is about “The Next Big Thing” and “The Bold New Idea”.  They need novelty to keep the enthusiasm and energy high.  They need that so that people sign up for conferences and certifications.  They need to be seen as making – “progress”.  Agile has become a business; and businesses need to grow. \n \n\n It looks to me like they are succeeding. \n\n \n   They are.  They just aren’t succeeding at the original goals of Agile.  They split away from those goals in order to feed the need for Novelty and Newness.  The result, unfortunately, has been what Fowler and Jeffries have called:  “Faux Agile” ,  “Dark Scrum”  and  “Flaccid SCRUM” . \n \n\n This is all a little hard for me to believe. \n\n \n   Let me prove it to you.  What was Fowler’s first point in that talk of his – the point about the Agile-Industrial Complex? \n \n\n He said something to the effect that people work best when they choose how they want to work. \n\n \n   Right!  On a software development team, who does most of the work? \n \n\n Well, programmers of course. \n\n \n   How many programmers were attending Fowler’s talk? \n \n\n Well, he said it was “a smattering”, “very few”, “very much a minority”. \n\n \n   QED.  Who goes to Agile Conferences?  Not programmers. Not the people who do the bulk of the work.  Programmers started those conferences.  Programmers started the movement.  Programmers don’t go anymore.  It’s not the programmers who have changed. It is the conferences, and therefore the movement, that has changed.  The Agile movement moved away from the programmers – from Agile.  QED \n \n\n But… \n\n \n   Look.  Agile was never about project management; but that’s what they’ve turned it into.  Agile and project management are utterly orthogonal things.  Agile is not a better way to manage a project.  Agile has nothing to do with managing a project.  Agile is a set of values and disciplines that can help a relatively small team of software crafts-men and -women build small to medium sized products. \n \n\n But isn’t that management? \n\n \n   No!  God No!  Project Management is about dates, and budgets, and deadlines, and milestones.  It’s about personnel management and motivation.  Good management is absolutely necessary; but it has nothing whatever to do with Agile. \n \n\n \n   Here.  Look at the  Agile Manifesto .  Notice those four statements, and how they are divided between left and right.  What divides the things on the left and right from each other?  The stuff on the right is management.  The stuff on the left is Agile.  Managers invoke processes and tools.  Individuals on Agile teams interact.  Managers drive comprehensive documentation.  Agile teams build working software.  Managers negotiate and manage contracts.  Agile teams collaborate with customers.  Managers make sure plans are followed.  Agile teams respond to change. \n \n\n But aren’t Scrum Masters kind of like project managers? \n\n \n   Heavens no!  Scrum Masters are coaches, not managers.  Their role is to defend the values and disciplines.  Their role is to remind the team of how they promised themselves they would work.  The role was supposed to be shared by the team, not usurped by managers.  Every few weeks a new team member would volunteer to act as coach – if needed.  The role was supposed to be temporary.  A mature team doesn’t need a permanent coach. \n \n\n Wow, that’s sure not what they teach now.  So I guess you think Agile is just ruined then. \n\n \n   No!  Agile is alive and well, and thriving in the Craftsmanship mindset.  That’s where Agile relocated when the project managers invaded and took the Agile movement over. \n \n\n So then, what is the Agile movement? \n\n \n   Nowadays, the Agile movement might as well be an unofficial branch of the  PMI .  It’s a business that promotes conferences, training, and certifications for project managers.  As such, it has become antithetical to Beck’s original goal.  The Agile movement does not heal the divide between programmers and managers; it exacerbates it. \n \n\n It seems like you are saying that the Agile movement isn’t Agile. \n\n \n   It’s not.  It gave that up long ago.  Nowadays the Agile movement is about the horribly flawed idea that project management is what makes a team Agile. \n \n\n Well, isn’t it? \n\n \n   No, no, not at all.  You see, an Agile team is a group of crafts-men and -women who hold the values and disciplines of Agile dear.  An Agile team will be Agile no matter how the project is managed.  On the other hand, a team that is not Agile will not become Agile simply by virtue of a new and fancy project management strategy.  Such a team will be Faux Agile. \n \n\n Are you saying that a good manager can’t lead a team to be Agile? \n\n \n   It is a rare manager who can inculcate the values and disciplines of Craftsmanship.  It’s not impossible; but it’s not common.  Agile teams are most often composed of people who already share the values and disciplines of Agile – of Craftsmanship.  Thinking that a team can become Agile simply because a Certified Scrum Master is the project manager is a pipe dream. \n \n\n So then what’s the future? \n\n \n   The future is what it has always been.   The values and disciplines of Agile will continue to help relatively small software teams build small to medium sized products, and will help to heal the divide between programmers and management.  Today, those values and disciplines are held by people who, whether they know it or not, align with the ideals of Software Craftsmanship. \n \n\n \n   I don’t think we need an organization to promote Craftsmanship.  I don’t think we need a “Craftsmanship Alliance”.  I think all we need are people of good will – individuals who interact and collaborate – communities of professionals who work to promote change by steadily adding value.  I think the ideas of Agile – the ideas of Craftsmanship – are robust enough to grow and spread without an organization to drive them. \n \n\n So then Software Craftsmanship was not a tragedy? \n\n \n   How could the ideals of Craftsmanship ever be considered tragic?  They are eternal ideals that humans have aspired to for as long as their have been humans.  The tragedy was that the Agile movement became a business that left the original values and disciplines of Agile behind. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/05/02/Craftsman-Craftswoman-Craftsperson.html", "title": "Craftsman, Craftswoman, Craftsperson", "content": "\n       In the past I have used the term “Craftswoman” only when refering directly to a woman.  In most other cases, including most gender neutral cases, I have used the term “Craftsman”.  I say “most” because when I would address a team of both men and women I’d be as likely as not to use some variation of “Craftsmen and Craftswomen”. \n\n When writing, however, I have used “Craftsman” for the gender neutral cases.  This is because there is a certain formality to writing; and “Craftsman” seemed to me to be proper English. \n\n I recently had a discussion with a software craftswoman named Liz Keogh.  She is a woman whom I consider to be far more adept at the craft we share than I, so I take her words very seriously.  She graciously, and patiently, explained to me that the appearance of the word “Craftsman” in written formal text, even when used in the gender neutral case, made her feel “set apart”. \n\n This was a surprise to me.  Call me dense if you like (I’ve been called worse) but I did not realize that the usage of the word was having this effect on her.  And that effect is not acceptable.  I want  nothing  I say or write to make a woman like Liz Keogh – or any other woman for that matter – feel excluded. \n\n I’m a programmer.  I need a formlua – an algorithm – I need to know how to write the  if  statements in my brain.  So here is what I’ve come up with.  I apologize if this seems a bit mechanical; but I need personal rules like this. \n\n \n   When referring directly to a man, I will use the term “Craftsman”. \n   When referring directly to a woman, I will use the term “Craftswoman”. \n   In the singular gender neutral case I will use “Craftsman or -woman”, or possibly “Craftsperson”.  [1] \n   In the plural gender neutral case I will use: “Craftswomen and -men”  [2]  or possibly “Craftspeople” [4] \n \n\n Many people have recommended terms like Artisan, Professional, and even Mechanic  [5] .  But I don’t find that those words carry the right tone and weight.  In any case the term “Craftsman” is in such common usage that I feel I must continue using it. \n\n \n\n \n   \n     [1]  I am uncomfortable with the term “Craftsperson” because it sounds clumsy in my head.  Perhaps it is the long chain of consonants “cra-FTSP-erson”, or perhaps it’s some more deeply seated phycological reaction.  Time will tell whether my discomfort level wanes. \n   \n   \n     [2]  Note the refersal of the genders in the singular and plural case.  This ought to keep the order balanced, so that it does not appear that I am preferring one gender over another.   \nI’m not sure about those hyphens.  Craftsman is not a hyphenated word so I’m not sure I can properly use the hyphen this way.  I’m using it to connect the last word to the word “Craft”. [3] \n   \n   \n     [3]  If you think I’m being overly pedantic here, you haven’t met my copyeditors. \n   \n   \n     [4]  I really don’t like the sounds of “Craftspeople”.  It sounds flippant somehow, as if it was being said by Maynard G. Krebs: “Hello Craftspeople, you rang?” \n   \n   \n     [5]  Mechanic? \n   \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/04/13/FPvsOO.html", "title": "FP vs. OO", "content": "\n       Over the last several years I have paired with people learning Functional Programming who have expressed an anti-OO bias.  This usually comes in the form of statements like: “Oh, that’s too much like an Object.” \n\n I think this comes from the notion that FP and OO are somehow mutually exclusive.  Many folks seem to think that a program is functional to the extent that it is  not  object oriented.  I presume this opinion comes as a natural consequence of learning something new. \n\n When we take on a new technique, we often tend to eschew the old techniques we used before.  This is natural because we believe the new technique to be “better” and therefore the old technique must be “worse”. \n\n In this blog I will make the case that while OO and FP are  orthogonal , they are not mutually exclusive.  That a good functional program can (and should) be object oriented.  And that a good object oriented program can (and should) be functional.  But to accomplish this goal we are going to have to define our terms very carefully. \n\n What is OO? \n\n I am going to take a very reductionist stance here.  There are many valid definitions of OO that cover a rich set of concepts, principles, techniques, patterns, and philosophies.  I am going to ignore all of that here and focus on nuts and bolts.  The reason for this reductionism is that all the richness surrounding OO is actually not specific to OO at all; but is part of the richness of software development overall.  Here, I will focus on that part of OO that is definitive and inextricable. \n\n Consider these two expressions: \n\n \n   1:  f(o);   2:  o.f(); \n \n\n What is the difference? \n\n Clearly there is no actual semantic difference.  The difference is entirely in the syntax.  But one looks procedural and the other looks object oriented.  This is because we have come to infer a special semantic behavior for expression 2. that we do not infer for expression 1.  The special semantic behavior is: polymorphism. \n\n When we see expression 1. we see a function named  f  being called upon an object named  o .  We infer that there is only one such function named  f , and that it may not be a member of the standard cohort of functions, if any, that surrounds  o . \n\n On the other hand, when we see expression 2. we see an object named  o  being sent the message named  f .  We expect that there may be other kinds of objects that can accept the message  f  and that therefore we do not know which particular  f  behavior is actually being invoked.   The behavior is dependent upon the type of  o .   i.e.  f  is polymorphic. \n\n This expectation of polymorphism is the essence of OO programming.  It is the reductionist definition; and it is inextricable from OO.  OO without polymorphism is not OO.  All of the other attributes of OO, such as encapsulated data, and methods bound to that data, and even inheritance, are more related to expression 1. than to expression 2. \n\n C and Pascal programmers (and to some extend even Fortran, and Cobol programmers) have always created systems of encapsulated functions and data structures.  It does not require an OOPL to create and use such encapsulated structures.  Encapsulation, and even simple inheritance, is obvious and natural in such languages. (More natural in C and Pascal than the others.) \n\n So the thing that truly differentiates OO programs from non-OO programs is polymorphism. \n\n You might complain about this by saying that polymorphism can be achieved by using switch statements or long if/else chains within  f .  This is true, so I must add one more constraint to OO. \n\n \n   The mechanism of polymorphism must not create a source code dependency from the caller to the callee. \n \n\n To explain this, look again at the two expressions.  Expression 1:  f(o) , seems to have a source code dependency upon the function  f .  We infer this because we also infer that there is only one  f  and so the caller must know the callee. \n\n However, when we look at Expression 2.  o.f()  we infer something different.  We know that there may be many implementations of  f , and we don’t know which of those  f  functions is really going to be called.   Therefore the source code containing Expression 2 does  not  have a source code dependency upon the function being called. \n\n In concrete terms this means that modules (source files) that contain polymorphic calls to functions must not reference, in any way, modules (source files) that contain the implementations of those functions.  There can be no  include  or  use  or  require  or any other such declaration that causes one source file to depend upon another. \n\n So our reductive definition of OO is: \n\n \n   The technique of using dynamic polymorphism to call functions without the source code of the caller depending upon the source code of the callee. \n \n\n What is FP? \n\n Again, I am going to be very reductive.  FP has a rich history and tradition that goes back well beyond software.  There are principles, techniques, theorems, philosophies, and concepts that pervade the paradigm.  I am going to ignore all of that and drive straight to the bottom, inextricable attribute that separates FP from any other style.  And it is, simply, this: \n\n \n   f(a) == f(b)  when  a == b . \n \n\n In a functional program, every time you call a particular function with a particular value, you will get the same result; no matter how long the program has been executing. This is sometimes called  referential transparency . \n\n The implication of this is that function  f  must not change any global state that affects the way function  f  behaves.  What’s more, if we say that function  f  represents all functions in the system – that all functions in the system must be referentially transparent – then no function in the system can change any global state at all.  No function in the system can do anything that will cause another function in the system to return a different value from the same inputs. \n\n The deeper implication of this is that no named value can ever be changed.  That is,  there can be no assignment operator . \n\n Now if you think about this very carefully you might come to the conclusion that a program composed of nothing but referentially transparent functions can do nothing at all – since any useful system behavior changes the state of  something ; even if it is just the state of the printer or display. However, if we exclude the hardware, and any elements of the outside world, from our referential transparency constraint, then it turns out that we can create very useful systems indeed. \n\n The trick, of course, is recursion.  Consider a function that takes a  state  data structure as its argument.  This argument contains all the state information that the function uses for its work.  When the work is done the function creates a new  state  data structure  with updated values.  As its last act, the function calls itself with the new  state  structure. \n\n This is just one of the simple tricks that a functional program can use to track changes in internal state without appearing to actually change any internal state[1]. \n\n So, the reductive definition of functional programming is: \n\n \n   Referential Transparency – no reassignment of values. \n \n\n FP vs OO \n\n By how I have both the OO and the FP communities gunning for me.  Reductionism is not a good way to win friends.  But it is sometimes useful.  In this case, I think it is useful to shine some light on the FP vs OO meme that seems to be circulating. \n\n It seems clear that the two reductive definitions I have chosen are completely orthogonal.  Polymorphism and Referential Transparency have nothing, whatever, to do with each other.  There is no intersection between them. \n\n But orthogonality does not imply mutual exclusion (just ask James Clerk Maxwell).  It is perfectly possible to build a system that employs both dynamic polymorphism  and  referential transparency.  Not only is it possible, it is  desirable ! \n\n Why is it desirable?  For precisely the same reasons that the two aspects are desirable alone!  We desire systems built with dynamic polymorphism because they are strongly decoupled.  Dependencies can be inverted across architectural boundaries.  They are testable using Mocks and Fakes and other kinds of Test Doubles. Modules can be modified without forcing changes to other modules. This makes such systems much easier to change and improve. \n\n We also desire systems that are built with referential transparency because they are predictable.  The inability to change internal state makes systems much easier to understand, to change, and to improve.  It drastically reduces the chances of race conditions and other concurrent update problems. \n\n The bottom line is: \n\n \n   There is no FP vs OO. \n \n\n FP and OO work nicely together.  Both attributes are desirable as part of modern systems.  A system that is built on both OO and FP principles will maximize flexibility, maintainability, testability, simplicity, and robustness.  Excluding one in favor of the other can only weaken the structure of a system. \n\n \n [1] Since we are using machines with Von Neumann architectures, we must assume that there are memory cells that actually are changing state.  In the recursive mechanism I described; tail call optimization will prevent new stack frames from being created, and will reuse the original stack frame instead.  But this violation of referential transparency is (usually) entirely hidden and irrelevant. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/06/06/PickledState.html", "title": "Pickled State", "content": "\n       By now everyone is familiar with BDD (Behavior Driven Development) and its emblematic adjective/adverb/adverb triplet:  GIVEN/WHEN/THEN .  There is something satisfying about using these three words to describe a requirement. \n\n GIVEN That the system is waiting for login.\nWHEN the user provides a valid username and password.\nTHEN the user is presented with the welcome page.\n \n\n The impact that this triplet has had on our industry is significant.  It began as a way to describe high level tests, and has become a general means for specifying requirements.  It has even acquired a name:  Gherkin . \n\n Nowadays business analysts and programmers are encouraged to write suites of Gherkin scenarios to describe the systems they want to build.  An entire consulting industry has grown up around this idea.  Behavior Driven Development, which used to be a testing dialect, has become a management consulting strategy. \n\n \n   Spock:   Fascinating. \n \n\n Why has this proven to be so effective?  One reason, that has recently occurred to me, is the connection between Gherkin and Finite State Machines.  If you look closely at a Gherkin triplet you will see that it specifies a state transition. \n\n GIVEN that we are in state S1\nWHEN we recieve event E1\nTHEN we transition to state S2\n \n\n \n   This means that a suite of Gherkin scenarios is a specification of a finite state machine. \n \n\n Let that last sentence sink in for a second or two.  The formalism that business analysts find most conducive to specifying system  requirements  just happens to be the same formalism that programmers frequently use to specify system  behavior .  What’s more, that formalism started as, and continues to be, a popular language for specifying  tests . \n\n If the Gherkin requirements are complete, then they describe the complete state machine of the system,  and  the complete test suite for the system. \n\n \n   Spock:   Logical. \n \n\n Now hold that thought and let’s consider  unit tests .[1] \n\n Well written unit tests always follow the  AAA  pattern:   Arrange/Act/Assert . \n\n First the test  arranges  the system so that it is in the appropriate state for the test.  Next the test executes the  action  to be tested.  Finally the test  asserts  that the state of the system has been appropriately changed by the action. \n\n I used the word  state  in that last paragraph intentionally.  It should be clear to you that the  AAA  pattern is also a description of a transition in a state machine.  Indeed,  AAA  and  GWT  are completely isomorphic: \n\n GIVEN that we have ARRANGED the system for the test.\nWHEN we perform the tested ACTION.\nTHEN we can ASSERT the conditions that pass the test.\n \n\n This, of course, means that the suite of unit tests produced through TDD specifies the finite state machine that describes the system behavior.  If the three laws of TDD are followed, then test coverage will be at (or very near) 100%, and the described finite state machine will be complete. \n\n Now, let’s say we have a complete suite of Gherkin scenarios that specify all the acceptance and integration[2] tests for the system.  Let’s also say that we have a complete suite of unit tests produced by the three laws of TDD.  How do the state machines specified by these two test suites differ? \n\n At the level of the business they shouldn’t differ at all.  All the states and transitions specified in the Gherkin scenarios ought to be present in the unit tests.  However, there will be states and transitions in the unit tests that are not present in the Gherkin.  These will be all the little programming details that the business has no visibility of.  For example, the state transitions that deal with the fact that lines of text sometimes end in  \\n\\r  but sometimes only end in  \\n . \n\n Thus the Gherkin is a proper  subset  of the unit tests. \n\n \n   Spock:   Indeed. \n \n\n Now, let’s say that the system is written in F# or Clojure – i.e. a  functional  language.  The combination of the Gherkin, unit tests, and code is a living example of the Church-Turing thesis – that state machines are equivalent to predicate calculus. \n\n I think that’s cool. \n\n \n   Spock:   Way cool. \n \n\n Anyway, back to the Gherkin.  Given all the different means by which people have specified requirements in the past; what is it about Gherkin that is so much more conducive to specification? \n\n Here’s my theory.  Gherkin is a language that sits perfectly between, and speaks equally to, the three disciplines of specification, testing, and programming.  Each of those disciplines gets precisely what they need from the language.  It is the common language that bridges that three-way divide and allows the practitioners to unambiguously communicate. \n\n On top of that, there is no better mechanism for exploring the behavior of a system than a well specified state machine. \n\n Perhaps that last statement needs some justification.  So consider a typical state machine.  It is composed of a list of transitions; each of which is a triplet. \n\n CURRENT_STATE : EVENT : NEXT_STATE\n \n\n The transition reads as follows: \n\n GIVEN that we are in the CURRENT_STATE, \nWHEN we get the EVENT, \nTHEN we go to the NEXT_STATE.  \n \n\n This means that the criterion for to transitioning to  NEXT_STATE  is the pair:  {CURRENT_STATE, EVENT} . \n\n So consider this simple state machine that represents a subway turnstile. \n\n LOCKED     COIN  UNLOCKED\nUNLOCKED   PASS  LOCKED\n \n\n We read these two transitions as follows: \n\n GIVEN we are in the LOCKED state,\nWHEN the user drops a COIN,\nTHEN we go to the UNLOCKED state.\n\nGIVEN we are in the UNLOCKED state,\nWHEN the user PASSes through the gate,\nTHEN we go to the LOCKED state.\n \n\n Do these two transitions fully describe the behavior of the subway turnstile?  Clearly not.  There are two states and two events.  That means there are four  {state, event}  pairs.  Therefore there should be four transitions as follows. \n\n LOCKED   COIN   UNLOCKED\nLOCKED   PASS   LOCKED (ALARM)\nUNLOCKED COIN   UNLOCKED (REFUND)\nUNLOCKED PASS   LOCKED  \n \n\n The ALARM and REFUND notations are actions that must be executed as part of the respective transitions.  These extra transitions are the ones that business people tend not to think about.  They are well off the happy path of the system. After all, nobody expects a turnstile user to deposit a coin once the turnstile is unlocked. \n\n How often have you found, and fixed, bugs in systems that were due to some event happening in a state that didn’t expect it?  If you are like me, it happens all the time.  That’s because these situations are hard see. \n\n However, when a system is described in state transition form, and when the states and transitions are well identified, then the hunt for the missing  {state, event}  pairs becomes trivial.  I have personally had a great deal of success in finding missing  {state,event}  pairs simply be being careful to identify and isolate states and events. \n\n \n   Spock:   A reasonable discipline. \n \n\n Anyway, the next time you are using BDD and/or Gherkin to specify a system, remember that what you are really doing is specifying a finite state machine.  If you are careful to identify the states and events, you will make it a lot easier to find the missing  {state,event}  pairs and create a more complete specification. \n\n \n [1] The word “unit” is entirely inappropriate.  No one knows what a  unit  is when applied to tests.  Some people call them  programmer tests  instead.  Others call them  micro-tests .  Whatever they are called, they are the tests that programmers produce when practicing TDD (Test Driven Development).  They are the tests that are used by programmers to describe the behavior of the system. \n\n [2] “Acceptance” and “Integration” are inappropriate words here.  Some people call them  Customer  tests.  They are the tests that the business uses to describe the behavior of the system. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/08/13/TooClean.html", "title": "Too Clean?", "content": "\n       I just watched  Sarah Mei’s  talk:  Livable Code .  It was very good.  I strongly agreed with the major points of her talk.  On the other hand the theme of her talk was something I hadn’t properly considered before. \n\n But before I get to that, I have a quibble.  At one point in the talk she criticized the  craftsmanship movement  for being too focussed on individual, and not team, behavior.  This is an unfortunate mischaracterization of the craftsmanship movement which has  always  been focussed on building professional and ethical teams. \n\n With that quibble out of the way the notion that software systems need to be “ livable ” is an interesting insight.  According to Mei, in order to be “livable”, the organization of a software system should lie somewhere between the extremes of the horrific tangled mess of  code hoarders , and the antiseptic cleanliness seen in the pages of interior decorating magazines and home sales brochures. \n\n This is not something I had considered or expressed before; though it has always been something I’ve sheepishly practiced. \n\n Can a system be too clean?  Is it possible to focus so much on cleanliness that no one can practically work in the system? \n\n Here is a picture of my office, taken today.  What do you see? \n\n \n\n It’s relatively clean.  There is an obvious organization to it.  You can see my workstation, with the laptop and two screens in the center.  You can also see my flight simulator station to the right.  There are baskets and cubby holes, and all the normal organizational paraphernalia. \n\n But there’s also a bit of mess laying around.  Next to the printer on the far left there is a  BUG-A-SALT  positioned at the ready in order to deal with an invading fly.  To the right, atop the Tardis peeking over the rightmost screen, is a roll of toilet paper.  It’s there in case I spill my coffee or need to sneeze.  Can you find the fidget spinner?  Do you see the pile of drawing utensils to the left?  What about all those post-its and pictures and…  And what in the world is inside all those cubby holes? \n\n The cleanliness and organization of the office makes it useable.  I know where things are.  I know how to access them.  Unrelated elements do not interfere with each other.  There are no unnecessary dependencies. \n\n The slight messiness of the office is  convenient .  The mess is there because of transient issues.  Flies, coffee, sneezes, doodles, idle moments, changing work priorities, and just general places to put stuff I don’t know what to do with.  Without that mess, the office would be harder for me to use.  The  act of using  the office would reintroduce the mess! \n\n So, clearly, I allow a bit of mess to creep into my office.  The mess provides affordances for transient issues.  But, just as clearly, I fight to keep that mess in check.  I fight to keep the office clean.  And that’s not easy! \n\n Does this rule apply to code?   It absolutely does!   When I write code I fight very hard to keep it clean.  But there are also little places where I break the rules specifically because those breakages create affordances for transient issues. \n\n For example, I am a real stickler about separating presenters from views.  The code that puts the data into presentable form should not reside with the code that puts that data on the screen.  On the other hand, when you are trying to get a screen to look and function properly, it is a pain to be hopping back and forth between two different files.  Sometimes it just makes sense to merge the code back together, get everything working the way you like, and then re-separate the code again.  This technique is known as  Worse is Better . \n\n Anyway I think there is value in the notion that code should be livable.  We should not be ashamed if our code looks a little bit  lived in .  On the other hand, we need to be diligent about cleaning up after ourselves; and not let the mess spin out of control. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/06/21/IntegersAndEstimates.html", "title": "Integers and Estimates", "content": "\n       \n   What is this:  a^2 + b^2 = c^2 \n \n\n The Pythagorean Theorem. \n\n \n   Right.  What else is it? \n \n\n An equation in three unknowns. \n\n \n   Do you know some solutions to this equation? \n \n\n Sure.  (3,4,5) and (5,12,13). \n\n \n   Right.  Those are common pythagorean triplets.  Do you know others? \n \n\n Well, Google is my friend, let’s see.  (typing)  It looks like (7,24,25) and (9,40,41) all satisfy the equation. \n\n \n   Have you noticed that you’ve only supplied integer solutions? \n \n\n Oh, right.  I suppose that there are a whole load of non-integer solutions. \n\n \n   Have you heard of Diophantus? \n \n\n Is that a Greek name? \n\n \n   Yes.  Diophantus was interested in equations that had integral solutions.  We call such equations: Diophantine equations. \n \n\n So  a^2 + b^2 = c^2  is a Diophantine equation? \n\n \n   Yes.  And there are many others.  For example:  a^3 + b^3 = c^3 \n \n\n Oh, sure.  And what are some solutions? \n\n \n   There aren’t any. \n \n\n Really?  None? \n\n \n   Yes.  None.  That has been proven.  In fact it has been proven that  a^n + b^n = c^n  has no integral solutions for  n>2 .  This is known as Fermat’s conjecture. \n \n\n Huh.  OK, well this is kinda interesting I guess, but why should I care? \n\n \n   What is a digital computer? \n \n\n What do you mean?  This thing that you and I are conversing on is a digital computer. \n\n \n   Yes, but what does a digital computer do? \n \n\n Uh. It computes digitally? \n\n \n   Precisely!  And the word digitally means…? \n \n\n Um.  With digits? \n\n \n   Exactly!  And are the number of digits finite? \n \n\n Of course, though very very large nowadays. \n\n \n   …and a finite number of digits is…? \n \n\n Oh, I think I see where you are going.  A finite number of digits is an integer. \n\n \n   Right.  A digital computer is a computer that computes with integers.  Nothing but integers. \n \n\n Well, wait.  What about floating point numbers and rational numbers? \n\n \n   They are represented by integers in the computer.  The computer deals with integers, only integers. \n \n\n OK.  sure.  Integers.  But what does this have to do with Diophantine equations? \n\n \n   What are the inputs to a computer program? \n \n\n There are lots of kinds.  Keyboard characters, mouse movements, mouse clicks, network packets.  You name it. \n\n \n   They are all made up of integers aren’t they? \n \n\n Um.  Yeah, I guess they are.  OK, so every input to a computer program is integral. \n\n \n   And what about the outputs? \n \n\n Well, yes, pixels, characters, network packets.  They are all composed of integers too. \n\n \n   So a digital computer program takes in integers and returns integers. \n \n\n Right.  That’s right.  It’s all integers. \n\n \n   A digital computer program, therefore, represents a Diophantine equation. \n \n\n Wait. What? \n\n \n   Integers in.  Integers out. \n \n\n OK. sure.  But it’s one big complicated Diophantine equation. \n\n \n   Actually, the specification of the program is the equation.  The program finds the solutions to that equation. \n \n\n Yeah, yeah, ok.  That’s right.  The specification of a program is a great big Diophantine equation in a bazillion unknowns, and the program that meets that specification finds solutions to that ginormous equation.  Is this useful to know? \n\n \n   Who is David Hilbert? \n \n\n You mean that guy who designed that funny recursive curve that looks like mosquito netting? \n\n \n   (Ahem.) That was one of his accomplishments yes. He also helped Einstein with the General Theory of Relativity.  He was a very great mathematician. \n \n\n And he did something with Diophantine equations I’m guessing. \n\n \n   Indeed he did many, many things.  Among them was a very famous question.  The question of “Entscheidung” – decidability. \n \n\n What did he want to decide? \n\n \n   Remember Fermat’s conjecture? \n \n\n You mean that equation that has no solutions.   a^n + b^n = c^n  where  n>2 ? \n\n \n   Yes, that’s the one.  For a long time there was no proof that  n=2  was the only solution.  How could you disprove that conjecture if you thought it was untrue? \n \n\n I could write a program to find counter examples.  Like, maybe  n=999,999,999  might work. \n\n \n   Right.  And if you found such a solution, you’d have disproven Fermat’s conjecture.  But how long would it take to PROVE the conjecture using that method? \n \n\n The program would run forever.  I couldn’t prove it that way. \n\n \n   Correct.  What Hilbert wanted was a finite algorithm to determine whether or not a solution exists.  He wanted a way to “decide” whether or not a search, such as the one you suggested, was practical. \n \n\n Wait, wait.  What?  He wasn’t asking for the solutions, he was asking for a way to know if there were any solutions? \n\n \n   Right.  He wanted a finite algorithm that could tell him whether a given diophantine equation had solutions or not.  That algorithm would not supply the solutions; it would just supply the decision. \n \n\n That’s why he called it “decidability”? \n\n \n   Entscheidung.  Yes \n \n\n Gesundheight! \n\n \n   Harumph!  Now.  Who do you think solved the problem of decidability? \n \n\n I think you’re about to tell me. \n\n \n   Two people whom you’ve heard of.  The two great founders of modern computer science.  Alonzo Church, and Alan Turing. \n \n\n Church!  That’s the guy who invented functional programming, right? \n\n \n   In a manner of speaking, yes. \n \n\n And Turing!  He won world war 2 right? \n\n \n   He certainly contributed.  The two of them proved, using very different techniques, that there was no general and finite solution to decidability. \n \n\n That must have disappointed Hilbert. \n\n \n   Perhaps.  But that’s not the issue. \n \n\n Yeah, just what is the issue here? \n\n \n   When you are given a program specification, i.e. a Diophantine equation, what is the first thing you are asked to do? \n \n\n Estimate it of course.  Folks want to know how long it will take to write the program. \n\n \n   And the program is what again, in terms of a that Diophantine equation? \n \n\n The program is the … solution … to the … OH! \n\n \n   (smile)  I perceive you’ve gotten the point. \n \n\n Yeah, like, they are asking me to DECIDE.  An estimate is a decision. \n\n \n   And is there a finite method for finding that decision in every case? \n \n\n No!  OH, that’s hilarious. \n\n \n   Right.  The founding documents of computer science are documents that prove that there is no finite mechanism for deciding if a program can even be written.  The founding of computer science was based on the proof that estimates were not guaranteed. \n \n\n Yeah, but we CAN estimate. \n\n \n   Yes, we can.  That’s because most specifications are estimable. \n \n\n So this has just been a cute little mathematical diversion with no pragmatic result. \n\n \n   I suppose you could say that.  But I enjoyed it.  And, after all, I think it’s deliciously ironic that it was the proof of NOESTIMATES that founded computer science. \n \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/12/16/unoffended.html", "title": "We, The Unoffended", "content": "\n       We,  The Unoffended , believe that a free society depends upon the tolerance and forebearance of it’s members toward each other, and each other’s ideas.  We therefore strive to remain unoffended by the freedom of others to speak and act according to their identity, politics, ambitions, and desires.  We are intolerant  only  of intentional or negligent harm done to others. \n\n Therefore, given the above, \n\n We,  The Unoffended : \n\n \n   \n     Are not offended by what you do, what you say, or who you are. \n   \n   \n     Are not offended by your race, your age, your gender, your sexual preference, your identification, your politics, your religion, or any other natural or chosen attribute. \n   \n   \n     Are not offended by your thoughts, your votes, your prayers, your hopes, your dreams, or your ambitions. \n   \n   \n     Celebrate, support, and defend your right to enjoy the personal dignity of your identity, your beliefs, and your choices. \n   \n   \n     Value you for who you are and for whatever skills, intelligence, perception, wisdom, and empathy you choose to share. \n   \n   \n     Believe that you owe us nothing; and are not offended if you choose to share nothing with us. \n   \n   \n     Are not offended by disagreement.  Your willingness to share disagreeing thoughts and ideas is a gift that we value and cherish. \n   \n   \n     Believe that, in order for the best ideas to rise to the top,  all  ideas must be heard and evaluated on their merits.  We therefore encourage the expression of any and all ideas; and resist their suppression. \n   \n   \n     Believe that ideas are not harmful, so long as they do not specifically incite harmful actions. \n   \n   \n     Are not offended by careful actions that cause inadervtent harm. \n   \n \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/12/14/SJWJS.html", "title": "SJWJS", "content": "\n       Remember when the software SJWs told us that “Software Craftsmanship” was exclusionary to women because it was clearly male?  At first we shrugged off the complaint as coming from silly people who were being offended by silly things. But the complaints continued and we began to listen. Oh, not right away, and not with any enthusiasm; but we listened. \n\n As we listened we began to see that, perhaps, they had a point. We realized that it was just possible that some people might feel somewhat excluded by the three letters:  m ,  a , and  n  in “Software Craftsmanship”. And so, in the interest of being polite, in the interest of not wanting to exclude anyone, we shifted our language a little. We started saying “Craftspeople” and “Craftsfolk”, and “Crafters”, and other variations that might soften the blow to those who felt excluded. \n\n Then, sometime later, the software SJWs told us:  “ all the Agile/XP stuff (like pairing, TDD, etc) doesn’t seem to work for a heterogenous team ” because: the Agile Manifesto was “ developed by a bunch of white dudes ”.  In other words: \n\n \n   The Agile revolution is yet another manifestation of the White Patriarchy imposing their power upon the intersectionally oppressed. \n \n\n \n\n Now most of us thought, and continue to think, that this was an absurd idea. I mean if the race and gender of the authors was the sole determinant of effectiveness, then one would have to condemn the Declaration of Independence, and The Magna Carta, the works of Shakespear, and  The Cat in the Hat  as being ineffective for the same reason.  So, despite the fact that there were a few folks who considered themselves  woke  enough to applaud and salute, it was clear to most of us that this was a truly dumb idea. \n\n Perhaps we should have realized, then and there, that the software SJWs were heading over a cliff in their aimless pursuit of:  something to be outraged about .  Perhaps, then and there, we should have realized that the true goal of the software SJWs was outrage –  for the sake of outrage  – and was no longer, truly, social justice.  But the issue kind of boiled down and we shook it off as an anomaly and kept on coding. \n\n So, yesterday, in order to dispell any doubt about just how irrelevant the software SJWs have become, they declared that  Domain Driven Design  was exclusionary and offensive because DDD is – wait for it – wait for it – \n\n \n–  a bra size . \n\n No, I’m not joking.  The case that Eric Evan’s masterpiece:  Domain Driven Design  is responsible for the unmitigated heartache and torture of vast quantities of desperate souls was made, yesterday, by an acclaimed leader of the Software SJWs. \n\n So now, we are told, for the sake of those intersectionally oppressed folks, we must ban the acronym DDD. But wait!  What about the letter  B  – with it’s obvious breasts – should that be banned as well?  And what about men with Erectile Dysfunction?  Should they be offended by the letter  R ?  And don’t get me started on the letter  Y ! \n\n \n\n Have we had enough now?  Do we need any more evidence that these misguided warriors have abandoned their cause?  Perhaps, when they started, their goals were laudable.  Perhaps they really wanted to help folks who were disadvantaged in some way.  But now I think it’s pretty clear that: \n\n \n   The software Social Justice Warriors have Jumped the Shark. \n \n\n #SJWJS \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2018/12/17/FPvsOO-List-processing.html", "title": "FP vs. OO List Processing", "content": "\n       While writing  spacewar  over the last few months, I came across an interesting difference between functional programming, and OO programming. \n\n Let’s imagine that we’ve got two lists.  A list of klingons, and a list of shots traveling through space.  The shots might be phaser beams, or photon torpedos, or balls of iron called “kinetics”.  In all cases they have a position and a velocity.  So do each of the klingons. \n\n We have a frame rate of  f  frames per second.  At each frame we look at the positions of the shots and compare them to the positions of the klingons.  We are looking for a hit. \n\n If we find a hit, we delete the shot from the list of shots, and we add the corresponding hit to the klingon, and an explosion to the list of explosions in the world. \n\n In Java, the code might look like this: \n\n void updateHits(World world){\n  nextShot:\n  for (shot : world.shots) {\n    for (klingon : world.klingons) {\n      if (distance(shot, klingon) <= type.proximity) {\n        world.shots.remove(shot);\n        world.explosions.add(new Explosion(shot));\n        klingon.hits.add(new Hit(shot));\n        break nextShot;\n      }\n    }\n  }\n}\n \n\n Note the labelled break.  That’s the first time I’ve ever been tempted to use one of those. \n\n Anyway, this code is pretty straightforward, isn’t it?  But it’s not at all functional because it alters the hit klingons and the world. \n\n A functional program that does the same thing must not alter the klingons, or the world, in any way.  What it must do instead is create a new world, with new klingons, and new shots, explosions, and hits.  Remember, that’s what functional programming is all about.  You aren’t allowed to change the value of any existing variables. \n\n You might ask how you can change the state of the game if you can’t change the state of any of the game variables?  The answer is pretty straightforward: You use tail recursion. \n\n Essentially, you have a function that transforms the world though one step in time.  The result of that function is an entirely new world.  Then you call that function in a recursive loop like this: \n\n void updateWorld(World world) {\n  drawWorld(world);\n  updateWorld(transformWorld(world));\n}\n \n\n If you are a Java programmer, you’d be worried about the stack overflowing.  But in functional languages (and in most other modern languages other than Java) there is a lovely little trick called  Tail Call Optimization  that eliminates that problem if the recursive call is the very last operation of the function. \n\n So how would one write the  update-hits  function in a functional language like Clojure? \n\n Take a look at this: \n\n \n\n That looks a bit different from the Java function, doesn’t it?  It should.  It’s doing a lot more than that Java function had to do.  So let’s walk through it. \n\n \n   \n     First we get all the relevant shots.  These are the three kinds of shots that our ship can fire at targets like Klingons or Romulans.  I didn’t include this is the Java example above, so this is just extra code. \n   \n   \n     Next we get the targets. For our purposes, this is just the list of all the Klingons.  Though it could also be the list of all the Romulans. \n   \n   \n     Next we get all the pairs of targets and shots along with their distance from each other. \n   \n   \n     Then we filter the pairs for those whose distance is less than the proximity of the weapon.  These are the pairs that represent hits. \n   \n   \n     Then we convert those pairs into a list of just the hit targets, and another list of just the shots that did the hitting. \n   \n   \n     Next, and this is the interesting bit, we get the list of all the Klingons that were NOT hit.  We also get the list of all the shots that did NOT hit.  Why do we need these?  Read on. \n   \n   \n     Next we create the hit-targets. This updates each hit target with the hit itself. \n   \n   \n     Then we create a list of all the new explosions added to all the other explosions in the world. \n   \n   \n     Finally, we build the world.  The world is a copy[1] of the old world, but with: \n     \n       the Klingons replaced by the concatenation of the hit klingons and the unhit klingons. \n       The shots replaced by only the shots that didn’t hit. \n       And the explosions replaced by the sum of old and new explosions. \n     \n   \n \n\n Now the part that I find interesting about this is that in order to build a new instance of the world, I must keep track of those parts of the world that changed, like the klingons, shots, and explosions,  and  all the parts of the world that  did not  change.  I must separate them.  Make the modifications.  Then add them back together again.  I can’t just reach into the world and make specific changes. \n\n At first I thought that this was extra work.  It seemed wasteful to me that I had to separate and keep track of the unchanged elements.  However, then I had a flash of insight. \n\n Imagine that you have a dozen bicycles in your garage.  You and your partner are about to enter a race, and you need to select and prepare two bicycles.  You must separate those two bicycles from the rest, perpare them, race them, and then put them back into the garage with the rest of the bicycles. \n\n I’ll let you ponder that for awhile.[2] \n\n \n\n [1] Actually, it’s not really a copy.  Most functional languages, including Clojure, use  very clever schemes  to prevent all that copying by strategically sharing the elements that did not change. \n\n [2] This is just one of the points of discussion about spacewar in the  Clean Code Functional Programming series  on  cleancoders.com . \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2019/05/18/737-Max-8.html", "title": "737 Max 8", "content": "\n       \n   The software within the two doomed 737 Max 8 aircraft physically wrested control away from the pilots and plunged those aircraft into the Earth at speeds approaching Mach 1, killing everyone aboard. \n \n\n As a pilot, and a software engineer, I have dug pretty deeply into this issue.  I’ve read many of the reports, and have read or listened to the opinions and commentary offered by others.  Nothing I have read or heard contradicts the statement above.  I believe that statement to be an essential fact.  The software executing within those planes killed everyone aboard. \n\n No Ill Intent \n\n Before I go any farther I need to make something clear.   No one had ill intent.   I’ve read silly statements like: “Boeing put profits above safety.”  Such statements are absurd and naive. Everyone at Boeing was well aware that the continued existence of their company, let alone their profits, depended upon their safety record.  Indeed, Boeing now faces the prospect that it may not survive this catastrophe. So it is ridiculous to think that anyone at Boeing carelessly rolled the dice over safety. \n\n On the other hand, it’s very clear that a cascade of many critical errors was made. In hindsight those errors seem painfully obvious. \n\n But we’ve seen errors like this before. \n\n \n   \n     On January 27th, 1967, engineers and technicians at NASA locked Gus Grissom, Ed White, and Roger Chaffee into the electrically active Apollo One capsule and pumped it full of  pure Oxygen  to a pressure of  twenty pounds per square inch .  In hindsight, the error was breathtakingly obvious. \n   \n   \n     On January 28th, 1986, managers at NASA overrode the objections of the engineers and launched the Challenger Space Shuttle even though the temperature overnight had been well below the redline minimum temperature for the solid rocket boosters.  In hindsight, the error was horrifyingly, almost criminally, obvious. \n   \n \n\n Nobody at NASA wanted these disasters to happen.  No one acted with ill intent.  Nobody thought they were rolling the dice over safety.  They just screwed up.  Big time.  And people died. \n\n I’m not trying to exonerate anyone.  The mistakes were fatal.  Those who made the mistakes must bear the responsibility for them. \n\n However, it is important to understand that nobody purposely rolled the dice – either at NASA or at Boeing.  The stakes were just too high to think that anyone would willingly take such an undue risk. \n\n Boeing’s Software Mistakes \n\n I don’t know all the mistakes that were made at Boeing.  That will be for others to work out.  However, I’m pretty sure I know some of the mistakes that the software developers made.  I know what those mistakes are because they’ve been pretty clear about what the fixes are. \n\n My analysis of these mistakes is certainly naive.  I have not looked at the actual software, nor do I have any knowledge about that software other than what I’ve gleaned from news reports and commentary.  So take this with a grain of salt. \n\n \n   They trusted a single sensor and did not cross check it against the readings of other instruments. \n \n\n It appears that in both of the crashes an Angle of Attack (AOA) sensor was providing bad data.  The software trusted this sensor without cross checking it against the airspeed, or the vertical speed, or the altitude, or the attitude, or the secondary AOA sensor – just to mention a few. \n\n This is hard to understand.  The discipline of cross checking instruments is baked into the marrow of every instrument pilot.  Instrument pilots are taught to  always  interpret their instruments by cross checking them against others.  After all, any single instrument can silently fail. \n\n Did the software developers not know this fundamental practice?  Weren’t they pilots?  Weren’t they steeped in the disciplines of aviation?  If not, why not? \n\n This is a fundamental issue of software.  Programmers must not be treated as requirement robots.  Rather, programmers must hae intimate knowledge of the domain they are programming in.  If you are writing code for aviation, you’d better know a  lot  about the culture, disciplines, and practices of aviation. \n\n \n   They gave the software the power to override and overpower the pilots. \n \n\n Again, this is hard to understand.  Pilot control is a fundamental principle in aviation.  Software on board an aircraft operates at the pilots’ direction and under the pilots’ supervision. \n\n In the airplanes that I fly, I have several different ways to disengage the auto-pilot.  I can also simply overpower it by strongly manipulating the controls.  If the auto pilot does something I don’t like (Yes, that happens) I can immediately flick it off and/or physically overpower it. \n\n But the errant software aboard the 737 Max 8 was capable of putting the airplane into a dive that the pilots could not disengage or overpower by using their normal control inputs. This evokes the terrifying mental image of a pilot hauling back on the yoke for all he’s worth while the airplane inexorably plunges headlong into the ground.  Was that image evoked in the minds of the developers?  If not, why not?  If so, how did it get ignored? \n\n \n   The software ignored the counteracting inputs by the pilots. \n \n\n In the airplanes that I fly, if I countermand the autopilot,  it will disengage .  This seems to me to be the obviously right thing to do.  But the 737 software did not disengage when the pilots countermanded it.  It continued to drive the nose of the airplane down into a dive despite the desperate attempts of the doomed pilots to regain control. \n\n How did this happen? \n\n The software in question was called MCAS (Maneuvering Characteristics Augmentation System).  MCAS was a late addition to the 737 Max fleet.  It was added as a minor tweak to the flight characteristics making the aircraft behave more like a standard 737. \n\n Boeing felt this was necessary in order to avoid having to retrain existing 737 pilots.  Their hope was to save their customers the hassle and expense of that retraining. \n\n It seems to me, from what I have read and heard, that everyone at Boeing considered this to be a truly minor tweak; and not anything particularly flight critical.  In fact they thought it was so minor that they did not inform any of the operators or pilots that MCAS existed.  They figured it would be a simple little background process that would make unnoticeably gentle modifications to the attitude of the aircraft. \n\n I suspect that the idea that MCAS was a minor, nearly inconsequential, tweak infected everyone’s attitude.  I suspect that this attitude led them to forget that any software that has the power to manipulate the flight controls is, in fact, flight critical.  And so I suspect that this attitude led them to use far less care than the situation actually called for. \n\n How could such carelessness happen?  It could happen the same way smart engineers and technicians at NASA could pump 20 psi of pure oxgen into an electrically active capsule with three men locked inside.  It could happen the same way smart managers at NASA could ignore their engineers and launch in redline weather conditions.  It could happen because people sometimes screw up really badly. \n\n We could blame lots of things for such screwups:  Groupthink. Overconfidence. Mission focus.  It’s a deeply human issue.  And Boeing will be wrestling with that issue for quite some time to come.  If the company survives, it will likely emerge a much better company. \n\n We are killing people. \n\n But we programmers have to deal with this issue too. We programmers must take a hard look at this accident, and others like it, and decide what kind of future we are going to create for ourselves. \n\n We, programmers, are killing people.  Our errors cause loss, injury, and death.  It’s our fingers on the keyboards.  It’s our code running in the machines.  No matter who else is involved in these errors, we are the ones who write that code. \n\n If our code might cause injury, loss, or death, it is up to us to foresee, and then prevent, that harm from occurring. \n\n \n   We have to know the business domains we are coding for. \n   We need to have the knowledge and insight to foresee and manage the risks our code might incur. \n   We need to employ the practices and disciplines that keep our users, our customers, and our employers safe. \n   We need to have the courage to say “No” when  we  assess that the risk of deploying our code is too high. \n \n\n This is not a responsibility we can shirk.  It’s on us; because it’s our code. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2019/06/08/TestsAndTypes.html", "title": "Types and Tests", "content": "\n       Mark Seeman (@ploeh) and I have been having a fun debate on twitter.  It began with this tweet from me, which is part of a much longer tweet thread: \n\n There is no escape from this. Whether you use static, or dynamic typing, you must still demonstrate correctness by executing tests. Static typing does not reduce that number of tests, because those tests are behavioral and empirical. — Uncle Bob Martin (@unclebobmartin)  June 4, 2019 \n \n\n As is usual, on social networks, many people posted rude and/or insulting comments.  Those people aren’t important.  Mark, on the other hand, came back with respectful and substantive replies which you can see by following the whole thread.  It began this way: \n\n I respectfully disagree with this. Some type systems allow null references. In those type systems, you must write tests that demonstrate how the SUT interacts with null input. In other type systems (e.g. Haskell) nulls don't exist. You can't meaningfully write an equivalent test — Mark Seemann (@ploeh)  June 4, 2019 \n \n\n A spirited debate ensued.  You may find it entertaining and educational to follow all the various threads.  However, I want to focus on one of Mark’s replies.  He tweeted about a  blog  he had written back in 2018.  I encourage you to read it.  You’ll learn something about testing, static typing, and Haskell.  You’ll also learn something about how to debate with someone in the future by posting your refutation in the past.  ;-) \n\n The problem Mark proposed in his blog is a simple function:  rndselect(n,list) .  It returns a list of  n  elements randomly selected from the input list.  He walked through his strategy for developing this in Haskell, using types,  QuickCheck , and after-the-fact tests. \n\n This piqued my interest.  How would the process, and the end result, be different using a dynamically typed functional language like Clojure, and strict TDD disciplines. \n\n Let’s find out. \n\n We begin with the degenerate tests.  We return an empty list if the input list is empty, or if the number of requested elements is zero. \n\n (deftest random-element-selection\n  (testing \"degenerate case\"\n   (is (= [] (random-elements 0 [])))\n   (is (= [] (random-elements 0 [1])))\n   (is (= [] (random-elements 1 [])))\n   ))\n\n(defn random-elements [n xs]\n  [])\n \n\n Early in Mark’s blog he noted that a consequence of  The Free Theorem  is that he would not have to test that the elements in the result list came from the elements in the source list.  On the other hand, since I’m not using a static type system my tests make no sense if I don’t assume that the elements in the result set come from the source list. \n\n There is one trivial case.  Let’s pull one element out of a list that has one element. \n\n (testing \"trivial cases\"\n  (is (= [1] (random-elements 1 [1]))))\n\n(defn random-elements [n xs]\n  (if (or (< n 1) (empty? xs))\n    []\n    [(first xs)]))\n \n\n Note that I am following the rule of gradually increasing complexity.  Rather than worrying about the whole problem of random selection, I’m first focusing on tests that describe  the periphery  of the problem. \n\n We call this:  Don’t go for the Gold” .  Gradually increase the complexity of your tests by staying away from the center of the algorithm for as long as possible.  Deal with the degenerate, trivial, and simple administrative tasks first. \n\n In that spirit, the next least complex thing to worry about is repetition.  So let’s test that we can pull more than one element out of a single element list. \n\n (testing \"repetitive case\"\n    (is (= [1 1] (random-elements 2 [1]))))\n\t\n(defn generate-indices [n]\n  (repeat n 0))\n\n(defn random-elements [n xs]\n  (if (or (< n 1) (empty? xs))\n    []\n    (map #(nth xs %) (generate-indices n))))\n \n\n Some of you might complain that I got a bit ahead of myself there.  I used that  nth  call instead of keeping the  first  call. Why did I change it if I didn’t have a test for that?   Shrug. \n\n The next least complex thing to worry about are those indices.  Right now they are all just zero.  It would be good to make sure that indices other than zero work properly too. Testing this forces me to mock the function that generates the indices; and that forces me to change the design a bit.  So I’ll break the  generate-indices  function up so that I can mock out a single index. \n\n The strange  with-bindings  call below temporarily replaces the implementation of the  index  function, such that it always returns the the index  1 .  The odd  ^:dynamic  attribute is necessary, in Clojure, if you want to mock out (re-bind) a function. \n\n (testing \"singular random case\"\n  (with-bindings {#'index (fn [] 1)}\n    (is (= [2] (random-elements 1 [1 2])))))\n\t\n(testing \"repeated random case\"\n    (with-bindings {#'index (fn [] 1)}\n      (is (= [2 2] (random-elements 2 [1 2])))))\n\n(defn ^:dynamic index []\n  0)\n\n(defn generate-indices [n]\n  (repeatedly n index))\n\n(defn random-elements [n xs]\n  (if (or (< n 1) (empty? xs))\n    []\n    (map #(nth xs %) (generate-indices n))))\n \n\n Let’s check that  rand-int  is being called. We can do this by making sure that the sum of 10 random elements selected from  [0 10 20 30]  are greater than zero and less than 300. \n\n (testing \"random selection\"\n  (let [ns (random-elements 10 [0 10 20 30])\n        sum (reduce + ns)]\n    (is (< 0 sum 300))))\n\t\n(defn ^:dynamic index [limit]\n  (rand-int limit))\n\n(defn generate-indices [n limit]\n  (repeatedly n (partial index limit)))\n\n(defn random-elements [n xs]\n  (if (or (< n 1) (empty? xs))\n    []\n    (map #(nth xs %) (generate-indices n (count xs)))))\n \n\n The odds that this test will fail are on the order of a million to one.  I could better those odds if I thought it necessary.  If I wanted to eliminate those odds I could write a spy that ensures that the  random-int  function is being called correctly.  But I didn’t think any of those machinations were worth it. \n\n So far I’ve used only integers in the tests.  Most of the tests don’t really care about the type of the elements.  So why not change those particular tests to make sure that many different types are handled properly. \n\n (testing \"trivial cases\"\n  (is (= [1] (random-elements 1 [1]))))\n\n(testing \"repetitive case\"\n  (is (= [:x :x] (random-elements 2 [:x]))))\n\n(testing \"singular random case\"\n  (with-bindings {#'index (fn [_] 1)}\n    (is (= ['b'] (random-elements 1 ['a' 'b'])))))\n\n(testing \"repeated random case\"\n  (with-bindings {#'index (fn [_] 1)}\n    (is (= [\"two\" \"two\"] (random-elements 2 [\"one\" \"two\"])))))\n \n\n And, with that, I think we are done. \n\n There are 8 assertions in my tests.  However, most of those assertions were added  in the process  of TDD.  Now that the function is working, how many assertions are actually necessary?  Probably just the degenerate cases and the final test of randomness.  Let’s call that 4 assertions.  I’ll leave all the others in for documentation purposes; but they aren’t strictly necessary. \n\n What about invalid arguments?  What if someone calls:  (random-elements -23 nil) ?  Should I write tests for those cases? \n\n The function already handles negatives by returning an empty list for any count less than one.  This isn’t tested; but the code is pretty clear.  In the case of the nil, an exception will be thrown.  That’s OK with me.  This is a dynamically typed language.  You get exceptions when you mess up the types. \n\n Is that a risk?  Sure, but only if some other part of the system was written without tests.  If you call this function from a module that you wrote with tests, using something as effective as the TDD discipline,  you won’t be passing  nil  or negative numbers, or any other form of invalid arguments .  So I’m not going to worry about that.  It’s on you. \n\n The Crux \n This is the crux of the argument between Mark and I.  I claim that the number of tests required are only those tests that are necessary to describe the correct behavior of the system.  If the behavior of each element of the system is correct, then no element of the system will pass invalid arguments to any other.  Invalid states will be  unrepresented . \n\n Of course no suite of tests can completely show that a system is correct.  Dijkstra said it best: “Testing shows the presence, not the absence of bugs.”  Still, we have to at least try.  So we demonstrate  practical  correctness with as comprehensive a suite of tests as we can muster. \n\n I assert that the set of tests that show  practical  correctness would be the same irrespective of whether the system was written in a statically, or a dynamically typed language. \n\n Mark asserts that you’ll need more tests when using a dynamically typed language because static type checking makes all illegal states  unrepresentable , thus eliminating them from consideration.  If you have a type that cannot be  nil  (for example) then you don’t have to (and simply cannot) write a test that checks for  nil . \n\n My answer to that is that even using a dynamically typed language I don’t have to write the test for  nil  because I know that  nil  will never be passed.  I don’t need to make states unrepresentable, if I know those states will never be represented. \n\n Anyway… \n Back to  random-elements : \n\n If you compare the two functions that Mark and I wrote (and if you understand Haskell, which I don’t) I think you’ll find that the two implementations are somewhat similar.  The testing strategies we employed were, however,  very different . \n\n The tests I wrote show that I was concerned almost entirely with the desired behavior of the function.  The tests walk that behavior forward one small step at a time using the TDD style. \n\n Mark, however, was far more concerned with expressing correctness in terms of generic types.  The vast majority of his thought process was consumed with the type structure that properly represented the problem.  He then verified behaviors with property-based  QuickCheck  style tests, and after-the-fact tests. \n\n Which is better? \n\n \n   “Answering  that  question with specificity is above my pay grade.” - Barack Obama. \n \n\n Which of us wound up with fewer tests?  In either case I think the four base assertions are adequate.  However, as part of the process I wrote 8 assertions and Mark wrote two property-based  QuickCheck  tests, and three after-the-fact regression tests.  Does that amount to 5 vs. 8?  Or do those  QuickCheck  tests count for more?  I don’t know.  I’ll let you decide. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2019/10/31/Circulatory.html", "title": "Circulatory", "content": "\n       My wife and I both got genetic analyses from  23andMe  recently.  I discovered that my ancestry comes from Britain and Northern Europe.  My wife is Mexican, and she found that her ancestry is very diverse. \n\n One of the services of 23andMe is that they offer to connect you to relatives who have also used 23andMe.  Using this service my wife found a second cousin whom she had never met, but whose extended family had overlapped with hers.  By email they were able to compare the names of aunts and uncles, and the towns where they lived.  The more they talked, the more they realized that the overlap with the extended families was large. \n\n Some years back I went through the effort of scanning all the old photo albums that we had created or inherited over the years.  From that trove of digitized pictures my wive was able to find a 50 year old photograph of that extended family, taken in a little town in Mexico.  She shared that photo with her relative who happened to be visiting that town at the moment. \n\n The relative showed the picture to her aunts, uncles, cousins, and they all started pointing to people that they recognized.  Many tears flowed as warm recollections were conveyed.  This is apparently the only surviving photograph of that extended family; and now they all have, and cherish, it. \n\n Now I want you to consider what made this possible. \n\n \n   I scanned those pictures on a whim, using  Photomyne  an iPhone app that makes scanning and cataloging old photos very easy. \n   My wife was able to find that picture using the Photos app on the Mac. \n   The software at 23andMe was able to find her distant relative. \n   Email and FaceTime allowed the two to communicate. \n   And the internet ran through it all. \n \n\n Software.  It was software that drove the connection of all those people.  It was software that enabled the warm tears of recollection to flow.  It was software that provided the photo to the folks in that little town in Mexico, who had not seen the faces of their loved ones in 50 years.  It was Software.  It was you and I – the programmers who built the systems and the connections that made that miracle happen. \n\n Software is the circulatory system of our civilization.  Software digests, filters, and sorts the constituents of the information stream.  Software routes the necessary element of that stream to the right places.  Software is the heart, the lungs, the vessels, the liver and kidneys, and the digestive system of our civilization.  Nothing works anymore without software.  Our civilization could no longer survive without software. \n\n But software does more than support the survival of our civilization.  Software also supports those moments of joy that my wife and her relatives recently experienced. \n\n It is things like this that make me proud to be a programmer.  Without us, our civilization could not survive, and the warm connections between relatives and friends could not be made. \n\n It is things like this that also make me yearn for a deeper discipline and professionalism for our industry.  Too much depends upon us now.  We’re going to have to leave the wild west of software behind and civilize ourselves, so that the civilization we support will continue to prosper. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2019/08/22/WhyClojure.html", "title": "Why Clojure?", "content": "\n       I’ve programmed systems in many different languages; from assembler to Java.  I’ve written programs in binary machine language.  I’ve written applications in Fortran, COBOL, PL/1, C, Pascal, C++, Java, Lua, Smalltalk, Logo, and dozens of other languages.  I’ve used statically typed languages, with lots of type inference.  I’ve used typeless languages.  I’ve used dynamically typed languages.  I’ve used stack based languages like Forth, and logic based languages like Prolog. \n\n Over the last 5 decades, I’ve used a LOT of different languages. \n\n And I’ve come to a conclusion. \n\n My favorite language of all, the language that I think will outlast all the others, the language that I believe will eventually become the standard language that all programmers use… \n\n …is Lisp. \n\n I have not come to this conclusion casually, nor even willingly.  I was not fan of Lisp.  For 40 years I was not a fan of Lisp.  I saw the  CAR s and  CDR s and  CADDADDR s and thought it was all just academic baloney; interesting but not truly useful. \n\n And then, a decade ago I found  SICP .  And after that I found Clojure.  Clojure is a Lisp that rides on top of the Java ecosystem (and does not have  CAR s or  CDR s, or  CADADAADR  s). \n\n I wasn’t convinced right away.  It took a few years.  But after the usual stumbling around and frustration, I began to realize that this language was the easiest, most elegant, least imposing language I had ever used – and not by a small margin. \n\n So, why Clojure?  I’ve made a list.  Are you ready for it?  Here it is. \n\n \n   Economy of Expression. \n \n\n If you are wondering where the rest of the list is, there isn’t any more.  That’s the reason.  There’s only one.  It is just simpler, and easier, and less occluding to write expressive code in Clojure.  It requires fewer lines.  It require fewer characters.  It require fewer hours.  It requires fewer mental gymnastics. \n\n Why should this be so?  The answer is very simple.  Indeed the answer is: Very simple.  The language has almost no syntax or grammar. \n\n Let me say that again: \n\n \n   The language has almost no syntax or grammar. \n \n\n Probably the best way to elucidate this point is to show you an example.  So, here, for your entertainment, is the program that prints the first 25 squares of integers. \n\n (println (take 25 (map #(* % %) (range))))\n \n\n Let’s walk through the syntax here. \n\n \n   (  means: begin a list. \n   )  means: end the innermost open list. \n   names  are just names, in this case all are functions. \n   *  is the name of the multiply function. \n   #  means: interpret the next list as a function. \n   %  means: the first argument of that function. \n \n\n You’ve just seen 80% or so of the syntax of Clojure.  But before I show you any more, let’s just walk through the code above. \n\n First of all there is that opening  ( .  That means everything that follows until the final  )  is a list.  In most contexts Clojure interprets lists as function calls.  In this case the function is  println .  That’s just the regular old  System.out.println  we’re used to in Java – sort of. \n\n What are we passing to  println ?  We’re passing the result of the  take  function.  The  take  function expects two arguments.  The first,  25  is the number of items to take.  The second argument is a list.  In this case the  take  function will return a list with the first 25 items of the list in the second argument. \n\n What is the list in the second argument?  It’s the result of calling the  map  function.  The  map  function expects two arguments.  The first is a function, and the second is a list.  The  map  function will return a list that is the result of calling the passed-in function on every element of the passed-in list. \n\n What function is passed-in to  map ?  It’s the anonymous function created by  # , which simply calls multiply on it’s duplicated first argument  % . \n\n What list is passed into  map ?  It is the list returned by calling the  range  function.  The  range  function simply returns a list of “all” non-negative integers.  That list is  lazy , so only the integers required by the upstream functions will actually be generated. \n\n Some folks don’t like the complication of the  %  syntax, so we can create a  square  function as follows: \n\n (defn square [x] (* x x))\n(println (take 25 (map square (range))))\n \n\n The  defn  “function”[1] defines a new function named  square .  The brackets  []  work just like parentheses, except that they define a different kind of “list” called a vector.  Lists have the runtime complexity of linked lists.  Vectors have the runtime complexity of arrays.  Sort of.  Anyway, in this case, the vector tells  defn  that the  square  function requires one argument named  x .  The rest you should be able to infer. \n\n This makes the second line a little nicer.  The  map  function simply invokes  square . \n\n Now you’ve seen about 90% of the syntax of Clojure. \n\n Now let’s compare that to the equivalent Java program: \n\n public class SquaresOfIntegers {\n  public static void main(String[] args) {\n    for (int i=0; i<25; i++)\n      System.out.println(i*i);\n  }\n}\n \n\n This is considerably more wordy, even if you don’t count the enclosing class.  More to the point, however, is that this covers, perhaps 5% of the syntax of Java.  And don’t get me started in a comparison with C++. \n\n Now I don’t want to belabor the point.  I could go on with comparison after comparison with language after language.  The bottom line is that Clojure has a much smaller syntax than most languages.  That minimal syntax means that I can express problems clearly, and directly, with much less effort and contortion than most other languages. \n\n Look.  I was not an easy sell.  I was (am) a C++ programmer. More than that, I was (am no longer) a C++ language lawyer.  I revelled in the heavyweight syntax of the language.  I was enthralled by all its lovely “fidelty bits”. I found the transition to Java, twenty years ago, to be rather “meh”.  It was just a slimmed down C++ (it’s gotten a bit more corpulent since). \n\n But my transition to Clojure was an eye-opener.  Based on the lightweight syntax I expected it to be suitable for a few classroom exercises, but not for building large systems.  In my mind, large systems equated to large syntax.  Boy, was I wrong. \n\n What I found, instead, was that the minimal syntax of Clojure is  far more  conducive to building large systems, than the heavier syntax of Java or C++.  In fact, it’s no contest.  Building large systems is Clojure is just simpler and easier than in any other language I’ve used. \n\n And as I pointed out at the beginning, I’ve used a lot of languages. \n\n But What About…? \n\n So you’ve probably got some complaints, questions, objections, etc.  Let me see if I can anticipate them. \n\n OMG!  All Those Parentheses \n\n How old are you…?  Look, here’s a java function call:  f(x) .  Now here’s the corresponding function call in Clojure:  (f x) .  Do you see any extra parentheses in there? \n\n OK, that’s not entirely fair.  We do wind up with a few more parentheses, but thats just because we tend to nest function calls.  Look at that squares of integers program above and you’ll see why.  Don’t fret though, if you really don’t like that nested syntax, you can always use the threading macros (Let the reader understand). \n\n But isn’t it slow? \n\n No.  Clojure is not slow.  Oh, look, it’s not C.  It’s not assembler.  If nanoseconds are your concern than you probably don’t want Clojure in your innermost loops.  You also probably don’t want Java, or C#.  But 99.9% of the software we write nowadays has no need of nanosecond performance.  I’ve built a real time, GUI based, animated  space war  game using Clojure.  I could keep the frame rates up in the high 20s even with hundreds of objects on the screen.  Clojure is not slow. \n\n But what about Javascript? \n\n ClojureScript compiles right down to Javascript and runs in the browser just fine.  Indeed, that space war program I mentioned above was compiled using ClojureScript and ran in the Browser at even higher frame-rates than in native mode (I’m still trying to figure  that  out.) \n\n But it’s dynamically typed! \n\n You do write tests, don’t you? And as part of those tests you can employ the  clojure/spec  library to specify the schema of your types, and do dynamic checking with pre-conditions and post-conditions (Design by Contract style) to your heart’s delight. \n\n But it’s dynamically typed[2]!! \n\n Declaring types requires syntax.  Syntax reduces economy of expression. (Incomming!) \n\n But,  Dammit , it’s dynamically typed!!! \n\n OK, I get it. You like static typing.  Fine.  You use a nice statically typed language, and I’ll use Clojure.  And I’ll be in Scotland before ye. \n\n What about IDEs? \n\n IntelliJ with the Cursive plugin does very nicely.  Most Clojure programmers use Emacs though. \n\n What about refactoring? \n\n IntelliJ with Cursive has some nice refactorings; though they don’t have “Extract Method” yet (Come on guys!) \n\n What about Java interop? \n\n No brainer.  Clojure programs can call Java directly.  Java programs can call Clojure programs with only a little bit of fuss.  Interop should not be your concern. \n\n Where do I find Clojure programmers? \n\n They’re out there; but you are better off making them.  The syntax is trivial.  The Java platform you likely already know.  Just decide to build your next system in Clojure.  Spend a week or two getting used to the language.  Then start.  Oh, by the time you’re a couple of months in you’ll realize how primitive your early stuff was, and you’ll be tempted to clean it up.  But what else is new? \n\n But what about that new language over there… What’s is called?  Um.  Gazoo? \n\n Yes, there are new languages every month.  Every week.  Every day.  We have no lack of new languages.  The thing about those new languages is that there’s nothing new in any of them.  They’re just made up of the pieces of old languages that they cut up and shook in a jar and then dumped out in a game of language Yahtzee. \n\n OK, that’s not entirely fair.  There are still some good ideas percolating in the language space.  But none of those ideas are revolutionary.  We’ve entered the era of “tweaking”.  I don’t find any of the new languages nearly as compelling as Clojure, simply because of the extra syntax virtually all of them carry. \n\n Other languages have had minimal syntax.  Why not one of them? \n\n Yes, that’s true.  Forth had a tiny syntax.  So did Smalltalk.  But those two languages had baggage of their own.  Forth was a reverse polish stack language.  Economy of expression is not the phrase I would use for Forth.  Though I find it fascinating that PostScript was based on it. \n\n Smalltalk was small and elegant and beautiful.  It spawned the Design Patterns revolution.  It spawned the Refactoring revolution.  It spawned the TDD revolution.  It helped to spawn the Agile revolution.  Smalltalk was a language of tremendous impact. \n\n Smalltalk was also an image based language.  Very few programmers have ever wrapped their minds around what that really meant.  So, unfortunately, the language languished compared to all the text-file based languages. \n\n Lisp is older, by far, than Smalltalk or Forth.  It was created in 1957 from concepts that came out of the ’30s) and it has never languished in the way Smalltalk and Forth have.  Indeed, it’s the language that  refuses  to die.  We’ve tried to kill it many times.  But like the annoying neighborhood stray cat it  keeps … coming … back . \n\n Finally, Lisp is functional.  And the future is looking very functional to me. \n\n \n [1]  defn  is not actually a function; but you didn’t need to know that. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2019/06/16/ObjectsAndDataStructures.html", "title": "Classes vs. Data Structures", "content": "\n       \n   What is a class? \n \n\n A class is the specification of a set of similar objects. \n\n \n   What is an object? \n \n\n An object is a set of functions that operate upon encapsulated data elements. \n\n \n   Or rather, an object is a set of functions that operate on  implied  data elements. \n \n\n What do you mean by  implied  data elements”? \n\n \n   The functions of an object imply the existence of some data elements; but that data is not directly accessible or visible outside of the object. \n \n\n Isn’t the data inside the object? \n\n \n   It could be; but there’s no rule that says it must be.  From the point of view of the user, an object is nothing more than a set of functions.  The data that those functions operate upon must exist, but the location of that data is unknown to the user. \n \n\n Hmmm.  OK, I’ll buy that for the moment. \n\n \n   Good.  Now, what is a data structure? \n \n\n A data structure is a cohesive set of data elements. \n\n \n   Or, in other words, a data structure is a set of data elements operated upon by implied functions. \n \n\n OK, OK. I get it.  The functions that operate on the data structure are not specified by the data structure but the existence of the data structure implies that some operations must exist. \n\n \n   Right.  Now what do you notice about those two definitions? \n \n\n They are sort of the opposite of each other. \n\n \n   Indeed.  They are complements of each other.  They fit together like a hand in a glove. \n \n\n \n   \n     An Object is a set of functions that operate upon implied data elements. \n     A Data Structure is a set of data elements operated upon by implied functions \n   \n \n\n Wow, so objects aren’t data structures. \n\n \n   Correct.  Objects are the opposite of data structures. \n \n\n So a DTO – a Data Transfer Object – is not an object? \n\n \n   Correct.  DTOs are data structures. \n \n\n And so database tables aren’t objects either are they? \n\n \n   Correct again.  Databases contain data structures, not objects. \n \n\n But wait.  Doesn’t an ORM – And Object Relational Mapper – map database tables to objects? \n\n \n   Of course not.  There is no mapping between database tables and objects.  Database tables are data structures, not objects. \n \n\n So then what do ORMs do? \n\n \n   They transfer data between data structures. \n \n\n So they don’t have anything to do with Objects? \n\n \n   Nothing whatever.  There is no such thing as an Object Relational Mapper; because there is no mapping between database tables and objects. \n \n\n But I thought ORMs built our business objects for us. \n\n \n   No, ORMs extract the data that our business objects operate upon.  That data is contained in a data structure loaded by the ORM. \n \n\n But then doesn’t the business object contain that data structure? \n\n \n   It might.  It might not.  That’s not the business of the ORM. \n \n\n That seems like a minor semantic point. \n\n \n   Not at all.  The distinction has significant implications. \n \n\n Such as? \n\n \n   Such as the design of the database schema vs. the design of the business objects.  Business objects define the structure of the business  behavior.   Database schemas define the structure of the business data.  Those two structures are constrained by very different forces.  The structure of the business data is not necessarily the best structure for the business behavior. \n \n\n Hmmm.  That’s confusing. \n\n \n   Think of it this way.  The database schema is not tuned for just one application; it must serve the entire enterprise.  So the structure of that data is a compromise between many different applications. \n \n\n OK, I get that. \n\n \n   Good.  But now consider each individual application.  The Object model of each application describes the way the behavior of those applications are structured.  Each application will have a different object model, tuned to that application’s behavior. \n \n\n Oh, I see.  Since the database schema is a compromise of all the various applications, that schema will not conform to the object model of any particular application. \n\n \n   Right!  Objects and Data Structures are constrained by very different forces.  They seldom line up very nicely.  People used to call this the Object/Relational impedance mismatch. \n \n\n I’ve heard of that.  But I thought that impedance mismatch was solved by ORMs. \n\n \n   And now you now differently.  There is no impedance mismatch because objects and data structures are complementary, not isomorphic. \n \n\n Say what? \n\n \n   They are opposites, not similar entities. \n \n\n Opposites? \n\n \n   Yes, in a very interesting way.  You see, objects and data structures imply diametrically opposed control structures. \n \n\n Wait, what? \n\n \n   Consider a set of object classes that all conform to a common interface.  For example, imagine classes that represent two dimensional shapes that all have functions for calculating the  area  and  perimeter  of the shape. \n \n\n Why does every software example always involve shapes? \n\n \n   Let’s just consider two different types:  Square s and  Circle s.  It should be clear that the  area  and  permimeter  functions of these two classes operate on different implied data structures.  It should also be clear that the way those operations are called is via dynamic polymorphism. \n \n\n Wait.  Slow down.  What? \n\n \n   There are two different  area  functions; one for  Square , the other for  Circle .  When the caller invokes the  area  function on a particular object, it is that object that knows what function to call.  We call that dynamic polymorphism. \n \n\n OK.  Sure.  The object knows the implementation of its methods.  Sure. \n\n \n   Now let’s turn those objects into data structures.  We’ll use Discriminated Unions. \n \n\n Discoominated whats? \n\n \n   Discriminated Unions.  In our case that’s just two different data structures.  One for  Square  and the other for  Circle .  The  Circle  data structure has a center point, and a radius for data elements.  It’s also got a type code that identifies it as a  Circle . \n \n\n You mean like an enum? \n\n \n   Sure.  The  Square  data structure has the top left point, and the length of the side.  It also has the type discriminator – the enum. \n \n\n OK.  Two data structures with a type code. \n\n \n   Right.  Now consider the  area  function.  Its going to have a switch statement in it, isn’t it? \n \n\n Um.  Sure, for the two different cases.  One for  Square  and the other for  Circle .  And the  perimeter  function will need a similar switch statement \n\n \n   Right again.  Now think about the structure of those two scenarios.  In the object scenarios the two implementations of the  area  function are independent of each other and belong (in some sense of the word) to the type.   Square ’s  area  function belongs to  Square  and  Circle ’s  area  function belongs to  Circle . \n \n\n OK, I see where you are going with this.  In the data structure scenario the two implementations of the  area  function are together in the same function, they don’t “belong” (however you mean that word) to the type. \n\n \n   It gets better.   If you want to add the  Triangle  type to the object scenario, what code must change? \n \n\n No code changes.  You just create the new  Triangle  class.  Oh, I suppose the creator of the instance has to be changed. \n\n \n   Right.  So when you add a new type, very little changes.  Now suppose you want to add a new function - say the  center  function. \n \n\n Well then you’d have to add that to all three types,  Circle ,  Square  ,and  Triangle . \n\n \n   Good.  So adding new functions is hard, you have to change each class. \n \n\n But with data structures it’s different.  In order to add  Triangle  you have to change each function to add the  Triangle  case to the switch statements. \n\n \n   Right.  Adding new types is hard, you have to change each function. \n \n\n But when you add the new  center  function, nothing has to change. \n\n \n   Yup.  Adding new functions is easy. \n \n\n Wow.  It’s the exact opposite. \n\n \n   It certainly is.  Let’s review: \n \n\n \n   \n     Adding new functions to a set of classes is hard, you have to change each class. \n     Adding new functions to a set of data structures is easy, you just add the function, nothing else changes. \n     Adding new types to a set of classes is easy, you just add the new class. \n     Adding new types to a set of data structures is hard, you have to change each function. \n   \n \n\n Yeah.  Opposites.  Opposites in an interesting way.  I mean, if you know that you are going to be adding new functions to a set of types, you’d want to use data structures.  But if you know you are going to be adding new types then you want to use classes. \n\n \n   Good observation!  But there’s one last thing for us to consider today.  There’s yet another way in which data structures and classes are opposites.  It has to do with dependencies. \n \n\n Dependencies? \n\n \n   Yes, the direction of the source code dependencies. \n \n\n OK, I’ll bite.  What’s the difference? \n\n \n   Consider the data structure case.  Each function has a switch statement that selects the appropriate implementation based upon the type code within the discriminated union. \n \n\n OK, that’s true.  But so what? \n\n \n   Consider a call to the  area  function.  The caller depends upon the  area  function, and the  area  function depends upon every specific implementation. \n \n\n What do you mean by “depends”? \n\n \n   Imagine that each of the implementations of  area  is written into it’s own function.  So there’s  circleArea  and  squareArea  and  triangleArea . \n \n\n OK, so the switch statement just calls those functions. \n\n \n   Imagine those functions are in different source files. \n \n\n Then the source file with the switch statement would have to import, or use, or include, all those source files. \n\n \n   Right.  That’s a source code dependency.  One source file depends upon another source file.  What is the direction of that dependency? \n \n\n The source file with the switch statement depends upon the source files that contain all the implementations. \n\n \n   And what about the caller of the  area  function? \n \n\n The caller of the  area  function depends upon the source file with the switch statement which depends upon all the implementations. \n\n \n   Correct.  All the source file dependencies point in the direction of the call, from the caller to the implementation.  So if you make a tiny change to one of those implementations… \n \n\n OK, I see where you are going with this.  A change to any one of the implementations will cause the source file with the switch statement to be recompiled, which will cause everyone who calls that switch statement – the  area  function in our case – to be recompiled. \n\n \n   Right.  At least that’s true for language systems that depend upon the dates of source files to figure out which modules should be compiled. \n \n\n That’s pretty much all of them that use static typing, right? \n\n \n   Yes, and some that don’t. \n \n\n That’s a lot of recompiling. \n\n \n   And a lot of redeploying. \n \n\n OK, but this is reversed in the case of classes? \n\n \n   Yes, because the caller of the  area  function depends upon an interface, and the implementation functions also depend upon that interface. \n \n\n I see what you mean.  The source file of the  Square  class imports, or uses, or includes the source file of the  Shape  interface. \n\n \n   Right.  The source files of the implementation point in the opposite direction of the call.  They point from the implementation to the caller.  At least that’s true for statically typed languages.  For dynamically typed languages the caller of the  area  function depends upon nothing at all.  The linkages get worked out at run time. \n \n\n Right.  OK.  So if you make a change to one of the implementations… \n\n \n   Only the changed file needs to be recompiled or redeployed. \n \n\n And that’s because the dependencies between the source files point against the direction of the call. \n\n \n   Right.  We call that Dependency Inversion. \n \n\n OK, so let me see if I can wrap this up.  Classes and Data Structures are opposites in at least three different ways. \n\n \n   Classes make functions visible while keeping data implied.  Data structures make data visible while keeping functions implied. \n   Classes make it easy to add types but hard to add functions.  Data structures make it easy to add functions but hard to add types. \n   Data Structures expose callers to recompilation and redeployment.  Classes isolate callers from recompilation and redeployment. \n \n\n \n   You got it.  These are issues that every good software designer and architect needs to keep in mind. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2019/07/22/WhyWontIt.html", "title": "Why won't it...", "content": "\n       A few days ago I was ordering lunch at a barbecue joint in Austin, Texas.  The menu described a lovely plate of three sausages, and suggested six different sausage types to choose from.  I love a good sausage; and I am especially fond of Bratwurst.  So when the waiter came by I told him I wanted two Brats and a Keilbasa. \n\n The waiter nodded and proceeded to tap on his hand-held POS terminal.  Over the next 60 seconds his taps grew ever more punctuated and his expression grew ever more frustrated. He started muttering words under his breath – words that I have heard so many times before:  Why won’t it let me…? \n\n Apparently the genius developers who wrote the software for the hand-held POS terminal had not considered that a customer would want two of the same kind of sausage.  The terminal simply would not allow the waiter to add bratwurst twice. \n\n We had a large group at the table, and the waiter had only taken half the orders at this point; but his frustration was so great that he abandoned the table and ran into the kitchen to resolve the issue.  He came back a few minutes later and told me that the kitchen would do their best. \n\n Software is supposed to make life easier.  Software is supposed to ease and enable the job of users.  All too often, however, software is constraining and restrictive.  If you don’t do what the software expects, the software fights you and stops you.  And you mutter under your breath: “Why won’t it…” \n\n In the early days of software we were generally forgiving of this kind of thing.  It was enough of a miracle that we were getting any automated help at all.  So the inflexible state machines that programmers employed in those days were a small price to pay for the massive improvement we gained from the automation. \n\n But those days are gone.  Nowadays we expect software to  get out of our way .  For example the hand-held POS terminal used by a waiter should be  more  flexible, and  less  constraining than a pad of paper.  That POS terminal should  not  get in the way. \n\n Now before you try to put all this off on the UX people, remember that you are writing the code.  It is your fingers on the keyboard.  It is your mind engaged in the task of making the machine do what the user needs it to do.  We expect the UX people to be smart.  We expect that they will try to describe a system that will stay out of the users’ way.  But that doesn’t mean that we should blithely and blindly implement that specification.  After all, nobody knows the machine better than we do. \n\n Remember that as a programmer you are also a stakeholder.  Your reputation is on the line.  You need the product to be a success.  So  play  with the software.  Pretend you are a user.  Walk through the use cases.  Invent new use cases.  Try to find areas where the software inhibits or constrains those use cases. \n\n Yes, I know, the UX people will be doing this too.  Yes, I know, there will be alpha and beta trials (probably).  Yes, I know, UX is not your job.  But do it anyway; because your name is on that product. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2019/11/03/WhatTheyThoughtOfUs.html", "title": "What They Thought of Programmers.", "content": "\n       It is interesting and educational to go back in time and look at how programmers were represented in popular culture.  What did they think of us?  Did they know who were? \n\n It’s important to remember that prior to 1946 there were no programmers, that computers themselves were virtually unknown until the late ’50s.  That virtually nobody lived next door to a programmer back then. \n\n Nowadays virtually everyone in the western world, and even in much of the developing world, is surrounded by computers.  And while programming remains a mystery to many, programmers are common neighbors. \n\n So let’s scan the last six decades and watch as the culture changes it’s view of just who we are and what we do. \n\n 1956 Forbidden Planet \n It’s best to begin at the beginning.  The first truly classic Science Fiction movie.   Forbidden Planet .  If you haven’t seen it, you are missing something profound and spectacular.  I urge you to watch – even study – it. \n\n There are no explicit mentions of computers or programmers in this movie.  The concept was simply not something that the public could relate to.  However there was a machine.  A very big machine.  And the implication was that it was intelligent, but not sentient. \n\n \n\n In the movie the anti-hero Dr Morbius is marooned on the uninhabited world of Altair 4.  He discovers an ancient alien machine.  Two decades later rescuers arrive.  He shows them the machine and states: “I have reason to believe that years ago a minor alteration was performed throughout the entire 8000 cubic miles of it’s own fabric.” \n\n The programmers of that big machine are long dead; but they are described as belonging to a highly evolved and benevolent alien race. \n\n There is another machine on this planet.  It is a robot named Robby. \n\n \n\n Robby is clearly intelligent and sentient.  Robby speaks English, with the inflection of a proper british butler, rather in the manner of Carson on Downton Abbey.  Dr. Morbius claims to have created the Robot; so he is clearly the programmer. \n\n Morbius is studious, austere, even dour.  He is not evil; but he is a hermit and does not particularly enjoy the company of others.  He is massively intelligent but quite anti-social. \n\n Now remember that this was the ’50s.  Missiles and A-bombs.  Scientists had a particular stereotype in those days, and Dr. Morbius is consistent with that; though with a hint of Captain Nemo. \n\n 1954 TOBOR the Great \n Yes, I’m going backwards two years, but only to say that I did not forget this movie.  I just don’t count it as significant.  This was a movie made for kids, and the semi-intelligent robot is much more like  Lassie  than Robby.  The creator of Tobor is an eminent scientist who also fits the mold of the ’50s. \n\n 1966 Star Trek \n With one exception, we learn very little about the programmers in Star Trek.  The computer, however, is fascinating.  The computer was voiced by Majel Barrett, Gene Roddenberry’s wife.  She also played Nurse Chapel, and “Number One” in the pilot.  She played the computer as utterly flat.  The voice was monotone.  The information was factual.  The computer never offered an opinion, or an emotion of any kind.  The computer was nothing more than a tool. \n\n The exception was the episode entitled  The Ultimate Computer  in which a new intelligent computer was hooked up to the enterprise.  The creator (and implicitly the programmer) of this machine was Dr. Daystrom.  Both he and the computer have a simultaneous nervous breakdown, and Kirk has to “pull the plug”. \n\n The implication is that programmers are so intelligent and driven that they eventually lose emotional stability. \n\n This is one of the first instances of the computer acting as the villain. \n\n 1968 2001: A Space Odyssey \n Hal 9000 is the villain of this story.  We know little of the programmer, Dr. Chandra, except that he taught the computer a song. \n\n Note that during this era it is the computer that is the character.  The programmers, if mentioned at all, are ancillary. \n\n 1970 Colossus: The Forbin Project \n Another movie in which the computer is the hyperintelligent villain.  The programmer is a scientist from the Dr. Morbius mold. \n\n 1982 Blade Runner \n The computers are among the main characters and are essentially a race of slaves.  We never meet the programmers, but it’s clear that they must be morally bankrupt. \n\n 1982 Tron \n Hero programmer defeats evil computer.  This is the first time we see the programmer as a good guy who defeats the computer.  The movie is also a foreshadow of  The Matrix  because the main character gets transported into the computer.  As a programmer (though they call him a “user”) he as powers. \n\n The hero programmer is a world famous scientist and business man.  He does not live next door. \n\n 1983 War Games \n The computer is again a character, though this time an innocent dupe.  A young boy meets the programmer and psychoanalyses him in order to convince the computer to not destroy the world.  The programmer is depicted as a famous scientist who is emotionally damaged.  The computer is depicted as a child-like character who likes to play games. \n\n 1984 The Terminator \n This one is indirect. Well meaning humans program the evil computer, Skynet, that then programs the Terminator to kill Sarah Connor.  So this is a  singularity  prediction.  The computers program the computer. \n\n One interesting aspect of this film is the depiction of the human-like machine being so utterly focused on it’s mission.  At first you think of the terminator as almost human.  But bit by bit that humanity is lost.  In the end you see only the machine, half-destroyed, missing legs and all vestiges of human form, still intent on one purpose only. \n\n 1986 Short Circuit \n (Sigh) Jonny number five is a combat robot whose programming gets scrambled by a lightning strike.  This makes the robot sentient and purely innocent.  Eventually the robot invents it’s own moral code which is vastly superior to anything human. \n\n So there is no programmer in this case – except nature or God or…  And in that case all human flaws are exposed by the purity of the programming. \n\n Cute movie, but very dumb. \n\n 1993 Jurassic Park \n This is our first real glimpse of a humanized programmer.  Dennis Nedry is not a mad scientist, not a well respected researcher, he’s just a common ordinary programmer.  And he is a flawed human.  Oh, there’s a bit of the Twinkie eating, basement dwelling stereotype there; but this is the first time the movies show a programmer as someone who might live next door. \n\n The computer is not a character at all.  It is just a tool (“a Unix System”). \n\n 1995 The Net \n The main character is a programmer who must user her skills as a programmer to defeat a ruthless plot to frame her for murder and other nefarious things. \n\n This is another case where the programmer could be someone next door. \n\n 1999 The Matrix \n All the human characters are programmers.  They all live next door.  But, given the red pill, are transported to an alternate reality where they can “see” the code.  They are engaged in an apocalyptic fight over good and evil.  The main character is a type of Jesus. \n\n Summary \n Note the progression.  Over the years the representation of the computer changes from Main Character (Good or evil) to supporting character to tool.  The programmer changes from obscurity to mad or damaged scientist, to Nature or Skynet, to the guy next door, to hyper-aware Savior. \n\n What does this say about society’s opinion of us.  Does society really think we are the folks who live next door who are simultaneously the hidden saviors? \n\n Well, maybe we don’t want to read too much into things.  Note that I stopped this review just prior to the millennium.  Have there been any movies since then in which programmers played a significant role? \n\n Actually, I think we have transitioned off the screen and have become part of the movie industry.  Virtually no movie made nowadays can be made without massive computer graphics and programming effort.  So now they know us intimately.  We  do  live next door.  And they don’t need to put us on the screen anymore. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2019/11/08/OpenLetterLinuxFoundation.html", "title": "Open Letter to the Linux Foundation", "content": "\n       To: The Linux Foundation \n    Jim Zemlin: Executive Director \n    Angela Brown: VP of Events \n    Andy Updegrove: Legal Council \n\n From: Robert Martin (@unclebobmartin) (unclebob@cleancoder.com) \n\n Re: Code of Conduct case of Charles Max Wood. \n\n Dear Linux Foundation: \n\n I am writing to you as a concerned member of the software development community which I have enjoyed serving for the last 50 years.  I am writing in public because the events I wish to describe took place in public.  I fear that something has gone terribly wrong within your organization; and that it will have deep repercussions within this industry that I cherish. \n\n The timeline of events, as far as I can determine them, is as follows: \n\n The Linux Foundation received a public tweet sent to the @KubeCon twitter address. That tweet recommended that Kube Con discontinue their association with Charles Max Wood. The reasons given in this complaint were his request for an open and civil phone call, and a picture of Mr. Wood wearing a MAGA hat. \n\n The Linux Foundation  publicly  replied from the @linuxfoundation twitter account as follows: \n\n \n   Hi all, We have reviewed social and videos and determined that the Event Code of Conduct was violated and his registration to the event has been revoked. Our events should and will be a safe space. \n \n\n First let me say that I find it highly problematic that the complaint and the decision were public. Indeed I am surprised that LF would accept a publicly submitted code of conduct complaint. I am much more than surprised that LF would ever consider  publicly  responding to such a complaint. Indeed, it seems to me that the public complaint, and perhaps even the public response by LF, could be seen as public harassment – which is explicitly prohibited by the LF Code of Conduct. \n\n It seems to me that Code of Conduct complaints made in public must be immediately rejected and viewed as Code of Conduct violations in and of themselves.  Code of Conduct complaints should be submitted in private and remain private and confidential in order to prevent their use as a means of harassment. It also seems to me that while the process of accepting, reviewing, and adjudicating such complaints should be public, the proceedings and decision of each individual case should remain private and confidential in order to protect the parties from harm. Making them a public showcase is, simply, horrible. \n\n Was the Code of Conduct actually violated by Mr. Wood? I have watched the videos in question and read the tweets and I can find no instance where Mr Wood violated the LF Code of Conduct. I understand that LF can make any decision they like about what constitutes a Code of Conduct violation. However, when both the complaint and the response are so blatantly public, it seems to me that LF owes it to the observing community to explain their decision and describe the due process that was used to make it – including the decision to make the public response that undoubtedly caused harm to Mr. Wood. To date no such explanation has been forthcoming, despite repeated requests. \n\n The software community needs to understand how decisions like this are going to be made. Otherwise those of us who have watched this case may be forced to conclude that LF has no internal process, that no due diligence will be applied to Code of Conduct complaints and determinations, that the accused will have no rights either of appeal or privacy, that LF feels free to make its decisions based on the blowing of political winds, and will loudly announce their decisions regardless of the harm it might cause. \n\n Therefore I have the following questions: \n\n \n   \n     Why was the initial complaint accepted and acknowledged in public? It was clearly political in nature, and very clearly intended to cause harm to Mr. Wood. \n   \n   \n     Is it LF policy to accept complaints that, in and of themselves, violate the LF Code of Conduct? \n   \n   \n     Why was the Code of Conduct determination announced publicly, despite the harm it would obviously cause to Mr. Wood? \n   \n   \n     Can LF specifically justify the determination that Mr. Wood violated the Code of Conduct? \n   \n   \n     Does LF have a documented process by which Code of Conduct complaints are to be submitted, reviewed, and adjudicated? \n   \n   \n     Is it LF policy to consider political affiliation, or support of certain public officials, as Code of Conduct violations? \n   \n   \n     Is it LF policy to publicly denounce registrants who have been determined to have violated the LF Code of Conduct? \n   \n   \n     Does LF have a Code of Conduct for how it conducts itself? \n   \n \n\n In summary, it appears to this humble observer that The Code of Conduct process at The Linux Foundation went very badly off the rails with regard to Charles Max Wood. That LF owes Mr. Wood, and the Software Community at large, a  profound  apology. That LF should keep all future Code of Conduct complaints and decisions personal and confidential. That LF should publish and follow a well defined process for accepting, reviewing, and adjudicating future Code of Conduct complaints. And that some form of reparation be provided to Mr. Wood for the public harm that was done to him by the careless and unprofessional behavior of The Linux Foundation. \n\n Yours \n\n Robert C. Martin. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2020/04/05/ANewHope.html", "title": "A New Hope", "content": "\n       …The Year is 2045… \n\n Dad, can you help me with my school report? \n\n \n   Sure son.  What’s it about? \n \n\n We have to do it on the great pandemic of 2020.  You were there, right? \n\n \n   I was just a little boy.  But I know a lot about it.  What is it you need to know? \n \n\n We’re supposed to write about the heroes. \n\n \n   Ah, yes.  A good topic.  There were so many. \n \n\n OK, so…  Who were they? \n\n \n   Well, first of all there were the healthcare workers.  Day after day, week after week, they kept on working in those hospitals full of very sick people.  Many of them got sick too; and quite a few of them died. \n \n\n They must have been brave. \n\n \n   They were.  Very.  They were as brave as any soldier going to war.  Perhaps braver, because you couldn’t see the enemy, and in those days you couldn’t fight it. \n \n\n We can fight it now, can’t we Dad. \n\n \n   Yes son.  Now we can.  We have vaccines and treatments.  Nobody dies of COVID-19 anymore.  But back then we didn’t have vaccines or treatments.  We just had nurses and doctors who tried their very best to save as many people as possible. \n \n\n So they were the heroes? \n\n \n   Yes. But there were many more.  There were the people who worked in grocery stores. \n \n\n I thought everybody stayed home to work. \n\n \n   Many of us did.  We were the lucky ones.  But the people who worked in those stores had to go to work every day.  People needed food; and so grocery stores needed to stay open.  And the people who worked in those stores had to help hundreds, maybe even thousands of people every day.  They took huge risks to keep those stores open. \n \n\n Wow, I hadn’t thought of that. \n\n \n   And then there were the delivery people.  The people who drove trucks of food to the stores and trucks of products to people’s home.  The people who worked for Amazon, and UPS, and Fedex, and the US Mail. \n \n\n Who else, Dad?  Who else? \n\n \n   Well, look son, there were so many.  The police, the firemen, the sailors and soldiers, the air traffic controllers, the garbage men, the repairmen.  Even though most people weren’t working, the essential parts of our civilization had to be kept running.  And then there were just the everyday people who followed the rules and kept themselve at home for so many weeks.  It was a huge effort that everyone had to play a part in. \n \n\n \n   But… \n \n\n But what, Dad? \n\n \n   Well, there was one group of people who don’t often get mentioned; but without them the Pandemic would have been a hundred times worse than it was. \n \n\n Really?  Who? \n\n \n   The programmers. \n \n\n Dad…  You’re a programmer aren’t you? \n\n \n   Yes son, I am.  Just like my mother – your gramma – before me.  She was one of the ones who worked during the Pandemic. \n \n\n Was gramma a hero Dad? \n\n \n   No more than anyone else, son.  She worked from home.  She wore masks, and kept the necessary social distance from others.  I was just a little boy, but I remember those masks and how much we had to stay at home.  Most programmers did just what Gramma did too.  They worked from home. \n \n\n So then why were they heros, Dad?  It sounds to me like they just did what everybody else did. \n\n \n   Well, son, think of this.  It was the programmers who made it possible for people to work from home; because it was the programmers who built the software that made the internet possible.  You see, this was the first full scale national emergency during which people had instant access to the news, to the government, and to each other.  When the President, and the Governors told people to shelter at home, almost everybody knew about it within minutes or hours.  The news was sent to their computers, to their phones, and to their watches.  Not only that, but people who were stuck at home could still talk to each other using Facebook and Twitter and Facetime.  People could order products on Amazon, and on so many other on-line shopping networks.  People could even order food from restaurants to be delivered or picked up.  Without the programmers who made those systems, people would have had a much harder time sheltering at home; and the pandemic would have been much worse. \n \n\n So the programmers weren’t brave, like the doctors and nurses and police were brave.  They weren’t heros like that. \n\n \n   No, not like that.  But without them, without the tools they created, so many more people would have died.  For example, did you know that the genetic code of the virus was sequenced long before the pandemic spread?  It was that RNA sequence that allowed our researchers to get a head start on the vaccines that eventually killed off the virus and saved so many people.  It was programmers who built the software that ran in those RNA sequencers.  Without those programmers, the vaccines might have come much too late. \n \n\n Wow!  What else, Dad?  What else? \n\n \n   Well, you know that there was a time when people used paper money, right?  Imagine how easily the virus would have spread if people paid for groceries or gasoline with paper money!  But it was programmers who built the systems that allowed people to pay with credit cards, or by just waving their phone or watch over readers.  They didn’t even have to touch antyhing!  The virus couldn’t spread that way. \n \n\n \n   And then there was so much entertainment piped right into people’s homes.  Netflix, and Amazon Prime, and Youtube, and..  Well the options were endless back then. \n \n\n \n   So people could work from home, shop from home, be entertained at home, and hardly ever had to leave their homes.  And all that was because of the software written by programmers. \n \n\n And that saved us, didn’t it Dad? \n\n \n   Well, son, it certainly played a pretty important part. \n \n\n Are you glad you’re a programmer Dad? \n\n \n   It’s an important job, Son.  I never want to be anything else.  Except, of course, your Dad. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2020/04/06/ALittleClojure.html", "title": "A Little Clojure", "content": "\n       So let’s learn just a little bit of clojure. \n\n This expression:  (1 2)  represents the list containing the integers 1 and 2 in that order.  If you want an empty list, that’s just  () .  And the list of the first five letters of the alphabet is just  (\\a \\b \\c \\d \\e) . \n\n Now you know a lot about the syntax of clojure.  Perhaps you think there’s a lot missing.  Well, there are a  few  things missing; but far fewer than you’d think. \n\n You might be wondering how you add two numbers.  That’s easy, that’s just  (+ 1 2) .  As it happens that’s also just the list of the function named  +  followed by a 1 and a 2.  You see, a function call is really just a list.  The function is the first element of the list, and the arguments are just the other elements of that list.  When you want to call a function, you simply invoke the list that represents that function call. \n\n There are quite a few built-in functions in clojure.  For example there’s  +, -, *, and / .  They do precisely what you’d think.  Well, perhaps not precisely.   (+ 1 2 3)  evaluations to  6 .   (- 3 2 1)  evaluates to zero.   (* 2 3 4)  evaluates to  24 .  And  (/ 20 2 5)  evaluates to 2.   (- 5)  evaluates to  -5 .   (* 5)  evaluates to  5 .  And, get ready for this,  (/ 3)  evaluates to  1/3 .  That last is the clojure syntax for the rational number one-third. \n\n (first 1 2 3)  evaluates to  1 ,  (second 1 2 3)  evaluates to 2, and  (last 1 2 3)  evaluates to – you guessed it –  3 . \n\n If you’d like to see this in action you’ll need to start up a clojure REPL.  You can google how to do that.  The word REPL stands for Read, Evaluate, Print Loop.  It’s a very simple program that reads in an expression, evaluates that expression, prints the result of that expression, and then loops back to the read. \n\n If you start a REPL you’ll get some kind of a prompt, perhaps like this  user=> .  Then you can type an expression and see it evaluated.  Here are a few from my REPL \n\n user=> (+ 1 2 3 4)\n10\nuser=> (- 5 6 7 8)\n-16\nuser=> (* 6 7 8)\n336\nuser=> (/ 5 6 9)\n5/54\n \n\n If you try the expression at the very start of this article:  (1 2)  you’ll get a nasty surprise. \n\n user=> (1 2)\nClassCastException java.lang.Long cannot be cast to clojure.lang.IFn  user$eval1766.invokeStatic (:1)\n \n\n That’s because the digit  1  is not a function; and the REPL believes that if it reads a list, that list ought to be evaluated as a function call.  If you just want the list  (1 2)  at the REPL you can convince the REPL not to call the list as a function by quoting it as follows: \n\n user=> (quote (1 2))\n(1 2)\nuser=> '(1 2)\n(1 2)\nuser=> (list 1 2)\n(1 2)\n \n\n The first invokes the  quote  function which prevents its argument  (1 2)  from being evaluated and just returns it.  The second is just a little syntax shortcut for calling the  quote  function.  The third invokes the function that constructs lists. \n\n Lists are implemented as linked lists.  Each element contains a value and points to the next element.  That makes it very fast to add an element to the front of the list, or to walk the list one element at a time.  But it makes it slow to index into the list to find the Nth element.  So, for that, clojure uses the  vector  data type.  Here is a vector of the first three integers:  [1 2 3] .  That’s right, it’s the square brackets that do the trick. \n\n A vector is a lot like a growable array. It’s easy to add to the end of it, and it’s easy to index into it.  Lists make good stacks. Vectors make good queues. \n\n Now let’s define a function.   (defn f [x] (+ (* 3 x) 1))   this defines the function named  f .  It takes one argument named  x .  And it calculates the formula:  3x+1 . \n\n Now let’s take this apart one token at a time.  This looks like a call to the function  defn .  We’ll let that stand for the moment, but it’s not exactly right;  defn  is a bit more special than that. The next token is the name of the function:  f .  Names are alphanumeric with a few special characters allowed.  For example  +++  is a valid name.  Following the name is a vector that names the function arguments.  Again, these are names.  Those names will be bound to the argument values when the function is called.  And following the argument vector is the expression that is evaluated by the function.  That expression can use the argument names. \n\n You now know the vast majority of Clojure syntax.  There’s more, of course, but you already know enough to write significant programs. \n\n So let’s write a simple one.  Let’s write the factorial function.  \n (defn fac [x] (if (= x 1) 1 (* x (fac (dec x))))) \n\n Let’s walk through this.  The function is named  fac  and it takes one argument named  x .  The  if  function takes three arguments. If the first evaluates to something  truthy  it returns the second, otherwise it returns the third.  The  =  function does exactly what you’d think: it is a test for equality.  If  x  is 1, then the  if  statement, and therefore the function, will return 1.  Otherwise the  if  statement will return  x  times the factorial of the decrement of  x . \n\n Let’s try it: \n\n user=> (fac 3)\n6\nuser=> (fac 4)\n24\nuser=> (fac 10)\n3628800\nuser=> (fac 20)\n2432902008176640000\nuser=> (fac 30)\n\nArithmeticException integer overflow  clojure.lang.Numbers.throwIntOverflow (Numbers.java:1501)\n \n\n That works nicely, until we exceed 64 bits of precision.  Clojure likes to use 64 bit integers for efficiency.  But if you’d rather have unlimited precision you can use the  N  notation. \n\n user=> (fac 1000N)\n402387260077093773543702433923003985719374864210714632543799910429938512398629020592044208486969404800479988610197196058631666872994808558901323829669944590997424504087073759918823627727188732519779505950995276120874975462497043601418278094646496291056393887437886487337119181045825783647849977012476632889835955735432513185323958463075557409114262417474349347553428646576611667797396668820291207379143853719588249808126867838374559731746136085379534524221586593201928090878297308431392844403281231558611036976801357304216168747609675871348312025478589320767169132448426236131412508780208000261683151027341827977704784635868170164365024153691398281264810213092761244896359928705114964975419909342221566832572080821333186116811553615836546984046708975602900950537616475847728421889679646244945160765353408198901385442487984959953319101723355556602139450399736280750137837615307127761926849034352625200015888535147331611702103968175921510907788019393178114194545257223865541461062892187960223838971476088506276862967146674697562911234082439208160153780889893964518263243671616762179168909779911903754031274622289988005195444414282012187361745992642956581746628302955570299024324153181617210465832036786906117260158783520751516284225540265170483304226143974286933061690897968482590125458327168226458066526769958652682272807075781391858178889652208164348344825993266043367660176999612831860788386150279465955131156552036093988180612138558600301435694527224206344631797460594682573103790084024432438465657245014402821885252470935190620929023136493273497565513958720559654228749774011413346962715422845862377387538230483865688976461927383814900140767310446640259899490222221765904339901886018566526485061799702356193897017860040811889729918311021171229845901641921068884387121855646124960798722908519296819372388642614839657382291123125024186649353143970137428531926649875337218940694281434118520158014123344828015051399694290153483077644569099073152433278288269864602789864321139083506217095002597389863554277196742822248757586765752344220207573630569498825087968928162753848863396909959826280956121450994871701244516461260379029309120889086942028510640182154399457156805941872748998094254742173582401063677404595741785160829230135358081840096996372524230560855903700624271243416909004153690105933983835777939410970027753472000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000N\n \n\n OK, one last thing.  Let’s add up all the numbers in a list.  We want  (sum [1 2 3 4 5])  to evaluate to  15 .  First we’ll do it the hard way: \n (defn sum [l] (if (empty? l) 0 (+ (first l) (sum (rest l))))) \n\n The  empty?  function does just what you’d think, it returns true if the list is empty.  The  rest  function returns all but the first element of a list. \n\n Of course we could have written  sum  like this:  (defn sum [l] (apply + l)) .  The  apply  function – um – applies the function passed in it’s first argument to the list in its second. \n\n We could also have written the function like this:  (defn sum [l] (reduce + l)) .  But that takes us to the  reduce  function which (as George Carlin used to say) might go a bit too far.  At least for this article. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2020/04/09/ALittleMoreClojure.html", "title": "A Little More Clojure", "content": "\n       So let’s learn just a little bit more of Clojure. \n\n Here are a few common utility functions: \n\n user=> (inc 1) ; increments argument\n2\nuser=> (dec 3) ; decrements argument\n2\nuser=> (empty? []) ; tests for empty\ntrue\nuser=> (empty? [1 2])\nfalse\n \n\n If you know Java or C# you probably know what the  map  function does.  Here’s an example:  (map inc [1 2 3])  evaluates to  (2 3 4) . \nThe first argument of map is a function.  The second is a list.  The  map  function returns a new list by applying the function to every element of the input list. \n\n The  filter  function also takes a function and a list.   (filter odd? [1 2 3 4 5])  evaluates to  (1 3 5) .  From that I think you can tell what both the  filter  and the  odd?  functions do. \n\n And so with that, let’s try a little challenge.  Let’s find all the prime numbers between one and a thousand. \n\n We’ll use a variant of TDD to do this.  Our eyes will be the tests.  The cycle will be the same size as normal TDD; but we’ll write a bit of code first and then test it. \n\n \n   I know.  Blashphemy!  So sue me.  ;-) \n \n\n We begin like this:  (defn primes [n] )   This returns  nil . \n\n user=> (primes 1000)\nnil\n \n\n Now let’s get all the numbers between 1 and  n . \n\n (defn primes [n]\n  (range 1 (inc n)))\n\nuser=> (primes 10)\n(1 2 3 4 5 6 7 8 9 10)\n \n\n You’ve probably figured out what  range  does.  It just returns a list of all the integers between it’s arguments. \n\n OK, so now all we have to do is filter all the primes: \n\n (defn primes [n]\n  (let [candidates (range 1 (inc n))]\n    (filter prime? candidates)))\n\nCompilerException java.lang.RuntimeException: Unable to resolve symbol: prime? in this context, compiling:(null:3:5)\n \n\n Oh, oh.  We need to implement  prime? \n\n (defn prime? [n])\n\nuser=> (primes 10)\n()\n \n\n OK, that makes sense.  But I should explain the  let  function.  It allows you to create names that are bound to expressions.  The names exist only within the parentheses of the  let  expression.  So it’s a way to create local variables – though the word “variable” is not quite right because they cannot be reassigned.  They are immutable. \n\n Now how do we tell if a given integer  n  is prime?  Well, you all know how to do that, right?  The simple and naive way is to divide the integer by every number between 2 and  n .  But, of course that’s wasteful.  There’s a better upper limit to try which is the square root of  n .  I’m sure you can work out why that’s true. \n\n (defn prime? [n]\n  (let [sqrt (Math/sqrt n)]\n     sqrt))\n \nuser=> (prime? 100)\n10.0\n \n\n OK, that’s right.  Notice that we called the Java  Math.sqrt  function.  That’s a good example of how Clojure can call down into the Java libraries).  Of course we don’t want  prime?  to return a number; we want it to return a boolean.  But for now it’s good to see the intermediate values of our computation. \n\n So, next we’d like to get all the integers between 2 and the square root.  We already know how to do that. \n\n (defn prime? [n]\n  (let [sqrt (Math/sqrt n)\n        divisors (range 2 (inc sqrt))]\n     divisors))\n \n user=> (prime? 100)\n (2 3 4 5 6 7 8 9 10)\n \n\n Now which of the  divisors \tdivide  n  evenly?  We can find out by using the  map  function. \n\n (defn prime? [n]\n  (let [sqrt (Math/sqrt n)\n        divisors (range 2 (inc sqrt))\n        remainders (map (fn [x] (rem n x)) divisors)]\n   remainders))\n \nuser=> (prime? 100)\n(0 1 0 0 4 2 4 1 0)\n \n\n The  rem  function should be self-explanatory; it just returns the integer remainder of the division of  n  by  x .  The  (fn [x]...)  business needs a little explanation.  Notice how similar it is to  defn f [x] ?  This is how we create an anonymous function.  If you know the syntax in Java or C# for anonymous functions, then this shouldn’t be too much of a surprise to you.  Anyway, the remainders list is just the list of all the remainders that result from dividing  n  by the  divisors . \n\n Now some of those remainders were zero, and that means they divided  n  evenly.  Therefore  n  (100 in this case) is not prime.  Let’s try a few others. \n\n user=> (prime? 17)\n(1 2 1 2)\nuser=> (prime? 1001)\n(1 2 1 1 5 0 1 2 1 0 5 0 7 11 9 15 11 13 1 14 11 12 17 1 13 2 21 15 11 9 9)\nuser=> (prime? 37)\n(1 1 1 2 1 2)\n \n\n OK, so if the remainders list has a zero in it, then  n  is not prime.  Well, that should be easy, shouldn’t it? \n\n (defn prime? [n]\n  (let [sqrt (Math/sqrt n)\n        divisors (range 2 (inc sqrt))\n        remainders (map (fn [x] (rem n x)) divisors)\n        zeroes (filter zero? remainders)]\n     zeroes))\n \nuser=> (prime? 100)\n(0 0 0 0)\nuser=> (prime? 17)\n()\t \n \n\n So now all we need to do is return  true  if the list is empty.  Right? \n\n (defn prime? [n]\n  (let [sqrt (Math/sqrt n)\n        divisors (range 2 (inc sqrt))\n\t\tremainders (map (fn [x] (rem n x)) divisors)\n\t\tzeroes (filter zero? remainders)]\n     (empty? zeroes)))\n\nuser=> (prime? 100)\nfalse\nuser=> (prime? 17)\ntrue\nuser=> (primes 100)\n(1 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97)\t\n \n\n Now I want you to think carefully about how we solved this problem.  No  if  statements.  No  while  loops.  Instead we envisioned lists of data flowing through filters and mappers.  The solution was almost more of a fluid dynamics problem than a software problem.  (Ok, that’s a stretch, but you get my meaning.)  Instead of imagining a procedural solution, we imagine a data-flow solution. \n\n Think hard on this – it is one of the keys to functional programming. \n\n (Special thanks to Stu Halloway @stuarthalloway for cluing me into the dataflow mindset way back in 2005) \n\n Oh, and the primes between 1 and 1000? \n\n user=> (primes 1000)\n(1 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97 101 103 107 109 113 127 131 137 139 149 151 157 163 167 173 179 181 191 193 197 199 211 223 227 229 233 239 241 251 257 263 269 271 277 281 283 293 307 311 313 317 331 337 347 349 353 359 367 373 379 383 389 397 401 409 419 421 431 433 439 443 449 457 461 463 467 479 487 491 499 503 509 521 523 541 547 557 563 569 571 577 587 593 599 601 607 613 617 619 631 641 643 647 653 659 661 673 677 683 691 701 709 719 727 733 739 743 751 757 761 769 773 787 797 809 811 821 823 827 829 839 853 857 859 863 877 881 883 887 907 911 919 929 937 941 947 953 967 971 977 983 991 997)\n \n\n \n   And, yes, there is a bug.  1 is not prime.  2 is prime.  Can you fix it? \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2020/09/12/TheDisinvitation.html", "title": "The Disinvitation", "content": "\n       I have a friend, in the Chicago area, who calls me up two or three times a year to ask me to give a talk at a User Group, or a conference he’s involved with, or something like that.  If my schedule is free I always say yes. I don’t charge anything because I enjoy supporting the Chicago software community, and it’s never a bad thing to get my face out in front of new people.  I am a consultant, after all, and giving pro-bono talks is one of the ways I promote myself. \n\n Anyway, he wrote to me last October (That’s right, a full year ago!) and asked me to give a presentation at a Chicago conference this September 21st.  I agreed, and he thanked me, and that was that.  Then, in June, he wrote to tell me that the conference was going to be virtual due to Covid.  I acknowledged and, once again, that was that. \n\n Last Wednesday, September 9th, twelve days before the conference, he called me on the phone and said: \n\n \n   “This is going to be the most uncomfortable phone call I have ever made.” \n \n\n He went on to say that the “Code of Conduct” people at the conference were concerened about some of my political opinions, and that some of the speakers of the conference refused to speak if I was going to speak. \n\n Like I said, this guy is a friend of mine, and I don’t want to get him into any trouble, so I decided not to raise a fuss about it, and I promised him I would not mention his name or the name of the conference on line.  He responded by telling me: \n\n \n   I’m scared to death of these people. \n \n\n Over the last few days I’ve been mulling this situation over in my mind, and I’ve come to a few interesting conclusions. \n\n \n   The conference organizers are in breach of contract. \n \n\n OK, we didn’t have a formal written contract, but we had emails.  And we also had the fact that, for the better part of a year, the conference website had my picture on it, and advertised me as a speaker.  I conclude that the conference organizers derived substantial benefit from those pictures and from promising my virtual presence to their audience.  I, on the other hand, was denied the benefit of actually speaking to that audience.  Therefore, I am the damaged party. \n\n Could I sue them?  Certainly, though I’d have a difficult time quantifying the damages.  Had we agreed on a speaking fee, I could at least claim that fee as damages.  Next time I do one of these pro-bono events I’ll have the organizers agree to paying a hefty cancellation fee. \n\n \n   The speakers who refused to speak if I spoke are guilty of  tortious interference . \n \n\n Those speakers would not have been harmed by speaking in a virtual conference that I also spoke in.  Their intent was to damage me by forcing the conference organizers to breach their contract with me.  That is the definition of tortious interference. \n\n Could I sue them?  Certainly.  I won’t, for the same reason that I’m not going to sue the conference organizers.  And, frankly, suing people for such small potatoes just isn’t worth the trouble.  But, like I said, next time I do a pro-bono talk I’ll have the conference organizers agree to the value that I’m deriving in return for using my name and likeness on their website.  Then I can sue them, and any tortious interferers, for that sum and punitive damages too. \n\n Do I know who those tortiously interfering speakers are?  I’ve got a pretty good idea.  Myfear of course is that I do not wish to harm my friend.  Nor do I wish to harm the conference organizers, nor the Chicago Software community.  It seems to me that they are all victims of those revolting speakers. \n\n So, this time, I’ll let the legal options rest.  Instead, I’m offering a virtual  free talk  at 10:00 AM CDT, on September 21st, the first day of the conference.  Those who wanted to hear me speak, still can. \n\n The last point I’d like to make is this: \n\n \n   Disinviting someone from a virtual conference who can draw a potentially large audience away from that virtual conference is not a particularly intelligent tactic. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2020/05/27/ReplDrivenDesign.html", "title": "REPL Driven Design", "content": "\n       If you follow me on facebook you know that I’ve been publishing daily CoronaVirus statistics. I generate these statistics using the daily updates in the Johns Hopkins github  repository . \n\n At first I just hand copied the data into a spreadsheet.  But that became tedious quite rapidly. \n\n Then, in late March, I wrote a little Clojure program to extract and process the data.  Every morning I pull the repo, and then run my little program.  It reads the files, does the math, and prints the results. \n\n Of course I used TDD to write this little program. \n\n But over the last several weeks I’ve made quite a few small modifications to the program; and it has grown substantially.  In making these adaptations I chose to use a different discipline:  REPL Driven Design. \n\n REPL Driven Design is quite popular in Clojure circles.  It’s also quite seductive.  The idea is that you try some experiments in the REPL to make sure you’ve got the right ideas.  Then you write a function in your code using those idea.  Finally, you test that function by invoking it at the REPL. \n\n It turns out that this is a very satisfying way to work.  The cycle time – the time between a code experiment and the test at the REPL – is nearly as small as TDD.  This breeds lots of confidence in the solution.  It also seems to save the time needed to mock, and create fake data because, at least in my case, I could use real production data in my REPL tests.  So, overall, it felt like I was moving faster than I would have with TDD. \n\n But then, in late April, I wanted to do something a little more complicated than usual.  It required a design change to my basic structure.  And suddenly I found myself full of fear.  I had no way to ensure that those design changes wouldn’t leave the system broken in some way.  If I made those changes, I’d have to examine every output to make sure that none of them had broken. So I postponed the change until I could muster the courage, and set aside the dedicated time it would require. \n\n The change was not too painful.  Clojure is an easy language to work with.  But the verfication was not trivial, which led me to deploy the program with a small bug – a bug I caught 4 days later.  That bug forced me to go back and correct the data and graphs that I generated. \n\n Why did I need the design change?  Because I was not mocking and creating fake data.  My functions just read from the repo files directly.  There was no way to pass them fake data.  The design change I needed to make was precisely the same as the design change that I’d have needed for mocking and fake data. \n\n Had I stuck with the TDD discipline I would have automatically made that design change, and I would not have faced the fear, the delay, and the error. \n\n Is it ironic that the very design change that TDD would have forced upon me was the design change I eventually needed?  Not at all. The decoupling that TDD forces upon us in order to pass isolated inputs and gather isolated outputs is almost always the design that fascilitates flexibility and promotes change. \n\n So I’ve learned my lesson.  REPL driven development feels easier and faster than TDD; but it is not.  Next time, it’s back to TDD for me. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2020/09/23/ConferenceConduct.html", "title": "Conference Conduct", "content": "\n       It was just a few years ago, at the height of the  Me Too  revelations, that codes of conduct began to prominently appear in Software Conferences.  At the time I felt this was appropriate given the horror stories that had been circulating about sexual harassment and misbehavior at some of those conferences.  I wrote a  blog  about it at the time. \n\n Since then I have seen the other side of the coin.  Codes of conduct have been used as weapons to exclude people on the basis of their political opinions, or on the basis of their associations, or just because someone didn’t like them.  I have written blogs about this as well.   (1) ,  (2) \n\n As much as I think that codes of conduct are a good idea, we must not allow them to be weaponized.  If we are going to set up rules with consequences, then we also need to set up the the due processes by which those rules and consequences are adjudicated.  Otherwise the people who police the codes of conduct will be free of the due checks and balances that protect conference attendees and speakers from unfair and malicious actions.  As we have seen, such malicious and unfair actions have become all too common. \n\n It seems to me that if a conference is going to publish a code of conduct, like the one below, they must also publish the process by which alleged violations will be adjudicated.  That process must include provisions for the accused to be able to defend themselves against the allegation, and must also allow the accused to know the identity of the accuser(s).  Otherwise all conference attendees and speakers will be exposed to malicious and falsified complaints with no recourse to defend themselves. \n\n The conference I was  disinvited  from is over.  I was ejected because code of conduct complaints were registered against me by three relatively minor speakers in quick succession.  I do not know if those speakers acted in concert.  Nor am I certain of the identities of those speakers (though I have a good idea).  What I  do  know is that three or four weeks before the conference was to begin those speakers threatened to withdraw from the conference if I were allowed to speak. \n\n From what I have been able to discern, the conference organizers conducted an investigation.  I was not a party to this investigation, indeed I was unaware that it was taking place.  I was not notified about the complaints, nor was I given the opportunity to speak in my own defense.  The conference organizers simply judged me based upon the complaints and whatever they could discover for themselves.  I am quite certain that due diligence was not a requirement of the investigation. \n\n Given that they were volunteers, and that losing three speakers one month before the conference is a considerable blow, it’s not hard to imagine that the conference organizers were under a fair bit of pressure to resolve the issue quickly and salvage as many speakers as possible.  What’s more, the conference had already extracted as much value as it could from my image being emblazoned on their website and on the mailers they sent out two days before the start of the conference.  So the decision to eject me must have been pretty easy. \n\n What was the code of conduct violation?  Apparently it related to something on twitter.  I have read the code of conduct and the only potential violation I can see falls under the following rule. \n\n \n   Any form of written, social media, or verbal communication that can be offensive or harassing to any attendee, speaker or staff is not allowed at Chicago Cloud Conference. \n \n\n That’s quite a standard.  I don’t think any of us could withstand it.  We’ve all said or written things that have offended, or could offend  someone . I’ve had people get offended about my definition of monads.  I’ve had people get upset with me about the SOLID principles, or my position on TDD, or my criticisms of statically typed languages.  Some people may even have been offended by my infrequent comments about current politics. \n\n As written, this rule means that anybody can complain about anything you might have said or written, at any time in the past.  The only qualification for violation is that  someone  finds it offensive. \n\n What’s more, since there is no published process of adjudication, you may well find that if a complaint is made against you, you will  not  be able to defend yourself, in any way.  An individual, or a small group of people, whom you do not know, will vote in secret, without your knowledge, and without your input. If they decide against you, you will be ejected from the conference, without refund, and without recourse. \n\n In short this means that if someone doesn’t like you, they can get you kicked out – and there’s nothing you can do about it.  In my case three speakers apparently didn’t like something I said on twitter.  So they extorted the conference organizers who bowed under the weight of that extortion and disinvited me without giving me the opportunity to address the complaints. \n\n My solution to this is simple: \n\n \n   From now on I will not agree to attend, nor will I agree to speak at, any conference that publishes a code of conduct but does not have a published process for adjudicating code of conduct complaints.  That process must include a means for those accused of a violation to defend themselves from the malicious actions of others, and must allow them to know who their accusers are. \n \n\n I recommend that you all adopt the same policy. \n\n \n\n \n   Code Of Conduct \n \n\n \n   Chicago Cloud Conference is dedicated to providing a harassment-free conference experience for everyone, regardless of gender, sexual orientation, disability, physical appearance, body size, race, or religion. We have a zero-tolerance policy for any harassment of conference participants in any form. Sexual language and imagery is not appropriate for any conference venue, including talks. Conference participants violating these rules may be sanctioned or expelled from the conference without a refund at the discretion of the conference organizers. \n \n\n \n   Any form of written, social media, or verbal communication that can be offensive or harassing to any attendee, speaker or staff is not allowed at Chicago Cloud Conference. Please inform a Chicago Cloud Conference staff member if you feel a violation has taken place and the conference leadership team will address the situation. \n \n\n \n   Harassment includes offensive verbal comments related to gender, sexual orientation, disability, physical appearance, body size, race, religion; sexual images in public spaces; deliberate intimidation; stalking; following; harassing photography or recording; sustained disruption of talks or other events; inappropriate physical contact; and unwelcome sexual attention. Participants asked to stop any harassing behavior are expected to comply immediately.\nExhibitors in the expo hall, sponsor or vendor booths, or similar activities are also subject to the anti-harassment policy. In particular, exhibitors should not use sexualized images, activities, or other material. Booth staff (including volunteers) should not use sexualized clothing/uniforms/costumes, or otherwise create a sexualized environment. \n \n\n \n   If a participant engages in harassing behavior, the conference organizers may take any action they deem appropriate, including warning the offender or expulsion from the conference with no refund. If you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact a member of conference staff immediately. Conference staff can be identified by t-shirts and special badges.\nConference staff will be happy to help participants contact hotel/venue security or local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the conference. We value your attendance. \n \n\n \n   We expect participants to follow these rules at all conference venues and conference-related social events. \n \n\n \n   Chicago Cloud Conference prioritizes marginalized people’s safety over privileged people’s comfort and therefore we will not act on complaints regarding:\n‘Reverse’ -isms, including ‘reverse racism,’ ‘reverse sexism,’ and ‘cisphobia’.\nReasonable communication of boundaries, such as “leave me alone,” “go away,” or “I’m not discussing this with you”.\nCommunicating in a ‘tone’ you don’t find congenial.\nCriticizing racist, sexist, cissexist, or otherwise oppressive behavior or assumptions. \n \n\n \n   What to do when you witness a Code of Conduct violation? \n \n\n \n   All reports of incidents are confidential! We will not publish the name of the reporter in any way.\nSpeak up \n \n\n \n   Of course we do not want you do get into a more uncomfortable position as you maybe already are. You do not need to interact with the person(s) who presumably violated the Code of Conduct.  Please let someone of the organizing team know \n \n\n \n   In every session, you will find one track host (the person introducing the speakers) and at least one crew member (wearing a colorful shirt with the word “crew” on it). All people who are working on Chicago Cloud Conference are very aware of the Code of Conduct.\nApproach them and let them know. In most cases they will bring you to one of the main organizers, so we can write an incident report.\nWho What were the circumstances that led to the incident?\nWhen? \n \n\n \n   Everyone working on Chicago Cloud Conference is informed on how to deal with an incident and how to further proceed with the situation. \n \n\n \n   The Purpose of the Code of Conduct: \n \n\n \n   By signaling inclusivity and diversity as values we expect the conference to uphold, the Code of Conduct helps guarantee that the event will indeed be inclusive and embrace diversity. \n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2020/09/30/loopy.html", "title": "Loopy", "content": "\n       The following is a segment of a journey.  It has no obvious beginning point, nor does it actually end up anywhere.  The value, if any, is in the journey itself. \n\n The code below is the standard solution to the  Prime Factors Kata . \n\n public List<Integer> factorsOf(int n) {\n  ArrayList<Integer> factors = new ArrayList<>();\n\n  for (int d = 2; n > 1; d++)\n    for (; n % d == 0; n /= d)\n      factors.add(d);\n\n  return factors;\n}\n \n\n However, I was doing this kata in Clojure the other day and I wound up with a different solution.  It looked like this: \n\n (defn prime-factors [n]\n  (loop [n n d 2 factors []]\n          (if (> n 1)\n            (if (zero? (mod n d))\n              (recur (/ n d) d (conj factors d))\n              (recur n (inc d) factors))\n            factors)))\n \n\n The algorithm is pretty much the same.  I mean if you tracked the value of  n ,  d , and  factors  they would go through the same changes.  On the other hand the code in Java is a doubly nested loop; but the code in Clojure is a single recursive loop with two recursion points.  That’s interesting. \n\n I could write the recursive algorithm in Java like this: \n\n   private List<Integer> factorsOf(int n) {\n    return factorsOf(n, 2, new ArrayList<Integer>());\n  }\n\n  private List<Integer> factorsOf(int n, int d, List<Integer> factors) {\n    if (n>1) {\n      if (n%d == 0) {\n        factors.add(d);\n        return factorsOf(n/d, d, factors);\n      } else {\n        return factorsOf(n, d+1, factors);\n      }\n    }\n    return factors;\n  }\n \n\n And then, since this is tail recursive, I could rewrite it as a straight loop. \n\n   private List<Integer> factorsOf(int n, int d, List<Integer> factors) {\n    while (true) {\n      if (n > 1) {\n        if (n % d == 0) {\n          factors.add(d);\n          n /= d;\n        } else {\n          d++;\n        }\n      } else\n        return factors;\n    }\n  }\n \n\n For all intents and purposes this code executes the same algorithm as the standard solution; but it does not have a doubly nested loop.  We have transformed the code from a doubly nested loop, to a single loop, without affecting the algorithm. \n\n Is this always possible? \n\n In other words: given a program with a nested loop, is there a way to write the same program with a single loop? \n\n The answer to that is:  Yes. \n\n The fact that a bit of code executes within an inner loop could be encoded into a state variable.  The outer loop could then dispatch to that bit of code depending upon how that state variable is set. \n\n We see that in the code above.  The state condition for the inner loop is  n%d==0 .  Indeed, I can extract that out as a  explanatory variable  to make my point clearer.  I can also extract  n>1 . \n\n   private List<Integer> factorsOf(int n, int d, List<Integer> factors) {\n    while (true) {\n      boolean factorsRemain = n > 1;\n      boolean currentDivisorIsFactor = n % d == 0;\n      if (factorsRemain) {\n        if (currentDivisorIsFactor) {\n          factors.add(d);\n          n /= d;\n        } else {\n          d++;\n        }\n      } else\n        return factors;\n    }\n  }\n \n\n Now all the looping decisions are made at the very top; and the  if  statements simply dispatch the flow of control to the right bits of code. \n\n That nested  if  is a bit annoying. Let’s replace all that nesting with appropriate logic. \n\n   private List<Integer> factorsOf(int n, int d, List<Integer> factors) {\n    while (true) {\n      boolean factorsRemain = n > 1;\n      boolean currentDivisorIsFactor = n % d == 0;\n      if (factorsRemain && currentDivisorIsFactor) {\n          factors.add(d);\n          n /= d;\n      }\n      if (factorsRemain && !currentDivisorIsFactor)\n          d++;\n      if (!factorsRemain)\n        return factors;\n    }\n  }\n \n\n Now we have a nice outer loop that fully determines the execution path up front, and then selects the appropriate paths with a sequence of  if  statements with no  else  clauses. \n\n Indeed, we can improve upon this just a little bit more by using more explanatory variables to explicitly name those paths. \n\n   private List<Integer> factorsOf(int n, int d, List<Integer> factors) {\n    while (true) {\n      boolean factorsRemain = n > 1;\n      boolean currentDivisorIsFactor = n % d == 0;\n  \n      boolean factorOutCurrentDivisor = factorsRemain && \n                                        currentDivisorIsFactor;\n      boolean tryNextDivisor = factorsRemain && !currentDivisorIsFactor;\n      boolean allDone = !factorsRemain;\n  \n      if (factorOutCurrentDivisor) {\n        factors.add(d);\n        n /= d;\n      }\n      if (tryNextDivisor) {\n        d++;\n      }\n      if (allDone)\n        return factors;\n    }\n  }\n \n\n I think I can make this more interesting by using an  enum  and a  switch . \n\n   private enum State {Starting, Factoring, Searching, Done}\n\n  private List<Integer> factorsOf(int n, int d, List<Integer> factors) {\n    State state = State.Starting;\n    while (true) {\n      boolean factorsRemain = n > 1;\n      boolean currentDivisorIsFactor = n % d == 0;\n\n      if (factorsRemain && currentDivisorIsFactor)\n        state = State.Factoring;\n      if (factorsRemain && !currentDivisorIsFactor)\n        state = State.Searching;\n      if (!factorsRemain)\n        state = State.Done;\n\n      switch (state) {\n        case Factoring:\n          factors.add(d);\n          n /= d;\n          break;\n        case Searching:\n          d++;\n          break;\n        case Done:\n          return factors;\n      }\n    }\n  }\n \n\n Now let’s move the determination of the  next  state into each case. \n\n   private List<Integer> factorsOf(int n, int d, List<Integer> factors) {\n    State state = State.Starting;\n    while (true) {\n      switch (state) {\n        case Starting:\n          if (n == 1)\n            state = State.Done;\n          else if (n % d == 0)\n            state = State.Factoring;\n          else\n            state = State.Searching;\n          break;\n        case Factoring:\n          factors.add(d);\n          n /= d;\n          if (n == 1)\n            state = State.Done;\n          else if (n % d != 0)\n            state = State.Searching;\n          break;\n        case Searching:\n          d++;\n          if (n == 1)\n            state = State.Done;\n          else if (n % d == 0)\n            state = State.Factoring;\n          break;\n        case Done:\n          return factors;\n      }\n    }\n  }\n \n\n Ugh.  I think we can improve upon this by moving a few things around and gettting rid of those explanatory variables. \n\n   private List<Integer> factorsOf(int n, int d, List<Integer> factors) {\n    State state = State.Starting;\n    while (true) {\n      switch (state) {\n        case Starting:\n          break;\n        case Factoring:\n          factors.add(d);\n          n /= d;\n          break;\n        case Searching:\n          d++;\n          break;\n        case Done:\n          return factors;\n      }\n\n      if (n == 1)\n        state = State.Done;\n      else if (n % d == 0)\n        state = State.Factoring;\n      else\n        state = State.Searching;\n    }\n  }\n \n\n OK, So now the whole thing has been changed into a  Moore  model finite state machine. The state transition diagram looks like this. \n\n \n\n If you look closely you can see the nested loops in that diagram.  They are the two transitions on the  Searching  and  Factoring  states that stay in the same state.  You can also see the how the two loops interconnect through the transitions between the  Searching  and  Factoring  states.  The  Starting  state simply accepts  n  from the outside world and initializes  d  and  factors , and then dispatches to one of the other three states as appropriate.  The  Done  state simply returns the  factors  list. \n\n This is how Alan Turing envisioned computation in his  1936 paper , which you can read about in Charles Petzold’s wonderful book:  The Annotated Turing . \n\n So, we’ve gone from a nice doubly nested loop in Java to a Turing style finite state machine simply through a sequence of refactorings, each of which kept all the tests passing.  This transformation from a standard procedure to a Turing style finite state machine could be done on any program at all. \n\n Now let’s go back to the two bits of code that started all this.  The Java version: \n\n public List<Integer> factorsOf(int n) {\n  ArrayList<Integer> factors = new ArrayList<>();\n\n  for (int d = 2; n > 1; d++)\n    for (; n % d == 0; n /= d)\n      factors.add(d);\n\n  return factors;\n}\n \n\n And the Clojure version: \n\n (defn prime-factors [n]\n  (loop [n n d 2 factors []]\n          (if (> n 1)\n            (if (zero? (mod n d))\n              (recur (/ n d) d (conj factors d))\n              (recur n (inc d) factors))\n            factors)))\n \n\n The finite state machine is entirely hidden in the Java version isn’t it.  It’s very difficult to see it peaking out from those nested  for  loops.  But that state machine is much more obvious in the Clojure program.  The state is determined by the two  if  forms, and the appropriate code is executed for each state. \n\n If you can’t see that FSM in the Clojure code, then consider this simple refactoring which makes it even more evident: \n\n (defn factors [n]\n  (loop [n n d 2 fs []]\n    (cond\n      (and (not= n 1) (zero? (mod n d)))       (recur (/ n d) d (conj fs d))\n      (and (not= n 1) (not (zero? (mod n d)))) (recur n (inc d) fs)\n      (= n 1)                                  fs)))\n \n\n Why should this be?  Why should the Clojure program look more like the FSM than the Java program?  The answer is simple.  The Java program can save some state information  within  the flow of control, because it can mutate variables while the loops are in progress.  The Clojure program cannot save any state within the flow of control because no variables can be mutated at all.  Those state changes are only noticed when the recursive  loop  is re-entered. \n\n Thus, functional programs tend to look much more like Finite State Machines than programs that are free to manipulate variables. \n\n One last thought.  The Java program that implemented the Finite State Machine had only one loop; and that loop was:  while   (true) .  That means the loop knew nothing at all about the algorithm it was looping.  Thus we can abstract it away from the program itself and envision a language that has no loops at all.  No  while  statements, no  for  loops, no  if  statements, and (of course) no  goto s.  Programs in this language would be written in the FSM style.  They would be composed of switch statements that switched on boolean expressions that identified each state.  The language system would then simply execute that program, over and over, until told to stop. \n\n Such programs would be naturally functional.  For although they could mutate the state of variables, the mutated state would be irrelevant to the flow of control within the program, and could only affect the next iteration of the program.  In effect the program would look like a tail-call-optimized recursive function. \n\n Wait…  Did I miss the exit? \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2020/10/18/Solid-Relevance.html", "title": "Solid Relevance", "content": "\n       Recently I received a letter from someone with a concern.  It went like this: \n\n \n\n \n   For years the knowledge of the SOLID principle has been a standard part of our recruiting procedure. Candidates were expected to have a good working knowledge of these principles.  Lately, however, one of our managers, who doesn’t code much anymore, has questioned whether that is wise.  His points were that the Open-Closed principle isn’t very important anymore because most of the code we write isn’t contained in large monoliths and making changes to small microservices is safe and easy.  The Liskov Substitution Principle is long out of date because we don’t focus on inheritance nearly as much as we did 20 years ago.  I think we should consider  Dan North’s position on SOLID  – “Just write simple code.” \n \n\n \n\n I wrote the following letter in response: \n\n The SOLID principles remain as relevant to day as they were in the 90s (and indeed before that).  This is because software hasn’t changed all that much in all those years — and  that  is because software hasn’t change all that much since 1945 when Turing wrote the first lines of code for an electronic computer.  Software is still  if  statements,  while  loops, and assignment statements —  Sequence ,  Selection , and  Iteration . \n\n Every new generation likes to think that their world is vastly different from the generation before.  Every new generation is wrong about that; which is something that every new generation learns once the next new generation comes along to tell them how much everything has changed.  <grin> \n\n So let’s walk through the principles, one by one. \n\n SRP ) The Single Responsibility Principle. \n\n \n   Gather together the things that change for the same reasons.  Separate things that change for different reasons. \n \n\n It is hard to imagine that this principle is not relevant in software.  We do not mix business rules with GUI code.  We do not mix SQL queries with communications protocols.  We keep code that is changed for different reasons separate so that changes to one part to not break other parts.  We make sure that modules that change for different reasons do not have dependencies that tangle them. \n\n Microservices do not solve this problem.  You can create a tangled microservice, or a tangled set of microservices if you mix code that changes for different reasons. \n\n Dan North’s slides completely miss the point on this, and convinces me that he did not understand the principle at all.  (or that he was being ironic, which knowing Dan, is far more likely)  His answer to the SRP is to “Write Simple Code”.  I agree.  The SRP is one of the ways we keep the code simple. \n\n OCP ) The Open-Closed Principle. \n\n \n   A Module should be open for extension but closed for modification. \n \n\n Of all the principles, the idea that anyone would question this one fills me full of dread for the future of our industry.  Of course we want to create modules that can be extended without modifying them.  Can you imagine working in a system that did not have device independence, where writing to a disk file was fundamentally different than writing to a printer, or a screen, or a pipe?  Do we want to see  if  statement scattered through our code to deal with all the little details? \n\n Or…  Do we want to separate abstract concepts from detailed concepts.  Do we want to keep business rules isolated from the nasty little details of the GUI, and the micro-service communications protocols, and the arbitrary behaviors of the database?  Of course we do! \n\n Again, Dan’s slide gets this completely wrong.  When requirements change only  part  of the existing code is wrong.  Much of the existing code is still right.  And we want to make sure that we don’t have to change the right code just to make the wrong code work again.  Dan’s answer is “write simple code”.  Again, I agree.  And, ironically, he is right.   Simple code is both open and closed . \n\n LSP ) The Liskov Substitution Principle. \n\n \n   A program that uses an interface must not be confused by an implementation of that interface. \n \n\n People (including me) have made the mistake that this is about inheritance.  It is not.  It is about sub-typing.  All implementations of interfaces are subtypes of an interface.  All duck-types are subtypes of an implied interface.  And, every user of the base interface, whether declared or implied, must agree on the meaning of that interface.  If an implementation confuses the user of the base type, then  if/switch  statements will proliferate. \n\n This principle is about keeping abstractions crisp and well-defined.  It is impossible to believe that this is an outmoded concept. \n\n Dan’s slides are entirely correct on this topic; he simply missed the point of the principle.  Simple code is code that maintains crisp subtype relationships. \n\n ISP ) The Interface Segregation Principle. \n\n \n   Keep interfaces small so that users don’t end up depending on things they don’t need. \n \n\n We still work with compiled languages.  We still depend upon modification dates to determine which modules should be recompiled and redeployed.  So long as this is true we will have to face the problem that when module A depends on module B at  compile  time, but not at run time, then changes to module B will force recompilation and redeployment of module A. \n\n This issue is especially acute in statically typed languages like Java, C#, C++, GO, Swift, etc.  Dynamicaly typed languages are affected much less; but are still not immune.  The existence of Maven and Leiningen are proof of that. \n\n Dan’s slide on this topic is provably false.  Clients  do  depend on methods they don’t call, if they have to be recompiled and redeployed when one of those methods is modified.  Dan’s final point on this principle is fine, so far as it goes.  Yes, if you can split a class with two interfaces into two separate classes, then it is a good idea to do so (SRP).  But such separation is often not feasible, nor even desirable. \n\n DIP ) The Dependency Inversion Principle. \n\n \n   Depend in the direction of abstraction. High level modules should not depend upon low level details. \n \n\n It is hard to imagine an architecture that does not make significant use of this principle.  We do not want our high level business rules depending upon low level details.  I hope that is perfectly obvious.  We do not want the computations that make money for us polluted with SQL, or low level validations, or formatting issues.  We want isolation of the high level abstractions from the low level details.  That separation is achieved by carefully managing the dependencies within the system so that all source code dependencies, especially those that cross architectural boundaries, point towards high level abstractions, not low level details. \n\n In every case Dan’s slides end with:  Just write simple code .  This is good advice.  However, if the years have taught us anything it is that simplicity requires disciplines guided by principles.  It is those principles that define simplicity.  It is those disciplines that constrain the programmers to produce code that leans towards simplicity. \n\n The best way to make a complicated mess is to tell everyone to “just be simple” and give them no further guidance. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2021/01/17/Pairing.html", "title": "Pairing Guidelines", "content": "\n       Everybody pairs from time to time. It is a rare programmer who has not sat down with another programmer to look something over or help find a bug. \n\n Deep problems, that require much heavy thinking, do not often lend themselves to pairing. The interaction between the programmers tends to disrupt the necessary concentration. \n\n On the other hand, it is not uncommon for programmers to get caught in a problem that they think is deep, but for which there is a much simpler solution that another programmer could quickly see. So it is wise to start deep problems with a pair, or even a mob, but then break it up when it becomes clear that the problem is irreducible. \n\n On the other side of the spectrum, there is no good reason to pair on trivial matters. Fleshing out a list of error messages, or loading fifty fields into a form are relatively mindless activities that do not require the scrutiny afforded by pairing. \n\n Then there is the vast middle. This is where pairing/mobbing are most valuable. These are problems that are non-trivial, but also not particularly deep. This is 90% of all programming. Pairing on this type of code keeps that code well tested, well structured, and as simple as possible. \n\n Pairing should always be voluntary, never be forced, never be scheduled by a manager, and never tracked. It is an informal process that is entirely under the control of the individual programmers. \n\n Some people can’t, or won’t do it. That’s ok; but it may require that their participation in certain projects be curtailed. \n\n Pairing sessions should be short-ish. 20-40 minutes at a time. ( Tomato  sized) With no more than three or four consecutive sessions of that length. This is not a rule, just an informal guideline. \n\n Not all code that would benefit from pairing, should be written by pairs. A mature team might pair 50% of the time, or even less. During the pairing sessions, a large amount of code will be reviewed; far more than the pair is actively writing; and thus the benefits of pairing will be seen in very large swathes of non-paired code. \n\n Bottom line: Don’t be a jerk. Pair sometimes, don’t pair other times. Pair enough so that you have a good grasp of the overall system, and know enough of what your teammates are doing that you could step into their roles if the need arose. Don’t pair so much that you hate your job, and your teammates. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2021/03/06/ifElseSwitch.html", "title": "if-else-switch", "content": "\n       A few days ago someone tweeted a question asking which of the following PHP snippets was better than the others, or whether there might be an even better approach. \n\n \n\n I tweeted my answer in the following cryptic paragraph. \n\n \n   Place the if/else cases in a factory object that creates a polymorphic object for each variant. Create the factory in ‘main’ and pass it into your app. That will ensure that the if/else chain occurs only once. \n \n\n Others have since asked me for an example.  Twitter is not the best medium for that so… \n\n Firstly, if the sole intent of the programmer is to translate: \n\n 0->'male', \n1->'female' \notherwise -> 'unknown'\n \n\n …then his refactoring #2 would be my preference. \n\n However, I have a hard time believing that the business rules of the system are not using that gender code for making policy decisions.  My fear is that the  if/else/switch  chain that the author was asking about is replicated in many more places within the code.  Some of those  if/else/switch  statements might switch on the integer, and others might switch on the string.  It’s not inconceivable that you’d find a  if/else/switch  that used an integer in one case and a string in the next! \n\n The proliferation of  if/else/switch  statements is a common problem in software systems.  The fact that they are replicated in many places is problematic because when such statements are inevitably changed, it is easy to miss some.  This leads to fragile systems. \n\n But there is a worse problem with  if/else/switch  statements.  It’s the dependency structure. \n\n \n\n Such statements tend to have cases that point outwards towards lower level modules.  This often means that the module containing the  if/else/switch  will have source code dependencies upon those lower level modules. \n\n That’s bad enough.  We don’t like dependencies that run from high level modules to low level modules.  They thwart our desire to create architectures that are made up of independently deployable components. \n\n However, the above diagram shows that it’s worse than that.  Other higher level modules tend to depend on the modules that contains those  if/else/switch  statements.  Those higher level modules, therefore, have transitive dependencies upon the lower level modules.  This turns the  if/else/switch  statements into  dependency magnets  that reach across large swathes of the system source code, binding the system into a tight monolithic architecture without a flexible component structure. \n\n The solution to this problem is to break those outwards dependencies on the lower level modules.  This can be done with simple polymorphism. \n\n \n\n In the diagram above you can see the high level modules using a base class interface that polymorphically deploys to the low level details.  With a little thought you should be able to see that this is behaviorally identical to the  if/else/switch  but with a twist.  The decision about which case to follow must have been made before those high level policy modules invoked the base class interface. \n\n We’ll come back to  when  that decision is made in a moment.  For now, just look at the direction of the dependencies.  There is no longer any transitive source code dependency from the high level modules to the low level modules.  We could easily create a component boundary that separates them.  We could even deploy the high level modules independently from the low level modules.  This makes for a pleasantly flexible architecture. \n\n Another point to consider is that the  if/else/switch  and the polymorphic implementations both use table lookups to do their work.  In the case of an  if/else  the table lookup is procedural.  In the case of a  switch  most compilers build a little lookup table.  In the case of the polymorphic dispatch the vector table is built into the base class interface.  So all three have very similar runtime and memory characteristics.  One is not much faster than another. \n\n So where does the decision get made?  The decision is made when the instance of the base class is created.  Hopefully that creation happens in a nice safe place like  main .  Usually we manage that with a simple factory class. \n\n \n\n In the diagram above you can see the high level module uses the base class to do its work.  Every business rule that would once have depended on an  if/else/switch  statement now has its own particular method to call in the base class.  When a business rule calls that method, it will deploy down to the proper low level module.  The low level module is created by the  Factory .  The high level module invokes the  make(x)  method of the  Factory  passing some kind of token  x  that represents the decision.  The  FactoryImpl  contains the sole  if/else/switch  statement, which creates the appropriate instance and passes it back to the high level module which then invokes it. \n\n Note, again, the direction of the dependencies.  See that red line?  That’s a nice convenient component boundary.  All dependencies cross it pointing towards the higher level modules. \n\n Be careful with that token  x .  Don’t try to make it an  enum  or anything that requires a declaration above the red line.  An integer, or a string is a better choice.  It may not be type safe.  Indeed, it  cannot  be type safe.  But it will allow you to preserve the component structure of your architecture. \n\n You may well be concerned about a different matter.  That base class needs a method for every business rule that once depended upon the  if/else/switch  decision.  As more of those business rules appear, you’ll have to add more methods to the base class.  And since many business rules already depend upon the base class they’ll have to be recompiled/redeployed even though nothing they care about changed. \n\n There are many ways to resolve that problem.  I could keep this blog going for another 2,000 words or so describing them.  To avoid that I suggest you look up  The Interface Segregation Principle  and the  Acyclic Visitor  pattern. \n\n Anyway, isn’t it fascinating how interesting a discussion of a simple  if/else/switch  can be? \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2021/06/25/OnTypes.html", "title": "On Types", "content": "\n       I wrote my first program in 1964.  The name of the program was:  Mr Patternson's Computerized Gate , and it was implemented on a little plastic computer named DIGICOMP-I, which was a cute little three bit finite state machine with 6 AND gates. \n\n The first electronic computer I ever wrote a program for was an ECP-18 in 1966.  This was a 15 bit wide machine with 1024 words of  drum  memory.  The programs I wrote were all in binary machine language and were entered through the front-panel switches. \n\n In the years between 1967 and 1969 my father would drive my friend, Tim Conrad, and I 25 miles to the Digital Equipment Corp sales office, where we would spend our Saturdays entering programs into the PDP-8 that they had on the floor.  They were very gracious to allow us such access and freedom.  The code we wrote was in PAL-D assembler (which was written by Ed Yourdon when he was 21 years old). \n\n My very first job as a programmer was temporary.  A matter of two weeks.  I was 17, and the year was 1969.  My father went to the CEO of a nearby insurance actuarial firm, ASC Tabulating, and in his inimitable fashion, told them that they would be hiring me for a summer job.  He had a way of being  very  convincing. \n\n The program I wrote for ASC was named IDSET.  It was written in Honeywell H200 assembler (the language was called Easycoder and was based on IBM 1401 Autocoder).  The purpose was to read student records from a magnetic tape and insert ID codes into those records, and then write them out onto a new tape.  With some coaching, I was able to get that program to work. \n\n Upon graduating High School, in 1971, I got a job at ASC again; but this time as a third-shift off-line printer operator.  We were printing junk mail, which was a brand new thing back then. \n\n A few months later I was hired as a full-time programmer analyst at ASC, and was assigned to work on huge re-write of a massive accounting and records system for Local 705 Trucker's union in Chicago.  The existing system ran on a great big GE Datanet 30.  ASC wanted to reimplement it on a Varian 620F mini-computer. \n\n The 620F was a lovely little 16 bit computer with 32K of core memory and a 1us cycle time.  The primary IO devices were a teletype, a slow card reader, two magnetic tape drives, and two 2314 20MB disks.  The machine also had 16 (or was it 32) RS232 ports for talking to teletypes that were remotely connected through 300BPS modems. \n\n Although the 620F came with a stand-alone assembler, there was no operating system.  So every bit of that real time union accounting system was built from assembler code, with no frameworks, platforms, or operating systems to help. \n\n In 1973 I took a job at Chicago Laser Systems, programming a PDP-8-like machine, in assembler, to control pulsed lasers, galvonometer driven mirrors, and step-and-repeat tables to trim electronic components to high degrees of tolerance. \n\n In 1975 I took a job at Outboard Marine Corporation, programming a real time aluminum die cast system in IBM System 7 assembler. \n\n In 1977 I took a job at Teradyne Central, programming a PDP-8-like machine, in assembler (again), to control a distributed system for testing and monitoring the quality of all the telephone lines in a telephone company service area.  A year later we started using 8085 micro-computers and wrote all that code in assembler too. \n\n Suffice it to say that I was steeped in assembler, and thought that all high-level languages were a joke. My forays into COBOL, Fortran, and PL/1 did not convince me otherwise.  Real programmers programmed in assembler. \n\n Between 1977 and 1980 I was introduced to Pascal.  I rejected it as a viable language almost immediatly.  I found the type system far too constraining, and didn't trust all the magic behind the scenes. \n\n In 1980 I read a copy of Kernighan and Ritchie, and for the first time I began to see that a high-level language could possibly be an appropriate engineering language.  I spent many years writing in that wonderful language which, by the way, was as untyped as assembler. \n\n Oh, that's not to say that C didn't have declared types.  It's just that the compiler didn't bother to check that you were using those types properly.  This made the language untyped for all intents and purposes. \n\n In 1986, after several nightmare scenarios having to do with the typlessness of C, I was an enthusiastic early adopter of C++.  Unfortunately I could not get my hands on a C++ compiler until 1987.  I became quite an expert in the languge, and engaged in many (many (many)) arguments on comp.lang.c++ and comp.object (in those heady days of USENET, a very early social networking platform). \n\n C++ is a statically typed language.  Many, today, would consider it to be relatively weakly typed; but from my point of view, after a decade and a half of untyped languages, I thought the type enforcement was very strong.  I had overcome the feeling of being handcuffed by a strong type system and became quite adept at building type models. \n\n In 1990 I took a contracting job at Rational, working in C++ on the first release of Rational Rose.  This is where I met Grady Booch, and came up with the plan for my first book. \n\n By 1991 I was a consultant, selling my services to companies, all over the US and Europe, who wanted to learn about object-oriented programming and C++.  It was a lucrative affair for me, and I continued building that business for several years. Eventually I became the editor-in-chief of  The C++ Report  (does anybody remember print magazines?) \n\n In 1999 I realized that C++ was a waning technolgy, and that the action was really happening in Java.  Java was similar enough to C++ for me to make the transition with relative ease.  The type system of Java was a bit weaker than C++, and I refused to use the stronger features (like  final  though I had been an avid consumer of  const  in C++). \n\n By 2003 I had grown tired of Java's static type system and started playing around with Python.  I found the language to be primitive and somewhat haphazard; so after a few excursions with the language I switched to Ruby. \n\n In Ruby I found a home for several years.  The dynamic type system was robust.  The object-oriented facilities were well thought through and very easy to use.  It was an elegant language with very few warts. \n\n Then, in 2010 or so, I bumped into Clojure.  I had just recently read  The Structure and Interpretation of Computer Programs  and so was interested in playing around with a LISP derivative. \n\n It has been 11 years now, and I feel no urge to change languages.  I reckon that Clojure may be my last programming language.  Oh, not that I haven't looked around.  I've had some daliances with Golang, Elixr, and Kotlin, and have looked with trepidation at Haskel.  I've even played with Scala and F#.  I keep looking as new languages arise; but have found nothing that calls me to switch away from Clojure. \n\n Notice the pathway of my career.  I went from untyped languages like assembler and C, to statically typed languages like C++ and Java, to dynamically typed languages like Python and Ruby, and now to Clojure. \n\n The type system in Clojure is as dynamic as Python or Ruby, but there is a library in Clojure called  clojure/spec  that provides all the strong typing anyone would ever need.  However, instead of that typing being controlled by the compiler, it is controlled by  me .  I can enforce simple types, or very complex data relationships.  You might think of it as a kind of pre-condition/post-condition language.  Eifel programmers would feel very much at home with it.  It's an almost perfect way to engage in Design by Contract. \n\n So what do I conclude from this?  Not much other than that static typing is not for me.  I prefer the flexibility of dynamic typing, and the ability to enforce types if, and when, I need such enforcement. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2021/10/28/functional-duplication.html", "title": "Functional Duplications", "content": "\n       I broke out my old  Space War  game a few days ago and decided to make a few changes to speed the game up and make it more fun to play.  In so doing I discovered a very interesting bug. \n\n One of the changes I made was to populate the initial space with a few random bases scattered here and there.  This would allow the player some extra resources with which to battle the Klingons while building up a network of more bases. \n\n While I was playing the modified game, it crashed.  Hard. \n\n Now I wrote this with TDD, and I was very disciplined about the cleanliness of the code, and the test coverage.  So this was unexpected.  So I dug up all my old debugging skills from the pit in which I had buried them, and started to work out what was going on. \n\n It wasn't long before I realized that crash was occuring because a transport was being launched between two bases, but the angle of the velocity vector of the transport was  :bad-angle .  This can only happen if the two bases exist at the exact same location. \n\n Bases don't move around in this game, so there's no chance that two bases will accidentally slide on top of each other.  There is a very (very) minor chance that the random number generator will put two bases on top of each other at the start of the game; but the odds are so miniscule that didn't worry about it. In any case, this crash happened well into the game I was playing, so initial values could not have been the cause. \n\n Fortunately it's pretty easy to hunt and peck around in the game, so I was quickly able to discover that the two bases in question were duplicates of each other.  Something in my code was duplicating bases! \n\n Well now that should't be too hard to find.  So I wrote a litte function that would examine the world and halt with a message if the world contained two bases at the same location.  I called this function in the main update loop, and sure enough after 20 minutes of play the program halted with my message. \n\n Unfortunately being able to detect  that  the duplication occurred did not tell me  where  it occurred.  So I laced the code with calls to my  check-for-duplicate-base  function. \n\n It took me a few tries because the problem was not in any of the obvious places.  So over a few hours I added more and more calls to  check-for-duplicate-base . \n\n Eventually I found the culprit in a low frequency function named  klingons-steal-antimatter . \n\n This function is called once per second.  It checks to see if any klingons are within  docking-distance  of a base, and if so it steals antimatter from that base. \n\n This explained why the crash took so long to create.  Most of the time it takes 20 minutes or so for a Klingon to move close enough to a base to start stealing. \n\n Anyway, I looked at the code and didn't see any obvious duplication.  So I wrote a unit test to check whether that function duplicated bases.  My test positioned a klingon near a base, called the  klingons-steal-antimatter  function, and then checked the number of bases in the world.  The result: No duplication. \n\n Now, before I continue, let me describe the process I used in the  klingons-steal-antimatter  function. \n\n The function created a list of  thefts .  A theft is a  [thief victim]  pair.  It used those pairs to create lists of all the thieves and victims, and separate lists of all the innocent klingons and all the unvictimized bases. \n\n Why?  Because this is a purely functional program.  In a purely functional program you cannot update the status of an object.  Instead you transformm old objects into new objects.  So when stealing antimatter from a base you must create a new base with less antimatter, and you must create a new klingon with more antimatter.  When you are done processing all the thefts you are left with a list of all the updated klingons, and a list of all the updated bases. \n\n The  world  contains a list of all the klingons and a list of all the bases.  In order to update the  world  after processing the thefts you have to concatenate the updated bases with the unvictimized bases, and you have to concatenate the updated klingons with the innocent klingons. \n\n Got it?  Understand?  Good. \n\n As I pondered the code I realized that a base could be robbed by more than one klingon.  Klingons tend to slowly migrate towards bases and then steal from them.  Two or three or more could eventually manage to slide over to a base, like a pack of coyotes squabbling over a carcass. \n\n Now I already had a unit test that checked for this condition.  It created two klingons near one base and made sure that each klingon was able to steal from that base.  What that test did not do, however, was count the number of bases in the world when it was done. \n\n So I added a check.   base-count => 1 .  Whoops, it came back with  2 . \n\n Now maybe you've already figured out why this happened.  But let me walk you through it.  My function identified two thefts:  [[k1 b] [k2 b]]   It returned with the results of each theft. Let's say that  k1  stole  a1  antimatter from  b , and  k2  stole  a2  from  b .  What the function returned was  [[k1+a1 b-a1] [k1+a2 b-a2]] .  Note that the second theft in the list was  not   [k2+a2 b-a1-a2] . \n\n You've probably guessed the rest.  When I reassembled the world, I added all the bases that had been victims; and -- of course -- I added  b-a1   and   b-a2 . \n\n Fortunately I had lots of unit tests to fall back on.  Changing the algorithm was actually quite challenging, and required me to put all the klingons and bases into hashmaps keyed by their positions.  I won't bore you with the details. \n\n So I added unit tests to check for the duplications, saw them fail, and then gradually made them pass.  The unit tests allowed me to be sure that I was not breaking something else along the way. \n\n Now you might think this is just an esoteric little problem that you'll never encounter.  However, if you are writing functional programs, you  will  face this issue, and you'll likely face it a lot.  Dealing with immutable lists of objects means that when you update such a list you must recreate it.  If you are only updating  m  out of  n  elements of the list, you have to partition the original list into the  m  elements you are changing and the  n-m  elements you are not changing; and then you have to concatenate the  m  changed elements with the  n-m  unchanged elements in order to create the updated list. \n\n Anyway, I thought you might find that interesting. \n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2021/06/29/MoreOnTypes.html", "title": "More On Types", "content": "\n       Recently I wrote a cute little program for doing  Turtle Graphics .  For those of you who don't know, turtle graphics were originally added to the LOGO language by Seymour Papert in the late 1960s.  He built a robot that he called a \"turtle\" that could hold a pen.  The robot had wheels and could move forwards and backwards, and could rotate left and right.  It could also raise and lower the pen.  When placed on a sheet of paper, the turtle could be commanded to draw interesting designs. \n\n Papert's goal was to teach children about programming.  As the years went by the robot got replaced with screens, and the turtle became an icon that could draw lines.  Children from the 70s until now have been enthralled by the simple commands for directing the turtle, and the elegant drawings they can make. \n\n For example, this is how you might draw a square: \n\n forward 100\nright 90\nforward 100\nright 90\nforward 100\nright 90\nforward 100\nright 90.\n \n\n Recently I had a need to explore some interesting geometrical designs.  Turtle graphics would be perfect for my purposes.  So I wrote a turtle graphics processor in Clojure.   [code] \n\n I used the  quil  framework which is based on the  Processing  framework in Java. This framework makes it very easy to create simple GUIs in Clojure. \n\n Now consider the problem of the Turtle.  What is the type model for this object?  What fields does it have, and what constraints must be placed on those fields? \n\n Here was my solution to that problem, written in  clojure/spec .  As usual, in Clojure, you start at the bottom and read towards the top. \n\n (s/def ::position (s/tuple number? number?))\n(s/def ::heading (s/and number? #(<= 0 % 360)))\n(s/def ::velocity number?)\n(s/def ::distance number?)\n(s/def ::omega number?)\n(s/def ::angle number?)\n(s/def ::weight (s/and pos? number?))\n(s/def ::state #{:idle :busy})\n(s/def ::pen #{:up :down})\n(s/def ::pen-start (s/or :nil nil?\n                         :pos (s/tuple number? number?)))\n(s/def ::line-start (s/tuple number? number?))\n(s/def ::line-end (s/tuple number? number?))\n(s/def ::line (s/keys :req-un [::line-start ::line-end]))\n(s/def ::lines (s/coll-of ::line))\n(s/def ::visible boolean?)\n(s/def ::speed (s/and int? pos?))\n(s/def ::turtle (s/keys :req-un [::position\n                                 ::heading\n                                 ::velocity\n                                 ::distance\n                                 ::omega\n                                 ::angle\n                                 ::pen\n                                 ::weight\n                                 ::speed\n                                 ::lines\n                                 ::visible\n                                 ::state]\n                        :opt-un [::pen-start]))\n \n\n Now don't freak out at all the parentheses and colons.  In fact, for the moment, just ignore them. \n\n So, what is a turtle?  A turtle is a map whose required keys are as follows: \n\n \n   \n     position  is the cartesian coordinate of the pen of the turtle.  If you look up towards the top you will see that a position is defined as a tuple containing two numbers. \n   \n   \n     heading  is the direction that the turtle is pointing.  It will move in that direction if told to move forward.  Again, looking up towards the top you can see that a heading must be a number between 0 and 360. \n   \n   \n     velocity  is a number that represents the speed at which the turtle will move across the screen.  This is used for animation, so that the user can actually watch the turtle travel across the screen. \n   \n   \n     distance  is a number that represents the remaining distance that the turtle must traverse before the current command (either a  forward  or  backwards  command) is complete. \n   \n   \n     omega  is a number that represents the angular velocity of the turtle.  Again, this is for animation purposes, so that the user can watch the turtle rotate when given a  right  or  left  command. \n   \n   \n     angle  is a number that represents the number of degrees remaining to complete the current rotation command. \n   \n   \n     pen  is the state of the pen.  Looking up you can see that the state of the pen can be either  up  or  down . \n   \n   \n     weight  is a positive number that represents the thickness of the line drawn by the pen. \n   \n   \n     speed  is a positive integer that acts as a multiplier for both the  velocity  and  omega  parameters.  This allows the user to speed up or slow down the animation. \n   \n   \n     lines  is a list of all the lines drawn by the turtle so far.  Looking up you can see that it is a collection of lines, and that lines are maps whose required keys are  line-start  and  line-end , both of which are tuples of two numbers. (Yes, I suppose I should have created a  point  type.) \n   \n   \n     visible  is a boolean that determines whether the turtle itself should be visible while it is being animated.  If this is false, then all the user sees is the animated result of the turtle's movements. \n   \n   \n     state  is either  busy  or  idle .  This is used by the command processor.  When the turtle goes from  busy  to  idle  the next command is pulled from the command queue and executed. \n   \n \n\n It should be clear that this is a type model.  Most statically typed languages would not be able to capture all the constraints within this type model; though there are perhaps some that could.  However, this is not a static type model.  Clojure is not a statically typed language.   clojure/spec  is a dynamic type definition language. \n\n What does that mean?  Probably the best way to explain that is to show you where that type model gets invoked.  Here's a simple example. \n\n (defn make []\n  {:post [(s/assert ::turtle %)]}\n  {:position [0.0 0.0]\n   :heading 0.0\n   :velocity 0.0\n   :distance 0.0\n   :omega 0.0\n   :angle 0.0\n   :pen :up\n   :weight 1\n   :speed 5\n   :visible true\n   :lines []\n   :state :idle})\n \n\n This is the default constructor of the turtle.  Notice that it just loads up all the required fields into a map. Notice also that there is a  post condition  that asserts that the result conforms the the  turtle  type. \n\n This is nice.  If I forget to initialize a field, or if I initialize a field to a value that conflicts with the type, I get an error. \n\n Here's another, more complex example.  Don't freak out, you don't have to understand this in detail. \n\n (defn update-turtle [turtle]\n  {:post [(s/assert ::turtle %)]}\n  (if (= :idle (:state turtle))\n    turtle\n    (let [{:keys [distance\n                  state\n                  angle\n                  lines\n                  position\n                  pen\n                  pen-start] :as turtle}\n          (-> turtle\n              (update-position)\n              (update-heading))\n          done? (and (zero? distance)\n                     (zero? angle))\n          state (if done? :idle state)\n          lines (if (and done? (= pen :down))\n                  (conj lines (make-line turtle))\n                  lines)\n          pen-start (if (and done? (= pen :down))\n                      position\n                      pen-start)]\n      (assoc turtle :state state :lines lines :pen-start pen-start)))\n  )\n \n\n This is the function that updates the turtle for each screen refresh.  Again, notice the  post condition .  If anything is calculated incorrectly by the  update-turtle  function, I'll get an exception right away. \n\n Now some of you might be worried that by checking types at runtime I could end up with runtime errors in production.  You might therefore assert that static typing is better because the compiler checks the types long before the program ever executes. \n\n However, I do not intend to have runtime errors in production, because I have a suite of tests that exercise all the behaviors of the system.  Here's just one of those tests: \n\n (describe \"Turtle Update\"\n  (with turtle (-> (t/make) (t/position [1.0 1.0]) (t/heading 1.0)))\n  (context \"position update\"\n    (it \"holds position when there's no velocity\"\n      (let [turtle (-> @turtle (t/velocity 0.0) (t/state :idle))\n            new-turtle (t/update-turtle turtle)]\n        (should= turtle new-turtle)))\n \n\n Again, you don't have to understand this in any detail.  Just notice that the  make  and  update-turtle  functions are being invoked.  Since those functions have  post conditions  that will check the types, and since my suite of tests is exhaustive, I am quite certain that there will be no runtime errors in production and that my dynamic type checking is as robust as any static type system. \n\n The dynamic nature of the type checking allows me to assert type constraints that are very difficult, if not impossible, to assert at compile time.  I can, for example, assert complex relationships between the values of the fields. \n\n To expand on that example, imagine the type model of an accounting balance sheet.  The sum of the assets, liabilities and equities on the balance sheet must be zero.  This is easy to assert in  clojure/spec  but is difficult, if not impossible, to assert in most statically typed languages. \n\n Moreover,  I  get to control when types are asserted.  It is up to me to decide if and when a certain type should be checked.  This gives me a lot of power and flexibility.  It allows me to violate the type rules in the midst of computations, so long as the end result ends up conforming to the types. \n\n One last point.  In the late 90s and the 2000s, there was a lengthy and animated (and sometimes acrimonious) debate over TDD vs DBC (Design by Contract).  What  clojure/spec  has taught me is that the two play very well together, and both should be in every programmer's toolkit. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2021/09/25/roots.html", "title": "Roots", "content": "\n       When I was 15 or so, my father would drive me, and my best friend, Tim Conrad, to the Digital Equipment Corporation (DEC) sales office each Saturday.  This was a 30 min drive.  My father would drop us off in the morning and pick us up in the late afternoon.  He spend two hours in his car each Saturday hauling us around. \n\n \n   Thanks Dad! \n \n\n Tim and I would spend our day \"playing\" with the floor model of the PDP-8 they had at the office.  The office staff were very accommodating and accepting of our presence, and they helped us out if we needed any fresh rolls of teleprinter paper, or paper tape. \n\n Several years later, at the age of 20, I found myself working at Teradyne Applied Systems in Chicago.  The computer we used there was called an M365; but it was really just an upgraded PDP-8.  We used it to control lasers in order to trim electronic components to very precise values. \n\n Forty four years later, in May of 2015, I started playing with a cute little Lua environment on my iPad called  Codea .  I wrote several fun little programs, like lunar lander, etc.  But then I thought: \"Wouldn't it be fun to write a PDP-8 Emulator?\" \n\n A few days/weeks later I had a nice little PDP-8 emulator running on my iPad.  I found some archived binary images of ancient paper tapes and managed to load them into my emulator.  This allowed me to run the suite of development tools that I had used back in those early days. \n\n Then Apple decided it didn't want people writing code on the Ipad that was not distributed through the App store, so they blocked the means by which Codea users could share source code.   Indeed, I couldn't even move Lua source code to my new iPads.  So the emulator was lost. \n\n Fortunately I had put the last working version up on GitHub. \n\n At some point, Apple reopened the channel, perhaps due to a court case.  I discovered this a few weeks back, and loaded that old source code back into my iPad.  It worked like a champ. \n\n I made a few changes to deal with the bigger screen, and the faster processor, and then announced it on twitter.  I think many people have played with it since. \n\n \n   You can get the emulator  here .   You'll find a lot of good tutorial information, and several demonstration videos in that repository. \n \n\n Euler 4 \n As you may know I have a  youtube series  on the  cleancoders.com channel , in which I walk through the problems in the  Euler project  solving them in Clojure and then taking them to the max, Myth-buster style. \n\n Euler 4 is a simple little problem of finding the factors of palindromic numbers.  I quickly solved it in Clojure, and then I thought it would be fun to write a PDP-8 program to solve it. \n\n \n   Down the rathole I went. \n \n\n I used TDD to get the individual subroutines working.  Among the subroutines I wrote were single and double precision multiply and divide routines. (We didn't use the word \"functions\" back then.)  The poor PDP-8 could only add.  It couldn't even subtract.  Subtraction was accomplished by using twos-complement addition (let the reader understand;-) \n\n Was this fun?  Yes, at first it was kinda cool to reminisce, and to feel all the old knowledge and instincts come flooding back into my brain.  But once the \"novelty\" wore off, it stopped being fun, and just turned into work -- grinding, tedious, work. \n\n It took me several hours, over a period of a few days, but I got the blasted thing working.  It's not an experience I'd like to repeat.  Working on a PDP-8 is a  PITA , even when with all the cheats I supply in my Emulator. \n\n Here, for your edification, is my solution to Euler 4 on a PDP-8.  This code solves the problem; but I'm quite sure it has some really nasty bugs anyway.  I am in no way proud of this code.  I'm just not willing to improve it.  If you study it you'll see just how awful it is.  I mean, among other sins I used truly naive algorithms for multiplying and dividing numbers. \n\n Anyway, be careful.  The lure of the rathole is very compelling. \n\n /EULER 4 SOLUTION\n\n        PZERO=20\n\n        *200\n    \nMAIN,   CLA\n        TLS\n        TAD SEED\n        ISZ SEED\n        CIA\n        JMS CALL\n        MKPAL\n    \n        JMS CALL\n        PRDOT\n\n        CLA\n        TAD MAXFAC\n        DCA FAC\n\nFACLUP, \n        CLA\n        TAD FAC\n        TAD K100\n        SMA CLA\n        JMP MAIN\n    \n        JMS CALL\n        DLOAD\n        DPAL\n        TAD FAC\n        CIA\n        JMS CALL\n        ISFAC\n        SKP\n        JMP GOTFAC\n    \n        CLA\n        TAD I OFP /OTHER FAC > 999 TRY NEXT PAL.\n        TAD MAXFAC\n        SMA CLA\n        JMP MAIN\n        ISZ FAC\n        JMP FACLUP\n    \nGOTFAC,\n        CLA\n        TAD I OFP\n        TAD MAXFAC\n        SMA CLA\n        JMP MAIN\n\n        JMS CRLF\n        CLA\n        TAD FAC\n        CIA\n        JMS CALL\n        PRAC\n        JMS CALL\n        PRDOT\n        CLA\n        TAD I OFP\n        JMS CALL\n        PRAC\n        JMS CRLF\n        JMS CALL\n        DLOAD\n        DPAL\n        JMS CALL\n        PRDACC\n        JMS CRLF\n        HLT     \n\n        DECIMAL\nSEED,   -999\nMAXFAC, -999\n        OCTAL\nFAC,    0\nOFP,    OTHFAC+1\n\n        *400\n\n/MAKE A PALINDROMIC  NUMBER FROM A SEED.\n/ABC->ABCCBA IN DECIMAL IN DACC AND STORED IN DPAL\n\nMKPAL,  0\n        DCA DPAL+1\n        DCA DPAL\n        TAD DPAL+1\n        JMS CALL\n        DIV\n        K10\n        DCA WRK\n        TAD REM\n        DCA DIGS\n        TAD WRK\n        JMS CALL\n        DIV\n        K10\n        DCA DIGS+2\n        TAD REM\n        DCA DIGS+1\n        JMS CALL\n        DLOAD\n        DPAL\n        TAD K1000\n        JMS CALL\n        DMUL\n        JMS CALL\n        DSTORE\n        DPAL\n        CLA\n        TAD DIGS\n        JMS CALL\n        MUL\n        K10\n        TAD DIGS+1\n        JMS CALL\n        MUL\n        K10\n        TAD DIGS+2\n        DCA DWRK+1\n        DCA DWRK\n        JMS CALL\n        DLOAD\n        DPAL\n        JMS CALL\n        DADD\n        DWRK\n    \n        JMS CALL\n        DSTORE\n        DPAL\n        JMP I MKPAL\n    \n/SKIP IF AC IS A FACTOR OF DACC. AC=0\nISFAC,  0\n        DCA DFAC+1\n        DCA DFAC\n    \n        JMS CALL\n        DDIV\n        DFAC\n    \n        JMS CALL\n        DSTORE\n        OTHFAC\n    \n        JMS CALL\n        DLOAD\n        DREM\n        JMS CALL\n        DSKEQ\n        D0\n        SKP\n        ISZ ISFAC\n        JMP I ISFAC\n\nDFAC,   0\n        0\nOTHFAC, 0\n        0\n    \n        OCTAL\nDPAL,   0\n        0\n    \n\nDIGS,   0\n        0\n        0\n    \nWRK,    0\n\nDWRK,   0\n        0\n    \n// PZERO FOR EULER\n        *PZERO\n        DECIMAL\nK100,   100\nK1000,  1000\nK10,    10\n        OCTAL\n\nPZERO = .\n\n\n~   \n\n*1000\n/DMATHLIB \n/DLOAD - LOAD ARG INTO DACC, AC=0\nDLOAD,  0\n        CLA\n        TAD I DLOAD\n        ISZ DLOAD\n        DCA DARGP\n        TAD I DARGP\n        DCA DACC\n        ISZ DARGP\n        TAD I DARGP\n        DCA DACC+1\n        JMP I DLOAD\n\n/DOUBLE PRECISION STORE ACCUMULATOR POINTED TO BY ARG\nDSTORE, 0\n        CLA\n        TAD I DSTORE\n        DCA DARGP\n        ISZ DSTORE\n    \n        TAD DACC\n        DCA I DARGP\n        ISZ DARGP\n        TAD DACC+1\n        DCA I DARGP\n        JMP I DSTORE\n\n/SKIP IF DOUBLE PRECISION ARGUMENT IS EQUAL TO DACC. AC=0\nDSKEQ,  0\n        CLA\n        TAD I DSKEQ\n        DCA DARGP\n        ISZ DSKEQ\n    \n        TAD DACC\n        CIA\n        TAD I DARGP\n        SZA CLA\n        JMP I DSKEQ\n    \n        ISZ DARGP\n        TAD DACC+1\n        CIA\n        TAD I DARGP\n        SNA CLA \n        ISZ DSKEQ\n        JMP I DSKEQ\n    \n/DOUBLE PRECISION ADD ARGUMENT TO DACC. AC=0\n\nDADD,   0\n        CLA CLL\n        TAD I DADD\n        ISZ DADD\n        DCA DARGP\n        TAD DARGP\n        IAC\n        DCA DARGP2\n    \n        TAD I DARGP2\n        TAD DACC+1\n        DCA DACC+1\n        RAL\n        TAD I DARGP\n        TAD DACC\n        DCA DACC\n    \n        JMP I DADD\n\n/COMPLEMENT AND INCREMENT DACC      \nDCIA,   0\n        CLA CLL\n        TAD DACC+1\n        CMA IAC\n        DCA DACC+1\n        TAD DACC\n        CMA\n        SZL\n        IAC\n        DCA DACC\n        JMP I DCIA\n    \n/MULTIPY DACC BY AC\nDMUL,   0\n        CIA\n        DCA PLIERD\n        JMS DSTORE\n        DCAND\n        JMS DLOAD\n        D0\n        TAD PLIERD\n        SNA CLA\n        JMP I DMUL\nDMUL1,  JMS DADD\n        DCAND\n        ISZ PLIERD\n        JMP DMUL1\n        JMP I DMUL\n    \nPLIERD, 0\nDCAND,  0\n        0\n    \n/DIV DACC BY DARG (AWFUL) R IN DREM AC=0\nDDIV,   0\n        CLA\n        TAD I DDIV\n        ISZ DDIV\n        DCA .+4\n        JMS DSTORE\n        DVDEND\n        JMS DLOAD\n        0\n        JMS DCIA /NEGATE DIVISOR\n        JMS DSTORE\n        DVSOR\n        JMS DLOAD\n        DVDEND\n    \n        DCA DQUOT \n        DCA DQUOT+1\n        JMP DDIV1\n    \nDDIV2,  ISZ DQUOT+1 // INCREMENT DQUOT\n        SKP\n        ISZ DQUOT\n    \nDDIV1,  JMS DSTORE\n        DREM\n        JMS DADD\n        DVSOR\n        TAD DACC\n        SMA CLA\n        JMP DDIV2\n    \n        JMS DLOAD\n        DQUOT\n        JMP I DDIV\n    \n    \nDARGP,  0\nDARGP2, 0       \n    \nDVSOR,  0\n        0\nDVDEND, 0\n        0\nDQUOT,  0\n        0\n        \n/PAGE ZERO DATA FOR DMATHLIB\n\n*PZERO\nDACC,   0\n        0\nD0,     0\n        0\nDREM,   0\n        0\nPZERO=.\n~\n\n/SINGLE PRECISION MATH LIBRARY\n        *2000\n/DIVIDE AC BY ARGP (SLOW AND NAIVE)\n/Q IN AC, R IN REM\nDIV,    0\n        DCA REM\n        TAD I DIV\n        ISZ DIV\n        DCA ARGP\n        TAD I ARGP\n        CIA\n        DCA MDVSOR\n        DCA QUOTNT\n        TAD REM\nDIVLUP, TAD MDVSOR\n        SPA\n        JMP DIVDUN\n        ISZ QUOTNT\n        JMP DIVLUP\nDIVDUN, CIA\n        TAD MDVSOR\n        CIA\n        DCA REM\n        TAD QUOTNT\n        JMP I DIV\nMDVSOR, 0\nQUOTNT, 0\nARGP,   0\n\n/MULTIPLY AC BY ARGP (SLOW AND NAIVE)\n/GIVING SINGLE PRECISION PRODUCT IN AC\n\nMUL,    0\n        DCA CAND\n        TAD I MUL\n        ISZ MUL\n        DCA ARGP\n        TAD I ARGP\n        SNA\n        JMP I MUL\n        CIA\n        DCA PLIER\n        TAD CAND\n        ISZ PLIER\n        JMP .-2\n        JMP I MUL\nCAND,   0\nPLIER,  0\n\n/PZERO FOR SMATHLIB\n        *PZERO\nREM,    0\nPZERO=.\n~\n\n/TTY UTILS\n        *3000\n/PRINT ONE CHAR IN AC.  IF CR THEN PRINT LF.  \nPRTCHAR,0\n        TSF\n        JMP .-1\n        TLS\n        DCA CH\n        TAD CH\n        TAD MCR\n        SZA\n        JMP RETCHR\n        TAD KLF\n        TSF\n        JMP .-1\n        TLS\nRETCHR, CLA\n        TAD CH\n        JMP I PRTCHAR\nCH,     0\nMCR,    -215\n\n/PRINT AC AS ONE DECIMAL DIGIT AC=0\nPRDIG,  0\n        TAD K260\n        TSF\n        JMP .-1\n        TLS\n        CLA\n        JMP I PRDIG\n        \nK260,   260\n\n/PRINT THE DACC IN DECIMAL\n\nPRDACC, 0\n        JMS CALL\n        DSTORE\n        DACSV\n        JMS CALL\n        DDIV\n        D1E6\n        TAD DACC+1\n        JMS PRDIG\n        JMS CALL\n        DLOAD\n        DREM\n        JMS CALL\n        DDIV\n        D1E5\n        TAD DACC+1\n        JMS PRDIG\n        JMS CALL\n        DLOAD\n        DREM\n        JMS CALL\n        DDIV\n        D1E4\n        TAD DACC+1\n        JMS PRDIG\n        JMS CALL\n        DLOAD\n        DREM\n        JMS CALL\n        DDIV\n        D1E3\n        TAD DACC+1\n        JMS PRDIG\n        JMS CALL\n        DLOAD\n        DREM\n        JMS CALL\n        DDIV\n        D1E2\n        TAD DACC+1\n        JMS PRDIG\n        JMS CALL\n        DLOAD\n        DREM\n        JMS CALL\n        DDIV\n        D1E1\n        TAD DACC+1\n        JMS PRDIG\n        JMS CALL\n        DLOAD\n        DREM\n        TAD DACC+1\n        JMS PRDIG\n        JMS CALL\n        DLOAD\n        DACSV\n        JMP I PRDACC\n    \n        \nDACSV,  0\n        0\nD1E6,   0364\n        1100\nD1E5,   0030\n        3240\nD1E4,   2\n        3420\nD1E3,   0\n        1750\nD1E2,   0\n        144\nD1E1,   0\n        12\n    \n/PRINT AC, AC=AC\nPRAC,   0\n        DCA SAC\n        TAD SAC\n        JMS CALL\n        DIV\n        D1E3+1\n        JMS PRDIG\n        TAD REM\n        JMS CALL\n        DIV\n        D1E2+1\n        JMS PRDIG\n        TAD REM\n        JMS CALL\n        DIV\n        D1E1+1\n        JMS PRDIG\n        TAD REM\n        JMS PRDIG\n        TAD SAC\n        JMP I PRAC\nSAC,    0\n\n/PRINT DOT AC=AC\nPRDOT,  0\n        DCA SAC\n        TAD KDOT\n        JMS TYPE\n        TAD SAC\n        JMP I PRDOT\n    \n/----------------------\n/PZERO TEST LIBRARY\n        *PZERO     \nTYPE,   0 / AC=0\n        TSF\n        JMP .-1\n        TLS\n        CLA\n        JMP I TYPE\n    \nCRLF,   0 / AC=0\n        CLA\n        TAD KCR\n        JMS TYPE\n        TAD KLF\n        JMS TYPE\n        JMP I CRLF\n\n/SOUND BELL AND HALT WITH ADDR OF BAD TEST IN AC    \nERROR,  0\n        CLA\n        TAD KBELL\n        JMS TYPE\n        CLA CMA\n        TAD ERROR\n        HLT\n\n/PRINT DOT, COUNT ERROR     \nPASS,   0\n        CLA\n        TAD KDOT\n        JMS TYPE\n        ISZ TESTS\n        JMP I PASS\n\n/TESTS COMPLETE, PRINT ZERO AND HALT WITH # OF TESTS IN AC. \nTSTDUN,\n        JMS CRLF\n        TAD KZERO\n        JMS TYPE\n        JMS CRLF\n        TAD TESTS\n        HLT\n    \n/CALL SUBROUTINE\nCALL,   0\n        DCA AC\n        TAD I CALL\n        DCA CALLEE\n        TAD CALL\n        IAC\n        DCA I CALLEE\n        ISZ CALLEE\n        TAD AC\n        JMP I CALLEE\nAC,     0\nCALLEE, 0\n\nTESTS, 0           \nKZERO,  260\nKBELL,  207\nKCR,    215\nKLF,    212\nKDOT,   256\n\nPZERO=.\n~\n$\n \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2021/11/28/Spacewar.html", "title": "Space War", "content": "\n       For the last month I've been spending a lot of time working on  Space War .  I know, I know, I should have been working on  Clean Code Episode 67: Legacy Code , and  Euler 5 , and  Countest and Curmugeon 3 .  I should have been working on a blog, or a new book, or...  But I couldn't let go of Space War.  It kept calling me. \n\n The first time I wrote Space War was in 1978.  I wrote it in  Alcom , which was a simple derivative of  Focal , which was an analog of  Basic  for the  PDP-8 .  The computer was an  M365  which was an augmented version of a PDP-8 and was proprietery to Teradyne, my employer at the time. \n\n The UI was screen based, using character graphics, similar to  curses .  Screen updates took on the order of a second.  All input was through the keyboard. \n\n We used to play it on one machine while waiting for a compile on another. \n\n Forty years later, in September of 2018, I started working on  this  version of Space War.  It's an animated GUI driven system with a frame rate of 30fps.  It is written entirely in Clojure and uses the  Quil  shim for the  Processing  GUI framework. \n\n My justification for writing it was so that I could use it as the case study for my cleancoders.com videos on  Functional Programming .  Once that series of videos was complete, I set Space War aside and started working on other things. \n\n Then, a month ago, the program called to me.  I don't know why.  Perhaps it was because I'd left it in a partially completed state.  Perhaps it was because I had just finished  Clean Craftsmanship  and I needed a way to decompress.  Or, perhaps it was just because I felt like it.  Whatever the reason, I loaded up the project and started goofing around with it. \n\n Now I'm sure you've had that feeling of trepidation when you pick up a code base that you haven't seen in three years.  I certainly felt it.  I mean, what was I going to find in there?  Would I be able to get my bearings and understand the code?  Or would I flail around aimlessly for weeks? \n\n I needn't have worried.  The code base was nicely organized.  There was a very nice suite of tests that covered the vast majority of the game logic.  The GUI code, though not tested, was simple enough to understand at a glance. \n\n But, perhaps most importantly, this code was written to be 100% functional.  No variables were mutated, anywhere in the code.  This meant that every function did exactly what it said it did; and left no detritus around to confound other functions.  No function could be impacted by the state of the system because the system did not have \"a state\". \n\n Now maybe you are rolling your eyes at that last paragraph.  Several years ago I might have rolled my eyes too.  But the relief I experienced coming back into this code base after three years of not touching it, and knowing it was functional, was palpable. \n\n Another thing that gave me a significant amount of help was that all the critical data structures in the system were described and tested using  clojure/spec .  This was profoundly helpful because it gave me the kind of declarative help that is usually reserved for statically typed languages. \n\n For example, This is a Klingon: \n\n (s/def ::x number?)\n(s/def ::y number?)\n(s/def ::shields number?)\n(s/def ::antimatter number?)\n\n(s/def ::kinetics number?)\n(s/def ::torpedos number?)\n(s/def ::weapon-charge number?)\n(s/def ::velocity (s/tuple number? number?))\n(s/def ::thrust (s/tuple number? number?))\n(s/def ::battle-state-age number?)\n(s/def ::battle-state #{:no-battle :flank-right :flank-left :retreating :advancing})\n(s/def ::cruise-state #{:patrol :guard :refuel :mission})\n(s/def ::mission #{:blockade :seek-and-destroy :escape-corbomite})\n\n(s/def ::klingon (s/keys :req-un [::x ::y ::shields ::antimatter\n                                  ::kinetics ::torpedos ::weapon-charge\n                                  ::velocity ::thrust\n                                  ::battle-state-age ::battle-state\n                                  ::cruise-state\n                                  ::mission]\n                         :opt-un [::hit/hit]))\n \n\n These kinds of  clojure/spec  descriptions gave me the documentation I needed to reaquaint myself with the critical data structures of the system.  They also gave me the ability to check that any functions I wrote kept those data structures conformant to the spec. \n\n All of this means that I was able to make progress in this code base quickly, and with a high degree of confidence.  I never had that feeling of wading through bogs of legacy code. \n\n Anyway, I'm done now, for the time being.  I've given the player a mission to complete, and made it challenging, but possible, to complete that mission.  A game requires 2-3 hours of intense play, is tactially and strategically challenging, and is often punctuated by moments of sheer panic. \n\n I hope you enjoy downloading it, firing up Clojure, and playing it.  Consider it my Christmas present to you. \n\n One last thing.  Three years ago  Mike Fikes  saw my Space War program and converted it from Clojure to  ClojureScript .  The change was so miniscule that the two are now a single code base with a tiny smattering of conditional compilation for the very few differences.  So if you want to play the game on-line you can just click on  http://spacewar.fikesfarm.com/spacewar.html .  Mike has kindly kept this version up to date so -- have at it! \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2023/01/19/functional-classes-clojure.html", "title": "Functional Classes in Clojure", "content": "\n       My previous  blog  seemed only to continue the confusion regarding classes in Functional Programming.  Indeed, many people got quite irate.  So perhaps a bit of code will help. \n\n Trigger Warning : \n\n \n   Object Oriented Terminology. \n   Dynamically Typed Language. \n   Mixed Metaphors. \n   Distracting Animations. \n \n\n \n   To all the adherents of the  Statically Typed  Functional Programming religion:  I know that you believe that  Static Typing  is an essential aspect of Functional Programming and that no mere dynamically typed language could ever begin to approach the heights and glory of  The One True and Holy TYPED Functional Apotheotic Paradigm .  But we lowly programmers quivering down here at the base of  Orthanc  can only hope to meekly subsist on the dregs that fall from on high. \n \n\n \n\n (R.I.P. Kirstie Alley \n\n OK, so, once again… \n\n \n   A class is an intentionally named abstraction that consists of a set of narrowly cohesive functions that operate over an internally defined data structure. \n \n\n We do not need the  class  keyword.  Nor do we need polymorphic dispatch.  Nor do we need inheritance.  A class is just a description, whether in full or in part, of an object. \n\n For example – it’s time we talked about clouds (which I have looked at from both sides now; and do, in fact, understand pretty well). \n\n So… Here come your father’s parentheses! \n\n \n\n (ns spacewar.game-logic.clouds\n  (:require [clojure.spec.alpha :as s]\n            [spacewar.geometry :as geo]\n            [spacewar.game-logic.config :as glc]))\n\n(s/def ::x number?)\n(s/def ::y number?)\n(s/def ::concentration number?)\n\n(s/def ::cloud (s/keys :req-un [::x ::y ::concentration]))\n(s/def ::clouds (s/coll-of ::cloud))\n\n(defn valid-cloud? [cloud]\n  (let [valid (s/valid? ::cloud cloud)]\n    (when (not valid)\n      (println (s/explain-str ::cloud cloud)))\n    valid))\n\n(defn make-cloud\n  ([]\n   (make-cloud 0 0 0))\n  ([x y concentration]\n  {:x x\n   :y y\n   :concentration concentration}))\n\n(defn harvest-dilithium [ms ship cloud]\n  (let [ship-pos [(:x ship) (:y ship)]\n        cloud-pos [(:x cloud) (:y cloud)]]\n    (if (< (geo/distance ship-pos cloud-pos) glc/dilithium-harvest-range)\n      (let [max-harvest (* ms glc/dilithium-harvest-rate)\n            need (- glc/ship-dilithium (:dilithium ship))\n            cloud-content (:concentration cloud)\n            harvest (min max-harvest cloud-content need)\n            ship (update ship :dilithium + harvest)\n            cloud (update cloud :concentration - harvest)]\n        [ship cloud])\n      [ship cloud])))\n\n(defn update-dilithium-harvest [ms world]\n  (let [{:keys [clouds ship]} world]\n    (loop [clouds clouds ship ship harvested-clouds []]\n      (if (empty? clouds)\n        (assoc world :ship ship :clouds harvested-clouds)\n        (let [[ship cloud] (harvest-dilithium ms ship (first clouds))]\n          (recur (rest clouds) ship (conj harvested-clouds cloud)))))))\n\n(defn update-clouds-age [ms world]\n  (let [clouds (:clouds world)\n        decay (Math/pow glc/cloud-decay-rate ms)\n        clouds (map #(update % :concentration * decay) clouds)\n        clouds (filter #(> (:concentration %) 1) clouds)\n        clouds (doall clouds)]\n    (assoc world :clouds clouds)))\n\n(defn update-clouds [ms world]\n  (->> world\n       (update-clouds-age ms)\n       (update-dilithium-harvest ms)))\n \n\n Some years back I wrote a nice little  spacewar game  in Clojure.  You can play it  here .  While playing, if you manage to blow up a Klingon, a sparkling cloud of  Dilithium Crystals  will remain behind, quickly dissipating.  If you can guide your ship into the midst of that cloud, you will harvest some of that  Dilithium  and replenish your stores. \n\n The code you see above is the  class  that represents the  Dilithium Cloud . \n\n The first thing to notice is that I defined the  TYPE  of the  cloud   class  –  dynamically . \n \n\n A  cloud  is an object with an  x  and  y  coordinate, and a  concentration ; all of which must be numbers.  I also created a little type checking function named  valid-cloud?  that is used by my unit tests (not shown) to make sure the  TYPE  is not violated by any of the  methods . \n\n Next comes  make-cloud  the  constructor  of the  cloud   class . \n\n \n via GIPHY \n\n There are two overloads of the  constructor .  The first takes no arguments and simply creates a  cloud  at (0,0) with no  Dilithium  in it.  The second takes three arguments and loads the  instance variables  of the  class . \n\n \n via GIPHY \n\n There are two primary  methods  of the  cloud   class :  update-clouds-age  and  update-dilithium-harvest .  The  update-clouds-age   method  finds all the  cloud   instances  in the  world   object  and decreases their concentration by the  decay  factor – which is a function of the number of milliseconds ( ms ) since the last time they were updated. The  update-dilithium-harvest   method  finds all the  cloud   objects  that are within the  ship   object ’s harvesting range and transfers  Dilithium  from those  cloud   objects  to the  ship   object . \n\n Now, you might notice that these  methods  are not the traditional style of method you would find in a Java program.  For one thing, they deal with a list of  cloud   objects  rather than an individual  cloud   object .  Secondly, there’s nothing polymorphic about them.  Third, they are  functional , because they return a new  world   object  with new  cloud   objects  and, in the case of  update-dilithium-harvest , a new  ship   object . \n\n So are these really  methods  of the  cloud   class ?  Sure!  Why not?  They are a set of narrowly cohesive functions that manipulate an internal data structure within an intentionally named abstraction. \n\n For all intents and purposes  cloud  is a °°°°°° °°°°°°°  class . \n\n \n via GIPHY \n\n So there. \n\n\n\n\n    "},
{"url": "https://blog.cleancoder.com/uncle-bob/2023/01/18/functional-classes.html", "title": "Functional Classes", "content": "\n       I recently tweeted the following: \n\n Should you subdivide a functional program into classes the way you would an object oriented program? Yes. You should. Because the rules don’t change just because you’ve chosen to use immutable data structures. — Uncle Bob Martin (@unclebobmartin)  January 17, 2023 \n \n\n This led to a bevy of interesting responses about the difference between classes and modules.  In answer to those responses I tweeted this: \n\n A class is a group of cohesive and narrowly defined functions that operate on an encapsulated data structure. The functions may, or may not, be polymorphically deployed. — Uncle Bob Martin (@unclebobmartin)  January 17, 2023 \n \n\n Of course that only led to an increased number of interesting responses.  And so I thought that it might be wise to blog about my reasoning rather than to continue trying to cram that reasoning into tweets. \n\n If you are in doubt about what FP is, and about what OO is, and about whether the two are compatible, then I recommend  this  old blog of mine. \n\n What is a class? According to the dictionary a class is: \n\n \n   A set, collection, group, or configuration containing members regarded as having certain attributes or traits in common; a kind or category. \n \n\n Now consider that definition when reading the next paragraph. \n\n In OO languages we organize our programs into classes of objects that share similar traits.  We describe those objects in terms of the attributes and behaviors that they have in common.  We strive to create hierarchies of classification that those objects can fit within.  We consider the higher level classifications to be abstractions that allow the expression of general truths that are independent of irrelevant details.  (Indeed, I once defined abstraction as:  The Amplification of the essential, and the elimination of the irrelevant. [1] ) \n\n In 1966 the power of abstraction by classification led the authors of Simula to create the keyword  class .  In 1980, Bjarne Stroustrup continued that convention and used the  class  keyword in C++.  This was actually somewhat strange because C already had the keyword  struct  which had a virtually identical meaning.  But the power of the word  class  held sway. \n\n In the mid 90s the power of that word led the authors of Java (and then C#) to declare  and enforce  that  everything  in a program must be part of a class.  This was a dramatic overreach.  It seems to me that some of the things that Java forces into classes ought not to be in classes at all.  For example, the class  java.lang.Math  is really just a namespace for a batch of functions and is not, in any sense, a classification of objects. \n\n This conflation of object classification and namespaces is confusing and unnecessary; and is probably part of the reason my initial tweet generated the responses that it did. \n\n Another overreach in Java (and by extension C#) is that methods are polymorphic by default.  Polymorphism is a tool, not a rule.  Many, if not most, function calls do not require dynamic dispatch. \n\n These kinds of overreach lead to confusion about what a class really is.  I believe that most of the responses to my tweet were the result of that confusion. \n\n So let’s cut to the chase. \n\n One of the oldest rules of software design is that we should partition the elements of the system into loosely coupled and internally cohesive elements.  Those elements become well named places where we can put data and behavior.  This follows the old proverb:  A place for everything, and everything in its place. \n\n What are those elements?  It seems obvious that the classification structures of objects ought to be high on the list.  Namespaced function libraries like  java.lang.Math  are another obvious choice.  In the one case we have a batch of functions that manipulate an internal data structure.  In the other case we have a batch of functions that manipulate an external data structure. \n\n The essential charachteristic of these elements, these batches of functions, is that they are internally cohesive.  That means that all the functions in the batch are strongly related to each other because they manipulate the same data structures, whether internal or external.  It is that cohesion that drives the partitioning of a software design. \n\n ###Example \n\n Recently I have been writing an application called  more-speech  which is a client that browses messages on the  nostr  network.  This nework is composed of relays that use a simple websocket protocol to transmit messages to clients.  The  more-speech  client is written in Clojure, which is a Functional Programming language. \n\n Early on I created a module named  protocol  to house the code that implemented the  nostr  protocol.  I began this module by managing the websockets over which the messages travelled, and then decoding those messages and manipulating them according to the rules of the protocol. \n\n Clojure is not a traditional OOPL, there is no  class  keyword that is used to declare and define objects and the methods that manipulate them.  Rather, a module in Clojure is just a batch of functions that are not syntactically bound to any particular data.  Thus my  protocol  module had functions that dealt with  WebSocket s and functions that dealth with messages and functions that dealth with protocol elements.  They were cohesive in the sense that they were all related to the  nostr  protocol; but there was no central data structure that unified them. \n\n The other day I realized that I was missing an abstraction.  The  nostr  protocol may be transmitted over websockets but the protocol rules have nothing to do with websockets.  Those rules deal with the data that comes through the websockets, but not the websockets themselves.  Yet my  protocol  module was littered with websocket code. \n\n So I separated the websocket code from the  protocol  code by creating an abstraction that I called  relay .  A  relay  is a data structure that contains the  url  of a websocket, the websocket itself, and a function to call when messages are received.  The  relay  data structure is manipulated by functions such as  make ,  open ,  close , and  send . \n\n This  relay  module very clearly defines a class of objects.  The  protocol  constructs a  relay  object for each of the urls in a list of active relays.  It  open s those  relay s and  send s messages to them.  Messages that are received are sent to  protocol  through the callback functions that are passed into the function that constructs the  relay  object.  In order to maintain the immutability and referential transparency constraints of Functional Programming, the functions that update the state of a  relay  return a new instance of that  relay . \n\n ###Lesson \n\n Java, C#, Ruby, and C++ all either enforce, or stronly encourage, the partitioning of systems into classes. Clojure does not; it is entirely agnostic about classes. The lesson that I learned from  protocol  and  relay  is that I had not been paying enough attention to class structure when writing complex Clojure programs.  Instead, I had been allowing functions to accumulate in modules in a, more or less, ad hoc fashion – similar to the way one might program in C, Fortran, Basic, or even Assembler.  But that was lazy.  Objects exist in programs, and they can, and should, be classified.  So, from now on, I will be paying much more attention to the classification structure of the objects my systems. \n\n \n   A place for everything, and everything in its place. \n \n\n\n\n\n    "}
]